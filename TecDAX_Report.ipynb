{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "---\n",
    "\n",
    "# Computational Finance and Financial Management\n",
    "\n",
    "## Analysis, replication and forecasting of the German TecDAX Stock Index\n",
    "\n",
    " By **Merlin Bartel** and **Max Veltwisch**\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Trying to predict the stock market is an enticing prospect to us, not so much as a desire for material gain, but for the challenge. Already in 1965 the Economist and Nobel Price winner Paul Samuelson postulated in his influencial paper \"Properly Anticipated Prices Fluctuate Randomly\" that returns of the past have no prediction power for future returns. He concluded that market prices could be considered a Martingale. This in turn implies that the analysis and knownledge about the historical price development of an underlying is of no use for future price predictions. Nevertheless by looking at the daily up and downs of the market, we imagine there must be patterns we can model to beat all odds. Our analysis follows 4 sections:\n",
    "\n",
    "1. **Exploratory analysis**: Analysing the historical price data of the TecDAX Index over time.\n",
    "2. **Replication**: Applying a Deep-Learning method to predict the weights of the stocks contained in the TecDAX Index.\n",
    "3. **Additive Model Forecasting**: Applying the Facebook Prophet model on time-series price data of the TecDAX Index in order to forcast future price movements.\n",
    "4.  **Recurrent Neural Network Forecasting**: Applying a Long Short-Term Memory (LSTM) model on time-series price data of the TecDAX Index in order to forecast future price movements.\n",
    "\n",
    "Section 1 serves the goal to better understand the general structure and charateristics of the historical price development of the TecDAX Index. Section 2 differs from the following sections by trying to estimate the weights of the stocks contained in the TecDAX Index in a way that replication becomes as precise as possible. Section 3 and 4 make use of different model approaches with the goal of recognising patterns in the historical price data of the TecDAX Index and using them to forecast future Index prices.\n",
    "\n",
    "\n",
    "Before we start with our research we load the required python packages into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hide_input": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#loading required packages\n",
    "import math\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import optimizers\n",
    "\n",
    "import statistics\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy import special, optimize\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Exploratory Analysis\n",
    "---\n",
    "### Data\n",
    "\n",
    "For this report we are looking at price data of the TecDAX Index from 01.01.2007 - 01.06.2019.  In section 2 we use the price data of all stocks contained in the TexDAX Index from 01.07.2015 - 31.05.2019. The variable of interest in all sections is the adjusted closing price. The following two tables show the structure of both datasets. The source of the datasets is: finance.yahoo.com in combination with onvista.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>749.690002</td>\n",
       "      <td>759.510010</td>\n",
       "      <td>749.309998</td>\n",
       "      <td>759.070007</td>\n",
       "      <td>759.070007</td>\n",
       "      <td>126187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>761.119995</td>\n",
       "      <td>778.429993</td>\n",
       "      <td>761.119995</td>\n",
       "      <td>777.400024</td>\n",
       "      <td>777.400024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>772.880005</td>\n",
       "      <td>772.880005</td>\n",
       "      <td>763.270020</td>\n",
       "      <td>765.960022</td>\n",
       "      <td>765.960022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>764.059998</td>\n",
       "      <td>765.049988</td>\n",
       "      <td>759.840027</td>\n",
       "      <td>763.309998</td>\n",
       "      <td>763.309998</td>\n",
       "      <td>135319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-08</th>\n",
       "      <td>763.150024</td>\n",
       "      <td>767.419983</td>\n",
       "      <td>762.119995</td>\n",
       "      <td>764.450012</td>\n",
       "      <td>764.450012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2007-01-02  749.690002  759.510010  749.309998  759.070007  759.070007   \n",
       "2007-01-03  761.119995  778.429993  761.119995  777.400024  777.400024   \n",
       "2007-01-04  772.880005  772.880005  763.270020  765.960022  765.960022   \n",
       "2007-01-05  764.059998  765.049988  759.840027  763.309998  763.309998   \n",
       "2007-01-08  763.150024  767.419983  762.119995  764.450012  764.450012   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2007-01-02  126187800  \n",
       "2007-01-03          0  \n",
       "2007-01-04          0  \n",
       "2007-01-05  135319300  \n",
       "2007-01-08          0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing TecDAX Index price data\n",
    "TecDAX = pd.read_csv(\"TECDAX.csv\", index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "# extracting adjusted close prices (variable of interest)\n",
    "TecDAX_close = TecDAX[\"Adj Close\"]\n",
    "\n",
    "# showing the first 5 observations\n",
    "TecDAX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TecDAX</th>\n",
       "      <th>1&amp;1 Drillisch</th>\n",
       "      <th>Aixtron</th>\n",
       "      <th>Aumann</th>\n",
       "      <th>Adva</th>\n",
       "      <th>Bechtle</th>\n",
       "      <th>Cancom</th>\n",
       "      <th>Carl Zeiss Meditec</th>\n",
       "      <th>CompuGroup</th>\n",
       "      <th>Dialog Semiconductor</th>\n",
       "      <th>...</th>\n",
       "      <th>SLM</th>\n",
       "      <th>SMA</th>\n",
       "      <th>Software</th>\n",
       "      <th>Stratec</th>\n",
       "      <th>Süss</th>\n",
       "      <th>Telefonica</th>\n",
       "      <th>Telekom</th>\n",
       "      <th>United Internet</th>\n",
       "      <th>Wirecard</th>\n",
       "      <th>Xing</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>1672.390015</td>\n",
       "      <td>35.488747</td>\n",
       "      <td>6.138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.735</td>\n",
       "      <td>32.312878</td>\n",
       "      <td>15.390124</td>\n",
       "      <td>23.140257</td>\n",
       "      <td>30.854900</td>\n",
       "      <td>50.439999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.845375</td>\n",
       "      <td>22.504152</td>\n",
       "      <td>46.892555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.897666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.481964</td>\n",
       "      <td>35.000416</td>\n",
       "      <td>149.166504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-07</th>\n",
       "      <td>1658.420044</td>\n",
       "      <td>35.128826</td>\n",
       "      <td>6.186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.510</td>\n",
       "      <td>32.572384</td>\n",
       "      <td>15.279471</td>\n",
       "      <td>23.116240</td>\n",
       "      <td>30.434217</td>\n",
       "      <td>49.540001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.977575</td>\n",
       "      <td>22.481371</td>\n",
       "      <td>46.043972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.854635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.891987</td>\n",
       "      <td>35.253502</td>\n",
       "      <td>144.268646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-07</th>\n",
       "      <td>1656.959961</td>\n",
       "      <td>35.119827</td>\n",
       "      <td>6.127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.774</td>\n",
       "      <td>32.317509</td>\n",
       "      <td>15.206487</td>\n",
       "      <td>23.250748</td>\n",
       "      <td>30.743683</td>\n",
       "      <td>49.669998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.438433</td>\n",
       "      <td>22.196175</td>\n",
       "      <td>46.775349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.830477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.915779</td>\n",
       "      <td>35.551250</td>\n",
       "      <td>143.056290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-07</th>\n",
       "      <td>1651.089966</td>\n",
       "      <td>34.354984</td>\n",
       "      <td>6.089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.700</td>\n",
       "      <td>32.901394</td>\n",
       "      <td>15.133503</td>\n",
       "      <td>23.130648</td>\n",
       "      <td>30.980619</td>\n",
       "      <td>50.200001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.193369</td>\n",
       "      <td>21.840816</td>\n",
       "      <td>46.039280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.769329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.663612</td>\n",
       "      <td>35.169140</td>\n",
       "      <td>142.037933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-07</th>\n",
       "      <td>1625.170044</td>\n",
       "      <td>32.789299</td>\n",
       "      <td>5.983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.590</td>\n",
       "      <td>32.794811</td>\n",
       "      <td>16.021078</td>\n",
       "      <td>22.909670</td>\n",
       "      <td>31.498013</td>\n",
       "      <td>48.735001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.825769</td>\n",
       "      <td>22.082279</td>\n",
       "      <td>46.976948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.656845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.187820</td>\n",
       "      <td>34.479359</td>\n",
       "      <td>138.206909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TecDAX  1&1 Drillisch  Aixtron  Aumann   Adva    Bechtle  \\\n",
       "Date                                                                        \n",
       "2015-01-07  1672.390015      35.488747    6.138     0.0  8.735  32.312878   \n",
       "2015-02-07  1658.420044      35.128826    6.186     0.0  8.510  32.572384   \n",
       "2015-03-07  1656.959961      35.119827    6.127     0.0  8.774  32.317509   \n",
       "2015-06-07  1651.089966      34.354984    6.089     0.0  8.700  32.901394   \n",
       "2015-07-07  1625.170044      32.789299    5.983     0.0  8.590  32.794811   \n",
       "\n",
       "               Cancom  Carl Zeiss Meditec  CompuGroup  Dialog Semiconductor  \\\n",
       "Date                                                                          \n",
       "2015-01-07  15.390124           23.140257   30.854900             50.439999   \n",
       "2015-02-07  15.279471           23.116240   30.434217             49.540001   \n",
       "2015-03-07  15.206487           23.250748   30.743683             49.669998   \n",
       "2015-06-07  15.133503           23.130648   30.980619             50.200001   \n",
       "2015-07-07  16.021078           22.909670   31.498013             48.735001   \n",
       "\n",
       "            ...  SLM        SMA   Software    Stratec  Süss  Telefonica  \\\n",
       "Date        ...                                                           \n",
       "2015-01-07  ...  0.0  19.845375  22.504152  46.892555   0.0    3.897666   \n",
       "2015-02-07  ...  0.0  20.977575  22.481371  46.043972   0.0    3.854635   \n",
       "2015-03-07  ...  0.0  20.438433  22.196175  46.775349   0.0    3.830477   \n",
       "2015-06-07  ...  0.0  20.193369  21.840816  46.039280   0.0    3.769329   \n",
       "2015-07-07  ...  0.0  19.825769  22.082279  46.976948   0.0    3.656845   \n",
       "\n",
       "            Telekom  United Internet   Wirecard        Xing  \n",
       "Date                                                         \n",
       "2015-01-07      0.0        38.481964  35.000416  149.166504  \n",
       "2015-02-07      0.0        37.891987  35.253502  144.268646  \n",
       "2015-03-07      0.0        37.915779  35.551250  143.056290  \n",
       "2015-06-07      0.0        37.663612  35.169140  142.037933  \n",
       "2015-07-07      0.0        37.187820  34.479359  138.206909  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing stock data contained in TecDAX Index\n",
    "TecDAX_stocks = pd.read_csv(\"Alltogether.csv\", encoding='latin-1')\n",
    "TecDAX_stocks[\"Date\"] = pd.to_datetime(TecDAX_stocks[\"Date\"])\n",
    "\n",
    "# setting \"Date\" as index\n",
    "TecDAX_stocks.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# showing the first 5 observations\n",
    "TecDAX_stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Visualising the data\n",
    "Visualizing our time-series data enables us to make inferences about important components, such as trend and seasonality and correlation. By plotting the price development of the TecDAX Index over time, we can easily see that the Index has instances of both positive and negative trend, with the later one dominating in the long-run. We also see the price reaction of the Index to the fincial crisis of 2008 and the increasing insecurity at the beginning of 2016. \n",
    "\n",
    "We can further compute the daily returns of the prices and verify Paul Samuelsons statement about the unpredictability of stock returns. Just by plotting the daily returns we can already guess that making acurate predictions about future returns will be a hard, if not a outrageous task. Instead of following recurring patterns the returns seem more like white noise fluctuating around a center somewhere at 0. To further confirm this first impression we make use of the autocorrelation function (ACF). This functions helps us understand the correlation component of different returns at different time lags. Lag refers to the time difference between one observation and a previous observation in a dataset. By plotting the ACF we indeed see, that there is almost no statistically significant correlation between todays returns and those in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hide_input": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualising historical Index price movement\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "TecDAX_close.plot(figsize=(16,8), color=\"royalblue\")\n",
    "plt.xlabel(\"Date\",fontsize=18,color=\"black\")\n",
    "plt.ylabel(\"Price\",fontsize=18,color=\"black\")\n",
    "plt.title(\"TecDAX Index - Price development\",fontweight=\"bold\",fontsize=22)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculating the daily Index returns\n",
    "daily_returns = TecDAX_close.shift(1) / TecDAX_close - 1\n",
    "\n",
    "# visualising the daily Index returns\n",
    "daily_returns.plot(figsize=(16,8), color=\"royalblue\")\n",
    "plt.xlabel(\"Date\",fontsize=18,color=\"black\")\n",
    "plt.ylabel(\"Returns\",fontsize=18,color=\"black\")\n",
    "plt.title(\"TecDAX Index - Daily returns\", fontweight=\"bold\",fontsize=22)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualising the autocorrelation of Index returns\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plot_acf(daily_returns[1:], lags=60, color=\"royalblue\")\n",
    "plt.title(\"TecDAX Index - Autocorrelation of returns\", fontweight=\"bold\",fontsize=22)\n",
    "plt.xlabel(\"Time lags\",fontsize=18,color=\"black\")\n",
    "plt.ylabel(\"Correlation\",fontsize=18,color=\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "---\n",
    "## 2. Replication \n",
    "---\n",
    "\n",
    "The first model we create is a very lean model, which sets a nice starting point into the world of Deep-Learning. The model is trained on regressing the TecDAX Index on daily historical stock price data.\n",
    "\n",
    "We use price data of the TecDAX Index reaching from the 01.07.2015 to 01.06.2019 to attain maximum data quality.\n",
    "As you will see, the optimal lean model does not have Hidden Layers and only one neuron in the Input Layer and is not affected by Overfitting. \n",
    "As so, it perfectly shows the minimum criteria for single neurons to be useful. To increase the prediction power of this model, it is indispensable to iterate the model over several weeks and to run tests with different Hyperparameters in order to find the optimal setup. The predictions and metrics of the validationset portraying the missing Overfitting are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing and indexing stock and index data\n",
    "TecDAX_stocks = pd.read_csv(\"Alltogether.csv\", encoding='latin-1') \n",
    "TecDAX_stocks[\"Date\"] = pd.to_datetime(TecDAX_stocks[\"Date\"])\n",
    "TecDAX_stocks.set_index(\"Date\", inplace=True)  \n",
    "\n",
    "# separating TecDAX Index data from stock data\n",
    "y = TecDAX_stocks[\"TecDAX\"]\n",
    "y = y.values.reshape(len(y),1)\n",
    "x = TecDAX_stocks.iloc [0:,1:]\n",
    "x = x.values.reshape(len(y),41)\n",
    "\n",
    "# defining prediction horizon\n",
    "test_size = 27\n",
    "train_size  = len(y) - test_size\n",
    "\n",
    "# splitting the data into training and test set\n",
    "y_train, y_test = y[0:train_size], y[train_size:len(y)]\n",
    "x_train, x_test = x[0:train_size,:], x[train_size:len(y),:]\n",
    "\n",
    "# extracting validation set from training set\n",
    "train_size2  = int(train_size * 0.7)\n",
    "\n",
    "# creating buffer-variable to combine TecDAX data with underlying stock data\n",
    "buff = np.append(y_train, x_train, axis =1)\n",
    "\n",
    "# randomizing sample and splitting TecDAX data from stock data again\n",
    "np.random.shuffle(buff)\n",
    "y_buff = buff[:,0]\n",
    "x_buff = buff[:,1:]\n",
    "np.reshape(x_buff, (len(x_buff),41))\n",
    "np.reshape(y_buff, (len(x_buff),1))\n",
    "\n",
    "# rearranging original Data\n",
    "y_train, y_valid = y_buff[0:train_size2], y_buff[train_size2:train_size]\n",
    "x_train, x_valid = x_buff[0:train_size2,:], x_buff[train_size2:train_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# building the neural network\n",
    "def index_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    model.add(Dense(1,activation='linear',input_dim=x_train.shape[1])) #the activation function can be linear, since the network consists of only 1 neuron\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1)) # Scalar Regression -> No Actication Function\n",
    "    \n",
    "    # compiling the model, defining optimizer and loss function\n",
    "    model.compile(optimizer='Adam',loss='mean_absolute_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 675 samples, validate on 290 samples\n",
      "Epoch 1/10000\n",
      "675/675 [==============================] - 2s 2ms/step - loss: 2188.7721 - val_loss: 2171.3162\n",
      "Epoch 2/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 2172.2453 - val_loss: 2155.0869\n",
      "Epoch 3/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 2155.4058 - val_loss: 2137.0749\n",
      "Epoch 4/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 2135.9076 - val_loss: 2116.1354\n",
      "Epoch 5/10000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 2113.3837 - val_loss: 2091.9602\n",
      "Epoch 6/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 2087.6462 - val_loss: 2064.5926\n",
      "Epoch 7/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 2058.5542 - val_loss: 2033.9354\n",
      "Epoch 8/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 2026.3393 - val_loss: 2000.2008\n",
      "Epoch 9/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 1990.9744 - val_loss: 1962.8258\n",
      "Epoch 10/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 1951.9359 - val_loss: 1922.4520\n",
      "Epoch 11/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 1910.0622 - val_loss: 1879.2177\n",
      "Epoch 12/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 1865.3813 - val_loss: 1832.8379\n",
      "Epoch 13/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 1817.1294 - val_loss: 1783.0252\n",
      "Epoch 14/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 1765.5363 - val_loss: 1729.5090\n",
      "Epoch 15/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 1710.2396 - val_loss: 1672.9719\n",
      "Epoch 16/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 1651.9815 - val_loss: 1613.2750\n",
      "Epoch 17/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 1590.7037 - val_loss: 1550.2917\n",
      "Epoch 18/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 1526.0160 - val_loss: 1484.5825\n",
      "Epoch 19/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 1458.5431 - val_loss: 1415.6760\n",
      "Epoch 20/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 1387.8479 - val_loss: 1342.9619\n",
      "Epoch 21/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1313.6271 - val_loss: 1267.3944\n",
      "Epoch 22/10000\n",
      "675/675 [==============================] - 0s 124us/step - loss: 1236.0510 - val_loss: 1188.2435\n",
      "Epoch 23/10000\n",
      "675/675 [==============================] - 0s 129us/step - loss: 1155.3279 - val_loss: 1106.5834\n",
      "Epoch 24/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 1071.2837 - val_loss: 1021.1305\n",
      "Epoch 25/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 984.2088 - val_loss: 932.6659\n",
      "Epoch 26/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 894.3730 - val_loss: 841.4005\n",
      "Epoch 27/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 800.8884 - val_loss: 746.7124\n",
      "Epoch 28/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 704.9855 - val_loss: 649.1661\n",
      "Epoch 29/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 605.5996 - val_loss: 548.6821\n",
      "Epoch 30/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 503.4940 - val_loss: 445.7108\n",
      "Epoch 31/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 398.3602 - val_loss: 339.2093\n",
      "Epoch 32/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 290.2926 - val_loss: 229.7077\n",
      "Epoch 33/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 179.6624 - val_loss: 117.7534\n",
      "Epoch 34/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 70.7622 - val_loss: 35.4001\n",
      "Epoch 35/10000\n",
      "675/675 [==============================] - 0s 111us/step - loss: 34.3468 - val_loss: 33.3931\n",
      "Epoch 36/10000\n",
      "675/675 [==============================] - 0s 107us/step - loss: 31.9313 - val_loss: 31.5766\n",
      "Epoch 37/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 30.5306 - val_loss: 30.2430\n",
      "Epoch 38/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 29.2996 - val_loss: 29.2661\n",
      "Epoch 39/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 28.3863 - val_loss: 28.4528\n",
      "Epoch 40/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 27.5904 - val_loss: 27.7433\n",
      "Epoch 41/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 26.8999 - val_loss: 27.1280\n",
      "Epoch 42/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 26.4638 - val_loss: 26.6464\n",
      "Epoch 43/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 25.9105 - val_loss: 26.1114\n",
      "Epoch 44/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 25.4748 - val_loss: 25.6230\n",
      "Epoch 45/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 25.0638 - val_loss: 25.2147\n",
      "Epoch 46/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 24.7686 - val_loss: 24.8260\n",
      "Epoch 47/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 24.5241 - val_loss: 24.5280\n",
      "Epoch 48/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 24.2324 - val_loss: 24.2308\n",
      "Epoch 49/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 23.9464 - val_loss: 23.8547\n",
      "Epoch 50/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 23.6821 - val_loss: 23.5896\n",
      "Epoch 51/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 23.4089 - val_loss: 23.2903\n",
      "Epoch 52/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 23.2500 - val_loss: 23.1460\n",
      "Epoch 53/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 22.9451 - val_loss: 22.8118\n",
      "Epoch 54/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 22.7674 - val_loss: 22.6553\n",
      "Epoch 55/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 22.6113 - val_loss: 22.4086\n",
      "Epoch 56/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 22.3867 - val_loss: 22.3070\n",
      "Epoch 57/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 22.2083 - val_loss: 22.0762\n",
      "Epoch 58/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 22.1871 - val_loss: 22.1199\n",
      "Epoch 59/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 21.9490 - val_loss: 21.8197\n",
      "Epoch 60/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 21.8217 - val_loss: 21.6950\n",
      "Epoch 61/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 21.6691 - val_loss: 21.5839\n",
      "Epoch 62/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 21.5672 - val_loss: 21.4736\n",
      "Epoch 63/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 21.6133 - val_loss: 21.3581\n",
      "Epoch 64/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 21.8293 - val_loss: 21.4709\n",
      "Epoch 65/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 21.3595 - val_loss: 21.1606\n",
      "Epoch 66/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 21.3060 - val_loss: 21.1160\n",
      "Epoch 67/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 21.0452 - val_loss: 21.2873\n",
      "Epoch 68/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 21.2450 - val_loss: 20.9119\n",
      "Epoch 69/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 20.8664 - val_loss: 20.7723\n",
      "Epoch 70/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 20.8431 - val_loss: 20.6812\n",
      "Epoch 71/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 20.7411 - val_loss: 20.6179\n",
      "Epoch 72/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 20.9372 - val_loss: 20.9523\n",
      "Epoch 73/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 20.8382 - val_loss: 20.5379\n",
      "Epoch 74/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 20.6249 - val_loss: 20.4378\n",
      "Epoch 75/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 20.5148 - val_loss: 20.3011\n",
      "Epoch 76/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 57us/step - loss: 20.3708 - val_loss: 20.2934\n",
      "Epoch 77/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 20.3124 - val_loss: 20.1649\n",
      "Epoch 78/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 20.2485 - val_loss: 20.2139\n",
      "Epoch 79/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 20.2174 - val_loss: 20.0232\n",
      "Epoch 80/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 20.1421 - val_loss: 20.0463\n",
      "Epoch 81/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 20.1609 - val_loss: 19.9780\n",
      "Epoch 82/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 20.1017 - val_loss: 19.8873\n",
      "Epoch 83/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 19.9070 - val_loss: 19.8323\n",
      "Epoch 84/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 19.9809 - val_loss: 19.7667\n",
      "Epoch 85/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 19.7632 - val_loss: 19.8003\n",
      "Epoch 86/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 19.8247 - val_loss: 19.7058\n",
      "Epoch 87/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 19.6678 - val_loss: 19.6153\n",
      "Epoch 88/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 19.7162 - val_loss: 19.4639\n",
      "Epoch 89/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 19.4748 - val_loss: 19.4252\n",
      "Epoch 90/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 19.3899 - val_loss: 19.4011\n",
      "Epoch 91/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 19.3180 - val_loss: 19.2740\n",
      "Epoch 92/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 19.2083 - val_loss: 19.1658\n",
      "Epoch 93/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 19.1679 - val_loss: 19.1206\n",
      "Epoch 94/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 19.3249 - val_loss: 19.1132\n",
      "Epoch 95/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.9624 - val_loss: 19.0423\n",
      "Epoch 96/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 19.0362 - val_loss: 18.8937\n",
      "Epoch 97/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 18.9681 - val_loss: 18.7982\n",
      "Epoch 98/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.9867 - val_loss: 18.7385\n",
      "Epoch 99/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.9648 - val_loss: 18.6883\n",
      "Epoch 100/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.7207 - val_loss: 18.8456\n",
      "Epoch 101/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 18.8666 - val_loss: 19.1992\n",
      "Epoch 102/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 18.6033 - val_loss: 18.5561\n",
      "Epoch 103/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 18.5177 - val_loss: 18.5465\n",
      "Epoch 104/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 18.3995 - val_loss: 18.3235\n",
      "Epoch 105/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 18.4213 - val_loss: 18.2360\n",
      "Epoch 106/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.5147 - val_loss: 18.6367\n",
      "Epoch 107/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 18.2501 - val_loss: 18.0869\n",
      "Epoch 108/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.1744 - val_loss: 17.9871\n",
      "Epoch 109/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.0343 - val_loss: 17.9737\n",
      "Epoch 110/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 18.0500 - val_loss: 17.8934\n",
      "Epoch 111/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 17.9308 - val_loss: 17.7914\n",
      "Epoch 112/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 17.9046 - val_loss: 17.7274\n",
      "Epoch 113/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 17.7789 - val_loss: 17.6739\n",
      "Epoch 114/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 17.6486 - val_loss: 17.7148\n",
      "Epoch 115/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 17.9843 - val_loss: 17.5239\n",
      "Epoch 116/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 17.8480 - val_loss: 17.4705\n",
      "Epoch 117/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 17.4416 - val_loss: 17.4067\n",
      "Epoch 118/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 17.3514 - val_loss: 17.2941\n",
      "Epoch 119/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 17.3203 - val_loss: 17.3639\n",
      "Epoch 120/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 17.2231 - val_loss: 17.1627\n",
      "Epoch 121/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 17.2240 - val_loss: 17.1577\n",
      "Epoch 122/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 17.1333 - val_loss: 17.1929\n",
      "Epoch 123/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 17.1394 - val_loss: 17.0566\n",
      "Epoch 124/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 17.1496 - val_loss: 17.0264\n",
      "Epoch 125/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 17.0534 - val_loss: 17.2381\n",
      "Epoch 126/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 17.1561 - val_loss: 16.9136\n",
      "Epoch 127/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 16.8152 - val_loss: 16.7426\n",
      "Epoch 128/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 16.7382 - val_loss: 16.7086\n",
      "Epoch 129/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 16.6350 - val_loss: 16.8422\n",
      "Epoch 130/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 16.7299 - val_loss: 16.6186\n",
      "Epoch 131/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 16.5995 - val_loss: 16.6085\n",
      "Epoch 132/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 16.7116 - val_loss: 16.5874\n",
      "Epoch 133/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 16.4547 - val_loss: 16.4624\n",
      "Epoch 134/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 16.5292 - val_loss: 16.4014\n",
      "Epoch 135/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 16.3604 - val_loss: 16.4710\n",
      "Epoch 136/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 16.4708 - val_loss: 16.4351\n",
      "Epoch 137/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 16.5255 - val_loss: 16.2554\n",
      "Epoch 138/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 16.2592 - val_loss: 16.2232\n",
      "Epoch 139/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 16.1373 - val_loss: 16.2455\n",
      "Epoch 140/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 16.3728 - val_loss: 16.1291\n",
      "Epoch 141/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 16.1644 - val_loss: 16.8900\n",
      "Epoch 142/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 16.5738 - val_loss: 16.0765\n",
      "Epoch 143/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 16.3273 - val_loss: 16.1336\n",
      "Epoch 144/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 16.4253 - val_loss: 16.1062\n",
      "Epoch 145/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 16.2167 - val_loss: 16.4037\n",
      "Epoch 146/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 16.0584 - val_loss: 15.9882\n",
      "Epoch 147/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.9051 - val_loss: 16.9868\n",
      "Epoch 148/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 16.5714 - val_loss: 16.3631\n",
      "Epoch 149/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.9104 - val_loss: 16.2616\n",
      "Epoch 150/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.8061 - val_loss: 15.7611\n",
      "Epoch 151/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.6719 - val_loss: 15.7255\n",
      "Epoch 152/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 15.8647 - val_loss: 15.7042\n",
      "Epoch 153/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 15.6369 - val_loss: 15.7556\n",
      "Epoch 154/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 15.5541 - val_loss: 15.7886\n",
      "Epoch 155/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.5742 - val_loss: 15.6996\n",
      "Epoch 156/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.5307 - val_loss: 15.6395\n",
      "Epoch 157/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.4297 - val_loss: 15.8755\n",
      "Epoch 158/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 15.5932 - val_loss: 15.5485\n",
      "Epoch 159/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 15.3753 - val_loss: 15.5762\n",
      "Epoch 160/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.3560 - val_loss: 15.7481\n",
      "Epoch 161/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.3136 - val_loss: 15.7193\n",
      "Epoch 162/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 15.8515 - val_loss: 15.4562\n",
      "Epoch 163/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.3530 - val_loss: 15.6405\n",
      "Epoch 164/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 15.4661 - val_loss: 15.4344\n",
      "Epoch 165/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.2615 - val_loss: 15.6889\n",
      "Epoch 166/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.5926 - val_loss: 15.6307\n",
      "Epoch 167/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.4245 - val_loss: 15.5387\n",
      "Epoch 168/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.1388 - val_loss: 15.3046\n",
      "Epoch 169/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.1011 - val_loss: 15.2534\n",
      "Epoch 170/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 15.0247 - val_loss: 15.2370\n",
      "Epoch 171/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.1640 - val_loss: 15.3224\n",
      "Epoch 172/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 15.0005 - val_loss: 15.1856\n",
      "Epoch 173/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.3840 - val_loss: 15.1667\n",
      "Epoch 174/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 14.9779 - val_loss: 15.2977\n",
      "Epoch 175/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.8595 - val_loss: 15.1203\n",
      "Epoch 176/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 15.0410 - val_loss: 15.1184\n",
      "Epoch 177/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.8985 - val_loss: 15.1317\n",
      "Epoch 178/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.8780 - val_loss: 15.0487\n",
      "Epoch 179/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 14.9122 - val_loss: 15.9868\n",
      "Epoch 180/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 15.1459 - val_loss: 14.9830\n",
      "Epoch 181/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.7755 - val_loss: 14.9452\n",
      "Epoch 182/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.7556 - val_loss: 14.9903\n",
      "Epoch 183/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 14.6753 - val_loss: 14.9064\n",
      "Epoch 184/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.7602 - val_loss: 14.9725\n",
      "Epoch 185/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.6233 - val_loss: 14.8902\n",
      "Epoch 186/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.6361 - val_loss: 14.8859\n",
      "Epoch 187/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 14.6349 - val_loss: 14.9769\n",
      "Epoch 188/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 14.5506 - val_loss: 14.8481\n",
      "Epoch 189/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 14.8098 - val_loss: 14.9339\n",
      "Epoch 190/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.8333 - val_loss: 14.7778\n",
      "Epoch 191/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.8412 - val_loss: 14.8509\n",
      "Epoch 192/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.9286 - val_loss: 14.7707\n",
      "Epoch 193/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.9117 - val_loss: 14.8486\n",
      "Epoch 194/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 15.1905 - val_loss: 14.8264\n",
      "Epoch 195/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.3828 - val_loss: 14.6543\n",
      "Epoch 196/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 14.4137 - val_loss: 15.1086\n",
      "Epoch 197/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 14.5533 - val_loss: 14.7282\n",
      "Epoch 198/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.6659 - val_loss: 14.8660\n",
      "Epoch 199/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 14.5465 - val_loss: 14.6593\n",
      "Epoch 200/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.2469 - val_loss: 14.9206\n",
      "Epoch 201/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 14.3278 - val_loss: 15.1441\n",
      "Epoch 202/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.3491 - val_loss: 14.5801\n",
      "Epoch 203/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.3810 - val_loss: 14.5087\n",
      "Epoch 204/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.1652 - val_loss: 14.5320\n",
      "Epoch 205/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.2710 - val_loss: 14.5157\n",
      "Epoch 206/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.2194 - val_loss: 14.4826\n",
      "Epoch 207/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 15.0596 - val_loss: 14.4186\n",
      "Epoch 208/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 14.3204 - val_loss: 14.7481\n",
      "Epoch 209/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.3061 - val_loss: 14.4087\n",
      "Epoch 210/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 14.3644 - val_loss: 14.3703\n",
      "Epoch 211/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 14.1257 - val_loss: 14.3439\n",
      "Epoch 212/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 14.0779 - val_loss: 14.5360\n",
      "Epoch 213/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 14.3608 - val_loss: 14.3436\n",
      "Epoch 214/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 14.1493 - val_loss: 14.5757\n",
      "Epoch 215/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 14.1425 - val_loss: 14.2753\n",
      "Epoch 216/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 14.0573 - val_loss: 14.4377\n",
      "Epoch 217/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.8552 - val_loss: 14.3958\n",
      "Epoch 218/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.8457 - val_loss: 14.1974\n",
      "Epoch 219/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.8226 - val_loss: 14.2254\n",
      "Epoch 220/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.8332 - val_loss: 14.2062\n",
      "Epoch 221/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.9312 - val_loss: 14.2764\n",
      "Epoch 222/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 13.8738 - val_loss: 14.3489\n",
      "Epoch 223/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 13.9704 - val_loss: 14.1295\n",
      "Epoch 224/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 14.0363 - val_loss: 14.1193\n",
      "Epoch 225/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.7905 - val_loss: 14.1112\n",
      "Epoch 226/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.7854 - val_loss: 14.0896\n",
      "Epoch 227/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.9898 - val_loss: 14.0789\n",
      "Epoch 228/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 13.8048 - val_loss: 14.0387\n",
      "Epoch 229/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 13.8605 - val_loss: 14.1038\n",
      "Epoch 230/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.9259 - val_loss: 14.3478\n",
      "Epoch 231/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.7751 - val_loss: 14.0529\n",
      "Epoch 232/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.7035 - val_loss: 14.0773\n",
      "Epoch 233/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 13.7830 - val_loss: 14.0926\n",
      "Epoch 234/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.6025 - val_loss: 13.9530\n",
      "Epoch 235/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.7047 - val_loss: 13.9499\n",
      "Epoch 236/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.5770 - val_loss: 13.9234\n",
      "Epoch 237/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.6489 - val_loss: 13.8775\n",
      "Epoch 238/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.5545 - val_loss: 13.8346\n",
      "Epoch 239/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 13.5834 - val_loss: 13.8671\n",
      "Epoch 240/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.5817 - val_loss: 13.8278\n",
      "Epoch 241/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 13.4359 - val_loss: 13.8196\n",
      "Epoch 242/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.5452 - val_loss: 13.9354\n",
      "Epoch 243/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.6531 - val_loss: 13.8879\n",
      "Epoch 244/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.5014 - val_loss: 13.7295\n",
      "Epoch 245/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.4414 - val_loss: 13.7013\n",
      "Epoch 246/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.2913 - val_loss: 13.7829\n",
      "Epoch 247/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.3713 - val_loss: 13.7119\n",
      "Epoch 248/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.2889 - val_loss: 13.6959\n",
      "Epoch 249/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.3065 - val_loss: 13.7573\n",
      "Epoch 250/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.4927 - val_loss: 13.8344\n",
      "Epoch 251/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.2701 - val_loss: 13.8235\n",
      "Epoch 252/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.2833 - val_loss: 13.6551\n",
      "Epoch 253/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.3777 - val_loss: 13.6268\n",
      "Epoch 254/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.2429 - val_loss: 13.9285\n",
      "Epoch 255/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.6095 - val_loss: 13.7314\n",
      "Epoch 256/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.4777 - val_loss: 13.5708\n",
      "Epoch 257/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.4646 - val_loss: 13.6909\n",
      "Epoch 258/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.2728 - val_loss: 13.5250\n",
      "Epoch 259/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 13.2439 - val_loss: 13.6279\n",
      "Epoch 260/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.0862 - val_loss: 13.5310\n",
      "Epoch 261/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.1611 - val_loss: 13.5327\n",
      "Epoch 262/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.0649 - val_loss: 13.4700\n",
      "Epoch 263/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.9928 - val_loss: 13.5149\n",
      "Epoch 264/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 13.0019 - val_loss: 13.4150\n",
      "Epoch 265/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.3352 - val_loss: 13.6769\n",
      "Epoch 266/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 13.3943 - val_loss: 13.4172\n",
      "Epoch 267/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 13.1617 - val_loss: 13.4651\n",
      "Epoch 268/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.9291 - val_loss: 13.3827\n",
      "Epoch 269/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.9098 - val_loss: 13.3940\n",
      "Epoch 270/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.9783 - val_loss: 13.4421\n",
      "Epoch 271/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.8527 - val_loss: 13.3566\n",
      "Epoch 272/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 12.8045 - val_loss: 13.3320\n",
      "Epoch 273/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.8904 - val_loss: 13.3936\n",
      "Epoch 274/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 13.0432 - val_loss: 13.4508\n",
      "Epoch 275/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.9638 - val_loss: 13.2914\n",
      "Epoch 276/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.8170 - val_loss: 13.2456\n",
      "Epoch 277/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.8482 - val_loss: 13.4317\n",
      "Epoch 278/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 12.7647 - val_loss: 13.3098\n",
      "Epoch 279/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.7587 - val_loss: 13.2242\n",
      "Epoch 280/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.7755 - val_loss: 13.2864\n",
      "Epoch 281/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 12.7702 - val_loss: 13.2263\n",
      "Epoch 282/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.7269 - val_loss: 13.3930\n",
      "Epoch 283/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.6185 - val_loss: 13.3299\n",
      "Epoch 284/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 13.3274 - val_loss: 13.2137\n",
      "Epoch 285/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 12.7101 - val_loss: 13.2053\n",
      "Epoch 286/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 12.7994 - val_loss: 13.2945\n",
      "Epoch 287/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.8042 - val_loss: 13.1657\n",
      "Epoch 288/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 12.6356 - val_loss: 13.1453\n",
      "Epoch 289/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 12.6420 - val_loss: 13.4489\n",
      "Epoch 290/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 13.0105 - val_loss: 13.4534\n",
      "Epoch 291/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 12.9960 - val_loss: 13.1816\n",
      "Epoch 292/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.4766 - val_loss: 13.2318\n",
      "Epoch 293/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.4603 - val_loss: 13.1988\n",
      "Epoch 294/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.4764 - val_loss: 13.1009\n",
      "Epoch 295/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.4375 - val_loss: 13.0512\n",
      "Epoch 296/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.4814 - val_loss: 13.7216\n",
      "Epoch 297/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.7771 - val_loss: 13.3720\n",
      "Epoch 298/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.3866 - val_loss: 13.3913\n",
      "Epoch 299/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.5002 - val_loss: 12.9573\n",
      "Epoch 300/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.5930 - val_loss: 12.9373\n",
      "Epoch 301/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.3363 - val_loss: 12.9361\n",
      "Epoch 302/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.3414 - val_loss: 12.9793\n",
      "Epoch 303/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.3513 - val_loss: 12.9094\n",
      "Epoch 304/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 12.4492 - val_loss: 12.9247\n",
      "Epoch 305/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 12.2701 - val_loss: 12.8770\n",
      "Epoch 306/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.3822 - val_loss: 12.8655\n",
      "Epoch 307/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 12.4409 - val_loss: 12.9642\n",
      "Epoch 308/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.2302 - val_loss: 12.9197\n",
      "Epoch 309/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.4975 - val_loss: 12.8945\n",
      "Epoch 310/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.3168 - val_loss: 12.8427\n",
      "Epoch 311/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.2008 - val_loss: 12.8022\n",
      "Epoch 312/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.4036 - val_loss: 13.0402\n",
      "Epoch 313/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 12.4072 - val_loss: 12.8679\n",
      "Epoch 314/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.2560 - val_loss: 12.8605\n",
      "Epoch 315/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 12.2899 - val_loss: 12.8037\n",
      "Epoch 316/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.1532 - val_loss: 12.9135\n",
      "Epoch 317/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.3524 - val_loss: 12.7421\n",
      "Epoch 318/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.2859 - val_loss: 12.7157\n",
      "Epoch 319/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.1580 - val_loss: 13.0168\n",
      "Epoch 320/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 12.2773 - val_loss: 13.0260\n",
      "Epoch 321/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.3326 - val_loss: 12.8519\n",
      "Epoch 322/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.4206 - val_loss: 12.9359\n",
      "Epoch 323/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.1906 - val_loss: 12.6868\n",
      "Epoch 324/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.0847 - val_loss: 12.6804\n",
      "Epoch 325/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.1221 - val_loss: 12.7193\n",
      "Epoch 326/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 12.0140 - val_loss: 12.8415\n",
      "Epoch 327/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 12.0037 - val_loss: 12.6879\n",
      "Epoch 328/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.9106 - val_loss: 12.6092\n",
      "Epoch 329/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.9238 - val_loss: 12.6049\n",
      "Epoch 330/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.8818 - val_loss: 12.6482\n",
      "Epoch 331/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.9874 - val_loss: 12.7456\n",
      "Epoch 332/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.8673 - val_loss: 12.5829\n",
      "Epoch 333/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.8909 - val_loss: 12.6436\n",
      "Epoch 334/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 12.0426 - val_loss: 12.6283\n",
      "Epoch 335/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 11.9859 - val_loss: 13.0618\n",
      "Epoch 336/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 11.9289 - val_loss: 12.5242\n",
      "Epoch 337/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 11.8623 - val_loss: 12.5530\n",
      "Epoch 338/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 12.0706 - val_loss: 12.6902\n",
      "Epoch 339/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.7229 - val_loss: 12.6066\n",
      "Epoch 340/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.8748 - val_loss: 12.4483\n",
      "Epoch 341/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.7972 - val_loss: 12.4540\n",
      "Epoch 342/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.8993 - val_loss: 12.6863\n",
      "Epoch 343/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.8426 - val_loss: 12.5107\n",
      "Epoch 344/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.7148 - val_loss: 12.7727\n",
      "Epoch 345/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 12.3721 - val_loss: 12.5066\n",
      "Epoch 346/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.6676 - val_loss: 12.4097\n",
      "Epoch 347/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.6375 - val_loss: 12.3891\n",
      "Epoch 348/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.8380 - val_loss: 12.4358\n",
      "Epoch 349/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.6796 - val_loss: 12.4617\n",
      "Epoch 350/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.6806 - val_loss: 12.4185\n",
      "Epoch 351/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.7872 - val_loss: 12.9492\n",
      "Epoch 352/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.6881 - val_loss: 13.0435\n",
      "Epoch 353/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.8532 - val_loss: 12.3414\n",
      "Epoch 354/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.6161 - val_loss: 12.2856\n",
      "Epoch 355/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.6276 - val_loss: 12.4006\n",
      "Epoch 356/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.6279 - val_loss: 12.4692\n",
      "Epoch 357/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 11.5401 - val_loss: 12.3088\n",
      "Epoch 358/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 11.5077 - val_loss: 12.2329\n",
      "Epoch 359/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.7257 - val_loss: 12.2358\n",
      "Epoch 360/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 11.4982 - val_loss: 12.2269\n",
      "Epoch 361/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 11.4574 - val_loss: 12.3639\n",
      "Epoch 362/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 11.4732 - val_loss: 12.4453\n",
      "Epoch 363/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 11.5749 - val_loss: 12.3350\n",
      "Epoch 364/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 11.3723 - val_loss: 12.3455\n",
      "Epoch 365/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.4113 - val_loss: 12.2397\n",
      "Epoch 366/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.8372 - val_loss: 12.2978\n",
      "Epoch 367/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.3468 - val_loss: 12.2843\n",
      "Epoch 368/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.4901 - val_loss: 12.1776\n",
      "Epoch 369/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.3358 - val_loss: 12.1638\n",
      "Epoch 370/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 11.3731 - val_loss: 12.1494\n",
      "Epoch 371/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.3366 - val_loss: 12.1289\n",
      "Epoch 372/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.2911 - val_loss: 12.3645\n",
      "Epoch 373/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.6143 - val_loss: 12.3836\n",
      "Epoch 374/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.5493 - val_loss: 12.1737\n",
      "Epoch 375/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.2469 - val_loss: 12.1229\n",
      "Epoch 376/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.5851 - val_loss: 12.5415\n",
      "Epoch 377/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.7565 - val_loss: 12.2855\n",
      "Epoch 378/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.3880 - val_loss: 12.0587\n",
      "Epoch 379/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.2956 - val_loss: 12.0908\n",
      "Epoch 380/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 11.2255 - val_loss: 12.1137\n",
      "Epoch 381/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 11.3306 - val_loss: 12.0606\n",
      "Epoch 382/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.2886 - val_loss: 12.0884\n",
      "Epoch 383/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.3039 - val_loss: 12.2030\n",
      "Epoch 384/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.2040 - val_loss: 11.9689\n",
      "Epoch 385/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.1806 - val_loss: 12.3564\n",
      "Epoch 386/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.7955 - val_loss: 12.5921\n",
      "Epoch 387/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.2676 - val_loss: 12.0641\n",
      "Epoch 388/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.2464 - val_loss: 12.1512\n",
      "Epoch 389/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.1927 - val_loss: 12.1184\n",
      "Epoch 390/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.0803 - val_loss: 12.0074\n",
      "Epoch 391/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.5728 - val_loss: 11.8979\n",
      "Epoch 392/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.4233 - val_loss: 11.9385\n",
      "Epoch 393/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.1565 - val_loss: 12.1182\n",
      "Epoch 394/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 11.1816 - val_loss: 12.0023\n",
      "Epoch 395/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.0410 - val_loss: 11.8623\n",
      "Epoch 396/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.0775 - val_loss: 11.9441\n",
      "Epoch 397/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.1235 - val_loss: 11.8724\n",
      "Epoch 398/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 11.0562 - val_loss: 11.8212\n",
      "Epoch 399/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.0247 - val_loss: 12.1366\n",
      "Epoch 400/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 11.4941 - val_loss: 12.0113\n",
      "Epoch 401/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 11.1121 - val_loss: 12.1602\n",
      "Epoch 402/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.9959 - val_loss: 11.8416\n",
      "Epoch 403/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.9351 - val_loss: 11.8472\n",
      "Epoch 404/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.9041 - val_loss: 12.0373\n",
      "Epoch 405/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.9326 - val_loss: 11.8143\n",
      "Epoch 406/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.0048 - val_loss: 11.9430\n",
      "Epoch 407/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.0120 - val_loss: 11.7717\n",
      "Epoch 408/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.0088 - val_loss: 11.8040\n",
      "Epoch 409/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.2908 - val_loss: 11.8159\n",
      "Epoch 410/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.1330 - val_loss: 11.8277\n",
      "Epoch 411/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.0086 - val_loss: 11.7728\n",
      "Epoch 412/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.8822 - val_loss: 11.7768\n",
      "Epoch 413/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.8074 - val_loss: 11.7685\n",
      "Epoch 414/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.0123 - val_loss: 11.7775\n",
      "Epoch 415/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.0265 - val_loss: 11.7131\n",
      "Epoch 416/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.8713 - val_loss: 11.7169\n",
      "Epoch 417/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.9812 - val_loss: 11.7097\n",
      "Epoch 418/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.9889 - val_loss: 11.7622\n",
      "Epoch 419/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.8075 - val_loss: 11.6656\n",
      "Epoch 420/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.8919 - val_loss: 11.6588\n",
      "Epoch 421/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.9789 - val_loss: 11.8012\n",
      "Epoch 422/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 10.8307 - val_loss: 11.6501\n",
      "Epoch 423/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.7483 - val_loss: 11.8501\n",
      "Epoch 424/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.0911 - val_loss: 11.8730\n",
      "Epoch 425/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.4879 - val_loss: 12.2560\n",
      "Epoch 426/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 11.4815 - val_loss: 12.5109\n",
      "Epoch 427/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 11.0266 - val_loss: 11.8118\n",
      "Epoch 428/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.7509 - val_loss: 11.7009\n",
      "Epoch 429/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.9146 - val_loss: 11.6791\n",
      "Epoch 430/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 11.1541 - val_loss: 11.5889\n",
      "Epoch 431/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.6572 - val_loss: 11.7135\n",
      "Epoch 432/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.6558 - val_loss: 11.5727\n",
      "Epoch 433/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.7914 - val_loss: 11.5790\n",
      "Epoch 434/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.6574 - val_loss: 11.7000\n",
      "Epoch 435/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.7402 - val_loss: 11.5313\n",
      "Epoch 436/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.8372 - val_loss: 11.6532\n",
      "Epoch 437/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 10.6193 - val_loss: 11.5638\n",
      "Epoch 438/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.6250 - val_loss: 11.8450\n",
      "Epoch 439/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.6701 - val_loss: 11.4972\n",
      "Epoch 440/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.6807 - val_loss: 11.5963\n",
      "Epoch 441/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.6871 - val_loss: 11.4501\n",
      "Epoch 442/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.6147 - val_loss: 11.5875\n",
      "Epoch 443/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.7220 - val_loss: 11.6113\n",
      "Epoch 444/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.5204 - val_loss: 11.4521\n",
      "Epoch 445/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.5140 - val_loss: 11.7079\n",
      "Epoch 446/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.6407 - val_loss: 11.4448\n",
      "Epoch 447/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 10.5038 - val_loss: 11.6732\n",
      "Epoch 448/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.6270 - val_loss: 11.4748\n",
      "Epoch 449/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.5569 - val_loss: 11.4202\n",
      "Epoch 450/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.5886 - val_loss: 11.7041\n",
      "Epoch 451/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.6374 - val_loss: 11.4031\n",
      "Epoch 452/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.6104 - val_loss: 11.6136\n",
      "Epoch 453/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.5667 - val_loss: 11.5509\n",
      "Epoch 454/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 10.4728 - val_loss: 11.6637\n",
      "Epoch 455/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.7932 - val_loss: 11.5373\n",
      "Epoch 456/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 10.5335 - val_loss: 11.4304\n",
      "Epoch 457/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 10.5602 - val_loss: 11.6608\n",
      "Epoch 458/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.5173 - val_loss: 11.5389\n",
      "Epoch 459/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.4514 - val_loss: 11.3734\n",
      "Epoch 460/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.4202 - val_loss: 11.5623\n",
      "Epoch 461/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.3509 - val_loss: 12.3188\n",
      "Epoch 462/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 11.0216 - val_loss: 11.9486\n",
      "Epoch 463/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.6664 - val_loss: 11.3679\n",
      "Epoch 464/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.5139 - val_loss: 11.3388\n",
      "Epoch 465/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.3602 - val_loss: 11.2984\n",
      "Epoch 466/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.3841 - val_loss: 11.3053\n",
      "Epoch 467/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.4372 - val_loss: 11.2993\n",
      "Epoch 468/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.3365 - val_loss: 11.2706\n",
      "Epoch 469/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.5271 - val_loss: 11.2830\n",
      "Epoch 470/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.4728 - val_loss: 11.3154\n",
      "Epoch 471/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.3271 - val_loss: 11.3889\n",
      "Epoch 472/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.5327 - val_loss: 11.6078\n",
      "Epoch 473/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.4037 - val_loss: 11.3243\n",
      "Epoch 474/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 10.3751 - val_loss: 11.2246\n",
      "Epoch 475/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.3344 - val_loss: 11.3723\n",
      "Epoch 476/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.3495 - val_loss: 11.2516\n",
      "Epoch 477/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.2704 - val_loss: 11.2016\n",
      "Epoch 478/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.2652 - val_loss: 11.2360\n",
      "Epoch 479/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 10.3398 - val_loss: 11.2029\n",
      "Epoch 480/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.2144 - val_loss: 11.1882\n",
      "Epoch 481/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.3083 - val_loss: 11.1988\n",
      "Epoch 482/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.2667 - val_loss: 11.1731\n",
      "Epoch 483/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.2030 - val_loss: 11.1746\n",
      "Epoch 484/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.2922 - val_loss: 11.2090\n",
      "Epoch 485/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.7541 - val_loss: 11.5273\n",
      "Epoch 486/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.3604 - val_loss: 11.4080\n",
      "Epoch 487/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.3479 - val_loss: 11.1185\n",
      "Epoch 488/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.2829 - val_loss: 11.1249\n",
      "Epoch 489/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.1997 - val_loss: 11.0854\n",
      "Epoch 490/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.2404 - val_loss: 11.5527\n",
      "Epoch 491/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.2326 - val_loss: 11.5304\n",
      "Epoch 492/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.2497 - val_loss: 11.0935\n",
      "Epoch 493/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.2329 - val_loss: 11.0875\n",
      "Epoch 494/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.1363 - val_loss: 11.6472\n",
      "Epoch 495/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.6271 - val_loss: 11.1731\n",
      "Epoch 496/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.5303 - val_loss: 11.0892\n",
      "Epoch 497/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.1608 - val_loss: 11.0888\n",
      "Epoch 498/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.1934 - val_loss: 11.0801\n",
      "Epoch 499/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.3476 - val_loss: 11.0179\n",
      "Epoch 500/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.1712 - val_loss: 11.0414\n",
      "Epoch 501/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.1873 - val_loss: 11.2956\n",
      "Epoch 502/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.1778 - val_loss: 11.1353\n",
      "Epoch 503/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.1042 - val_loss: 11.1424\n",
      "Epoch 504/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.0797 - val_loss: 10.9847\n",
      "Epoch 505/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.1244 - val_loss: 10.9862\n",
      "Epoch 506/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.0516 - val_loss: 10.9736\n",
      "Epoch 507/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.1022 - val_loss: 11.2373\n",
      "Epoch 508/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.0842 - val_loss: 10.9963\n",
      "Epoch 509/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.1063 - val_loss: 11.0789\n",
      "Epoch 510/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 10.0233 - val_loss: 11.1429\n",
      "Epoch 511/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.9714 - val_loss: 11.1406\n",
      "Epoch 512/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.1223 - val_loss: 11.2488\n",
      "Epoch 513/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 10.2629 - val_loss: 11.0688\n",
      "Epoch 514/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.0509 - val_loss: 10.9799\n",
      "Epoch 515/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.0513 - val_loss: 10.9328\n",
      "Epoch 516/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.1173 - val_loss: 11.3155\n",
      "Epoch 517/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.9520 - val_loss: 10.9692\n",
      "Epoch 518/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.9816 - val_loss: 11.0256\n",
      "Epoch 519/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.1184 - val_loss: 11.3619\n",
      "Epoch 520/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.9078 - val_loss: 10.8867\n",
      "Epoch 521/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.9065 - val_loss: 11.1291\n",
      "Epoch 522/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.0963 - val_loss: 10.8866\n",
      "Epoch 523/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.0370 - val_loss: 10.9714\n",
      "Epoch 524/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.0909 - val_loss: 10.8536\n",
      "Epoch 525/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 9.9414 - val_loss: 10.8808\n",
      "Epoch 526/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.0151 - val_loss: 11.1767\n",
      "Epoch 527/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.9233 - val_loss: 11.0103\n",
      "Epoch 528/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.8918 - val_loss: 10.9493\n",
      "Epoch 529/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.9558 - val_loss: 10.9920\n",
      "Epoch 530/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.0906 - val_loss: 11.1443\n",
      "Epoch 531/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.9128 - val_loss: 10.8719\n",
      "Epoch 532/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 9.9770 - val_loss: 10.7797\n",
      "Epoch 533/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.8420 - val_loss: 10.9389\n",
      "Epoch 534/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.8117 - val_loss: 10.8523\n",
      "Epoch 535/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.7946 - val_loss: 10.8081\n",
      "Epoch 536/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.9140 - val_loss: 10.8051\n",
      "Epoch 537/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.8410 - val_loss: 10.7404\n",
      "Epoch 538/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.8391 - val_loss: 10.7461\n",
      "Epoch 539/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.8145 - val_loss: 11.1345\n",
      "Epoch 540/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.9073 - val_loss: 10.8543\n",
      "Epoch 541/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.0689 - val_loss: 10.7411\n",
      "Epoch 542/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.8204 - val_loss: 11.0087\n",
      "Epoch 543/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.8797 - val_loss: 10.7253\n",
      "Epoch 544/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.9356 - val_loss: 10.8265\n",
      "Epoch 545/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.9050 - val_loss: 10.7050\n",
      "Epoch 546/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.7484 - val_loss: 10.7224\n",
      "Epoch 547/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 9.8378 - val_loss: 10.7426\n",
      "Epoch 548/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 10.0202 - val_loss: 10.7214\n",
      "Epoch 549/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.7887 - val_loss: 10.7663\n",
      "Epoch 550/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 10.0134 - val_loss: 10.7157\n",
      "Epoch 551/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.7300 - val_loss: 10.6900\n",
      "Epoch 552/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.6616 - val_loss: 10.7399\n",
      "Epoch 553/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.8202 - val_loss: 10.6647\n",
      "Epoch 554/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.6773 - val_loss: 10.6905\n",
      "Epoch 555/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.7058 - val_loss: 10.7870\n",
      "Epoch 556/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.7264 - val_loss: 10.8038\n",
      "Epoch 557/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.8343 - val_loss: 10.7117\n",
      "Epoch 558/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.6490 - val_loss: 10.6702\n",
      "Epoch 559/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.6165 - val_loss: 11.0256\n",
      "Epoch 560/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.6496 - val_loss: 10.8802\n",
      "Epoch 561/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.7765 - val_loss: 10.6531\n",
      "Epoch 562/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.9102 - val_loss: 10.6915\n",
      "Epoch 563/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.6518 - val_loss: 10.6248\n",
      "Epoch 564/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.7480 - val_loss: 10.8032\n",
      "Epoch 565/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.6114 - val_loss: 10.5820\n",
      "Epoch 566/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.5245 - val_loss: 11.0554\n",
      "Epoch 567/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.8744 - val_loss: 10.7124\n",
      "Epoch 568/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.5692 - val_loss: 10.7400\n",
      "Epoch 569/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 9.7505 - val_loss: 10.6588\n",
      "Epoch 570/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.5596 - val_loss: 10.8245\n",
      "Epoch 571/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 10.1114 - val_loss: 10.9133\n",
      "Epoch 572/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.5563 - val_loss: 10.5450\n",
      "Epoch 573/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.7420 - val_loss: 10.8049\n",
      "Epoch 574/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.7742 - val_loss: 10.6482\n",
      "Epoch 575/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.6464 - val_loss: 10.7093\n",
      "Epoch 576/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.5653 - val_loss: 10.5317\n",
      "Epoch 577/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.6394 - val_loss: 10.5648\n",
      "Epoch 578/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.7069 - val_loss: 10.6573\n",
      "Epoch 579/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.5100 - val_loss: 10.4877\n",
      "Epoch 580/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.5879 - val_loss: 10.4938\n",
      "Epoch 581/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 9.6774 - val_loss: 10.7228\n",
      "Epoch 582/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.6086 - val_loss: 10.7073\n",
      "Epoch 583/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.5012 - val_loss: 10.4730\n",
      "Epoch 584/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.4548 - val_loss: 10.5700\n",
      "Epoch 585/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.5219 - val_loss: 10.7265\n",
      "Epoch 586/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.6006 - val_loss: 10.4944\n",
      "Epoch 587/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.8549 - val_loss: 10.8065\n",
      "Epoch 588/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 9.5996 - val_loss: 10.3808\n",
      "Epoch 589/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.4735 - val_loss: 10.5495\n",
      "Epoch 590/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.5075 - val_loss: 10.4555\n",
      "Epoch 591/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 9.5318 - val_loss: 10.4007\n",
      "Epoch 592/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.4259 - val_loss: 10.4206\n",
      "Epoch 593/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.4136 - val_loss: 10.4833\n",
      "Epoch 594/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.4779 - val_loss: 11.0970\n",
      "Epoch 595/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.5762 - val_loss: 10.5220\n",
      "Epoch 596/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.4384 - val_loss: 10.4025\n",
      "Epoch 597/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.5655 - val_loss: 10.4071\n",
      "Epoch 598/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.4485 - val_loss: 10.6026\n",
      "Epoch 599/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.4733 - val_loss: 10.4068\n",
      "Epoch 600/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.3756 - val_loss: 10.9517\n",
      "Epoch 601/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 10.0222 - val_loss: 11.3879\n",
      "Epoch 602/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.6136 - val_loss: 10.3796\n",
      "Epoch 603/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.3756 - val_loss: 10.8848\n",
      "Epoch 604/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 9.6817 - val_loss: 10.4061\n",
      "Epoch 605/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.4562 - val_loss: 10.3888\n",
      "Epoch 606/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.5490 - val_loss: 10.4344\n",
      "Epoch 607/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.3640 - val_loss: 10.3537\n",
      "Epoch 608/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 49us/step - loss: 9.5982 - val_loss: 10.3294\n",
      "Epoch 609/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.3306 - val_loss: 10.3064\n",
      "Epoch 610/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.3712 - val_loss: 10.3021\n",
      "Epoch 611/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.2680 - val_loss: 10.2905\n",
      "Epoch 612/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.5701 - val_loss: 10.4531\n",
      "Epoch 613/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.4434 - val_loss: 10.3376\n",
      "Epoch 614/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.3363 - val_loss: 10.3806\n",
      "Epoch 615/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2640 - val_loss: 10.5127\n",
      "Epoch 616/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.2542 - val_loss: 10.6437\n",
      "Epoch 617/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 9.4648 - val_loss: 10.4585\n",
      "Epoch 618/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.3226 - val_loss: 10.3131\n",
      "Epoch 619/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.3124 - val_loss: 10.2804\n",
      "Epoch 620/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2563 - val_loss: 10.3261\n",
      "Epoch 621/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.3578 - val_loss: 10.2613\n",
      "Epoch 622/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.2557 - val_loss: 10.2653\n",
      "Epoch 623/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2114 - val_loss: 10.9311\n",
      "Epoch 624/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.4355 - val_loss: 10.2869\n",
      "Epoch 625/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2879 - val_loss: 10.4107\n",
      "Epoch 626/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.3180 - val_loss: 10.2720\n",
      "Epoch 627/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.3404 - val_loss: 10.3953\n",
      "Epoch 628/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.3367 - val_loss: 10.3898\n",
      "Epoch 629/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.4302 - val_loss: 10.6209\n",
      "Epoch 630/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.3135 - val_loss: 10.3003\n",
      "Epoch 631/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.3300 - val_loss: 10.1887\n",
      "Epoch 632/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1509 - val_loss: 10.1643\n",
      "Epoch 633/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.2119 - val_loss: 10.2228\n",
      "Epoch 634/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.1612 - val_loss: 10.1693\n",
      "Epoch 635/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2373 - val_loss: 10.3311\n",
      "Epoch 636/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.1750 - val_loss: 10.1745\n",
      "Epoch 637/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.1689 - val_loss: 10.4871\n",
      "Epoch 638/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.9614 - val_loss: 11.1964\n",
      "Epoch 639/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.4930 - val_loss: 10.1893\n",
      "Epoch 640/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.6311 - val_loss: 10.6226\n",
      "Epoch 641/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.4426 - val_loss: 10.5021\n",
      "Epoch 642/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2295 - val_loss: 10.1619\n",
      "Epoch 643/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1837 - val_loss: 10.3035\n",
      "Epoch 644/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.2248 - val_loss: 10.1403\n",
      "Epoch 645/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1505 - val_loss: 10.1457\n",
      "Epoch 646/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.0631 - val_loss: 10.6569\n",
      "Epoch 647/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.3321 - val_loss: 10.1411\n",
      "Epoch 648/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2176 - val_loss: 10.5115\n",
      "Epoch 649/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.5196 - val_loss: 10.5227\n",
      "Epoch 650/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.1294 - val_loss: 10.0958\n",
      "Epoch 651/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.1255 - val_loss: 10.0743\n",
      "Epoch 652/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.2944 - val_loss: 10.1948\n",
      "Epoch 653/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.0821 - val_loss: 10.0901\n",
      "Epoch 654/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.3077 - val_loss: 10.2105\n",
      "Epoch 655/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.1852 - val_loss: 10.0733\n",
      "Epoch 656/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1219 - val_loss: 10.6695\n",
      "Epoch 657/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.2584 - val_loss: 10.2461\n",
      "Epoch 658/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1564 - val_loss: 10.0551\n",
      "Epoch 659/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1527 - val_loss: 10.0588\n",
      "Epoch 660/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.0867 - val_loss: 10.0895\n",
      "Epoch 661/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.0089 - val_loss: 10.0563\n",
      "Epoch 662/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 9.2175 - val_loss: 10.0675\n",
      "Epoch 663/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.0957 - val_loss: 10.0932\n",
      "Epoch 664/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1780 - val_loss: 10.1190\n",
      "Epoch 665/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.9892 - val_loss: 10.1618\n",
      "Epoch 666/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1271 - val_loss: 10.2591\n",
      "Epoch 667/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.2941 - val_loss: 10.0520\n",
      "Epoch 668/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.0546 - val_loss: 10.0445\n",
      "Epoch 669/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.9421 - val_loss: 10.0611\n",
      "Epoch 670/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.1581 - val_loss: 10.0328\n",
      "Epoch 671/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.2166 - val_loss: 10.4114\n",
      "Epoch 672/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.9853 - val_loss: 10.0123\n",
      "Epoch 673/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.0371 - val_loss: 10.3616\n",
      "Epoch 674/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.0046 - val_loss: 10.1277\n",
      "Epoch 675/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.0147 - val_loss: 10.4269\n",
      "Epoch 676/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.1334 - val_loss: 10.3528\n",
      "Epoch 677/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.2526 - val_loss: 10.3750\n",
      "Epoch 678/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 9.1736 - val_loss: 10.0433\n",
      "Epoch 679/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.0602 - val_loss: 9.9617\n",
      "Epoch 680/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.8898 - val_loss: 9.9507\n",
      "Epoch 681/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8787 - val_loss: 10.1251\n",
      "Epoch 682/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.9590 - val_loss: 10.3551\n",
      "Epoch 683/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8898 - val_loss: 9.9320\n",
      "Epoch 684/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 8.9051 - val_loss: 9.9636\n",
      "Epoch 685/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.9318 - val_loss: 10.0412\n",
      "Epoch 686/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.6781 - val_loss: 10.6309\n",
      "Epoch 687/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.0921 - val_loss: 10.1196\n",
      "Epoch 688/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.8086 - val_loss: 10.0050\n",
      "Epoch 689/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 9.1975 - val_loss: 10.0815\n",
      "Epoch 690/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.9015 - val_loss: 9.9544\n",
      "Epoch 691/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.8656 - val_loss: 9.8871\n",
      "Epoch 692/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8079 - val_loss: 9.9214\n",
      "Epoch 693/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.8129 - val_loss: 9.8836\n",
      "Epoch 694/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8221 - val_loss: 9.8703\n",
      "Epoch 695/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.9373 - val_loss: 9.9286\n",
      "Epoch 696/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8371 - val_loss: 9.8537\n",
      "Epoch 697/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.8764 - val_loss: 10.2860\n",
      "Epoch 698/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.9750 - val_loss: 9.9312\n",
      "Epoch 699/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8149 - val_loss: 10.0576\n",
      "Epoch 700/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.2538 - val_loss: 10.0323\n",
      "Epoch 701/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.0034 - val_loss: 10.0477\n",
      "Epoch 702/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.7737 - val_loss: 10.9487\n",
      "Epoch 703/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 9.6427 - val_loss: 10.6345\n",
      "Epoch 704/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.9768 - val_loss: 9.8537\n",
      "Epoch 705/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8231 - val_loss: 9.9305\n",
      "Epoch 706/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.0876 - val_loss: 10.1378\n",
      "Epoch 707/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7467 - val_loss: 9.8272\n",
      "Epoch 708/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.7816 - val_loss: 9.8869\n",
      "Epoch 709/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.9140 - val_loss: 9.9401\n",
      "Epoch 710/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8422 - val_loss: 9.8542\n",
      "Epoch 711/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7455 - val_loss: 9.8417\n",
      "Epoch 712/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7341 - val_loss: 10.1080\n",
      "Epoch 713/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8228 - val_loss: 9.8016\n",
      "Epoch 714/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.7170 - val_loss: 10.0440\n",
      "Epoch 715/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.7951 - val_loss: 9.7709\n",
      "Epoch 716/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7038 - val_loss: 9.7893\n",
      "Epoch 717/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.7363 - val_loss: 9.8138\n",
      "Epoch 718/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.6649 - val_loss: 9.8081\n",
      "Epoch 719/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.6934 - val_loss: 9.8289\n",
      "Epoch 720/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7933 - val_loss: 9.8695\n",
      "Epoch 721/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 9.0252 - val_loss: 9.8571\n",
      "Epoch 722/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.7163 - val_loss: 9.9257\n",
      "Epoch 723/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.8460 - val_loss: 9.7668\n",
      "Epoch 724/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 8.7974 - val_loss: 9.8616\n",
      "Epoch 725/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.6506 - val_loss: 9.7898\n",
      "Epoch 726/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.8532 - val_loss: 9.7210\n",
      "Epoch 727/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.6665 - val_loss: 10.1375\n",
      "Epoch 728/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 9.1130 - val_loss: 9.7290\n",
      "Epoch 729/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.9601 - val_loss: 9.8806\n",
      "Epoch 730/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.9518 - val_loss: 9.9687\n",
      "Epoch 731/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7889 - val_loss: 9.7419\n",
      "Epoch 732/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.6404 - val_loss: 9.8365\n",
      "Epoch 733/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.6667 - val_loss: 9.7930\n",
      "Epoch 734/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.8401 - val_loss: 9.9989\n",
      "Epoch 735/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.6904 - val_loss: 9.7025\n",
      "Epoch 736/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.8555 - val_loss: 9.8139\n",
      "Epoch 737/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.6523 - val_loss: 9.6991\n",
      "Epoch 738/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.8652 - val_loss: 10.1046\n",
      "Epoch 739/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.6023 - val_loss: 9.6857\n",
      "Epoch 740/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.6033 - val_loss: 9.7329\n",
      "Epoch 741/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.6913 - val_loss: 9.6674\n",
      "Epoch 742/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.6950 - val_loss: 9.6885\n",
      "Epoch 743/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.9258 - val_loss: 10.6089\n",
      "Epoch 744/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.7941 - val_loss: 10.0576\n",
      "Epoch 745/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.6840 - val_loss: 9.6917\n",
      "Epoch 746/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.6069 - val_loss: 9.6338\n",
      "Epoch 747/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.6312 - val_loss: 9.6684\n",
      "Epoch 748/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7410 - val_loss: 9.6350\n",
      "Epoch 749/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7195 - val_loss: 9.6758\n",
      "Epoch 750/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 8.5798 - val_loss: 9.6500\n",
      "Epoch 751/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 8.7065 - val_loss: 9.7293\n",
      "Epoch 752/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 8.6081 - val_loss: 10.0555\n",
      "Epoch 753/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.6910 - val_loss: 9.6548\n",
      "Epoch 754/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.6561 - val_loss: 9.6971\n",
      "Epoch 755/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.6368 - val_loss: 9.6004\n",
      "Epoch 756/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5297 - val_loss: 9.6747\n",
      "Epoch 757/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.6044 - val_loss: 9.7793\n",
      "Epoch 758/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.7160 - val_loss: 9.6371\n",
      "Epoch 759/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.5267 - val_loss: 9.6156\n",
      "Epoch 760/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.6371 - val_loss: 9.6914\n",
      "Epoch 761/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 8.4977 - val_loss: 9.7507\n",
      "Epoch 762/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.5986 - val_loss: 9.8756\n",
      "Epoch 763/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 9.0442 - val_loss: 9.7161\n",
      "Epoch 764/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.5026 - val_loss: 9.6472\n",
      "Epoch 765/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.4949 - val_loss: 9.6264\n",
      "Epoch 766/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.7107 - val_loss: 9.6084\n",
      "Epoch 767/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.5959 - val_loss: 9.7095\n",
      "Epoch 768/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5006 - val_loss: 9.6046\n",
      "Epoch 769/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.4007 - val_loss: 10.0794\n",
      "Epoch 770/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.6530 - val_loss: 9.7922\n",
      "Epoch 771/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.4826 - val_loss: 9.6047\n",
      "Epoch 772/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.4830 - val_loss: 9.5788\n",
      "Epoch 773/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.5333 - val_loss: 9.5875\n",
      "Epoch 774/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.5906 - val_loss: 9.5505\n",
      "Epoch 775/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.5579 - val_loss: 10.0406\n",
      "Epoch 776/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.5553 - val_loss: 9.5727\n",
      "Epoch 777/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.4334 - val_loss: 9.6152\n",
      "Epoch 778/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.4303 - val_loss: 9.5568\n",
      "Epoch 779/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.4394 - val_loss: 9.6359\n",
      "Epoch 780/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5033 - val_loss: 10.1154\n",
      "Epoch 781/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.9121 - val_loss: 10.4558\n",
      "Epoch 782/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 9.1042 - val_loss: 9.7704\n",
      "Epoch 783/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.8140 - val_loss: 9.4971\n",
      "Epoch 784/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 8.4647 - val_loss: 9.5512\n",
      "Epoch 785/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.4786 - val_loss: 9.5183\n",
      "Epoch 786/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.4645 - val_loss: 9.6854\n",
      "Epoch 787/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.4986 - val_loss: 9.4871\n",
      "Epoch 788/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3879 - val_loss: 9.4863\n",
      "Epoch 789/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5368 - val_loss: 9.5173\n",
      "Epoch 790/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.3680 - val_loss: 9.9667\n",
      "Epoch 791/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.6957 - val_loss: 9.5678\n",
      "Epoch 792/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.4672 - val_loss: 9.4804\n",
      "Epoch 793/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.3490 - val_loss: 9.6060\n",
      "Epoch 794/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.6212 - val_loss: 9.4813\n",
      "Epoch 795/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 8.3696 - val_loss: 9.4467\n",
      "Epoch 796/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3597 - val_loss: 9.4931\n",
      "Epoch 797/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3699 - val_loss: 9.4734\n",
      "Epoch 798/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.5667 - val_loss: 9.4511\n",
      "Epoch 799/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3642 - val_loss: 9.4833\n",
      "Epoch 800/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.3979 - val_loss: 9.5152\n",
      "Epoch 801/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3435 - val_loss: 9.4680\n",
      "Epoch 802/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.5293 - val_loss: 9.4754\n",
      "Epoch 803/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.4304 - val_loss: 9.4579\n",
      "Epoch 804/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.3398 - val_loss: 9.9762\n",
      "Epoch 805/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.6115 - val_loss: 9.4711\n",
      "Epoch 806/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3825 - val_loss: 9.4677\n",
      "Epoch 807/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3832 - val_loss: 9.6041\n",
      "Epoch 808/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.3416 - val_loss: 9.4346\n",
      "Epoch 809/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.3463 - val_loss: 9.4625\n",
      "Epoch 810/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2717 - val_loss: 9.6783\n",
      "Epoch 811/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.4594 - val_loss: 9.6531\n",
      "Epoch 812/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.6213 - val_loss: 9.6556\n",
      "Epoch 813/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.7740 - val_loss: 9.5784\n",
      "Epoch 814/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.4358 - val_loss: 9.7261\n",
      "Epoch 815/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.3498 - val_loss: 9.4840\n",
      "Epoch 816/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2873 - val_loss: 9.4174\n",
      "Epoch 817/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.2492 - val_loss: 9.4243\n",
      "Epoch 818/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2448 - val_loss: 9.8447\n",
      "Epoch 819/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.3812 - val_loss: 9.4384\n",
      "Epoch 820/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.4459 - val_loss: 9.4249\n",
      "Epoch 821/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3113 - val_loss: 9.4512\n",
      "Epoch 822/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.2601 - val_loss: 9.4174\n",
      "Epoch 823/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.2134 - val_loss: 10.0522\n",
      "Epoch 824/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.5235 - val_loss: 9.5603\n",
      "Epoch 825/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.3134 - val_loss: 9.3628\n",
      "Epoch 826/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.5686 - val_loss: 9.7981\n",
      "Epoch 827/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.4156 - val_loss: 9.3870\n",
      "Epoch 828/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5028 - val_loss: 9.4883\n",
      "Epoch 829/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2827 - val_loss: 9.4986\n",
      "Epoch 830/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2789 - val_loss: 9.8165\n",
      "Epoch 831/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.6966 - val_loss: 9.5182\n",
      "Epoch 832/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.5448 - val_loss: 9.7242\n",
      "Epoch 833/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3140 - val_loss: 9.3931\n",
      "Epoch 834/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3291 - val_loss: 9.4982\n",
      "Epoch 835/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2982 - val_loss: 9.3854\n",
      "Epoch 836/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2093 - val_loss: 9.3462\n",
      "Epoch 837/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2103 - val_loss: 9.4769\n",
      "Epoch 838/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 8.1631 - val_loss: 9.4505\n",
      "Epoch 839/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3050 - val_loss: 9.3917\n",
      "Epoch 840/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.5860 - val_loss: 9.5344\n",
      "Epoch 841/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3079 - val_loss: 9.3050\n",
      "Epoch 842/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5169 - val_loss: 9.3394\n",
      "Epoch 843/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.3833 - val_loss: 9.8308\n",
      "Epoch 844/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5877 - val_loss: 9.3618\n",
      "Epoch 845/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.4146 - val_loss: 9.3624\n",
      "Epoch 846/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3058 - val_loss: 9.8783\n",
      "Epoch 847/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3485 - val_loss: 9.3080\n",
      "Epoch 848/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2078 - val_loss: 9.4138\n",
      "Epoch 849/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.5345 - val_loss: 9.6633\n",
      "Epoch 850/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.2454 - val_loss: 9.4649\n",
      "Epoch 851/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.2180 - val_loss: 9.2819\n",
      "Epoch 852/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1885 - val_loss: 9.4154\n",
      "Epoch 853/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2690 - val_loss: 9.5284\n",
      "Epoch 854/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.4623 - val_loss: 9.5074\n",
      "Epoch 855/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.3987 - val_loss: 9.3784\n",
      "Epoch 856/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.4534 - val_loss: 9.2596\n",
      "Epoch 857/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1738 - val_loss: 9.3017\n",
      "Epoch 858/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2663 - val_loss: 9.2783\n",
      "Epoch 859/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1164 - val_loss: 9.5428\n",
      "Epoch 860/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.1286 - val_loss: 9.3936\n",
      "Epoch 861/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2289 - val_loss: 9.2838\n",
      "Epoch 862/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1135 - val_loss: 9.3296\n",
      "Epoch 863/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.1784 - val_loss: 9.4187\n",
      "Epoch 864/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2174 - val_loss: 9.2758\n",
      "Epoch 865/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1617 - val_loss: 9.3654\n",
      "Epoch 866/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1438 - val_loss: 9.2493\n",
      "Epoch 867/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.1554 - val_loss: 9.2158\n",
      "Epoch 868/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0819 - val_loss: 9.3943\n",
      "Epoch 869/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1299 - val_loss: 9.2261\n",
      "Epoch 870/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2176 - val_loss: 9.2351\n",
      "Epoch 871/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1010 - val_loss: 9.3846\n",
      "Epoch 872/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.1893 - val_loss: 9.3920\n",
      "Epoch 873/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.1856 - val_loss: 9.2397\n",
      "Epoch 874/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0597 - val_loss: 9.2640\n",
      "Epoch 875/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1553 - val_loss: 9.1910\n",
      "Epoch 876/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.0476 - val_loss: 9.2130\n",
      "Epoch 877/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1670 - val_loss: 9.4199\n",
      "Epoch 878/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.2591 - val_loss: 9.2741\n",
      "Epoch 879/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2098 - val_loss: 9.3579\n",
      "Epoch 880/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0853 - val_loss: 9.2879\n",
      "Epoch 881/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.1951 - val_loss: 9.2203\n",
      "Epoch 882/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1623 - val_loss: 9.1822\n",
      "Epoch 883/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.1847 - val_loss: 9.3135\n",
      "Epoch 884/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0455 - val_loss: 9.2119\n",
      "Epoch 885/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0598 - val_loss: 9.5323\n",
      "Epoch 886/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0620 - val_loss: 9.7884\n",
      "Epoch 887/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.2059 - val_loss: 9.3147\n",
      "Epoch 888/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.1960 - val_loss: 9.2540\n",
      "Epoch 889/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0813 - val_loss: 9.2915\n",
      "Epoch 890/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0581 - val_loss: 9.2277\n",
      "Epoch 891/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3115 - val_loss: 9.1531\n",
      "Epoch 892/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.1934 - val_loss: 9.4058\n",
      "Epoch 893/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.2162 - val_loss: 9.8305\n",
      "Epoch 894/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.1324 - val_loss: 9.1867\n",
      "Epoch 895/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.0029 - val_loss: 9.1517\n",
      "Epoch 896/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0594 - val_loss: 9.1542\n",
      "Epoch 897/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0121 - val_loss: 9.1781\n",
      "Epoch 898/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0393 - val_loss: 9.2105\n",
      "Epoch 899/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0334 - val_loss: 9.1479\n",
      "Epoch 900/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9784 - val_loss: 9.1875\n",
      "Epoch 901/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0143 - val_loss: 9.4772\n",
      "Epoch 902/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.2817 - val_loss: 9.1348\n",
      "Epoch 903/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.9711 - val_loss: 9.1569\n",
      "Epoch 904/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.9476 - val_loss: 9.1295\n",
      "Epoch 905/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0359 - val_loss: 9.3105\n",
      "Epoch 906/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.2316 - val_loss: 9.2994\n",
      "Epoch 907/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0766 - val_loss: 9.2120\n",
      "Epoch 908/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.0678 - val_loss: 9.2379\n",
      "Epoch 909/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0238 - val_loss: 9.4291\n",
      "Epoch 910/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0178 - val_loss: 9.4635\n",
      "Epoch 911/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.6084 - val_loss: 9.1595\n",
      "Epoch 912/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9817 - val_loss: 9.1688\n",
      "Epoch 913/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9519 - val_loss: 9.3663\n",
      "Epoch 914/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3899 - val_loss: 9.2838\n",
      "Epoch 915/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 7.9978 - val_loss: 9.1553\n",
      "Epoch 916/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 8.2123 - val_loss: 9.4716\n",
      "Epoch 917/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0071 - val_loss: 9.1307\n",
      "Epoch 918/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9363 - val_loss: 9.1433\n",
      "Epoch 919/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9464 - val_loss: 9.2335\n",
      "Epoch 920/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9738 - val_loss: 9.1086\n",
      "Epoch 921/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.0640 - val_loss: 9.1205\n",
      "Epoch 922/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.9853 - val_loss: 9.7483\n",
      "Epoch 923/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.6153 - val_loss: 9.2472\n",
      "Epoch 924/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.3097 - val_loss: 9.2067\n",
      "Epoch 925/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9977 - val_loss: 9.1058\n",
      "Epoch 926/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0806 - val_loss: 10.3371\n",
      "Epoch 927/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.6743 - val_loss: 9.4213\n",
      "Epoch 928/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2179 - val_loss: 9.1022\n",
      "Epoch 929/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0546 - val_loss: 9.1655\n",
      "Epoch 930/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8901 - val_loss: 9.0820\n",
      "Epoch 931/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9397 - val_loss: 9.2314\n",
      "Epoch 932/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9616 - val_loss: 9.1123\n",
      "Epoch 933/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8755 - val_loss: 9.0710\n",
      "Epoch 934/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8546 - val_loss: 9.0155\n",
      "Epoch 935/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0434 - val_loss: 9.0974\n",
      "Epoch 936/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9367 - val_loss: 9.1607\n",
      "Epoch 937/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9540 - val_loss: 9.0426\n",
      "Epoch 938/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9490 - val_loss: 9.0922\n",
      "Epoch 939/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9663 - val_loss: 9.2062\n",
      "Epoch 940/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0719 - val_loss: 9.1479\n",
      "Epoch 941/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.9031 - val_loss: 9.0642\n",
      "Epoch 942/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8700 - val_loss: 9.0664\n",
      "Epoch 943/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.0396 - val_loss: 9.0129\n",
      "Epoch 944/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0108 - val_loss: 9.3055\n",
      "Epoch 945/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9447 - val_loss: 9.0237\n",
      "Epoch 946/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9869 - val_loss: 8.9860\n",
      "Epoch 947/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8156 - val_loss: 8.9938\n",
      "Epoch 948/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8681 - val_loss: 9.1730\n",
      "Epoch 949/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.8297 - val_loss: 9.0195\n",
      "Epoch 950/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8911 - val_loss: 9.0935\n",
      "Epoch 951/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.8400 - val_loss: 9.0320\n",
      "Epoch 952/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0030 - val_loss: 8.9814\n",
      "Epoch 953/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8114 - val_loss: 9.4557\n",
      "Epoch 954/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.0724 - val_loss: 8.9963\n",
      "Epoch 955/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9318 - val_loss: 9.8393\n",
      "Epoch 956/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2454 - val_loss: 9.2460\n",
      "Epoch 957/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9060 - val_loss: 9.0124\n",
      "Epoch 958/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0796 - val_loss: 9.0185\n",
      "Epoch 959/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9592 - val_loss: 9.2595\n",
      "Epoch 960/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8631 - val_loss: 9.2119\n",
      "Epoch 961/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9452 - val_loss: 9.1523\n",
      "Epoch 962/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.0900 - val_loss: 9.0229\n",
      "Epoch 963/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9196 - val_loss: 8.9566\n",
      "Epoch 964/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9539 - val_loss: 8.9575\n",
      "Epoch 965/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9486 - val_loss: 9.0765\n",
      "Epoch 966/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.8846 - val_loss: 8.9604\n",
      "Epoch 967/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.9628 - val_loss: 8.9685\n",
      "Epoch 968/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9079 - val_loss: 8.9831\n",
      "Epoch 969/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8140 - val_loss: 9.3083\n",
      "Epoch 970/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9130 - val_loss: 8.9343\n",
      "Epoch 971/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8223 - val_loss: 9.0188\n",
      "Epoch 972/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.7593 - val_loss: 8.9593\n",
      "Epoch 973/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.7714 - val_loss: 8.9185\n",
      "Epoch 974/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8559 - val_loss: 9.1295\n",
      "Epoch 975/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8238 - val_loss: 8.9919\n",
      "Epoch 976/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8600 - val_loss: 8.9816\n",
      "Epoch 977/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.8369 - val_loss: 8.9059\n",
      "Epoch 978/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8697 - val_loss: 8.9456\n",
      "Epoch 979/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7660 - val_loss: 8.9185\n",
      "Epoch 980/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7863 - val_loss: 9.0306\n",
      "Epoch 981/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.7578 - val_loss: 8.9502\n",
      "Epoch 982/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9151 - val_loss: 8.9903\n",
      "Epoch 983/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9861 - val_loss: 9.0362\n",
      "Epoch 984/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.1715 - val_loss: 9.5182\n",
      "Epoch 985/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8164 - val_loss: 9.1453\n",
      "Epoch 986/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.8566 - val_loss: 8.9775\n",
      "Epoch 987/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 7.7360 - val_loss: 8.9243\n",
      "Epoch 988/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9825 - val_loss: 8.9229\n",
      "Epoch 989/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.7751 - val_loss: 8.9135\n",
      "Epoch 990/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.7517 - val_loss: 8.9429\n",
      "Epoch 991/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.7373 - val_loss: 9.0932\n",
      "Epoch 992/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 7.9876 - val_loss: 8.8909\n",
      "Epoch 993/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.7039 - val_loss: 8.8643\n",
      "Epoch 994/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7012 - val_loss: 8.9030\n",
      "Epoch 995/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.7799 - val_loss: 9.0574\n",
      "Epoch 996/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.7970 - val_loss: 8.9467\n",
      "Epoch 997/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8234 - val_loss: 9.1228\n",
      "Epoch 998/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0524 - val_loss: 9.0615\n",
      "Epoch 999/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8916 - val_loss: 8.9364\n",
      "Epoch 1000/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9562 - val_loss: 9.1716\n",
      "Epoch 1001/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 8.0310 - val_loss: 9.1103\n",
      "Epoch 1002/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.1310 - val_loss: 9.3772\n",
      "Epoch 1003/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8588 - val_loss: 8.8715\n",
      "Epoch 1004/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.7031 - val_loss: 8.9935\n",
      "Epoch 1005/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.7223 - val_loss: 8.8582\n",
      "Epoch 1006/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7472 - val_loss: 9.0524\n",
      "Epoch 1007/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.9763 - val_loss: 8.8657\n",
      "Epoch 1008/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7640 - val_loss: 9.3479\n",
      "Epoch 1009/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.9736 - val_loss: 8.8641\n",
      "Epoch 1010/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7436 - val_loss: 8.8389\n",
      "Epoch 1011/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6719 - val_loss: 8.9208\n",
      "Epoch 1012/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8597 - val_loss: 8.8799\n",
      "Epoch 1013/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8636 - val_loss: 9.0169\n",
      "Epoch 1014/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7151 - val_loss: 9.2067\n",
      "Epoch 1015/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.0279 - val_loss: 9.3562\n",
      "Epoch 1016/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6902 - val_loss: 8.8831\n",
      "Epoch 1017/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6657 - val_loss: 8.8552\n",
      "Epoch 1018/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6628 - val_loss: 9.1387\n",
      "Epoch 1019/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.7194 - val_loss: 8.8186\n",
      "Epoch 1020/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 7.6461 - val_loss: 8.8189\n",
      "Epoch 1021/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6418 - val_loss: 8.8091\n",
      "Epoch 1022/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6580 - val_loss: 8.9571\n",
      "Epoch 1023/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.9479 - val_loss: 9.3318\n",
      "Epoch 1024/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.9613 - val_loss: 8.9819\n",
      "Epoch 1025/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6638 - val_loss: 8.8947\n",
      "Epoch 1026/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6497 - val_loss: 8.9900\n",
      "Epoch 1027/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.8619 - val_loss: 8.9366\n",
      "Epoch 1028/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6786 - val_loss: 8.8304\n",
      "Epoch 1029/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7087 - val_loss: 8.8870\n",
      "Epoch 1030/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.6436 - val_loss: 8.9405\n",
      "Epoch 1031/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7190 - val_loss: 8.8218\n",
      "Epoch 1032/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7046 - val_loss: 8.8660\n",
      "Epoch 1033/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7504 - val_loss: 8.8460\n",
      "Epoch 1034/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8438 - val_loss: 8.8303\n",
      "Epoch 1035/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6437 - val_loss: 9.0057\n",
      "Epoch 1036/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.0444 - val_loss: 10.2318\n",
      "Epoch 1037/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 8.2133 - val_loss: 9.1612\n",
      "Epoch 1038/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 8.0028 - val_loss: 8.8296\n",
      "Epoch 1039/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8538 - val_loss: 9.3986\n",
      "Epoch 1040/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8729 - val_loss: 8.9134\n",
      "Epoch 1041/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6613 - val_loss: 8.8117\n",
      "Epoch 1042/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7340 - val_loss: 9.0961\n",
      "Epoch 1043/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6640 - val_loss: 9.0285\n",
      "Epoch 1044/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.7198 - val_loss: 8.8775\n",
      "Epoch 1045/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6607 - val_loss: 8.8019\n",
      "Epoch 1046/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.7502 - val_loss: 8.8086\n",
      "Epoch 1047/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7079 - val_loss: 8.7751\n",
      "Epoch 1048/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6187 - val_loss: 9.0457\n",
      "Epoch 1049/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8598 - val_loss: 8.8616\n",
      "Epoch 1050/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6314 - val_loss: 8.8850\n",
      "Epoch 1051/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7392 - val_loss: 8.7733\n",
      "Epoch 1052/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7719 - val_loss: 8.7874\n",
      "Epoch 1053/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.6754 - val_loss: 8.7699\n",
      "Epoch 1054/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5537 - val_loss: 9.4927\n",
      "Epoch 1055/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7983 - val_loss: 8.8032\n",
      "Epoch 1056/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6691 - val_loss: 8.7510\n",
      "Epoch 1057/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6454 - val_loss: 9.0221\n",
      "Epoch 1058/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8032 - val_loss: 8.7575\n",
      "Epoch 1059/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5625 - val_loss: 8.8178\n",
      "Epoch 1060/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.6193 - val_loss: 8.7958\n",
      "Epoch 1061/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5232 - val_loss: 8.8636\n",
      "Epoch 1062/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.5984 - val_loss: 8.7395\n",
      "Epoch 1063/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5477 - val_loss: 8.8632\n",
      "Epoch 1064/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.5560 - val_loss: 8.9205\n",
      "Epoch 1065/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.8053 - val_loss: 8.7568\n",
      "Epoch 1066/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.6104 - val_loss: 8.9806\n",
      "Epoch 1067/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.5734 - val_loss: 8.8738\n",
      "Epoch 1068/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5370 - val_loss: 8.9370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1069/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.6614 - val_loss: 8.8930\n",
      "Epoch 1070/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6464 - val_loss: 8.7644\n",
      "Epoch 1071/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.6053 - val_loss: 8.7722\n",
      "Epoch 1072/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.6029 - val_loss: 8.7422\n",
      "Epoch 1073/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6525 - val_loss: 8.8562\n",
      "Epoch 1074/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6519 - val_loss: 8.7038\n",
      "Epoch 1075/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5335 - val_loss: 9.1395\n",
      "Epoch 1076/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6577 - val_loss: 8.8704\n",
      "Epoch 1077/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7209 - val_loss: 8.7317\n",
      "Epoch 1078/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.9278 - val_loss: 8.8260\n",
      "Epoch 1079/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7042 - val_loss: 9.4184\n",
      "Epoch 1080/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6248 - val_loss: 8.9317\n",
      "Epoch 1081/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5660 - val_loss: 8.7061\n",
      "Epoch 1082/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4798 - val_loss: 9.1511\n",
      "Epoch 1083/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6651 - val_loss: 8.6995\n",
      "Epoch 1084/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5548 - val_loss: 8.7175\n",
      "Epoch 1085/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.8048 - val_loss: 9.4857\n",
      "Epoch 1086/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 8.8527 - val_loss: 9.3940\n",
      "Epoch 1087/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7255 - val_loss: 8.8379\n",
      "Epoch 1088/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.5442 - val_loss: 8.7574\n",
      "Epoch 1089/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.5561 - val_loss: 8.7191\n",
      "Epoch 1090/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5040 - val_loss: 8.6979\n",
      "Epoch 1091/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.7443 - val_loss: 8.7325\n",
      "Epoch 1092/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 7.4964 - val_loss: 8.7329\n",
      "Epoch 1093/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4864 - val_loss: 8.8999\n",
      "Epoch 1094/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7370 - val_loss: 9.0969\n",
      "Epoch 1095/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5648 - val_loss: 8.8095\n",
      "Epoch 1096/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5441 - val_loss: 8.7922\n",
      "Epoch 1097/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.6087 - val_loss: 8.7765\n",
      "Epoch 1098/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.4879 - val_loss: 8.6786\n",
      "Epoch 1099/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4728 - val_loss: 8.7295\n",
      "Epoch 1100/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.5368 - val_loss: 8.6798\n",
      "Epoch 1101/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.6427 - val_loss: 8.6850\n",
      "Epoch 1102/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5620 - val_loss: 8.9489\n",
      "Epoch 1103/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.8679 - val_loss: 8.7017\n",
      "Epoch 1104/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6810 - val_loss: 8.6846\n",
      "Epoch 1105/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7706 - val_loss: 8.6528\n",
      "Epoch 1106/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4452 - val_loss: 8.7093\n",
      "Epoch 1107/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4899 - val_loss: 8.6629\n",
      "Epoch 1108/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4615 - val_loss: 8.6918\n",
      "Epoch 1109/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4728 - val_loss: 8.6705\n",
      "Epoch 1110/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.6521 - val_loss: 8.6748\n",
      "Epoch 1111/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6641 - val_loss: 8.6396\n",
      "Epoch 1112/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.5397 - val_loss: 8.6537\n",
      "Epoch 1113/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4607 - val_loss: 8.6329\n",
      "Epoch 1114/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4935 - val_loss: 8.6660\n",
      "Epoch 1115/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5896 - val_loss: 8.6658\n",
      "Epoch 1116/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4119 - val_loss: 8.6472\n",
      "Epoch 1117/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4828 - val_loss: 8.7430\n",
      "Epoch 1118/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4958 - val_loss: 8.6409\n",
      "Epoch 1119/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4119 - val_loss: 8.6084\n",
      "Epoch 1120/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.5158 - val_loss: 8.6088\n",
      "Epoch 1121/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5154 - val_loss: 8.7644\n",
      "Epoch 1122/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7709 - val_loss: 8.8127\n",
      "Epoch 1123/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4699 - val_loss: 8.6110\n",
      "Epoch 1124/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4002 - val_loss: 8.6206\n",
      "Epoch 1125/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 7.6019 - val_loss: 8.6210\n",
      "Epoch 1126/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4029 - val_loss: 8.7698\n",
      "Epoch 1127/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6380 - val_loss: 8.6076\n",
      "Epoch 1128/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5015 - val_loss: 9.0912\n",
      "Epoch 1129/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7766 - val_loss: 8.5994\n",
      "Epoch 1130/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4718 - val_loss: 8.7185\n",
      "Epoch 1131/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4540 - val_loss: 8.7445\n",
      "Epoch 1132/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.9070 - val_loss: 8.7483\n",
      "Epoch 1133/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4319 - val_loss: 8.5976\n",
      "Epoch 1134/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.5796 - val_loss: 8.7461\n",
      "Epoch 1135/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4520 - val_loss: 8.6303\n",
      "Epoch 1136/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4641 - val_loss: 8.8725\n",
      "Epoch 1137/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.8815 - val_loss: 9.1833\n",
      "Epoch 1138/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5011 - val_loss: 8.5944\n",
      "Epoch 1139/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.4405 - val_loss: 8.5898\n",
      "Epoch 1140/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4195 - val_loss: 8.7885\n",
      "Epoch 1141/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.4785 - val_loss: 8.6304\n",
      "Epoch 1142/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.6062 - val_loss: 8.8123\n",
      "Epoch 1143/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4431 - val_loss: 8.5837\n",
      "Epoch 1144/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4810 - val_loss: 8.5625\n",
      "Epoch 1145/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 7.6178 - val_loss: 8.9708\n",
      "Epoch 1146/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6085 - val_loss: 8.6634\n",
      "Epoch 1147/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4106 - val_loss: 9.1607\n",
      "Epoch 1148/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6174 - val_loss: 8.6934\n",
      "Epoch 1149/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4786 - val_loss: 8.5670\n",
      "Epoch 1150/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4320 - val_loss: 8.6391\n",
      "Epoch 1151/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4607 - val_loss: 8.6877\n",
      "Epoch 1152/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.9799 - val_loss: 8.6613\n",
      "Epoch 1153/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.3846 - val_loss: 8.5814\n",
      "Epoch 1154/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3456 - val_loss: 8.7033\n",
      "Epoch 1155/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3482 - val_loss: 8.5794\n",
      "Epoch 1156/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3640 - val_loss: 8.5674\n",
      "Epoch 1157/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3460 - val_loss: 8.6004\n",
      "Epoch 1158/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3993 - val_loss: 8.5647\n",
      "Epoch 1159/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.5952 - val_loss: 8.7068\n",
      "Epoch 1160/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3773 - val_loss: 8.5484\n",
      "Epoch 1161/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3417 - val_loss: 8.5260\n",
      "Epoch 1162/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3797 - val_loss: 8.5325\n",
      "Epoch 1163/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3527 - val_loss: 8.5347\n",
      "Epoch 1164/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.3286 - val_loss: 8.6702\n",
      "Epoch 1165/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3760 - val_loss: 8.5419\n",
      "Epoch 1166/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4129 - val_loss: 8.6458\n",
      "Epoch 1167/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4558 - val_loss: 8.5404\n",
      "Epoch 1168/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.5359 - val_loss: 8.9164\n",
      "Epoch 1169/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7072 - val_loss: 8.5376\n",
      "Epoch 1170/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4041 - val_loss: 8.9914\n",
      "Epoch 1171/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8677 - val_loss: 8.4987\n",
      "Epoch 1172/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3466 - val_loss: 8.5581\n",
      "Epoch 1173/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3634 - val_loss: 8.5238\n",
      "Epoch 1174/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.4146 - val_loss: 8.6193\n",
      "Epoch 1175/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.3982 - val_loss: 8.5181\n",
      "Epoch 1176/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3128 - val_loss: 8.5182\n",
      "Epoch 1177/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6290 - val_loss: 8.5169\n",
      "Epoch 1178/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.3348 - val_loss: 8.5839\n",
      "Epoch 1179/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3341 - val_loss: 8.9183\n",
      "Epoch 1180/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7483 - val_loss: 8.5623\n",
      "Epoch 1181/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.3070 - val_loss: 8.5002\n",
      "Epoch 1182/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3594 - val_loss: 8.5186\n",
      "Epoch 1183/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.5196 - val_loss: 8.5833\n",
      "Epoch 1184/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3934 - val_loss: 8.5702\n",
      "Epoch 1185/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3641 - val_loss: 8.6092\n",
      "Epoch 1186/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3238 - val_loss: 8.5114\n",
      "Epoch 1187/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4963 - val_loss: 8.5027\n",
      "Epoch 1188/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2935 - val_loss: 8.7588\n",
      "Epoch 1189/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4870 - val_loss: 8.5342\n",
      "Epoch 1190/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.3348 - val_loss: 8.5081\n",
      "Epoch 1191/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4841 - val_loss: 8.9640\n",
      "Epoch 1192/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5896 - val_loss: 8.9351\n",
      "Epoch 1193/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5790 - val_loss: 8.7586\n",
      "Epoch 1194/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 8.2425 - val_loss: 9.0566\n",
      "Epoch 1195/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.5034 - val_loss: 8.5512\n",
      "Epoch 1196/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2517 - val_loss: 8.5325\n",
      "Epoch 1197/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3158 - val_loss: 8.5454\n",
      "Epoch 1198/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4306 - val_loss: 8.5403\n",
      "Epoch 1199/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4728 - val_loss: 8.5930\n",
      "Epoch 1200/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4046 - val_loss: 8.5524\n",
      "Epoch 1201/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4168 - val_loss: 8.4957\n",
      "Epoch 1202/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2863 - val_loss: 8.4779\n",
      "Epoch 1203/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5813 - val_loss: 8.4506\n",
      "Epoch 1204/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3003 - val_loss: 8.5824\n",
      "Epoch 1205/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.6862 - val_loss: 8.4805\n",
      "Epoch 1206/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.2516 - val_loss: 8.5220\n",
      "Epoch 1207/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3650 - val_loss: 8.4671\n",
      "Epoch 1208/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.4117 - val_loss: 8.6553\n",
      "Epoch 1209/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5642 - val_loss: 8.5337\n",
      "Epoch 1210/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3282 - val_loss: 8.5540\n",
      "Epoch 1211/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4257 - val_loss: 8.5064\n",
      "Epoch 1212/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3495 - val_loss: 8.4776\n",
      "Epoch 1213/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.2600 - val_loss: 8.5835\n",
      "Epoch 1214/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2720 - val_loss: 8.7083\n",
      "Epoch 1215/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4737 - val_loss: 8.5312\n",
      "Epoch 1216/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3933 - val_loss: 8.4753\n",
      "Epoch 1217/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.2176 - val_loss: 8.4625\n",
      "Epoch 1218/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5342 - val_loss: 8.5614\n",
      "Epoch 1219/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.2417 - val_loss: 8.5199\n",
      "Epoch 1220/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3781 - val_loss: 8.7679\n",
      "Epoch 1221/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 7.2694 - val_loss: 8.4814\n",
      "Epoch 1222/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.2265 - val_loss: 8.6962\n",
      "Epoch 1223/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.3718 - val_loss: 8.5454\n",
      "Epoch 1224/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2433 - val_loss: 8.5019\n",
      "Epoch 1225/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2480 - val_loss: 8.6075\n",
      "Epoch 1226/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2209 - val_loss: 8.4390\n",
      "Epoch 1227/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2230 - val_loss: 8.6648\n",
      "Epoch 1228/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5652 - val_loss: 8.4532\n",
      "Epoch 1229/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.2654 - val_loss: 8.4827\n",
      "Epoch 1230/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.3422 - val_loss: 8.5051\n",
      "Epoch 1231/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.2479 - val_loss: 8.4154\n",
      "Epoch 1232/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.2295 - val_loss: 8.4939\n",
      "Epoch 1233/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3664 - val_loss: 8.7792\n",
      "Epoch 1234/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3120 - val_loss: 8.4618\n",
      "Epoch 1235/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.3641 - val_loss: 8.5915\n",
      "Epoch 1236/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2540 - val_loss: 8.5767\n",
      "Epoch 1237/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3032 - val_loss: 8.4712\n",
      "Epoch 1238/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3215 - val_loss: 8.4434\n",
      "Epoch 1239/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4058 - val_loss: 8.4727\n",
      "Epoch 1240/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.2576 - val_loss: 8.4421\n",
      "Epoch 1241/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2906 - val_loss: 8.4979\n",
      "Epoch 1242/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2236 - val_loss: 8.5549\n",
      "Epoch 1243/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5739 - val_loss: 8.4535\n",
      "Epoch 1244/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.2390 - val_loss: 8.4839\n",
      "Epoch 1245/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1953 - val_loss: 8.4243\n",
      "Epoch 1246/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2170 - val_loss: 8.7840\n",
      "Epoch 1247/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4759 - val_loss: 8.4449\n",
      "Epoch 1248/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3839 - val_loss: 8.4165\n",
      "Epoch 1249/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1768 - val_loss: 8.4060\n",
      "Epoch 1250/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1647 - val_loss: 8.4282\n",
      "Epoch 1251/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.1912 - val_loss: 8.4165\n",
      "Epoch 1252/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1910 - val_loss: 8.4312\n",
      "Epoch 1253/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2541 - val_loss: 8.6227\n",
      "Epoch 1254/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7927 - val_loss: 8.3889\n",
      "Epoch 1255/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.3869 - val_loss: 8.3934\n",
      "Epoch 1256/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.3888 - val_loss: 8.4456\n",
      "Epoch 1257/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3256 - val_loss: 8.4711\n",
      "Epoch 1258/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1939 - val_loss: 8.6609\n",
      "Epoch 1259/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2229 - val_loss: 8.4170\n",
      "Epoch 1260/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1524 - val_loss: 8.4091\n",
      "Epoch 1261/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1447 - val_loss: 8.4335\n",
      "Epoch 1262/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1939 - val_loss: 8.3906\n",
      "Epoch 1263/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1747 - val_loss: 8.4726\n",
      "Epoch 1264/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2094 - val_loss: 8.5378\n",
      "Epoch 1265/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7139 - val_loss: 8.7091\n",
      "Epoch 1266/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5821 - val_loss: 8.4560\n",
      "Epoch 1267/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2364 - val_loss: 8.4208\n",
      "Epoch 1268/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3905 - val_loss: 8.4609\n",
      "Epoch 1269/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2780 - val_loss: 8.3664\n",
      "Epoch 1270/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1776 - val_loss: 8.3593\n",
      "Epoch 1271/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1911 - val_loss: 8.4918\n",
      "Epoch 1272/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2409 - val_loss: 8.3579\n",
      "Epoch 1273/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2897 - val_loss: 8.4202\n",
      "Epoch 1274/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2737 - val_loss: 8.3457\n",
      "Epoch 1275/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1055 - val_loss: 8.4708\n",
      "Epoch 1276/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1799 - val_loss: 8.3557\n",
      "Epoch 1277/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4308 - val_loss: 8.5399\n",
      "Epoch 1278/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2103 - val_loss: 8.3557\n",
      "Epoch 1279/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5285 - val_loss: 9.0494\n",
      "Epoch 1280/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3321 - val_loss: 8.4121\n",
      "Epoch 1281/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1841 - val_loss: 8.3392\n",
      "Epoch 1282/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1517 - val_loss: 9.2858\n",
      "Epoch 1283/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.8067 - val_loss: 8.9164\n",
      "Epoch 1284/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1576 - val_loss: 8.3692\n",
      "Epoch 1285/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1162 - val_loss: 8.3708\n",
      "Epoch 1286/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3632 - val_loss: 8.3564\n",
      "Epoch 1287/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3988 - val_loss: 8.4323\n",
      "Epoch 1288/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0862 - val_loss: 8.5168\n",
      "Epoch 1289/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1496 - val_loss: 8.4966\n",
      "Epoch 1290/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2103 - val_loss: 8.3097\n",
      "Epoch 1291/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1429 - val_loss: 8.3183\n",
      "Epoch 1292/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1931 - val_loss: 8.3345\n",
      "Epoch 1293/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0929 - val_loss: 8.2929\n",
      "Epoch 1294/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1361 - val_loss: 8.4908\n",
      "Epoch 1295/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1472 - val_loss: 8.3254\n",
      "Epoch 1296/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1442 - val_loss: 8.3081\n",
      "Epoch 1297/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 7.1029 - val_loss: 8.2923\n",
      "Epoch 1298/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0725 - val_loss: 8.3055\n",
      "Epoch 1299/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0834 - val_loss: 8.3355\n",
      "Epoch 1300/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0972 - val_loss: 8.7370\n",
      "Epoch 1301/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1543 - val_loss: 8.2614\n",
      "Epoch 1302/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2356 - val_loss: 8.6978\n",
      "Epoch 1303/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1157 - val_loss: 8.3767\n",
      "Epoch 1304/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2621 - val_loss: 8.3251\n",
      "Epoch 1305/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.2232 - val_loss: 8.4011\n",
      "Epoch 1306/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.8368 - val_loss: 8.5154\n",
      "Epoch 1307/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.7878 - val_loss: 8.3690\n",
      "Epoch 1308/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.4954 - val_loss: 8.3654\n",
      "Epoch 1309/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0932 - val_loss: 8.3445\n",
      "Epoch 1310/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1113 - val_loss: 8.8481\n",
      "Epoch 1311/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.5186 - val_loss: 8.3488\n",
      "Epoch 1312/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1994 - val_loss: 8.3085\n",
      "Epoch 1313/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2457 - val_loss: 8.3357\n",
      "Epoch 1314/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1746 - val_loss: 8.2730\n",
      "Epoch 1315/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2427 - val_loss: 8.3608\n",
      "Epoch 1316/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0987 - val_loss: 8.5935\n",
      "Epoch 1317/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.2727 - val_loss: 8.2930\n",
      "Epoch 1318/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.0797 - val_loss: 8.4893\n",
      "Epoch 1319/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2210 - val_loss: 8.3760\n",
      "Epoch 1320/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1655 - val_loss: 8.2722\n",
      "Epoch 1321/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2971 - val_loss: 8.3687\n",
      "Epoch 1322/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0404 - val_loss: 8.3461\n",
      "Epoch 1323/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1758 - val_loss: 8.3390\n",
      "Epoch 1324/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0898 - val_loss: 8.3187\n",
      "Epoch 1325/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.1540 - val_loss: 8.3279\n",
      "Epoch 1326/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1860 - val_loss: 8.4767\n",
      "Epoch 1327/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3404 - val_loss: 8.2884\n",
      "Epoch 1328/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0713 - val_loss: 8.2629\n",
      "Epoch 1329/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0279 - val_loss: 8.2578\n",
      "Epoch 1330/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3435 - val_loss: 8.3963\n",
      "Epoch 1331/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.5096 - val_loss: 8.2517\n",
      "Epoch 1332/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1311 - val_loss: 8.2658\n",
      "Epoch 1333/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0951 - val_loss: 8.3147\n",
      "Epoch 1334/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2388 - val_loss: 8.4368\n",
      "Epoch 1335/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1144 - val_loss: 8.3811\n",
      "Epoch 1336/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4408 - val_loss: 8.2809\n",
      "Epoch 1337/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0732 - val_loss: 8.5174\n",
      "Epoch 1338/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1577 - val_loss: 8.2751\n",
      "Epoch 1339/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0427 - val_loss: 8.5536\n",
      "Epoch 1340/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3491 - val_loss: 8.3035\n",
      "Epoch 1341/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0961 - val_loss: 8.2607\n",
      "Epoch 1342/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0997 - val_loss: 8.2954\n",
      "Epoch 1343/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.0666 - val_loss: 8.3118\n",
      "Epoch 1344/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.0741 - val_loss: 8.8822\n",
      "Epoch 1345/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4513 - val_loss: 8.2399\n",
      "Epoch 1346/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0533 - val_loss: 8.2530\n",
      "Epoch 1347/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0679 - val_loss: 8.2637\n",
      "Epoch 1348/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0414 - val_loss: 8.2241\n",
      "Epoch 1349/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.0287 - val_loss: 8.2954\n",
      "Epoch 1350/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0969 - val_loss: 8.2613\n",
      "Epoch 1351/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1774 - val_loss: 8.2918\n",
      "Epoch 1352/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9632 - val_loss: 8.2275\n",
      "Epoch 1353/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0232 - val_loss: 8.3549\n",
      "Epoch 1354/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.3733 - val_loss: 9.0334\n",
      "Epoch 1355/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2340 - val_loss: 8.1742\n",
      "Epoch 1356/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9731 - val_loss: 9.2177\n",
      "Epoch 1357/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.5473 - val_loss: 8.2323\n",
      "Epoch 1358/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9956 - val_loss: 8.2356\n",
      "Epoch 1359/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0388 - val_loss: 8.3595\n",
      "Epoch 1360/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2054 - val_loss: 8.3193\n",
      "Epoch 1361/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0898 - val_loss: 8.2540\n",
      "Epoch 1362/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9866 - val_loss: 8.2684\n",
      "Epoch 1363/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1773 - val_loss: 8.2432\n",
      "Epoch 1364/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0528 - val_loss: 8.5034\n",
      "Epoch 1365/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1489 - val_loss: 8.2072\n",
      "Epoch 1366/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.9394 - val_loss: 8.4082\n",
      "Epoch 1367/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.0535 - val_loss: 8.2101\n",
      "Epoch 1368/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 7.2144 - val_loss: 8.1867\n",
      "Epoch 1369/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0244 - val_loss: 8.2573\n",
      "Epoch 1370/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0415 - val_loss: 8.1642\n",
      "Epoch 1371/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0138 - val_loss: 8.3444\n",
      "Epoch 1372/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.0456 - val_loss: 8.1704\n",
      "Epoch 1373/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 7.1423 - val_loss: 8.2072\n",
      "Epoch 1374/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 7.0067 - val_loss: 8.2233\n",
      "Epoch 1375/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9983 - val_loss: 8.1893\n",
      "Epoch 1376/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9770 - val_loss: 8.2643\n",
      "Epoch 1377/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9790 - val_loss: 8.1923\n",
      "Epoch 1378/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1305 - val_loss: 8.1788\n",
      "Epoch 1379/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9565 - val_loss: 8.1851\n",
      "Epoch 1380/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.9501 - val_loss: 8.3248\n",
      "Epoch 1381/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9589 - val_loss: 8.3203\n",
      "Epoch 1382/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9919 - val_loss: 8.2693\n",
      "Epoch 1383/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.1658 - val_loss: 8.1793\n",
      "Epoch 1384/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9955 - val_loss: 8.1484\n",
      "Epoch 1385/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.9565 - val_loss: 8.1883\n",
      "Epoch 1386/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9223 - val_loss: 8.2290\n",
      "Epoch 1387/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0804 - val_loss: 8.1887\n",
      "Epoch 1388/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0721 - val_loss: 8.4206\n",
      "Epoch 1389/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1953 - val_loss: 8.1670\n",
      "Epoch 1390/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9318 - val_loss: 8.2333\n",
      "Epoch 1391/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9123 - val_loss: 8.1471\n",
      "Epoch 1392/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0403 - val_loss: 8.1628\n",
      "Epoch 1393/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1850 - val_loss: 8.1743\n",
      "Epoch 1394/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0194 - val_loss: 8.3052\n",
      "Epoch 1395/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9595 - val_loss: 8.1744\n",
      "Epoch 1396/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9754 - val_loss: 8.1739\n",
      "Epoch 1397/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.5070 - val_loss: 8.1346\n",
      "Epoch 1398/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0360 - val_loss: 8.2097\n",
      "Epoch 1399/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.0046 - val_loss: 8.1555\n",
      "Epoch 1400/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0790 - val_loss: 8.1606\n",
      "Epoch 1401/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9823 - val_loss: 8.1401\n",
      "Epoch 1402/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9090 - val_loss: 8.2240\n",
      "Epoch 1403/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8954 - val_loss: 8.1481\n",
      "Epoch 1404/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.0679 - val_loss: 8.3092\n",
      "Epoch 1405/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0668 - val_loss: 8.2055\n",
      "Epoch 1406/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9468 - val_loss: 8.2718\n",
      "Epoch 1407/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9931 - val_loss: 8.1294\n",
      "Epoch 1408/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8845 - val_loss: 8.1577\n",
      "Epoch 1409/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1507 - val_loss: 8.1838\n",
      "Epoch 1410/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9262 - val_loss: 8.1210\n",
      "Epoch 1411/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.9989 - val_loss: 8.1424\n",
      "Epoch 1412/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2899 - val_loss: 8.7357\n",
      "Epoch 1413/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2374 - val_loss: 8.1002\n",
      "Epoch 1414/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9971 - val_loss: 8.1162\n",
      "Epoch 1415/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9193 - val_loss: 8.1384\n",
      "Epoch 1416/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9752 - val_loss: 8.2763\n",
      "Epoch 1417/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9478 - val_loss: 8.1325\n",
      "Epoch 1418/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9314 - val_loss: 8.1305\n",
      "Epoch 1419/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9426 - val_loss: 8.1167\n",
      "Epoch 1420/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.0076 - val_loss: 8.1623\n",
      "Epoch 1421/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1149 - val_loss: 8.6533\n",
      "Epoch 1422/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1202 - val_loss: 8.1997\n",
      "Epoch 1423/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9746 - val_loss: 8.4169\n",
      "Epoch 1424/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0398 - val_loss: 8.1841\n",
      "Epoch 1425/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9522 - val_loss: 8.1355\n",
      "Epoch 1426/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1402 - val_loss: 8.1164\n",
      "Epoch 1427/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0271 - val_loss: 8.1385\n",
      "Epoch 1428/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1390 - val_loss: 8.1289\n",
      "Epoch 1429/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8860 - val_loss: 8.2699\n",
      "Epoch 1430/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.9551 - val_loss: 8.2095\n",
      "Epoch 1431/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9540 - val_loss: 8.1181\n",
      "Epoch 1432/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.1302 - val_loss: 8.1136\n",
      "Epoch 1433/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9087 - val_loss: 8.1221\n",
      "Epoch 1434/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9004 - val_loss: 8.0909\n",
      "Epoch 1435/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8798 - val_loss: 8.0868\n",
      "Epoch 1436/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.8675 - val_loss: 8.1112\n",
      "Epoch 1437/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.9536 - val_loss: 8.1072\n",
      "Epoch 1438/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.9055 - val_loss: 8.4330\n",
      "Epoch 1439/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9601 - val_loss: 8.1546\n",
      "Epoch 1440/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0362 - val_loss: 8.6963\n",
      "Epoch 1441/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9696 - val_loss: 8.1975\n",
      "Epoch 1442/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9561 - val_loss: 8.3601\n",
      "Epoch 1443/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0394 - val_loss: 8.1457\n",
      "Epoch 1444/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9089 - val_loss: 8.1141\n",
      "Epoch 1445/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8509 - val_loss: 8.0970\n",
      "Epoch 1446/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8704 - val_loss: 8.0687\n",
      "Epoch 1447/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9942 - val_loss: 8.1595\n",
      "Epoch 1448/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8576 - val_loss: 8.1258\n",
      "Epoch 1449/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 49us/step - loss: 7.0709 - val_loss: 8.1234\n",
      "Epoch 1450/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.8766 - val_loss: 8.1520\n",
      "Epoch 1451/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.0064 - val_loss: 8.1687\n",
      "Epoch 1452/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9011 - val_loss: 8.0883\n",
      "Epoch 1453/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8393 - val_loss: 8.1439\n",
      "Epoch 1454/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8708 - val_loss: 8.0719\n",
      "Epoch 1455/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8245 - val_loss: 8.1828\n",
      "Epoch 1456/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9033 - val_loss: 8.0871\n",
      "Epoch 1457/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9233 - val_loss: 8.1901\n",
      "Epoch 1458/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.9024 - val_loss: 8.0840\n",
      "Epoch 1459/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9237 - val_loss: 8.0599\n",
      "Epoch 1460/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9077 - val_loss: 8.3314\n",
      "Epoch 1461/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.0207 - val_loss: 8.0986\n",
      "Epoch 1462/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8253 - val_loss: 8.1110\n",
      "Epoch 1463/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.0174 - val_loss: 8.3880\n",
      "Epoch 1464/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.1106 - val_loss: 8.2382\n",
      "Epoch 1465/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0128 - val_loss: 8.0711\n",
      "Epoch 1466/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8508 - val_loss: 8.0968\n",
      "Epoch 1467/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2398 - val_loss: 8.0374\n",
      "Epoch 1468/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9469 - val_loss: 8.0989\n",
      "Epoch 1469/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.2001 - val_loss: 8.4224\n",
      "Epoch 1470/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0230 - val_loss: 8.0498\n",
      "Epoch 1471/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8451 - val_loss: 8.1101\n",
      "Epoch 1472/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4265 - val_loss: 8.2901\n",
      "Epoch 1473/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9372 - val_loss: 8.1344\n",
      "Epoch 1474/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0067 - val_loss: 8.0451\n",
      "Epoch 1475/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8930 - val_loss: 8.0360\n",
      "Epoch 1476/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8839 - val_loss: 8.0195\n",
      "Epoch 1477/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7950 - val_loss: 8.0228\n",
      "Epoch 1478/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8393 - val_loss: 8.2366\n",
      "Epoch 1479/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8989 - val_loss: 8.0601\n",
      "Epoch 1480/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7925 - val_loss: 8.0400\n",
      "Epoch 1481/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7688 - val_loss: 8.5705\n",
      "Epoch 1482/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.4589 - val_loss: 8.0874\n",
      "Epoch 1483/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.3520 - val_loss: 8.4797\n",
      "Epoch 1484/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.0092 - val_loss: 8.1386\n",
      "Epoch 1485/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8562 - val_loss: 8.0196\n",
      "Epoch 1486/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9839 - val_loss: 7.9889\n",
      "Epoch 1487/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8128 - val_loss: 8.0580\n",
      "Epoch 1488/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7512 - val_loss: 8.9268\n",
      "Epoch 1489/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.6716 - val_loss: 8.1292\n",
      "Epoch 1490/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.1374 - val_loss: 8.0132\n",
      "Epoch 1491/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 7.4618 - val_loss: 8.1990\n",
      "Epoch 1492/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0234 - val_loss: 8.2693\n",
      "Epoch 1493/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9432 - val_loss: 8.0825\n",
      "Epoch 1494/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.7242 - val_loss: 8.6139\n",
      "Epoch 1495/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9973 - val_loss: 8.2070\n",
      "Epoch 1496/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.0383 - val_loss: 8.0118\n",
      "Epoch 1497/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8114 - val_loss: 8.0227\n",
      "Epoch 1498/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8025 - val_loss: 8.0787\n",
      "Epoch 1499/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.8955 - val_loss: 8.0549\n",
      "Epoch 1500/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7610 - val_loss: 8.0185\n",
      "Epoch 1501/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9656 - val_loss: 8.5789\n",
      "Epoch 1502/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9269 - val_loss: 8.2463\n",
      "Epoch 1503/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8906 - val_loss: 8.0966\n",
      "Epoch 1504/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.8836 - val_loss: 7.9716\n",
      "Epoch 1505/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.8057 - val_loss: 8.3750\n",
      "Epoch 1506/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8558 - val_loss: 7.9827\n",
      "Epoch 1507/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.2335 - val_loss: 8.2604\n",
      "Epoch 1508/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8844 - val_loss: 8.6571\n",
      "Epoch 1509/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2196 - val_loss: 8.3159\n",
      "Epoch 1510/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 6.8337 - val_loss: 8.4542\n",
      "Epoch 1511/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.6325 - val_loss: 8.3554\n",
      "Epoch 1512/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.0259 - val_loss: 8.6497\n",
      "Epoch 1513/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.2870 - val_loss: 8.0523\n",
      "Epoch 1514/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 7.0234 - val_loss: 8.0554\n",
      "Epoch 1515/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 7.0268 - val_loss: 8.4159\n",
      "Epoch 1516/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.0162 - val_loss: 7.9747\n",
      "Epoch 1517/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.8451 - val_loss: 8.3916\n",
      "Epoch 1518/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9516 - val_loss: 8.6620\n",
      "Epoch 1519/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9255 - val_loss: 7.9804\n",
      "Epoch 1520/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7683 - val_loss: 7.9839\n",
      "Epoch 1521/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6926 - val_loss: 8.0536\n",
      "Epoch 1522/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8548 - val_loss: 8.0092\n",
      "Epoch 1523/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.7548 - val_loss: 8.0560\n",
      "Epoch 1524/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7181 - val_loss: 7.9846\n",
      "Epoch 1525/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 6.7364 - val_loss: 7.9429\n",
      "Epoch 1526/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7242 - val_loss: 7.9648\n",
      "Epoch 1527/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.8249 - val_loss: 8.4208\n",
      "Epoch 1528/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8521 - val_loss: 7.9676\n",
      "Epoch 1529/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7636 - val_loss: 7.9428\n",
      "Epoch 1530/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0147 - val_loss: 8.1306\n",
      "Epoch 1531/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8259 - val_loss: 7.9959\n",
      "Epoch 1532/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8221 - val_loss: 8.2673\n",
      "Epoch 1533/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9122 - val_loss: 7.9480\n",
      "Epoch 1534/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7512 - val_loss: 7.9647\n",
      "Epoch 1535/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7056 - val_loss: 8.0201\n",
      "Epoch 1536/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8192 - val_loss: 8.1384\n",
      "Epoch 1537/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.7891 - val_loss: 8.0558\n",
      "Epoch 1538/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0986 - val_loss: 8.1844\n",
      "Epoch 1539/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8061 - val_loss: 7.9669\n",
      "Epoch 1540/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7429 - val_loss: 7.9482\n",
      "Epoch 1541/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7685 - val_loss: 7.9882\n",
      "Epoch 1542/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.8232 - val_loss: 7.9893\n",
      "Epoch 1543/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.9133 - val_loss: 7.9528\n",
      "Epoch 1544/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8726 - val_loss: 8.1264\n",
      "Epoch 1545/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7468 - val_loss: 8.0514\n",
      "Epoch 1546/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0286 - val_loss: 8.0140\n",
      "Epoch 1547/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7450 - val_loss: 7.9570\n",
      "Epoch 1548/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7425 - val_loss: 7.9516\n",
      "Epoch 1549/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6997 - val_loss: 8.1079\n",
      "Epoch 1550/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8258 - val_loss: 8.0201\n",
      "Epoch 1551/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9468 - val_loss: 8.0745\n",
      "Epoch 1552/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7789 - val_loss: 8.0157\n",
      "Epoch 1553/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7913 - val_loss: 7.9359\n",
      "Epoch 1554/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6946 - val_loss: 8.0006\n",
      "Epoch 1555/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7300 - val_loss: 7.9334\n",
      "Epoch 1556/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8748 - val_loss: 7.9665\n",
      "Epoch 1557/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8805 - val_loss: 8.1060\n",
      "Epoch 1558/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7756 - val_loss: 7.9430\n",
      "Epoch 1559/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7939 - val_loss: 7.9049\n",
      "Epoch 1560/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7242 - val_loss: 7.9891\n",
      "Epoch 1561/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7045 - val_loss: 8.0752\n",
      "Epoch 1562/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.6491 - val_loss: 8.2914\n",
      "Epoch 1563/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8970 - val_loss: 8.3055\n",
      "Epoch 1564/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8676 - val_loss: 7.9114\n",
      "Epoch 1565/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.7511 - val_loss: 7.9377\n",
      "Epoch 1566/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7380 - val_loss: 8.1922\n",
      "Epoch 1567/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0115 - val_loss: 8.4799\n",
      "Epoch 1568/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.6824 - val_loss: 8.0279\n",
      "Epoch 1569/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7877 - val_loss: 7.9456\n",
      "Epoch 1570/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7655 - val_loss: 7.9238\n",
      "Epoch 1571/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9232 - val_loss: 7.9645\n",
      "Epoch 1572/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.7081 - val_loss: 7.8781\n",
      "Epoch 1573/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.7807 - val_loss: 7.9318\n",
      "Epoch 1574/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6484 - val_loss: 7.8982\n",
      "Epoch 1575/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9563 - val_loss: 8.3590\n",
      "Epoch 1576/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.2106 - val_loss: 8.1338\n",
      "Epoch 1577/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6925 - val_loss: 7.8951\n",
      "Epoch 1578/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8894 - val_loss: 7.8798\n",
      "Epoch 1579/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7494 - val_loss: 7.9356\n",
      "Epoch 1580/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 7.2313 - val_loss: 8.5099\n",
      "Epoch 1581/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.8189 - val_loss: 8.1554\n",
      "Epoch 1582/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8850 - val_loss: 7.8357\n",
      "Epoch 1583/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6556 - val_loss: 7.9567\n",
      "Epoch 1584/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6553 - val_loss: 7.9494\n",
      "Epoch 1585/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8224 - val_loss: 7.8621\n",
      "Epoch 1586/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.7567 - val_loss: 7.8654\n",
      "Epoch 1587/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0442 - val_loss: 7.9902\n",
      "Epoch 1588/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7309 - val_loss: 7.8989\n",
      "Epoch 1589/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6990 - val_loss: 8.0032\n",
      "Epoch 1590/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0343 - val_loss: 8.0222\n",
      "Epoch 1591/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.9397 - val_loss: 7.8999\n",
      "Epoch 1592/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.7245 - val_loss: 7.9381\n",
      "Epoch 1593/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7688 - val_loss: 7.9716\n",
      "Epoch 1594/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6981 - val_loss: 7.8797\n",
      "Epoch 1595/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.6360 - val_loss: 7.8633\n",
      "Epoch 1596/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7042 - val_loss: 7.9227\n",
      "Epoch 1597/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7504 - val_loss: 7.8725\n",
      "Epoch 1598/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9087 - val_loss: 7.8938\n",
      "Epoch 1599/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5960 - val_loss: 8.4257\n",
      "Epoch 1600/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.1745 - val_loss: 7.8203\n",
      "Epoch 1601/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 6.6612 - val_loss: 7.8739\n",
      "Epoch 1602/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6960 - val_loss: 7.8199\n",
      "Epoch 1603/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7508 - val_loss: 7.8688\n",
      "Epoch 1604/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7590 - val_loss: 7.8826\n",
      "Epoch 1605/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6612 - val_loss: 7.8237\n",
      "Epoch 1606/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7025 - val_loss: 8.0837\n",
      "Epoch 1607/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.8975 - val_loss: 7.9372\n",
      "Epoch 1608/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6823 - val_loss: 7.7981\n",
      "Epoch 1609/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6895 - val_loss: 7.8127\n",
      "Epoch 1610/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6527 - val_loss: 7.9162\n",
      "Epoch 1611/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.7826 - val_loss: 7.8659\n",
      "Epoch 1612/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7835 - val_loss: 7.9279\n",
      "Epoch 1613/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.6326 - val_loss: 8.2655\n",
      "Epoch 1614/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7941 - val_loss: 7.8398\n",
      "Epoch 1615/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.7728 - val_loss: 8.2982\n",
      "Epoch 1616/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0760 - val_loss: 7.9042\n",
      "Epoch 1617/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7070 - val_loss: 7.9144\n",
      "Epoch 1618/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6249 - val_loss: 7.8506\n",
      "Epoch 1619/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6710 - val_loss: 8.3394\n",
      "Epoch 1620/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9853 - val_loss: 7.9218\n",
      "Epoch 1621/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.8174 - val_loss: 7.9031\n",
      "Epoch 1622/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5923 - val_loss: 7.7940\n",
      "Epoch 1623/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8061 - val_loss: 8.3844\n",
      "Epoch 1624/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8335 - val_loss: 7.8284\n",
      "Epoch 1625/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7366 - val_loss: 8.0009\n",
      "Epoch 1626/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6702 - val_loss: 7.8627\n",
      "Epoch 1627/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7101 - val_loss: 7.7973\n",
      "Epoch 1628/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6676 - val_loss: 7.8457\n",
      "Epoch 1629/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6411 - val_loss: 7.9530\n",
      "Epoch 1630/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6658 - val_loss: 7.9072\n",
      "Epoch 1631/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7047 - val_loss: 7.7926\n",
      "Epoch 1632/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8603 - val_loss: 7.8943\n",
      "Epoch 1633/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6346 - val_loss: 8.0472\n",
      "Epoch 1634/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6979 - val_loss: 7.9837\n",
      "Epoch 1635/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6930 - val_loss: 7.9392\n",
      "Epoch 1636/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6305 - val_loss: 7.8706\n",
      "Epoch 1637/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6978 - val_loss: 7.9964\n",
      "Epoch 1638/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.6512 - val_loss: 7.9256\n",
      "Epoch 1639/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6086 - val_loss: 7.8315\n",
      "Epoch 1640/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7040 - val_loss: 8.1182\n",
      "Epoch 1641/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6974 - val_loss: 7.8658\n",
      "Epoch 1642/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8256 - val_loss: 8.1898\n",
      "Epoch 1643/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6610 - val_loss: 7.8505\n",
      "Epoch 1644/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.6397 - val_loss: 7.9926\n",
      "Epoch 1645/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5964 - val_loss: 7.8138\n",
      "Epoch 1646/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6275 - val_loss: 8.2601\n",
      "Epoch 1647/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8622 - val_loss: 7.7886\n",
      "Epoch 1648/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5950 - val_loss: 7.8987\n",
      "Epoch 1649/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.6296 - val_loss: 7.8545\n",
      "Epoch 1650/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7346 - val_loss: 7.8390\n",
      "Epoch 1651/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6474 - val_loss: 7.8128\n",
      "Epoch 1652/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.6157 - val_loss: 7.8303\n",
      "Epoch 1653/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8248 - val_loss: 7.7787\n",
      "Epoch 1654/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5964 - val_loss: 7.7985\n",
      "Epoch 1655/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6250 - val_loss: 7.7839\n",
      "Epoch 1656/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5993 - val_loss: 7.7992\n",
      "Epoch 1657/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.6950 - val_loss: 7.7678\n",
      "Epoch 1658/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5708 - val_loss: 7.7622\n",
      "Epoch 1659/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5669 - val_loss: 7.7865\n",
      "Epoch 1660/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5792 - val_loss: 7.7667\n",
      "Epoch 1661/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.6720 - val_loss: 7.7543\n",
      "Epoch 1662/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5470 - val_loss: 7.8779\n",
      "Epoch 1663/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5808 - val_loss: 7.7894\n",
      "Epoch 1664/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.9466 - val_loss: 7.7653\n",
      "Epoch 1665/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8521 - val_loss: 8.4216\n",
      "Epoch 1666/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.9278 - val_loss: 8.2406\n",
      "Epoch 1667/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6888 - val_loss: 7.8900\n",
      "Epoch 1668/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.8910 - val_loss: 7.8305\n",
      "Epoch 1669/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6917 - val_loss: 7.8507\n",
      "Epoch 1670/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5883 - val_loss: 7.7947\n",
      "Epoch 1671/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5475 - val_loss: 7.7701\n",
      "Epoch 1672/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5785 - val_loss: 7.7483\n",
      "Epoch 1673/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6595 - val_loss: 7.9167\n",
      "Epoch 1674/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5552 - val_loss: 7.7558\n",
      "Epoch 1675/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5988 - val_loss: 7.8260\n",
      "Epoch 1676/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8250 - val_loss: 7.7451\n",
      "Epoch 1677/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 6.9154 - val_loss: 7.8559\n",
      "Epoch 1678/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 7.0638 - val_loss: 8.0395\n",
      "Epoch 1679/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6022 - val_loss: 7.7745\n",
      "Epoch 1680/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7005 - val_loss: 7.7497\n",
      "Epoch 1681/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7131 - val_loss: 7.8425\n",
      "Epoch 1682/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5552 - val_loss: 7.7672\n",
      "Epoch 1683/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6076 - val_loss: 7.8308\n",
      "Epoch 1684/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5698 - val_loss: 7.7461\n",
      "Epoch 1685/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6408 - val_loss: 7.8674\n",
      "Epoch 1686/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5992 - val_loss: 7.9044\n",
      "Epoch 1687/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6184 - val_loss: 7.8194\n",
      "Epoch 1688/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6184 - val_loss: 7.7676\n",
      "Epoch 1689/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5773 - val_loss: 7.7473\n",
      "Epoch 1690/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5186 - val_loss: 7.9442\n",
      "Epoch 1691/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5708 - val_loss: 7.7449\n",
      "Epoch 1692/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6846 - val_loss: 7.9840\n",
      "Epoch 1693/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7133 - val_loss: 7.7860\n",
      "Epoch 1694/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7537 - val_loss: 7.6889\n",
      "Epoch 1695/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5479 - val_loss: 8.0124\n",
      "Epoch 1696/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7287 - val_loss: 7.7665\n",
      "Epoch 1697/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6172 - val_loss: 7.9686\n",
      "Epoch 1698/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6231 - val_loss: 7.7299\n",
      "Epoch 1699/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6326 - val_loss: 7.7552\n",
      "Epoch 1700/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5642 - val_loss: 7.7236\n",
      "Epoch 1701/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5800 - val_loss: 7.7373\n",
      "Epoch 1702/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6120 - val_loss: 7.8374\n",
      "Epoch 1703/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.7322 - val_loss: 7.7648\n",
      "Epoch 1704/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5879 - val_loss: 8.2927\n",
      "Epoch 1705/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.0117 - val_loss: 7.7952\n",
      "Epoch 1706/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6481 - val_loss: 7.7353\n",
      "Epoch 1707/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5925 - val_loss: 7.7818\n",
      "Epoch 1708/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7493 - val_loss: 7.7548\n",
      "Epoch 1709/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7319 - val_loss: 7.7035\n",
      "Epoch 1710/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6219 - val_loss: 7.7385\n",
      "Epoch 1711/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.8899 - val_loss: 7.7675\n",
      "Epoch 1712/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4710 - val_loss: 7.7319\n",
      "Epoch 1713/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4575 - val_loss: 8.1501\n",
      "Epoch 1714/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6525 - val_loss: 7.6949\n",
      "Epoch 1715/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4896 - val_loss: 8.2533\n",
      "Epoch 1716/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.9340 - val_loss: 7.7293\n",
      "Epoch 1717/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4959 - val_loss: 7.7112\n",
      "Epoch 1718/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5443 - val_loss: 7.6746\n",
      "Epoch 1719/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4803 - val_loss: 7.6822\n",
      "Epoch 1720/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6028 - val_loss: 7.6916\n",
      "Epoch 1721/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4431 - val_loss: 8.0509\n",
      "Epoch 1722/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6834 - val_loss: 7.7488\n",
      "Epoch 1723/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6954 - val_loss: 7.9112\n",
      "Epoch 1724/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7921 - val_loss: 8.2210\n",
      "Epoch 1725/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7281 - val_loss: 7.7039\n",
      "Epoch 1726/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4981 - val_loss: 7.8663\n",
      "Epoch 1727/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5790 - val_loss: 7.7241\n",
      "Epoch 1728/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6799 - val_loss: 7.7073\n",
      "Epoch 1729/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6529 - val_loss: 7.6774\n",
      "Epoch 1730/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5840 - val_loss: 8.3147\n",
      "Epoch 1731/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7197 - val_loss: 7.7939\n",
      "Epoch 1732/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5504 - val_loss: 7.7371\n",
      "Epoch 1733/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.6379 - val_loss: 7.7038\n",
      "Epoch 1734/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4924 - val_loss: 8.2610\n",
      "Epoch 1735/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.8354 - val_loss: 7.7057\n",
      "Epoch 1736/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4967 - val_loss: 7.7722\n",
      "Epoch 1737/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4698 - val_loss: 7.6943\n",
      "Epoch 1738/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5405 - val_loss: 7.8443\n",
      "Epoch 1739/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.6945 - val_loss: 7.8586\n",
      "Epoch 1740/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4698 - val_loss: 7.7779\n",
      "Epoch 1741/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.6562 - val_loss: 7.6968\n",
      "Epoch 1742/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4881 - val_loss: 7.7805\n",
      "Epoch 1743/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4898 - val_loss: 7.9768\n",
      "Epoch 1744/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.6706 - val_loss: 7.6396\n",
      "Epoch 1745/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5164 - val_loss: 7.6645\n",
      "Epoch 1746/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7320 - val_loss: 7.8865\n",
      "Epoch 1747/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7859 - val_loss: 7.6700\n",
      "Epoch 1748/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 6.5278 - val_loss: 7.6425\n",
      "Epoch 1749/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5756 - val_loss: 7.6466\n",
      "Epoch 1750/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 7.0856 - val_loss: 7.6747\n",
      "Epoch 1751/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.7898 - val_loss: 7.7388\n",
      "Epoch 1752/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5796 - val_loss: 7.6018\n",
      "Epoch 1753/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 6.5993 - val_loss: 7.6421\n",
      "Epoch 1754/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.4393 - val_loss: 7.6558\n",
      "Epoch 1755/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4827 - val_loss: 7.7695\n",
      "Epoch 1756/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4933 - val_loss: 7.9453\n",
      "Epoch 1757/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7049 - val_loss: 7.6647\n",
      "Epoch 1758/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4589 - val_loss: 7.6857\n",
      "Epoch 1759/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7607 - val_loss: 7.7137\n",
      "Epoch 1760/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6952 - val_loss: 7.6924\n",
      "Epoch 1761/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4215 - val_loss: 7.7911\n",
      "Epoch 1762/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5813 - val_loss: 7.6706\n",
      "Epoch 1763/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5282 - val_loss: 7.6941\n",
      "Epoch 1764/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4751 - val_loss: 7.6663\n",
      "Epoch 1765/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4960 - val_loss: 7.6191\n",
      "Epoch 1766/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5118 - val_loss: 7.6988\n",
      "Epoch 1767/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6054 - val_loss: 7.7258\n",
      "Epoch 1768/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6055 - val_loss: 7.6614\n",
      "Epoch 1769/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5592 - val_loss: 7.6845\n",
      "Epoch 1770/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5816 - val_loss: 7.9351\n",
      "Epoch 1771/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6448 - val_loss: 7.6437\n",
      "Epoch 1772/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5071 - val_loss: 7.8223\n",
      "Epoch 1773/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4705 - val_loss: 7.6662\n",
      "Epoch 1774/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.5125 - val_loss: 7.6946\n",
      "Epoch 1775/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.5002 - val_loss: 7.6110\n",
      "Epoch 1776/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6021 - val_loss: 7.6449\n",
      "Epoch 1777/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5340 - val_loss: 7.6484\n",
      "Epoch 1778/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4302 - val_loss: 7.8765\n",
      "Epoch 1779/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.6654 - val_loss: 7.7140\n",
      "Epoch 1780/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5601 - val_loss: 7.6096\n",
      "Epoch 1781/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6082 - val_loss: 7.7134\n",
      "Epoch 1782/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.0095 - val_loss: 7.7653\n",
      "Epoch 1783/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4091 - val_loss: 7.6245\n",
      "Epoch 1784/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4138 - val_loss: 7.6849\n",
      "Epoch 1785/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4288 - val_loss: 7.7202\n",
      "Epoch 1786/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5050 - val_loss: 7.7224\n",
      "Epoch 1787/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5722 - val_loss: 8.3366\n",
      "Epoch 1788/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.7728 - val_loss: 7.6040\n",
      "Epoch 1789/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6270 - val_loss: 7.6565\n",
      "Epoch 1790/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5466 - val_loss: 7.7034\n",
      "Epoch 1791/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4676 - val_loss: 7.5939\n",
      "Epoch 1792/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4036 - val_loss: 7.6163\n",
      "Epoch 1793/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5250 - val_loss: 8.1473\n",
      "Epoch 1794/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7025 - val_loss: 7.7737\n",
      "Epoch 1795/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 7.1348 - val_loss: 7.5732\n",
      "Epoch 1796/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4009 - val_loss: 7.6145\n",
      "Epoch 1797/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4420 - val_loss: 7.6001\n",
      "Epoch 1798/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4287 - val_loss: 7.6993\n",
      "Epoch 1799/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4014 - val_loss: 7.5955\n",
      "Epoch 1800/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4835 - val_loss: 7.6222\n",
      "Epoch 1801/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4764 - val_loss: 7.6323\n",
      "Epoch 1802/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4335 - val_loss: 8.0269\n",
      "Epoch 1803/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.6164 - val_loss: 7.6170\n",
      "Epoch 1804/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4491 - val_loss: 7.6144\n",
      "Epoch 1805/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3937 - val_loss: 7.5973\n",
      "Epoch 1806/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4114 - val_loss: 7.5945\n",
      "Epoch 1807/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.4396 - val_loss: 7.5695\n",
      "Epoch 1808/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4432 - val_loss: 7.6035\n",
      "Epoch 1809/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4156 - val_loss: 7.7467\n",
      "Epoch 1810/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5288 - val_loss: 7.6426\n",
      "Epoch 1811/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5587 - val_loss: 7.6313\n",
      "Epoch 1812/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5047 - val_loss: 7.6147\n",
      "Epoch 1813/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4159 - val_loss: 7.6001\n",
      "Epoch 1814/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5179 - val_loss: 7.6803\n",
      "Epoch 1815/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.4118 - val_loss: 7.6047\n",
      "Epoch 1816/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4981 - val_loss: 7.5769\n",
      "Epoch 1817/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.4359 - val_loss: 7.5412\n",
      "Epoch 1818/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4137 - val_loss: 7.6289\n",
      "Epoch 1819/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4064 - val_loss: 7.6083\n",
      "Epoch 1820/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.3832 - val_loss: 7.5852\n",
      "Epoch 1821/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4273 - val_loss: 7.5693\n",
      "Epoch 1822/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6059 - val_loss: 7.6140\n",
      "Epoch 1823/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3343 - val_loss: 7.6080\n",
      "Epoch 1824/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8741 - val_loss: 7.5637\n",
      "Epoch 1825/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5008 - val_loss: 7.7723\n",
      "Epoch 1826/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.3892 - val_loss: 7.5662\n",
      "Epoch 1827/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3661 - val_loss: 7.5451\n",
      "Epoch 1828/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4036 - val_loss: 7.6027\n",
      "Epoch 1829/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 6.6887 - val_loss: 8.1002\n",
      "Epoch 1830/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5655 - val_loss: 7.7634\n",
      "Epoch 1831/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5710 - val_loss: 7.7243\n",
      "Epoch 1832/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5316 - val_loss: 7.6103\n",
      "Epoch 1833/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5687 - val_loss: 7.9249\n",
      "Epoch 1834/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4804 - val_loss: 7.7523\n",
      "Epoch 1835/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5581 - val_loss: 7.5272\n",
      "Epoch 1836/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4118 - val_loss: 8.1324\n",
      "Epoch 1837/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 7.3814 - val_loss: 7.7040\n",
      "Epoch 1838/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7675 - val_loss: 7.5949\n",
      "Epoch 1839/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3695 - val_loss: 7.5587\n",
      "Epoch 1840/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5192 - val_loss: 7.6409\n",
      "Epoch 1841/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4080 - val_loss: 7.6044\n",
      "Epoch 1842/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.4602 - val_loss: 8.4373\n",
      "Epoch 1843/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5971 - val_loss: 7.5817\n",
      "Epoch 1844/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5043 - val_loss: 7.5424\n",
      "Epoch 1845/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3497 - val_loss: 7.5535\n",
      "Epoch 1846/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3585 - val_loss: 7.5646\n",
      "Epoch 1847/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.5022 - val_loss: 7.5771\n",
      "Epoch 1848/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3593 - val_loss: 7.5550\n",
      "Epoch 1849/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.3318 - val_loss: 7.4840\n",
      "Epoch 1850/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3553 - val_loss: 7.8709\n",
      "Epoch 1851/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7903 - val_loss: 7.5022\n",
      "Epoch 1852/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3508 - val_loss: 7.5374\n",
      "Epoch 1853/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4690 - val_loss: 7.5061\n",
      "Epoch 1854/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5628 - val_loss: 7.5487\n",
      "Epoch 1855/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4172 - val_loss: 7.4908\n",
      "Epoch 1856/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.9144 - val_loss: 7.5294\n",
      "Epoch 1857/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7870 - val_loss: 8.3609\n",
      "Epoch 1858/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4911 - val_loss: 7.6239\n",
      "Epoch 1859/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6082 - val_loss: 7.5565\n",
      "Epoch 1860/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3464 - val_loss: 7.6333\n",
      "Epoch 1861/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4459 - val_loss: 7.5589\n",
      "Epoch 1862/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6015 - val_loss: 7.5605\n",
      "Epoch 1863/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3756 - val_loss: 7.5421\n",
      "Epoch 1864/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3663 - val_loss: 7.5351\n",
      "Epoch 1865/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3166 - val_loss: 7.6640\n",
      "Epoch 1866/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4488 - val_loss: 7.6240\n",
      "Epoch 1867/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 6.6106 - val_loss: 7.8682\n",
      "Epoch 1868/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5507 - val_loss: 7.5507\n",
      "Epoch 1869/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3149 - val_loss: 7.7563\n",
      "Epoch 1870/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6349 - val_loss: 7.6109\n",
      "Epoch 1871/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.3200 - val_loss: 7.4886\n",
      "Epoch 1872/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3306 - val_loss: 7.4967\n",
      "Epoch 1873/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5600 - val_loss: 7.5635\n",
      "Epoch 1874/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.4106 - val_loss: 7.8268\n",
      "Epoch 1875/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4055 - val_loss: 7.5105\n",
      "Epoch 1876/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4734 - val_loss: 7.5139\n",
      "Epoch 1877/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5149 - val_loss: 7.9364\n",
      "Epoch 1878/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.7352 - val_loss: 7.5440\n",
      "Epoch 1879/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4487 - val_loss: 7.4996\n",
      "Epoch 1880/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3013 - val_loss: 7.5211\n",
      "Epoch 1881/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5382 - val_loss: 7.4918\n",
      "Epoch 1882/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3423 - val_loss: 7.5054\n",
      "Epoch 1883/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3050 - val_loss: 7.9945\n",
      "Epoch 1884/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4704 - val_loss: 7.4947\n",
      "Epoch 1885/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4139 - val_loss: 7.4571\n",
      "Epoch 1886/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3552 - val_loss: 7.8344\n",
      "Epoch 1887/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6574 - val_loss: 7.6006\n",
      "Epoch 1888/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3800 - val_loss: 7.4598\n",
      "Epoch 1889/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2960 - val_loss: 7.5884\n",
      "Epoch 1890/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3397 - val_loss: 7.5023\n",
      "Epoch 1891/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3211 - val_loss: 7.5504\n",
      "Epoch 1892/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3137 - val_loss: 7.6547\n",
      "Epoch 1893/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3345 - val_loss: 8.1412\n",
      "Epoch 1894/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.5184 - val_loss: 7.4745\n",
      "Epoch 1895/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2857 - val_loss: 7.5377\n",
      "Epoch 1896/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5849 - val_loss: 7.4835\n",
      "Epoch 1897/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4338 - val_loss: 7.5351\n",
      "Epoch 1898/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3617 - val_loss: 7.5084\n",
      "Epoch 1899/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3003 - val_loss: 7.5103\n",
      "Epoch 1900/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3445 - val_loss: 7.4611\n",
      "Epoch 1901/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3813 - val_loss: 7.4894\n",
      "Epoch 1902/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.3058 - val_loss: 7.5201\n",
      "Epoch 1903/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3267 - val_loss: 7.4449\n",
      "Epoch 1904/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.4042 - val_loss: 7.5796\n",
      "Epoch 1905/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 7.1266 - val_loss: 7.5107\n",
      "Epoch 1906/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6988 - val_loss: 7.5195\n",
      "Epoch 1907/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2807 - val_loss: 8.1023\n",
      "Epoch 1908/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.6229 - val_loss: 7.4584\n",
      "Epoch 1909/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3018 - val_loss: 7.4327\n",
      "Epoch 1910/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3180 - val_loss: 7.5160\n",
      "Epoch 1911/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2929 - val_loss: 7.4618\n",
      "Epoch 1912/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2755 - val_loss: 7.4516\n",
      "Epoch 1913/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4076 - val_loss: 7.5052\n",
      "Epoch 1914/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2668 - val_loss: 7.5837\n",
      "Epoch 1915/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2500 - val_loss: 7.6356\n",
      "Epoch 1916/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5046 - val_loss: 7.4369\n",
      "Epoch 1917/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2497 - val_loss: 7.4906\n",
      "Epoch 1918/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3208 - val_loss: 7.6175\n",
      "Epoch 1919/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3165 - val_loss: 7.5460\n",
      "Epoch 1920/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2916 - val_loss: 7.4898\n",
      "Epoch 1921/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2762 - val_loss: 7.4694\n",
      "Epoch 1922/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2919 - val_loss: 7.4919\n",
      "Epoch 1923/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2928 - val_loss: 7.4885\n",
      "Epoch 1924/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2760 - val_loss: 7.5290\n",
      "Epoch 1925/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3338 - val_loss: 7.5894\n",
      "Epoch 1926/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2991 - val_loss: 7.4840\n",
      "Epoch 1927/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2437 - val_loss: 7.4338\n",
      "Epoch 1928/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2748 - val_loss: 7.6338\n",
      "Epoch 1929/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3568 - val_loss: 7.4662\n",
      "Epoch 1930/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4978 - val_loss: 8.2311\n",
      "Epoch 1931/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.5265 - val_loss: 7.4254\n",
      "Epoch 1932/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2586 - val_loss: 7.6948\n",
      "Epoch 1933/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.8348 - val_loss: 7.5586\n",
      "Epoch 1934/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3292 - val_loss: 7.6644\n",
      "Epoch 1935/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4003 - val_loss: 7.5505\n",
      "Epoch 1936/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2795 - val_loss: 7.4592\n",
      "Epoch 1937/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.4017 - val_loss: 7.4346\n",
      "Epoch 1938/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3173 - val_loss: 7.4768\n",
      "Epoch 1939/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3040 - val_loss: 7.5491\n",
      "Epoch 1940/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4334 - val_loss: 8.1728\n",
      "Epoch 1941/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.8345 - val_loss: 7.5158\n",
      "Epoch 1942/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5298 - val_loss: 7.4219\n",
      "Epoch 1943/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3953 - val_loss: 7.7419\n",
      "Epoch 1944/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2800 - val_loss: 7.4678\n",
      "Epoch 1945/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3824 - val_loss: 7.4595\n",
      "Epoch 1946/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.5274 - val_loss: 7.8266\n",
      "Epoch 1947/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5105 - val_loss: 7.5407\n",
      "Epoch 1948/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6007 - val_loss: 8.0448\n",
      "Epoch 1949/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4309 - val_loss: 7.4001\n",
      "Epoch 1950/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3934 - val_loss: 7.4814\n",
      "Epoch 1951/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2916 - val_loss: 7.3998\n",
      "Epoch 1952/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5602 - val_loss: 7.4478\n",
      "Epoch 1953/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2773 - val_loss: 7.5173\n",
      "Epoch 1954/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2883 - val_loss: 7.4282\n",
      "Epoch 1955/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3848 - val_loss: 7.3693\n",
      "Epoch 1956/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3511 - val_loss: 7.4161\n",
      "Epoch 1957/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3114 - val_loss: 7.4351\n",
      "Epoch 1958/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2687 - val_loss: 8.1222\n",
      "Epoch 1959/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.5291 - val_loss: 7.5258\n",
      "Epoch 1960/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2712 - val_loss: 7.4605\n",
      "Epoch 1961/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2218 - val_loss: 7.5819\n",
      "Epoch 1962/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3659 - val_loss: 7.3921\n",
      "Epoch 1963/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2239 - val_loss: 7.6728\n",
      "Epoch 1964/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3873 - val_loss: 7.3966\n",
      "Epoch 1965/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2195 - val_loss: 7.3730\n",
      "Epoch 1966/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2855 - val_loss: 7.3879\n",
      "Epoch 1967/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2505 - val_loss: 7.8003\n",
      "Epoch 1968/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3998 - val_loss: 7.3988\n",
      "Epoch 1969/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.4502 - val_loss: 7.3788\n",
      "Epoch 1970/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2395 - val_loss: 7.3952\n",
      "Epoch 1971/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2106 - val_loss: 7.4681\n",
      "Epoch 1972/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2872 - val_loss: 7.3754\n",
      "Epoch 1973/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2716 - val_loss: 7.3654\n",
      "Epoch 1974/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.3823 - val_loss: 7.4447\n",
      "Epoch 1975/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2124 - val_loss: 7.3937\n",
      "Epoch 1976/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2613 - val_loss: 7.3431\n",
      "Epoch 1977/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3052 - val_loss: 7.3974\n",
      "Epoch 1978/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2204 - val_loss: 7.3628\n",
      "Epoch 1979/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2280 - val_loss: 7.4220\n",
      "Epoch 1980/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2101 - val_loss: 7.3934\n",
      "Epoch 1981/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 49us/step - loss: 6.2299 - val_loss: 7.3647\n",
      "Epoch 1982/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3358 - val_loss: 7.5372\n",
      "Epoch 1983/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3918 - val_loss: 7.4195\n",
      "Epoch 1984/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1496 - val_loss: 7.8302\n",
      "Epoch 1985/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3566 - val_loss: 7.3857\n",
      "Epoch 1986/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2053 - val_loss: 7.3909\n",
      "Epoch 1987/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1779 - val_loss: 7.4553\n",
      "Epoch 1988/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.2256 - val_loss: 7.3880\n",
      "Epoch 1989/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5141 - val_loss: 7.6165\n",
      "Epoch 1990/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2866 - val_loss: 7.3342\n",
      "Epoch 1991/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2297 - val_loss: 7.3681\n",
      "Epoch 1992/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.1706 - val_loss: 7.3405\n",
      "Epoch 1993/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2307 - val_loss: 7.7893\n",
      "Epoch 1994/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4293 - val_loss: 7.3375\n",
      "Epoch 1995/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2396 - val_loss: 7.4126\n",
      "Epoch 1996/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1950 - val_loss: 7.4081\n",
      "Epoch 1997/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2072 - val_loss: 7.4684\n",
      "Epoch 1998/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.5672 - val_loss: 7.7237\n",
      "Epoch 1999/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.4173 - val_loss: 7.3713\n",
      "Epoch 2000/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3383 - val_loss: 7.6014\n",
      "Epoch 2001/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.2277 - val_loss: 7.5425\n",
      "Epoch 2002/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2575 - val_loss: 7.3581\n",
      "Epoch 2003/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1815 - val_loss: 7.3389\n",
      "Epoch 2004/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1963 - val_loss: 7.3294\n",
      "Epoch 2005/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2570 - val_loss: 7.3639\n",
      "Epoch 2006/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2870 - val_loss: 7.4520\n",
      "Epoch 2007/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2031 - val_loss: 7.3182\n",
      "Epoch 2008/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.2231 - val_loss: 7.3054\n",
      "Epoch 2009/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2697 - val_loss: 7.4995\n",
      "Epoch 2010/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3448 - val_loss: 7.3184\n",
      "Epoch 2011/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 6.5833 - val_loss: 7.4225\n",
      "Epoch 2012/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.7243 - val_loss: 7.3212\n",
      "Epoch 2013/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.1841 - val_loss: 7.3570\n",
      "Epoch 2014/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2120 - val_loss: 7.3943\n",
      "Epoch 2015/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2026 - val_loss: 7.3450\n",
      "Epoch 2016/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3461 - val_loss: 7.2873\n",
      "Epoch 2017/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.1779 - val_loss: 7.3590\n",
      "Epoch 2018/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 6.1749 - val_loss: 7.4852\n",
      "Epoch 2019/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4919 - val_loss: 7.6613\n",
      "Epoch 2020/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2277 - val_loss: 7.3565\n",
      "Epoch 2021/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3152 - val_loss: 7.3498\n",
      "Epoch 2022/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1481 - val_loss: 7.3222\n",
      "Epoch 2023/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1894 - val_loss: 7.2960\n",
      "Epoch 2024/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2919 - val_loss: 7.3839\n",
      "Epoch 2025/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.2063 - val_loss: 7.3242\n",
      "Epoch 2026/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.1637 - val_loss: 7.3146\n",
      "Epoch 2027/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.7831 - val_loss: 7.4033\n",
      "Epoch 2028/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3865 - val_loss: 7.4092\n",
      "Epoch 2029/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4185 - val_loss: 7.3129\n",
      "Epoch 2030/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1968 - val_loss: 7.3196\n",
      "Epoch 2031/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1996 - val_loss: 7.3805\n",
      "Epoch 2032/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2186 - val_loss: 7.2908\n",
      "Epoch 2033/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1808 - val_loss: 7.5763\n",
      "Epoch 2034/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2651 - val_loss: 7.3254\n",
      "Epoch 2035/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2214 - val_loss: 7.3568\n",
      "Epoch 2036/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2611 - val_loss: 7.2928\n",
      "Epoch 2037/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2488 - val_loss: 7.9379\n",
      "Epoch 2038/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.6238 - val_loss: 7.3193\n",
      "Epoch 2039/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2468 - val_loss: 7.3029\n",
      "Epoch 2040/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.2658 - val_loss: 7.3007\n",
      "Epoch 2041/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3405 - val_loss: 7.4019\n",
      "Epoch 2042/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1950 - val_loss: 7.2924\n",
      "Epoch 2043/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1123 - val_loss: 7.5872\n",
      "Epoch 2044/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2690 - val_loss: 7.3367\n",
      "Epoch 2045/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2568 - val_loss: 7.4414\n",
      "Epoch 2046/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1765 - val_loss: 7.3392\n",
      "Epoch 2047/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1327 - val_loss: 7.3793\n",
      "Epoch 2048/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1680 - val_loss: 7.4912\n",
      "Epoch 2049/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3071 - val_loss: 7.3347\n",
      "Epoch 2050/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1156 - val_loss: 7.3376\n",
      "Epoch 2051/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1443 - val_loss: 7.2874\n",
      "Epoch 2052/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1607 - val_loss: 7.4607\n",
      "Epoch 2053/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2472 - val_loss: 7.3361\n",
      "Epoch 2054/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4249 - val_loss: 7.3107\n",
      "Epoch 2055/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2965 - val_loss: 7.7864\n",
      "Epoch 2056/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1797 - val_loss: 7.3869\n",
      "Epoch 2057/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 6.3434 - val_loss: 7.6406\n",
      "Epoch 2058/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.3210 - val_loss: 7.2922\n",
      "Epoch 2059/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1003 - val_loss: 7.3665\n",
      "Epoch 2060/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1081 - val_loss: 7.7827\n",
      "Epoch 2061/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.2794 - val_loss: 7.2709\n",
      "Epoch 2062/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.1462 - val_loss: 7.3053\n",
      "Epoch 2063/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.0837 - val_loss: 7.3340\n",
      "Epoch 2064/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2390 - val_loss: 7.3172\n",
      "Epoch 2065/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.1167 - val_loss: 7.4381\n",
      "Epoch 2066/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.4341 - val_loss: 7.6306\n",
      "Epoch 2067/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1525 - val_loss: 7.4205\n",
      "Epoch 2068/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3589 - val_loss: 7.3123\n",
      "Epoch 2069/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1827 - val_loss: 7.2932\n",
      "Epoch 2070/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1265 - val_loss: 7.4251\n",
      "Epoch 2071/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3125 - val_loss: 7.2170\n",
      "Epoch 2072/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1423 - val_loss: 7.7260\n",
      "Epoch 2073/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2201 - val_loss: 7.2885\n",
      "Epoch 2074/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2228 - val_loss: 7.2865\n",
      "Epoch 2075/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1944 - val_loss: 7.3425\n",
      "Epoch 2076/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2727 - val_loss: 7.3191\n",
      "Epoch 2077/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3066 - val_loss: 7.3758\n",
      "Epoch 2078/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2675 - val_loss: 7.4162\n",
      "Epoch 2079/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3071 - val_loss: 7.2811\n",
      "Epoch 2080/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1587 - val_loss: 7.3261\n",
      "Epoch 2081/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.1391 - val_loss: 7.3158\n",
      "Epoch 2082/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2250 - val_loss: 7.2224\n",
      "Epoch 2083/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1400 - val_loss: 7.4537\n",
      "Epoch 2084/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1506 - val_loss: 7.3398\n",
      "Epoch 2085/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1998 - val_loss: 7.2739\n",
      "Epoch 2086/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4851 - val_loss: 7.9347\n",
      "Epoch 2087/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2871 - val_loss: 7.2855\n",
      "Epoch 2088/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2533 - val_loss: 7.2802\n",
      "Epoch 2089/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1501 - val_loss: 7.2225\n",
      "Epoch 2090/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.0879 - val_loss: 7.3259\n",
      "Epoch 2091/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.0921 - val_loss: 7.2435\n",
      "Epoch 2092/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1825 - val_loss: 7.3829\n",
      "Epoch 2093/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2580 - val_loss: 7.1993\n",
      "Epoch 2094/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0962 - val_loss: 7.2520\n",
      "Epoch 2095/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1648 - val_loss: 7.2561\n",
      "Epoch 2096/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1478 - val_loss: 7.1872\n",
      "Epoch 2097/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.4057 - val_loss: 7.5391\n",
      "Epoch 2098/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.1857 - val_loss: 7.2384\n",
      "Epoch 2099/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1851 - val_loss: 7.2896\n",
      "Epoch 2100/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1631 - val_loss: 7.5657\n",
      "Epoch 2101/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2104 - val_loss: 7.2996\n",
      "Epoch 2102/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.4263 - val_loss: 7.4223\n",
      "Epoch 2103/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3874 - val_loss: 7.2581\n",
      "Epoch 2104/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.0893 - val_loss: 7.2475\n",
      "Epoch 2105/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3977 - val_loss: 7.3716\n",
      "Epoch 2106/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.4212 - val_loss: 7.5527\n",
      "Epoch 2107/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1127 - val_loss: 7.1970\n",
      "Epoch 2108/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1448 - val_loss: 7.2871\n",
      "Epoch 2109/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0588 - val_loss: 7.2522\n",
      "Epoch 2110/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2945 - val_loss: 7.3506\n",
      "Epoch 2111/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.5659 - val_loss: 7.3220\n",
      "Epoch 2112/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.3704 - val_loss: 7.2349\n",
      "Epoch 2113/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3130 - val_loss: 7.2304\n",
      "Epoch 2114/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 6.2962 - val_loss: 7.3026\n",
      "Epoch 2115/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 6.0763 - val_loss: 7.3061\n",
      "Epoch 2116/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.0705 - val_loss: 7.1775\n",
      "Epoch 2117/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.2004 - val_loss: 7.2305\n",
      "Epoch 2118/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.0830 - val_loss: 7.1948\n",
      "Epoch 2119/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.1167 - val_loss: 7.2106\n",
      "Epoch 2120/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1120 - val_loss: 7.2582\n",
      "Epoch 2121/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3459 - val_loss: 7.2166\n",
      "Epoch 2122/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2924 - val_loss: 7.2719\n",
      "Epoch 2123/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.0509 - val_loss: 7.2626\n",
      "Epoch 2124/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0590 - val_loss: 7.2064\n",
      "Epoch 2125/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0985 - val_loss: 7.2088\n",
      "Epoch 2126/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2149 - val_loss: 7.3700\n",
      "Epoch 2127/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.1560 - val_loss: 7.2329\n",
      "Epoch 2128/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1447 - val_loss: 7.2210\n",
      "Epoch 2129/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 6.3891 - val_loss: 7.2646\n",
      "Epoch 2130/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1687 - val_loss: 7.1909\n",
      "Epoch 2131/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0982 - val_loss: 7.1988\n",
      "Epoch 2132/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0654 - val_loss: 7.2040\n",
      "Epoch 2133/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 6.2587 - val_loss: 7.2043\n",
      "Epoch 2134/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1675 - val_loss: 7.1849\n",
      "Epoch 2135/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0902 - val_loss: 7.4178\n",
      "Epoch 2136/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2345 - val_loss: 7.2559\n",
      "Epoch 2137/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0435 - val_loss: 7.3448\n",
      "Epoch 2138/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2236 - val_loss: 7.2874\n",
      "Epoch 2139/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.9845 - val_loss: 8.1743\n",
      "Epoch 2140/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.7215 - val_loss: 7.2316\n",
      "Epoch 2141/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0654 - val_loss: 7.1803\n",
      "Epoch 2142/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.0336 - val_loss: 7.1891\n",
      "Epoch 2143/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1392 - val_loss: 7.6071\n",
      "Epoch 2144/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.3834 - val_loss: 7.3771\n",
      "Epoch 2145/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1985 - val_loss: 7.2559\n",
      "Epoch 2146/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.4916 - val_loss: 7.4393\n",
      "Epoch 2147/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1602 - val_loss: 7.2292\n",
      "Epoch 2148/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0952 - val_loss: 7.2025\n",
      "Epoch 2149/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1275 - val_loss: 7.2324\n",
      "Epoch 2150/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3908 - val_loss: 7.3537\n",
      "Epoch 2151/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.3147 - val_loss: 7.7869\n",
      "Epoch 2152/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2572 - val_loss: 7.3229\n",
      "Epoch 2153/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1057 - val_loss: 7.3351\n",
      "Epoch 2154/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0718 - val_loss: 7.2065\n",
      "Epoch 2155/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1144 - val_loss: 7.3020\n",
      "Epoch 2156/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0905 - val_loss: 7.2405\n",
      "Epoch 2157/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0153 - val_loss: 7.3130\n",
      "Epoch 2158/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.0587 - val_loss: 7.2408\n",
      "Epoch 2159/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0857 - val_loss: 7.5338\n",
      "Epoch 2160/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.3331 - val_loss: 7.4532\n",
      "Epoch 2161/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2282 - val_loss: 7.2062\n",
      "Epoch 2162/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.2792 - val_loss: 7.1714\n",
      "Epoch 2163/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2978 - val_loss: 7.4244\n",
      "Epoch 2164/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.2565 - val_loss: 7.2335\n",
      "Epoch 2165/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2850 - val_loss: 7.2215\n",
      "Epoch 2166/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1173 - val_loss: 7.4509\n",
      "Epoch 2167/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.1782 - val_loss: 7.1923\n",
      "Epoch 2168/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0713 - val_loss: 7.2034\n",
      "Epoch 2169/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0485 - val_loss: 7.2876\n",
      "Epoch 2170/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0302 - val_loss: 7.1696\n",
      "Epoch 2171/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1923 - val_loss: 7.1149\n",
      "Epoch 2172/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1569 - val_loss: 7.4004\n",
      "Epoch 2173/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0762 - val_loss: 7.1445\n",
      "Epoch 2174/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0470 - val_loss: 7.1304\n",
      "Epoch 2175/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1670 - val_loss: 7.2977\n",
      "Epoch 2176/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0219 - val_loss: 7.4239\n",
      "Epoch 2177/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2473 - val_loss: 7.1777\n",
      "Epoch 2178/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0234 - val_loss: 7.4069\n",
      "Epoch 2179/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1582 - val_loss: 7.1362\n",
      "Epoch 2180/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9785 - val_loss: 7.2701\n",
      "Epoch 2181/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0294 - val_loss: 7.1455\n",
      "Epoch 2182/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0482 - val_loss: 7.1309\n",
      "Epoch 2183/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0721 - val_loss: 7.1600\n",
      "Epoch 2184/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1697 - val_loss: 7.7737\n",
      "Epoch 2185/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2589 - val_loss: 7.2964\n",
      "Epoch 2186/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2717 - val_loss: 7.5984\n",
      "Epoch 2187/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2231 - val_loss: 7.1523\n",
      "Epoch 2188/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1040 - val_loss: 7.4637\n",
      "Epoch 2189/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1203 - val_loss: 7.5029\n",
      "Epoch 2190/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1894 - val_loss: 7.2218\n",
      "Epoch 2191/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0434 - val_loss: 7.1575\n",
      "Epoch 2192/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0908 - val_loss: 7.2073\n",
      "Epoch 2193/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0282 - val_loss: 7.1018\n",
      "Epoch 2194/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9815 - val_loss: 7.1436\n",
      "Epoch 2195/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0195 - val_loss: 7.1296\n",
      "Epoch 2196/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0675 - val_loss: 7.4923\n",
      "Epoch 2197/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0438 - val_loss: 7.1698\n",
      "Epoch 2198/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2879 - val_loss: 7.2309\n",
      "Epoch 2199/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1420 - val_loss: 7.1558\n",
      "Epoch 2200/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0127 - val_loss: 7.1333\n",
      "Epoch 2201/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9755 - val_loss: 7.5722\n",
      "Epoch 2202/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.4229 - val_loss: 7.3461\n",
      "Epoch 2203/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.3528 - val_loss: 7.3000\n",
      "Epoch 2204/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0926 - val_loss: 7.3900\n",
      "Epoch 2205/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0440 - val_loss: 7.3546\n",
      "Epoch 2206/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1708 - val_loss: 7.3055\n",
      "Epoch 2207/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2479 - val_loss: 7.1819\n",
      "Epoch 2208/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0362 - val_loss: 7.2298\n",
      "Epoch 2209/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 6.0586 - val_loss: 7.1843\n",
      "Epoch 2210/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 6.0879 - val_loss: 7.1460\n",
      "Epoch 2211/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 6.0953 - val_loss: 7.1369\n",
      "Epoch 2212/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9962 - val_loss: 7.1177\n",
      "Epoch 2213/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0998 - val_loss: 7.1709\n",
      "Epoch 2214/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 6.1253 - val_loss: 7.1954\n",
      "Epoch 2215/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 6.0407 - val_loss: 7.1483\n",
      "Epoch 2216/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2073 - val_loss: 7.1766\n",
      "Epoch 2217/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.9836 - val_loss: 7.3883\n",
      "Epoch 2218/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0879 - val_loss: 7.2644\n",
      "Epoch 2219/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1621 - val_loss: 7.1451\n",
      "Epoch 2220/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0500 - val_loss: 7.1463\n",
      "Epoch 2221/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9696 - val_loss: 7.1353\n",
      "Epoch 2222/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9641 - val_loss: 7.0969\n",
      "Epoch 2223/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9770 - val_loss: 7.1211\n",
      "Epoch 2224/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9937 - val_loss: 7.1018\n",
      "Epoch 2225/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0136 - val_loss: 7.0893\n",
      "Epoch 2226/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1310 - val_loss: 7.1168\n",
      "Epoch 2227/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9803 - val_loss: 7.1545\n",
      "Epoch 2228/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1498 - val_loss: 7.1594\n",
      "Epoch 2229/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1392 - val_loss: 7.0665\n",
      "Epoch 2230/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9349 - val_loss: 7.0720\n",
      "Epoch 2231/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9476 - val_loss: 7.0968\n",
      "Epoch 2232/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2615 - val_loss: 7.1614\n",
      "Epoch 2233/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9501 - val_loss: 7.1786\n",
      "Epoch 2234/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0484 - val_loss: 7.7802\n",
      "Epoch 2235/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.2806 - val_loss: 7.1746\n",
      "Epoch 2236/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9311 - val_loss: 7.2448\n",
      "Epoch 2237/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9693 - val_loss: 7.1054\n",
      "Epoch 2238/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0576 - val_loss: 7.1031\n",
      "Epoch 2239/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1967 - val_loss: 7.0889\n",
      "Epoch 2240/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2952 - val_loss: 7.1256\n",
      "Epoch 2241/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1802 - val_loss: 7.1195\n",
      "Epoch 2242/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9717 - val_loss: 7.1019\n",
      "Epoch 2243/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9485 - val_loss: 7.0813\n",
      "Epoch 2244/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9776 - val_loss: 7.0790\n",
      "Epoch 2245/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0163 - val_loss: 7.3535\n",
      "Epoch 2246/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1206 - val_loss: 7.1310\n",
      "Epoch 2247/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9692 - val_loss: 7.1314\n",
      "Epoch 2248/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1847 - val_loss: 7.2802\n",
      "Epoch 2249/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.6344 - val_loss: 7.1107\n",
      "Epoch 2250/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0875 - val_loss: 7.1133\n",
      "Epoch 2251/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0975 - val_loss: 7.0649\n",
      "Epoch 2252/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1941 - val_loss: 7.3470\n",
      "Epoch 2253/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0412 - val_loss: 7.0797\n",
      "Epoch 2254/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9578 - val_loss: 7.0601\n",
      "Epoch 2255/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.9930 - val_loss: 7.0600\n",
      "Epoch 2256/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0220 - val_loss: 7.1447\n",
      "Epoch 2257/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0519 - val_loss: 7.5174\n",
      "Epoch 2258/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.3185 - val_loss: 7.0984\n",
      "Epoch 2259/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2265 - val_loss: 7.1136\n",
      "Epoch 2260/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0282 - val_loss: 7.0223\n",
      "Epoch 2261/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9612 - val_loss: 7.0438\n",
      "Epoch 2262/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9727 - val_loss: 7.0358\n",
      "Epoch 2263/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9440 - val_loss: 7.2584\n",
      "Epoch 2264/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1242 - val_loss: 7.1191\n",
      "Epoch 2265/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9181 - val_loss: 7.0492\n",
      "Epoch 2266/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2106 - val_loss: 7.0001\n",
      "Epoch 2267/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0200 - val_loss: 7.0242\n",
      "Epoch 2268/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9948 - val_loss: 7.2820\n",
      "Epoch 2269/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.1146 - val_loss: 7.0328\n",
      "Epoch 2270/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9176 - val_loss: 7.0672\n",
      "Epoch 2271/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9781 - val_loss: 7.0338\n",
      "Epoch 2272/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1801 - val_loss: 7.0286\n",
      "Epoch 2273/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1985 - val_loss: 7.0329\n",
      "Epoch 2274/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0040 - val_loss: 7.2296\n",
      "Epoch 2275/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1000 - val_loss: 7.0412\n",
      "Epoch 2276/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.9093 - val_loss: 7.1095\n",
      "Epoch 2277/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9346 - val_loss: 7.0345\n",
      "Epoch 2278/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8838 - val_loss: 7.1196\n",
      "Epoch 2279/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9522 - val_loss: 7.1722\n",
      "Epoch 2280/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1948 - val_loss: 6.9942\n",
      "Epoch 2281/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0461 - val_loss: 6.9923\n",
      "Epoch 2282/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9517 - val_loss: 7.0442\n",
      "Epoch 2283/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.9874 - val_loss: 7.0971\n",
      "Epoch 2284/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.2382 - val_loss: 7.1193\n",
      "Epoch 2285/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 5.9493 - val_loss: 7.1714\n",
      "Epoch 2286/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9458 - val_loss: 7.0769\n",
      "Epoch 2287/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 5.9190 - val_loss: 7.0293\n",
      "Epoch 2288/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9463 - val_loss: 7.1560\n",
      "Epoch 2289/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9882 - val_loss: 7.0450\n",
      "Epoch 2290/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.3322 - val_loss: 7.0908\n",
      "Epoch 2291/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1100 - val_loss: 7.0575\n",
      "Epoch 2292/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9470 - val_loss: 7.0640\n",
      "Epoch 2293/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9267 - val_loss: 7.0295\n",
      "Epoch 2294/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9292 - val_loss: 7.1538\n",
      "Epoch 2295/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9909 - val_loss: 7.0344\n",
      "Epoch 2296/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9452 - val_loss: 7.0237\n",
      "Epoch 2297/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0127 - val_loss: 7.5485\n",
      "Epoch 2298/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.4770 - val_loss: 7.0259\n",
      "Epoch 2299/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9948 - val_loss: 7.0605\n",
      "Epoch 2300/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9828 - val_loss: 7.0879\n",
      "Epoch 2301/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8821 - val_loss: 6.9729\n",
      "Epoch 2302/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9154 - val_loss: 7.2798\n",
      "Epoch 2303/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9622 - val_loss: 7.0587\n",
      "Epoch 2304/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9811 - val_loss: 7.3611\n",
      "Epoch 2305/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2042 - val_loss: 7.4031\n",
      "Epoch 2306/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0593 - val_loss: 7.0321\n",
      "Epoch 2307/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8929 - val_loss: 7.0473\n",
      "Epoch 2308/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9177 - val_loss: 7.0769\n",
      "Epoch 2309/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1552 - val_loss: 7.0632\n",
      "Epoch 2310/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0164 - val_loss: 7.4010\n",
      "Epoch 2311/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0154 - val_loss: 7.2367\n",
      "Epoch 2312/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1811 - val_loss: 7.7375\n",
      "Epoch 2313/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.3093 - val_loss: 7.0546\n",
      "Epoch 2314/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0208 - val_loss: 7.1321\n",
      "Epoch 2315/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9635 - val_loss: 7.0491\n",
      "Epoch 2316/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9323 - val_loss: 7.4559\n",
      "Epoch 2317/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2577 - val_loss: 7.0589\n",
      "Epoch 2318/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9336 - val_loss: 7.4002\n",
      "Epoch 2319/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0346 - val_loss: 7.1319\n",
      "Epoch 2320/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0874 - val_loss: 7.2745\n",
      "Epoch 2321/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0775 - val_loss: 7.0691\n",
      "Epoch 2322/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.3718 - val_loss: 7.5729\n",
      "Epoch 2323/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0121 - val_loss: 7.0336\n",
      "Epoch 2324/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9225 - val_loss: 7.0004\n",
      "Epoch 2325/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9040 - val_loss: 7.2309\n",
      "Epoch 2326/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9790 - val_loss: 7.0296\n",
      "Epoch 2327/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8825 - val_loss: 7.0286\n",
      "Epoch 2328/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0210 - val_loss: 7.0241\n",
      "Epoch 2329/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9356 - val_loss: 7.0594\n",
      "Epoch 2330/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0748 - val_loss: 7.0151\n",
      "Epoch 2331/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8768 - val_loss: 6.9532\n",
      "Epoch 2332/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0731 - val_loss: 7.1237\n",
      "Epoch 2333/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9620 - val_loss: 7.1878\n",
      "Epoch 2334/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0028 - val_loss: 7.0401\n",
      "Epoch 2335/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.9112 - val_loss: 7.0262\n",
      "Epoch 2336/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9899 - val_loss: 7.0815\n",
      "Epoch 2337/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0037 - val_loss: 7.0813\n",
      "Epoch 2338/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8640 - val_loss: 7.0336\n",
      "Epoch 2339/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8470 - val_loss: 7.2832\n",
      "Epoch 2340/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1183 - val_loss: 6.9953\n",
      "Epoch 2341/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9560 - val_loss: 7.0356\n",
      "Epoch 2342/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0510 - val_loss: 6.9895\n",
      "Epoch 2343/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9320 - val_loss: 6.9510\n",
      "Epoch 2344/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9022 - val_loss: 7.2484\n",
      "Epoch 2345/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0196 - val_loss: 7.1149\n",
      "Epoch 2346/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8645 - val_loss: 6.9778\n",
      "Epoch 2347/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8543 - val_loss: 6.9743\n",
      "Epoch 2348/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9523 - val_loss: 7.6020\n",
      "Epoch 2349/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 6.0906 - val_loss: 6.9710\n",
      "Epoch 2350/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9873 - val_loss: 6.9662\n",
      "Epoch 2351/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9093 - val_loss: 7.0390\n",
      "Epoch 2352/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1319 - val_loss: 7.0481\n",
      "Epoch 2353/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8514 - val_loss: 7.0637\n",
      "Epoch 2354/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8450 - val_loss: 7.0013\n",
      "Epoch 2355/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8958 - val_loss: 6.9431\n",
      "Epoch 2356/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8329 - val_loss: 6.9908\n",
      "Epoch 2357/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7921 - val_loss: 6.9802\n",
      "Epoch 2358/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9185 - val_loss: 6.9993\n",
      "Epoch 2359/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8108 - val_loss: 6.9677\n",
      "Epoch 2360/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 6.3051 - val_loss: 7.3523\n",
      "Epoch 2361/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 6.2648 - val_loss: 7.0257\n",
      "Epoch 2362/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9803 - val_loss: 7.0378\n",
      "Epoch 2363/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9954 - val_loss: 6.9733\n",
      "Epoch 2364/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.8760 - val_loss: 6.9536\n",
      "Epoch 2365/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8854 - val_loss: 6.9449\n",
      "Epoch 2366/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8655 - val_loss: 7.0680\n",
      "Epoch 2367/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9811 - val_loss: 7.0158\n",
      "Epoch 2368/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8658 - val_loss: 6.9290\n",
      "Epoch 2369/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.8408 - val_loss: 6.9216\n",
      "Epoch 2370/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1375 - val_loss: 7.1509\n",
      "Epoch 2371/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9543 - val_loss: 7.0206\n",
      "Epoch 2372/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9229 - val_loss: 7.0531\n",
      "Epoch 2373/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9343 - val_loss: 7.4160\n",
      "Epoch 2374/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9718 - val_loss: 6.9773\n",
      "Epoch 2375/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8051 - val_loss: 7.1326\n",
      "Epoch 2376/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9396 - val_loss: 6.9986\n",
      "Epoch 2377/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8414 - val_loss: 7.6435\n",
      "Epoch 2378/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9022 - val_loss: 6.9526\n",
      "Epoch 2379/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0274 - val_loss: 7.0124\n",
      "Epoch 2380/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1268 - val_loss: 6.9902\n",
      "Epoch 2381/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8665 - val_loss: 7.0025\n",
      "Epoch 2382/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0520 - val_loss: 6.9971\n",
      "Epoch 2383/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9715 - val_loss: 7.0637\n",
      "Epoch 2384/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9836 - val_loss: 7.3516\n",
      "Epoch 2385/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9400 - val_loss: 7.0417\n",
      "Epoch 2386/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9158 - val_loss: 7.0967\n",
      "Epoch 2387/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8257 - val_loss: 7.3460\n",
      "Epoch 2388/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0964 - val_loss: 7.0203\n",
      "Epoch 2389/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8102 - val_loss: 7.0010\n",
      "Epoch 2390/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.8525 - val_loss: 6.9879\n",
      "Epoch 2391/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7835 - val_loss: 7.5882\n",
      "Epoch 2392/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0928 - val_loss: 6.9064\n",
      "Epoch 2393/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8286 - val_loss: 7.0131\n",
      "Epoch 2394/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9439 - val_loss: 6.9818\n",
      "Epoch 2395/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8714 - val_loss: 7.1931\n",
      "Epoch 2396/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8294 - val_loss: 6.9865\n",
      "Epoch 2397/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8981 - val_loss: 6.9600\n",
      "Epoch 2398/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.8223 - val_loss: 7.2401\n",
      "Epoch 2399/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8776 - val_loss: 6.9191\n",
      "Epoch 2400/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8049 - val_loss: 7.2047\n",
      "Epoch 2401/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2030 - val_loss: 6.9707\n",
      "Epoch 2402/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0443 - val_loss: 6.9845\n",
      "Epoch 2403/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9022 - val_loss: 6.9521\n",
      "Epoch 2404/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8358 - val_loss: 6.9423\n",
      "Epoch 2405/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0649 - val_loss: 7.0107\n",
      "Epoch 2406/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2411 - val_loss: 6.9248\n",
      "Epoch 2407/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8960 - val_loss: 6.9911\n",
      "Epoch 2408/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8432 - val_loss: 7.4889\n",
      "Epoch 2409/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.3033 - val_loss: 6.9300\n",
      "Epoch 2410/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0165 - val_loss: 6.9671\n",
      "Epoch 2411/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8911 - val_loss: 6.9758\n",
      "Epoch 2412/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7842 - val_loss: 6.9758\n",
      "Epoch 2413/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9706 - val_loss: 7.0371\n",
      "Epoch 2414/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8501 - val_loss: 6.9611\n",
      "Epoch 2415/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7932 - val_loss: 6.9256\n",
      "Epoch 2416/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8030 - val_loss: 7.0498\n",
      "Epoch 2417/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8337 - val_loss: 7.2140\n",
      "Epoch 2418/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0799 - val_loss: 6.8660\n",
      "Epoch 2419/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7645 - val_loss: 6.8843\n",
      "Epoch 2420/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7996 - val_loss: 7.2376\n",
      "Epoch 2421/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8695 - val_loss: 6.9291\n",
      "Epoch 2422/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.8708 - val_loss: 7.0279\n",
      "Epoch 2423/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8810 - val_loss: 7.0055\n",
      "Epoch 2424/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8480 - val_loss: 7.0071\n",
      "Epoch 2425/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7822 - val_loss: 7.1351\n",
      "Epoch 2426/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7727 - val_loss: 6.9695\n",
      "Epoch 2427/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8829 - val_loss: 6.9117\n",
      "Epoch 2428/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7897 - val_loss: 7.1171\n",
      "Epoch 2429/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9051 - val_loss: 6.9256\n",
      "Epoch 2430/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7803 - val_loss: 6.9440\n",
      "Epoch 2431/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7874 - val_loss: 6.9661\n",
      "Epoch 2432/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8238 - val_loss: 6.9770\n",
      "Epoch 2433/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7984 - val_loss: 6.8992\n",
      "Epoch 2434/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8288 - val_loss: 6.9864\n",
      "Epoch 2435/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9233 - val_loss: 7.3981\n",
      "Epoch 2436/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8476 - val_loss: 6.9615\n",
      "Epoch 2437/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 6.0963 - val_loss: 7.1089\n",
      "Epoch 2438/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0194 - val_loss: 6.9113\n",
      "Epoch 2439/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9384 - val_loss: 6.9207\n",
      "Epoch 2440/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8733 - val_loss: 6.9733\n",
      "Epoch 2441/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.4260 - val_loss: 7.2823\n",
      "Epoch 2442/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9703 - val_loss: 7.8129\n",
      "Epoch 2443/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.1508 - val_loss: 6.9657\n",
      "Epoch 2444/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8606 - val_loss: 6.8921\n",
      "Epoch 2445/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7653 - val_loss: 6.8996\n",
      "Epoch 2446/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8556 - val_loss: 6.8862\n",
      "Epoch 2447/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.8362 - val_loss: 6.8810\n",
      "Epoch 2448/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7447 - val_loss: 6.9202\n",
      "Epoch 2449/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7378 - val_loss: 6.9021\n",
      "Epoch 2450/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7851 - val_loss: 6.8697\n",
      "Epoch 2451/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.8310 - val_loss: 6.9673\n",
      "Epoch 2452/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9678 - val_loss: 7.0031\n",
      "Epoch 2453/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8903 - val_loss: 7.0333\n",
      "Epoch 2454/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 5.9539 - val_loss: 7.1033\n",
      "Epoch 2455/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 5.7711 - val_loss: 6.9776\n",
      "Epoch 2456/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 5.8365 - val_loss: 6.9267\n",
      "Epoch 2457/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.7725 - val_loss: 6.8867\n",
      "Epoch 2458/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.8108 - val_loss: 6.9027\n",
      "Epoch 2459/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.8536 - val_loss: 7.1853\n",
      "Epoch 2460/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8873 - val_loss: 6.9092\n",
      "Epoch 2461/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9413 - val_loss: 7.2100\n",
      "Epoch 2462/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9741 - val_loss: 6.9165\n",
      "Epoch 2463/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.8526 - val_loss: 7.8932\n",
      "Epoch 2464/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1829 - val_loss: 6.8958\n",
      "Epoch 2465/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8146 - val_loss: 7.1008\n",
      "Epoch 2466/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9236 - val_loss: 6.9059\n",
      "Epoch 2467/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9642 - val_loss: 6.8380\n",
      "Epoch 2468/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7644 - val_loss: 6.8512\n",
      "Epoch 2469/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7290 - val_loss: 6.8458\n",
      "Epoch 2470/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7985 - val_loss: 6.8312\n",
      "Epoch 2471/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7551 - val_loss: 6.8388\n",
      "Epoch 2472/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9311 - val_loss: 7.8126\n",
      "Epoch 2473/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2429 - val_loss: 7.0245\n",
      "Epoch 2474/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7864 - val_loss: 6.8807\n",
      "Epoch 2475/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7485 - val_loss: 6.8764\n",
      "Epoch 2476/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7778 - val_loss: 7.2992\n",
      "Epoch 2477/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9743 - val_loss: 6.8478\n",
      "Epoch 2478/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 6.0664 - val_loss: 6.9059\n",
      "Epoch 2479/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9055 - val_loss: 7.2997\n",
      "Epoch 2480/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.8746 - val_loss: 6.8430\n",
      "Epoch 2481/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6955 - val_loss: 6.8266\n",
      "Epoch 2482/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8918 - val_loss: 6.8311\n",
      "Epoch 2483/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0692 - val_loss: 6.8734\n",
      "Epoch 2484/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7978 - val_loss: 6.8503\n",
      "Epoch 2485/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9238 - val_loss: 6.8700\n",
      "Epoch 2486/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7568 - val_loss: 6.8688\n",
      "Epoch 2487/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7216 - val_loss: 6.8706\n",
      "Epoch 2488/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7581 - val_loss: 6.8540\n",
      "Epoch 2489/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.1193 - val_loss: 6.9712\n",
      "Epoch 2490/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9390 - val_loss: 6.9501\n",
      "Epoch 2491/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7728 - val_loss: 6.8310\n",
      "Epoch 2492/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7121 - val_loss: 6.8789\n",
      "Epoch 2493/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7076 - val_loss: 6.9965\n",
      "Epoch 2494/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.4934 - val_loss: 6.9606\n",
      "Epoch 2495/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.1298 - val_loss: 6.9330\n",
      "Epoch 2496/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.2663 - val_loss: 7.4531\n",
      "Epoch 2497/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0821 - val_loss: 6.9495\n",
      "Epoch 2498/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7949 - val_loss: 6.8787\n",
      "Epoch 2499/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7235 - val_loss: 6.8130\n",
      "Epoch 2500/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7750 - val_loss: 6.9589\n",
      "Epoch 2501/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9597 - val_loss: 6.9360\n",
      "Epoch 2502/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8497 - val_loss: 6.9740\n",
      "Epoch 2503/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7699 - val_loss: 6.8325\n",
      "Epoch 2504/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7250 - val_loss: 6.8645\n",
      "Epoch 2505/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8754 - val_loss: 7.6049\n",
      "Epoch 2506/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.8759 - val_loss: 6.8086\n",
      "Epoch 2507/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7020 - val_loss: 6.7939\n",
      "Epoch 2508/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7151 - val_loss: 6.8204\n",
      "Epoch 2509/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7303 - val_loss: 6.8493\n",
      "Epoch 2510/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7203 - val_loss: 7.0458\n",
      "Epoch 2511/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8570 - val_loss: 6.8551\n",
      "Epoch 2512/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7406 - val_loss: 6.8035\n",
      "Epoch 2513/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 5.8287 - val_loss: 6.8921\n",
      "Epoch 2514/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8625 - val_loss: 6.8149\n",
      "Epoch 2515/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7073 - val_loss: 6.9774\n",
      "Epoch 2516/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7034 - val_loss: 6.8580\n",
      "Epoch 2517/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7070 - val_loss: 6.9641\n",
      "Epoch 2518/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7125 - val_loss: 6.8269\n",
      "Epoch 2519/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7993 - val_loss: 6.8093\n",
      "Epoch 2520/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7892 - val_loss: 6.7702\n",
      "Epoch 2521/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7426 - val_loss: 6.8137\n",
      "Epoch 2522/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6905 - val_loss: 6.8990\n",
      "Epoch 2523/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7385 - val_loss: 6.8055\n",
      "Epoch 2524/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7215 - val_loss: 7.5500\n",
      "Epoch 2525/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8691 - val_loss: 6.9283\n",
      "Epoch 2526/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8164 - val_loss: 6.8007\n",
      "Epoch 2527/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7818 - val_loss: 6.8357\n",
      "Epoch 2528/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6903 - val_loss: 6.7937\n",
      "Epoch 2529/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7060 - val_loss: 6.8918\n",
      "Epoch 2530/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7183 - val_loss: 6.9918\n",
      "Epoch 2531/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7662 - val_loss: 6.9558\n",
      "Epoch 2532/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8857 - val_loss: 6.9774\n",
      "Epoch 2533/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7868 - val_loss: 6.8344\n",
      "Epoch 2534/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7082 - val_loss: 6.8606\n",
      "Epoch 2535/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7653 - val_loss: 6.8534\n",
      "Epoch 2536/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9988 - val_loss: 6.9107\n",
      "Epoch 2537/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8443 - val_loss: 6.8107\n",
      "Epoch 2538/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7176 - val_loss: 6.8113\n",
      "Epoch 2539/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8379 - val_loss: 6.8689\n",
      "Epoch 2540/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8564 - val_loss: 6.8367\n",
      "Epoch 2541/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8935 - val_loss: 7.2447\n",
      "Epoch 2542/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7216 - val_loss: 6.7511\n",
      "Epoch 2543/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7811 - val_loss: 6.7683\n",
      "Epoch 2544/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.6902 - val_loss: 6.8238\n",
      "Epoch 2545/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7201 - val_loss: 6.9120\n",
      "Epoch 2546/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7240 - val_loss: 6.8191\n",
      "Epoch 2547/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7167 - val_loss: 6.7591\n",
      "Epoch 2548/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0121 - val_loss: 7.2020\n",
      "Epoch 2549/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0999 - val_loss: 6.8017\n",
      "Epoch 2550/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9784 - val_loss: 7.6280\n",
      "Epoch 2551/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9059 - val_loss: 6.7888\n",
      "Epoch 2552/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7525 - val_loss: 7.2513\n",
      "Epoch 2553/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9378 - val_loss: 6.8046\n",
      "Epoch 2554/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6776 - val_loss: 6.8906\n",
      "Epoch 2555/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7390 - val_loss: 6.7691\n",
      "Epoch 2556/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6691 - val_loss: 6.7909\n",
      "Epoch 2557/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6788 - val_loss: 6.7721\n",
      "Epoch 2558/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7117 - val_loss: 6.8830\n",
      "Epoch 2559/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9586 - val_loss: 6.7965\n",
      "Epoch 2560/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8109 - val_loss: 6.8830\n",
      "Epoch 2561/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7146 - val_loss: 7.0005\n",
      "Epoch 2562/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8740 - val_loss: 6.7808\n",
      "Epoch 2563/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7167 - val_loss: 6.8500\n",
      "Epoch 2564/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8014 - val_loss: 6.7829\n",
      "Epoch 2565/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7920 - val_loss: 6.9693\n",
      "Epoch 2566/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9504 - val_loss: 7.0055\n",
      "Epoch 2567/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7044 - val_loss: 6.8041\n",
      "Epoch 2568/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7406 - val_loss: 6.8217\n",
      "Epoch 2569/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7177 - val_loss: 6.7590\n",
      "Epoch 2570/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8042 - val_loss: 6.7756\n",
      "Epoch 2571/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6837 - val_loss: 6.9706\n",
      "Epoch 2572/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9118 - val_loss: 6.7472\n",
      "Epoch 2573/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.2052 - val_loss: 7.3369\n",
      "Epoch 2574/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7548 - val_loss: 6.8647\n",
      "Epoch 2575/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8735 - val_loss: 6.9748\n",
      "Epoch 2576/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6770 - val_loss: 6.7388\n",
      "Epoch 2577/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7958 - val_loss: 6.8165\n",
      "Epoch 2578/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6954 - val_loss: 7.3247\n",
      "Epoch 2579/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9110 - val_loss: 7.0124\n",
      "Epoch 2580/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6993 - val_loss: 6.8391\n",
      "Epoch 2581/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6521 - val_loss: 6.7841\n",
      "Epoch 2582/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6096 - val_loss: 6.7640\n",
      "Epoch 2583/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6412 - val_loss: 6.7654\n",
      "Epoch 2584/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6828 - val_loss: 6.7724\n",
      "Epoch 2585/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6753 - val_loss: 6.8304\n",
      "Epoch 2586/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6450 - val_loss: 6.7671\n",
      "Epoch 2587/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8407 - val_loss: 6.9736\n",
      "Epoch 2588/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6788 - val_loss: 6.8236\n",
      "Epoch 2589/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.6966 - val_loss: 6.7938\n",
      "Epoch 2590/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8281 - val_loss: 6.8600\n",
      "Epoch 2591/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7782 - val_loss: 6.7458\n",
      "Epoch 2592/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9707 - val_loss: 6.8982\n",
      "Epoch 2593/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7044 - val_loss: 6.9143\n",
      "Epoch 2594/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8668 - val_loss: 6.8029\n",
      "Epoch 2595/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7178 - val_loss: 6.7393\n",
      "Epoch 2596/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6587 - val_loss: 6.8468\n",
      "Epoch 2597/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7373 - val_loss: 6.7309\n",
      "Epoch 2598/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7101 - val_loss: 6.7583\n",
      "Epoch 2599/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6861 - val_loss: 6.7331\n",
      "Epoch 2600/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6775 - val_loss: 7.0331\n",
      "Epoch 2601/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7337 - val_loss: 6.7524\n",
      "Epoch 2602/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6294 - val_loss: 6.7578\n",
      "Epoch 2603/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6063 - val_loss: 6.8068\n",
      "Epoch 2604/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6760 - val_loss: 6.7495\n",
      "Epoch 2605/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7123 - val_loss: 6.7049\n",
      "Epoch 2606/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7941 - val_loss: 7.1494\n",
      "Epoch 2607/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7848 - val_loss: 6.9566\n",
      "Epoch 2608/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 6.0217 - val_loss: 6.7535\n",
      "Epoch 2609/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7758 - val_loss: 7.3827\n",
      "Epoch 2610/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8345 - val_loss: 6.7399\n",
      "Epoch 2611/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9085 - val_loss: 6.9250\n",
      "Epoch 2612/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6743 - val_loss: 6.6969\n",
      "Epoch 2613/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8629 - val_loss: 7.0432\n",
      "Epoch 2614/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 5.6899 - val_loss: 6.8420\n",
      "Epoch 2615/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7988 - val_loss: 7.1011\n",
      "Epoch 2616/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8015 - val_loss: 6.8384\n",
      "Epoch 2617/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6612 - val_loss: 6.8270\n",
      "Epoch 2618/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7751 - val_loss: 6.7950\n",
      "Epoch 2619/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7946 - val_loss: 6.8113\n",
      "Epoch 2620/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6688 - val_loss: 6.8050\n",
      "Epoch 2621/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7034 - val_loss: 6.7200\n",
      "Epoch 2622/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9174 - val_loss: 6.7196\n",
      "Epoch 2623/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6649 - val_loss: 6.7527\n",
      "Epoch 2624/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7288 - val_loss: 6.7214\n",
      "Epoch 2625/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8956 - val_loss: 6.6915\n",
      "Epoch 2626/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6530 - val_loss: 6.7776\n",
      "Epoch 2627/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6465 - val_loss: 6.7690\n",
      "Epoch 2628/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8353 - val_loss: 6.7084\n",
      "Epoch 2629/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9338 - val_loss: 6.8355\n",
      "Epoch 2630/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6873 - val_loss: 6.8379\n",
      "Epoch 2631/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9774 - val_loss: 6.8476\n",
      "Epoch 2632/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7028 - val_loss: 6.8064\n",
      "Epoch 2633/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7493 - val_loss: 6.7335\n",
      "Epoch 2634/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5621 - val_loss: 6.7808\n",
      "Epoch 2635/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6500 - val_loss: 6.7056\n",
      "Epoch 2636/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5707 - val_loss: 7.1773\n",
      "Epoch 2637/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8229 - val_loss: 6.7086\n",
      "Epoch 2638/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6818 - val_loss: 6.8056\n",
      "Epoch 2639/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6403 - val_loss: 6.8219\n",
      "Epoch 2640/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6220 - val_loss: 6.9773\n",
      "Epoch 2641/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.8257 - val_loss: 6.7095\n",
      "Epoch 2642/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6345 - val_loss: 6.8242\n",
      "Epoch 2643/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6509 - val_loss: 6.7450\n",
      "Epoch 2644/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6201 - val_loss: 6.7569\n",
      "Epoch 2645/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.7356 - val_loss: 6.7008\n",
      "Epoch 2646/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6599 - val_loss: 6.6626\n",
      "Epoch 2647/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7058 - val_loss: 6.6682\n",
      "Epoch 2648/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5998 - val_loss: 6.6672\n",
      "Epoch 2649/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6692 - val_loss: 6.9117\n",
      "Epoch 2650/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8332 - val_loss: 6.7915\n",
      "Epoch 2651/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9452 - val_loss: 6.8509\n",
      "Epoch 2652/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6551 - val_loss: 6.8546\n",
      "Epoch 2653/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6713 - val_loss: 6.7281\n",
      "Epoch 2654/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.6632 - val_loss: 6.6891\n",
      "Epoch 2655/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5431 - val_loss: 6.9966\n",
      "Epoch 2656/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7869 - val_loss: 6.8427\n",
      "Epoch 2657/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7009 - val_loss: 6.6326\n",
      "Epoch 2658/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6714 - val_loss: 6.6214\n",
      "Epoch 2659/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7901 - val_loss: 7.0054\n",
      "Epoch 2660/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7419 - val_loss: 6.6472\n",
      "Epoch 2661/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6708 - val_loss: 6.7289\n",
      "Epoch 2662/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8864 - val_loss: 6.6273\n",
      "Epoch 2663/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5698 - val_loss: 6.6386\n",
      "Epoch 2664/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5985 - val_loss: 6.6387\n",
      "Epoch 2665/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.6917 - val_loss: 6.7324\n",
      "Epoch 2666/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 5.7391 - val_loss: 6.7657\n",
      "Epoch 2667/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6618 - val_loss: 6.6816\n",
      "Epoch 2668/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7427 - val_loss: 7.1965\n",
      "Epoch 2669/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5994 - val_loss: 6.7306\n",
      "Epoch 2670/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8967 - val_loss: 6.8421\n",
      "Epoch 2671/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6666 - val_loss: 6.6490\n",
      "Epoch 2672/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6235 - val_loss: 6.6235\n",
      "Epoch 2673/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6985 - val_loss: 7.3891\n",
      "Epoch 2674/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9593 - val_loss: 6.6501\n",
      "Epoch 2675/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7222 - val_loss: 6.6816\n",
      "Epoch 2676/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6115 - val_loss: 6.6419\n",
      "Epoch 2677/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7256 - val_loss: 6.6486\n",
      "Epoch 2678/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6033 - val_loss: 6.8398\n",
      "Epoch 2679/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.9421 - val_loss: 6.9456\n",
      "Epoch 2680/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6697 - val_loss: 6.8712\n",
      "Epoch 2681/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5897 - val_loss: 6.6535\n",
      "Epoch 2682/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6057 - val_loss: 6.6968\n",
      "Epoch 2683/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6239 - val_loss: 7.0612\n",
      "Epoch 2684/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.8223 - val_loss: 6.6454\n",
      "Epoch 2685/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5961 - val_loss: 6.6141\n",
      "Epoch 2686/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7495 - val_loss: 7.1496\n",
      "Epoch 2687/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0429 - val_loss: 6.6787\n",
      "Epoch 2688/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.6183 - val_loss: 6.7140\n",
      "Epoch 2689/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6507 - val_loss: 6.6868\n",
      "Epoch 2690/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.6071 - val_loss: 6.6951\n",
      "Epoch 2691/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7342 - val_loss: 6.7310\n",
      "Epoch 2692/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7293 - val_loss: 6.8823\n",
      "Epoch 2693/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6440 - val_loss: 6.7851\n",
      "Epoch 2694/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7233 - val_loss: 6.9691\n",
      "Epoch 2695/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7583 - val_loss: 6.7217\n",
      "Epoch 2696/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5635 - val_loss: 6.8763\n",
      "Epoch 2697/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7609 - val_loss: 6.7927\n",
      "Epoch 2698/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9364 - val_loss: 6.7767\n",
      "Epoch 2699/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6665 - val_loss: 6.7053\n",
      "Epoch 2700/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8715 - val_loss: 6.6522\n",
      "Epoch 2701/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6691 - val_loss: 6.8207\n",
      "Epoch 2702/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9076 - val_loss: 6.6825\n",
      "Epoch 2703/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5987 - val_loss: 6.6369\n",
      "Epoch 2704/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5718 - val_loss: 6.7922\n",
      "Epoch 2705/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6322 - val_loss: 6.7347\n",
      "Epoch 2706/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.7767 - val_loss: 6.6495\n",
      "Epoch 2707/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5779 - val_loss: 6.6752\n",
      "Epoch 2708/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 5.5921 - val_loss: 6.8497\n",
      "Epoch 2709/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.6816 - val_loss: 6.7928\n",
      "Epoch 2710/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.6939 - val_loss: 6.7061\n",
      "Epoch 2711/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5687 - val_loss: 6.9590\n",
      "Epoch 2712/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7649 - val_loss: 6.6216\n",
      "Epoch 2713/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5758 - val_loss: 7.3654\n",
      "Epoch 2714/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 6.1095 - val_loss: 6.7446\n",
      "Epoch 2715/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5309 - val_loss: 6.9028\n",
      "Epoch 2716/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7946 - val_loss: 6.6436\n",
      "Epoch 2717/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 5.5592 - val_loss: 6.6273\n",
      "Epoch 2718/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 5.5382 - val_loss: 6.6428\n",
      "Epoch 2719/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 5.6690 - val_loss: 6.6603\n",
      "Epoch 2720/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 5.6569 - val_loss: 6.6795\n",
      "Epoch 2721/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 5.6055 - val_loss: 6.6893\n",
      "Epoch 2722/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 5.5479 - val_loss: 6.6235\n",
      "Epoch 2723/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 5.4960 - val_loss: 6.6302\n",
      "Epoch 2724/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 5.5787 - val_loss: 6.7830\n",
      "Epoch 2725/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 5.7382 - val_loss: 6.6290\n",
      "Epoch 2726/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5231 - val_loss: 6.8551\n",
      "Epoch 2727/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5485 - val_loss: 6.7378\n",
      "Epoch 2728/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7191 - val_loss: 6.7695\n",
      "Epoch 2729/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5323 - val_loss: 6.8190\n",
      "Epoch 2730/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7588 - val_loss: 6.7043\n",
      "Epoch 2731/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5826 - val_loss: 6.6003\n",
      "Epoch 2732/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5759 - val_loss: 7.1457\n",
      "Epoch 2733/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8006 - val_loss: 6.6546\n",
      "Epoch 2734/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5445 - val_loss: 6.6551\n",
      "Epoch 2735/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5522 - val_loss: 6.8654\n",
      "Epoch 2736/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6077 - val_loss: 6.6353\n",
      "Epoch 2737/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6224 - val_loss: 6.9270\n",
      "Epoch 2738/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8895 - val_loss: 7.2632\n",
      "Epoch 2739/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7129 - val_loss: 6.9745\n",
      "Epoch 2740/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6816 - val_loss: 6.7070\n",
      "Epoch 2741/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.7790 - val_loss: 6.6070\n",
      "Epoch 2742/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5668 - val_loss: 6.6076\n",
      "Epoch 2743/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5045 - val_loss: 6.6883\n",
      "Epoch 2744/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.6781 - val_loss: 6.6490\n",
      "Epoch 2745/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5792 - val_loss: 6.6131\n",
      "Epoch 2746/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5078 - val_loss: 6.6478\n",
      "Epoch 2747/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5181 - val_loss: 6.6097\n",
      "Epoch 2748/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 5.5889 - val_loss: 6.5709\n",
      "Epoch 2749/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 5.7305 - val_loss: 6.7146\n",
      "Epoch 2750/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5732 - val_loss: 6.5615\n",
      "Epoch 2751/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 5.5346 - val_loss: 6.5791\n",
      "Epoch 2752/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5768 - val_loss: 6.7062\n",
      "Epoch 2753/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5581 - val_loss: 6.6987\n",
      "Epoch 2754/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.9761 - val_loss: 7.1253\n",
      "Epoch 2755/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6160 - val_loss: 6.5970\n",
      "Epoch 2756/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5336 - val_loss: 6.6347\n",
      "Epoch 2757/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6181 - val_loss: 6.6301\n",
      "Epoch 2758/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.9075 - val_loss: 6.7612\n",
      "Epoch 2759/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5560 - val_loss: 6.8611\n",
      "Epoch 2760/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5642 - val_loss: 6.6911\n",
      "Epoch 2761/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6713 - val_loss: 6.5748\n",
      "Epoch 2762/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5244 - val_loss: 6.6735\n",
      "Epoch 2763/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6866 - val_loss: 6.7098\n",
      "Epoch 2764/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7509 - val_loss: 7.1174\n",
      "Epoch 2765/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7567 - val_loss: 6.6634\n",
      "Epoch 2766/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6608 - val_loss: 6.6029\n",
      "Epoch 2767/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5456 - val_loss: 6.5863\n",
      "Epoch 2768/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6235 - val_loss: 6.5758\n",
      "Epoch 2769/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5224 - val_loss: 6.6371\n",
      "Epoch 2770/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.5451 - val_loss: 6.5624\n",
      "Epoch 2771/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5196 - val_loss: 6.6122\n",
      "Epoch 2772/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5538 - val_loss: 6.6159\n",
      "Epoch 2773/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4807 - val_loss: 6.9901\n",
      "Epoch 2774/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6221 - val_loss: 6.5832\n",
      "Epoch 2775/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5595 - val_loss: 6.7842\n",
      "Epoch 2776/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.6167 - val_loss: 6.6665\n",
      "Epoch 2777/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6258 - val_loss: 7.9892\n",
      "Epoch 2778/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.9156 - val_loss: 6.7817\n",
      "Epoch 2779/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7545 - val_loss: 6.5808\n",
      "Epoch 2780/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5314 - val_loss: 6.8673\n",
      "Epoch 2781/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5856 - val_loss: 6.5841\n",
      "Epoch 2782/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5400 - val_loss: 7.0249\n",
      "Epoch 2783/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9674 - val_loss: 6.8536\n",
      "Epoch 2784/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 5.6989 - val_loss: 6.7558\n",
      "Epoch 2785/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5044 - val_loss: 6.8147\n",
      "Epoch 2786/10000\n",
      "675/675 [==============================] - 0s 44us/step - loss: 5.5469 - val_loss: 6.5761\n",
      "Epoch 2787/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4994 - val_loss: 6.5922\n",
      "Epoch 2788/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 5.4847 - val_loss: 6.6272\n",
      "Epoch 2789/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 5.4836 - val_loss: 6.5979\n",
      "Epoch 2790/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7894 - val_loss: 6.7106\n",
      "Epoch 2791/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5902 - val_loss: 6.5559\n",
      "Epoch 2792/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 5.4978 - val_loss: 6.7620\n",
      "Epoch 2793/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7468 - val_loss: 6.8443\n",
      "Epoch 2794/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7268 - val_loss: 7.0391\n",
      "Epoch 2795/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5682 - val_loss: 6.5622\n",
      "Epoch 2796/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4787 - val_loss: 6.6203\n",
      "Epoch 2797/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5555 - val_loss: 6.6312\n",
      "Epoch 2798/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.5555 - val_loss: 6.6152\n",
      "Epoch 2799/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7512 - val_loss: 6.6482\n",
      "Epoch 2800/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6481 - val_loss: 6.6008\n",
      "Epoch 2801/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6784 - val_loss: 6.6584\n",
      "Epoch 2802/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5445 - val_loss: 6.6565\n",
      "Epoch 2803/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5101 - val_loss: 6.8512\n",
      "Epoch 2804/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8727 - val_loss: 6.5676\n",
      "Epoch 2805/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7729 - val_loss: 6.5192\n",
      "Epoch 2806/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5219 - val_loss: 6.5813\n",
      "Epoch 2807/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5281 - val_loss: 6.8214\n",
      "Epoch 2808/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4729 - val_loss: 6.5758\n",
      "Epoch 2809/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4586 - val_loss: 6.5699\n",
      "Epoch 2810/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4875 - val_loss: 6.5179\n",
      "Epoch 2811/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4523 - val_loss: 6.5492\n",
      "Epoch 2812/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5946 - val_loss: 6.5449\n",
      "Epoch 2813/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.7617 - val_loss: 6.5648\n",
      "Epoch 2814/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4897 - val_loss: 6.7319\n",
      "Epoch 2815/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.9256 - val_loss: 6.5868\n",
      "Epoch 2816/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5836 - val_loss: 6.6782\n",
      "Epoch 2817/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.5739 - val_loss: 6.8761\n",
      "Epoch 2818/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 5.7380 - val_loss: 6.9134\n",
      "Epoch 2819/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 6.0034 - val_loss: 6.6766\n",
      "Epoch 2820/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4462 - val_loss: 6.6713\n",
      "Epoch 2821/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5907 - val_loss: 6.5670\n",
      "Epoch 2822/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6295 - val_loss: 6.5626\n",
      "Epoch 2823/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4802 - val_loss: 6.5636\n",
      "Epoch 2824/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5245 - val_loss: 6.5157\n",
      "Epoch 2825/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4797 - val_loss: 6.5353\n",
      "Epoch 2826/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.4883 - val_loss: 6.5543\n",
      "Epoch 2827/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4372 - val_loss: 6.5652\n",
      "Epoch 2828/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5417 - val_loss: 6.4972\n",
      "Epoch 2829/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5428 - val_loss: 6.7156\n",
      "Epoch 2830/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4818 - val_loss: 6.4893\n",
      "Epoch 2831/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6249 - val_loss: 6.7491\n",
      "Epoch 2832/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5173 - val_loss: 6.5971\n",
      "Epoch 2833/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5033 - val_loss: 6.6301\n",
      "Epoch 2834/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5920 - val_loss: 6.5019\n",
      "Epoch 2835/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4836 - val_loss: 6.5881\n",
      "Epoch 2836/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5182 - val_loss: 6.7043\n",
      "Epoch 2837/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6475 - val_loss: 6.5255\n",
      "Epoch 2838/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4902 - val_loss: 7.0142\n",
      "Epoch 2839/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6322 - val_loss: 6.6546\n",
      "Epoch 2840/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5970 - val_loss: 6.5907\n",
      "Epoch 2841/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5252 - val_loss: 6.4859\n",
      "Epoch 2842/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4926 - val_loss: 6.5244\n",
      "Epoch 2843/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5947 - val_loss: 6.6286\n",
      "Epoch 2844/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4618 - val_loss: 6.8182\n",
      "Epoch 2845/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.8451 - val_loss: 6.5341\n",
      "Epoch 2846/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 5.4643 - val_loss: 6.5245\n",
      "Epoch 2847/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.4982 - val_loss: 6.5043\n",
      "Epoch 2848/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5301 - val_loss: 6.5650\n",
      "Epoch 2849/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5320 - val_loss: 7.2901\n",
      "Epoch 2850/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6434 - val_loss: 6.4947\n",
      "Epoch 2851/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4599 - val_loss: 6.9575\n",
      "Epoch 2852/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5533 - val_loss: 6.4490\n",
      "Epoch 2853/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4214 - val_loss: 6.4723\n",
      "Epoch 2854/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4683 - val_loss: 6.5056\n",
      "Epoch 2855/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4396 - val_loss: 6.4956\n",
      "Epoch 2856/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4784 - val_loss: 6.6260\n",
      "Epoch 2857/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7248 - val_loss: 6.4969\n",
      "Epoch 2858/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5423 - val_loss: 7.0150\n",
      "Epoch 2859/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4781 - val_loss: 6.6080\n",
      "Epoch 2860/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4837 - val_loss: 6.5605\n",
      "Epoch 2861/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5655 - val_loss: 6.5469\n",
      "Epoch 2862/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4722 - val_loss: 6.6001\n",
      "Epoch 2863/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6571 - val_loss: 6.7728\n",
      "Epoch 2864/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4957 - val_loss: 6.4746\n",
      "Epoch 2865/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4600 - val_loss: 6.6077\n",
      "Epoch 2866/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4768 - val_loss: 6.5651\n",
      "Epoch 2867/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4992 - val_loss: 6.5881\n",
      "Epoch 2868/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4510 - val_loss: 6.4947\n",
      "Epoch 2869/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4372 - val_loss: 6.8373\n",
      "Epoch 2870/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6694 - val_loss: 6.5901\n",
      "Epoch 2871/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4807 - val_loss: 6.7130\n",
      "Epoch 2872/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4281 - val_loss: 6.7455\n",
      "Epoch 2873/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7654 - val_loss: 6.6115\n",
      "Epoch 2874/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5956 - val_loss: 6.6072\n",
      "Epoch 2875/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4677 - val_loss: 6.6865\n",
      "Epoch 2876/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4731 - val_loss: 6.6662\n",
      "Epoch 2877/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4266 - val_loss: 6.5966\n",
      "Epoch 2878/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5874 - val_loss: 6.8178\n",
      "Epoch 2879/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4880 - val_loss: 6.5567\n",
      "Epoch 2880/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5192 - val_loss: 6.5064\n",
      "Epoch 2881/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.4753 - val_loss: 6.5103\n",
      "Epoch 2882/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5676 - val_loss: 6.4947\n",
      "Epoch 2883/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4954 - val_loss: 6.5756\n",
      "Epoch 2884/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4593 - val_loss: 6.5775\n",
      "Epoch 2885/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.6085 - val_loss: 6.5034\n",
      "Epoch 2886/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3956 - val_loss: 6.8258\n",
      "Epoch 2887/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4753 - val_loss: 6.9001\n",
      "Epoch 2888/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6047 - val_loss: 6.4849\n",
      "Epoch 2889/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4568 - val_loss: 6.8057\n",
      "Epoch 2890/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4900 - val_loss: 6.8903\n",
      "Epoch 2891/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4863 - val_loss: 6.5274\n",
      "Epoch 2892/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5691 - val_loss: 6.5058\n",
      "Epoch 2893/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 5.4758 - val_loss: 6.4951\n",
      "Epoch 2894/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3968 - val_loss: 6.4725\n",
      "Epoch 2895/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7464 - val_loss: 7.1626\n",
      "Epoch 2896/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5930 - val_loss: 6.5463\n",
      "Epoch 2897/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4180 - val_loss: 6.6059\n",
      "Epoch 2898/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6492 - val_loss: 6.4793\n",
      "Epoch 2899/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4207 - val_loss: 6.5269\n",
      "Epoch 2900/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5023 - val_loss: 6.5065\n",
      "Epoch 2901/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4011 - val_loss: 6.6350\n",
      "Epoch 2902/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4462 - val_loss: 7.1107\n",
      "Epoch 2903/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.6646 - val_loss: 6.4706\n",
      "Epoch 2904/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5681 - val_loss: 6.5163\n",
      "Epoch 2905/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4922 - val_loss: 6.4854\n",
      "Epoch 2906/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4300 - val_loss: 6.4803\n",
      "Epoch 2907/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.5004 - val_loss: 6.7841\n",
      "Epoch 2908/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6570 - val_loss: 7.0130\n",
      "Epoch 2909/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5750 - val_loss: 6.5645\n",
      "Epoch 2910/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4575 - val_loss: 6.7257\n",
      "Epoch 2911/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7786 - val_loss: 6.5822\n",
      "Epoch 2912/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5685 - val_loss: 6.4795\n",
      "Epoch 2913/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6529 - val_loss: 6.4840\n",
      "Epoch 2914/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4490 - val_loss: 6.5831\n",
      "Epoch 2915/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4761 - val_loss: 6.7938\n",
      "Epoch 2916/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5717 - val_loss: 6.7589\n",
      "Epoch 2917/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5202 - val_loss: 6.5892\n",
      "Epoch 2918/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4655 - val_loss: 6.5799\n",
      "Epoch 2919/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4034 - val_loss: 6.5108\n",
      "Epoch 2920/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4062 - val_loss: 6.4825\n",
      "Epoch 2921/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3542 - val_loss: 6.6260\n",
      "Epoch 2922/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4658 - val_loss: 6.6717\n",
      "Epoch 2923/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5874 - val_loss: 7.0066\n",
      "Epoch 2924/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7531 - val_loss: 6.5783\n",
      "Epoch 2925/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4449 - val_loss: 6.8209\n",
      "Epoch 2926/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.7526 - val_loss: 6.4574\n",
      "Epoch 2927/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4929 - val_loss: 6.6257\n",
      "Epoch 2928/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5681 - val_loss: 6.4404\n",
      "Epoch 2929/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4003 - val_loss: 6.4987\n",
      "Epoch 2930/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4598 - val_loss: 6.4572\n",
      "Epoch 2931/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4800 - val_loss: 6.4704\n",
      "Epoch 2932/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5192 - val_loss: 6.4850\n",
      "Epoch 2933/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4108 - val_loss: 7.0602\n",
      "Epoch 2934/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4987 - val_loss: 6.4720\n",
      "Epoch 2935/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4746 - val_loss: 6.6434\n",
      "Epoch 2936/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4532 - val_loss: 6.5333\n",
      "Epoch 2937/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4255 - val_loss: 6.5715\n",
      "Epoch 2938/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5128 - val_loss: 6.5077\n",
      "Epoch 2939/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4165 - val_loss: 6.5769\n",
      "Epoch 2940/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4810 - val_loss: 6.5931\n",
      "Epoch 2941/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4423 - val_loss: 6.8414\n",
      "Epoch 2942/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5030 - val_loss: 6.4113\n",
      "Epoch 2943/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3611 - val_loss: 6.4729\n",
      "Epoch 2944/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4084 - val_loss: 6.5898\n",
      "Epoch 2945/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8147 - val_loss: 6.4799\n",
      "Epoch 2946/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4332 - val_loss: 6.6459\n",
      "Epoch 2947/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4710 - val_loss: 6.4575\n",
      "Epoch 2948/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3816 - val_loss: 6.7140\n",
      "Epoch 2949/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5069 - val_loss: 6.5603\n",
      "Epoch 2950/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3649 - val_loss: 6.4844\n",
      "Epoch 2951/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3760 - val_loss: 6.5017\n",
      "Epoch 2952/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4519 - val_loss: 6.5570\n",
      "Epoch 2953/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5734 - val_loss: 6.4467\n",
      "Epoch 2954/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5251 - val_loss: 6.3692\n",
      "Epoch 2955/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4012 - val_loss: 6.4357\n",
      "Epoch 2956/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4790 - val_loss: 6.5222\n",
      "Epoch 2957/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3794 - val_loss: 6.4961\n",
      "Epoch 2958/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4648 - val_loss: 6.4574\n",
      "Epoch 2959/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4640 - val_loss: 6.4772\n",
      "Epoch 2960/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5776 - val_loss: 6.4027\n",
      "Epoch 2961/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3481 - val_loss: 6.4346\n",
      "Epoch 2962/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3750 - val_loss: 6.5226\n",
      "Epoch 2963/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4825 - val_loss: 6.8034\n",
      "Epoch 2964/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4767 - val_loss: 6.4816\n",
      "Epoch 2965/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4091 - val_loss: 6.7181\n",
      "Epoch 2966/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5131 - val_loss: 6.5053\n",
      "Epoch 2967/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3984 - val_loss: 6.6111\n",
      "Epoch 2968/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5380 - val_loss: 6.4756\n",
      "Epoch 2969/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.3913 - val_loss: 6.5024\n",
      "Epoch 2970/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 5.4822 - val_loss: 7.1073\n",
      "Epoch 2971/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 6.0725 - val_loss: 6.4585\n",
      "Epoch 2972/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3552 - val_loss: 6.4791\n",
      "Epoch 2973/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4397 - val_loss: 7.1007\n",
      "Epoch 2974/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5262 - val_loss: 6.6069\n",
      "Epoch 2975/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3998 - val_loss: 6.4270\n",
      "Epoch 2976/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3583 - val_loss: 6.4417\n",
      "Epoch 2977/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4141 - val_loss: 6.5626\n",
      "Epoch 2978/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3308 - val_loss: 6.4761\n",
      "Epoch 2979/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4096 - val_loss: 6.7403\n",
      "Epoch 2980/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7048 - val_loss: 6.8808\n",
      "Epoch 2981/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4184 - val_loss: 6.4593\n",
      "Epoch 2982/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4037 - val_loss: 6.4248\n",
      "Epoch 2983/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4495 - val_loss: 6.4369\n",
      "Epoch 2984/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4044 - val_loss: 6.4244\n",
      "Epoch 2985/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4190 - val_loss: 6.4451\n",
      "Epoch 2986/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4466 - val_loss: 6.8312\n",
      "Epoch 2987/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8589 - val_loss: 6.5927\n",
      "Epoch 2988/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3735 - val_loss: 6.3671\n",
      "Epoch 2989/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3799 - val_loss: 6.5001\n",
      "Epoch 2990/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3882 - val_loss: 6.5096\n",
      "Epoch 2991/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3314 - val_loss: 6.4473\n",
      "Epoch 2992/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3529 - val_loss: 6.8396\n",
      "Epoch 2993/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4886 - val_loss: 6.4851\n",
      "Epoch 2994/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4194 - val_loss: 6.4673\n",
      "Epoch 2995/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5034 - val_loss: 6.4512\n",
      "Epoch 2996/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4303 - val_loss: 6.6853\n",
      "Epoch 2997/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5856 - val_loss: 6.9492\n",
      "Epoch 2998/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5507 - val_loss: 6.5983\n",
      "Epoch 2999/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4177 - val_loss: 6.3661\n",
      "Epoch 3000/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6467 - val_loss: 6.4638\n",
      "Epoch 3001/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 5.3281 - val_loss: 6.4480\n",
      "Epoch 3002/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4101 - val_loss: 6.3774\n",
      "Epoch 3003/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3886 - val_loss: 6.7157\n",
      "Epoch 3004/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6993 - val_loss: 6.5318\n",
      "Epoch 3005/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3753 - val_loss: 6.4084\n",
      "Epoch 3006/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4328 - val_loss: 6.4511\n",
      "Epoch 3007/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5462 - val_loss: 6.7157\n",
      "Epoch 3008/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4752 - val_loss: 6.3992\n",
      "Epoch 3009/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3867 - val_loss: 6.3876\n",
      "Epoch 3010/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3800 - val_loss: 6.3731\n",
      "Epoch 3011/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3412 - val_loss: 6.5056\n",
      "Epoch 3012/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6442 - val_loss: 6.3976\n",
      "Epoch 3013/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6428 - val_loss: 6.3721\n",
      "Epoch 3014/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3584 - val_loss: 6.3814\n",
      "Epoch 3015/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5582 - val_loss: 6.3996\n",
      "Epoch 3016/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4897 - val_loss: 6.3681\n",
      "Epoch 3017/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4803 - val_loss: 6.7159\n",
      "Epoch 3018/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3386 - val_loss: 6.3347\n",
      "Epoch 3019/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3745 - val_loss: 6.5568\n",
      "Epoch 3020/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3007 - val_loss: 6.3800\n",
      "Epoch 3021/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6102 - val_loss: 6.4344\n",
      "Epoch 3022/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4925 - val_loss: 6.3783\n",
      "Epoch 3023/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3973 - val_loss: 6.8023\n",
      "Epoch 3024/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6423 - val_loss: 6.3626\n",
      "Epoch 3025/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3616 - val_loss: 6.4863\n",
      "Epoch 3026/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4262 - val_loss: 6.6023\n",
      "Epoch 3027/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5584 - val_loss: 6.5450\n",
      "Epoch 3028/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4358 - val_loss: 7.2670\n",
      "Epoch 3029/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5576 - val_loss: 6.4202\n",
      "Epoch 3030/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4326 - val_loss: 6.3282\n",
      "Epoch 3031/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4841 - val_loss: 6.3519\n",
      "Epoch 3032/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7060 - val_loss: 6.4349\n",
      "Epoch 3033/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5186 - val_loss: 6.5041\n",
      "Epoch 3034/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3185 - val_loss: 6.3791\n",
      "Epoch 3035/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3603 - val_loss: 6.4549\n",
      "Epoch 3036/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4554 - val_loss: 6.3349\n",
      "Epoch 3037/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3630 - val_loss: 6.3542\n",
      "Epoch 3038/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3469 - val_loss: 6.7516\n",
      "Epoch 3039/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3246 - val_loss: 6.3512\n",
      "Epoch 3040/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3139 - val_loss: 6.3547\n",
      "Epoch 3041/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3046 - val_loss: 6.3827\n",
      "Epoch 3042/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5091 - val_loss: 6.3654\n",
      "Epoch 3043/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3543 - val_loss: 6.5475\n",
      "Epoch 3044/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2796 - val_loss: 6.5154\n",
      "Epoch 3045/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 5.3279 - val_loss: 6.4747\n",
      "Epoch 3046/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3849 - val_loss: 6.4439\n",
      "Epoch 3047/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3802 - val_loss: 6.5307\n",
      "Epoch 3048/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3261 - val_loss: 6.3544\n",
      "Epoch 3049/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3444 - val_loss: 6.4746\n",
      "Epoch 3050/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4321 - val_loss: 6.4333\n",
      "Epoch 3051/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2953 - val_loss: 6.4027\n",
      "Epoch 3052/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.6102 - val_loss: 6.4926\n",
      "Epoch 3053/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3819 - val_loss: 6.8248\n",
      "Epoch 3054/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6833 - val_loss: 6.3520\n",
      "Epoch 3055/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.6409 - val_loss: 6.5614\n",
      "Epoch 3056/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4965 - val_loss: 6.3783\n",
      "Epoch 3057/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3330 - val_loss: 6.3412\n",
      "Epoch 3058/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3850 - val_loss: 6.3915\n",
      "Epoch 3059/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3424 - val_loss: 6.3817\n",
      "Epoch 3060/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4862 - val_loss: 6.4845\n",
      "Epoch 3061/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3439 - val_loss: 6.4045\n",
      "Epoch 3062/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3449 - val_loss: 6.3611\n",
      "Epoch 3063/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3599 - val_loss: 6.3310\n",
      "Epoch 3064/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4261 - val_loss: 6.3363\n",
      "Epoch 3065/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3177 - val_loss: 6.7379\n",
      "Epoch 3066/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4967 - val_loss: 6.4321\n",
      "Epoch 3067/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5449 - val_loss: 6.6680\n",
      "Epoch 3068/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3402 - val_loss: 6.4356\n",
      "Epoch 3069/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2689 - val_loss: 6.3922\n",
      "Epoch 3070/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4955 - val_loss: 6.7066\n",
      "Epoch 3071/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3416 - val_loss: 6.3893\n",
      "Epoch 3072/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4205 - val_loss: 6.4811\n",
      "Epoch 3073/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.2988 - val_loss: 6.3952\n",
      "Epoch 3074/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3300 - val_loss: 6.3275\n",
      "Epoch 3075/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3325 - val_loss: 6.3413\n",
      "Epoch 3076/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.3618 - val_loss: 6.3503\n",
      "Epoch 3077/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.4370 - val_loss: 6.6492\n",
      "Epoch 3078/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.6576 - val_loss: 6.7268\n",
      "Epoch 3079/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4485 - val_loss: 6.3953\n",
      "Epoch 3080/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3776 - val_loss: 6.3891\n",
      "Epoch 3081/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3201 - val_loss: 6.4067\n",
      "Epoch 3082/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3664 - val_loss: 6.3257\n",
      "Epoch 3083/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3467 - val_loss: 6.4437\n",
      "Epoch 3084/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3280 - val_loss: 6.3535\n",
      "Epoch 3085/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5467 - val_loss: 6.7585\n",
      "Epoch 3086/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3056 - val_loss: 6.4540\n",
      "Epoch 3087/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3274 - val_loss: 6.3383\n",
      "Epoch 3088/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2948 - val_loss: 6.3156\n",
      "Epoch 3089/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5848 - val_loss: 6.4236\n",
      "Epoch 3090/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5144 - val_loss: 6.4104\n",
      "Epoch 3091/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3497 - val_loss: 6.3489\n",
      "Epoch 3092/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3108 - val_loss: 6.4097\n",
      "Epoch 3093/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3771 - val_loss: 6.4303\n",
      "Epoch 3094/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3867 - val_loss: 6.5046\n",
      "Epoch 3095/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2942 - val_loss: 6.4739\n",
      "Epoch 3096/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4800 - val_loss: 6.7841\n",
      "Epoch 3097/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.6197 - val_loss: 6.3029\n",
      "Epoch 3098/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3556 - val_loss: 6.3034\n",
      "Epoch 3099/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3504 - val_loss: 6.9673\n",
      "Epoch 3100/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5967 - val_loss: 6.3325\n",
      "Epoch 3101/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3215 - val_loss: 6.3623\n",
      "Epoch 3102/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3851 - val_loss: 6.2897\n",
      "Epoch 3103/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3304 - val_loss: 6.5220\n",
      "Epoch 3104/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.2957 - val_loss: 6.3980\n",
      "Epoch 3105/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3284 - val_loss: 6.3124\n",
      "Epoch 3106/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3131 - val_loss: 6.3311\n",
      "Epoch 3107/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3419 - val_loss: 6.4280\n",
      "Epoch 3108/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4489 - val_loss: 6.6053\n",
      "Epoch 3109/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4566 - val_loss: 6.3167\n",
      "Epoch 3110/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.3447 - val_loss: 7.6873\n",
      "Epoch 3111/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.9219 - val_loss: 6.6760\n",
      "Epoch 3112/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.8351 - val_loss: 6.4317\n",
      "Epoch 3113/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.4141 - val_loss: 6.2976\n",
      "Epoch 3114/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3682 - val_loss: 6.5700\n",
      "Epoch 3115/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5952 - val_loss: 6.3991\n",
      "Epoch 3116/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5020 - val_loss: 6.3147\n",
      "Epoch 3117/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3358 - val_loss: 6.7227\n",
      "Epoch 3118/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4020 - val_loss: 6.3143\n",
      "Epoch 3119/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3725 - val_loss: 6.3585\n",
      "Epoch 3120/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2965 - val_loss: 6.4641\n",
      "Epoch 3121/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 5.3320 - val_loss: 6.3230\n",
      "Epoch 3122/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.4228 - val_loss: 6.3041\n",
      "Epoch 3123/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4515 - val_loss: 6.2916\n",
      "Epoch 3124/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.4707 - val_loss: 6.4434\n",
      "Epoch 3125/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3105 - val_loss: 6.3425\n",
      "Epoch 3126/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3531 - val_loss: 6.5182\n",
      "Epoch 3127/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3981 - val_loss: 6.2857\n",
      "Epoch 3128/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3149 - val_loss: 6.3043\n",
      "Epoch 3129/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.2816 - val_loss: 6.2748\n",
      "Epoch 3130/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.4905 - val_loss: 6.7388\n",
      "Epoch 3131/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4777 - val_loss: 6.6506\n",
      "Epoch 3132/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4133 - val_loss: 6.5140\n",
      "Epoch 3133/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4771 - val_loss: 6.3821\n",
      "Epoch 3134/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2481 - val_loss: 6.6375\n",
      "Epoch 3135/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.6599 - val_loss: 7.5979\n",
      "Epoch 3136/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8506 - val_loss: 6.7835\n",
      "Epoch 3137/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3578 - val_loss: 6.3317\n",
      "Epoch 3138/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2843 - val_loss: 6.3680\n",
      "Epoch 3139/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2573 - val_loss: 6.3017\n",
      "Epoch 3140/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.2966 - val_loss: 7.1972\n",
      "Epoch 3141/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.8941 - val_loss: 6.4861\n",
      "Epoch 3142/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.7691 - val_loss: 6.2820\n",
      "Epoch 3143/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3749 - val_loss: 6.5582\n",
      "Epoch 3144/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2289 - val_loss: 6.6052\n",
      "Epoch 3145/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6771 - val_loss: 6.4603\n",
      "Epoch 3146/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3138 - val_loss: 6.2522\n",
      "Epoch 3147/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2870 - val_loss: 6.2599\n",
      "Epoch 3148/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.2691 - val_loss: 6.2744\n",
      "Epoch 3149/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2847 - val_loss: 6.3030\n",
      "Epoch 3150/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3414 - val_loss: 6.5398\n",
      "Epoch 3151/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2625 - val_loss: 6.2977\n",
      "Epoch 3152/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3059 - val_loss: 6.2904\n",
      "Epoch 3153/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4632 - val_loss: 6.7167\n",
      "Epoch 3154/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3154 - val_loss: 6.3732\n",
      "Epoch 3155/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2520 - val_loss: 6.2909\n",
      "Epoch 3156/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3584 - val_loss: 6.2713\n",
      "Epoch 3157/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2494 - val_loss: 6.4398\n",
      "Epoch 3158/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2942 - val_loss: 6.5592\n",
      "Epoch 3159/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.7696 - val_loss: 6.3105\n",
      "Epoch 3160/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2964 - val_loss: 6.4522\n",
      "Epoch 3161/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2908 - val_loss: 6.3076\n",
      "Epoch 3162/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3608 - val_loss: 6.9750\n",
      "Epoch 3163/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.5823 - val_loss: 6.3042\n",
      "Epoch 3164/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3070 - val_loss: 6.3060\n",
      "Epoch 3165/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2400 - val_loss: 6.2895\n",
      "Epoch 3166/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2815 - val_loss: 6.2998\n",
      "Epoch 3167/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3041 - val_loss: 6.3452\n",
      "Epoch 3168/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4578 - val_loss: 6.4976\n",
      "Epoch 3169/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3705 - val_loss: 6.3377\n",
      "Epoch 3170/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2473 - val_loss: 6.2505\n",
      "Epoch 3171/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2258 - val_loss: 6.3189\n",
      "Epoch 3172/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3887 - val_loss: 6.3275\n",
      "Epoch 3173/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3570 - val_loss: 6.5259\n",
      "Epoch 3174/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4267 - val_loss: 6.5609\n",
      "Epoch 3175/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7915 - val_loss: 6.2304\n",
      "Epoch 3176/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2397 - val_loss: 6.2499\n",
      "Epoch 3177/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.2549 - val_loss: 6.4018\n",
      "Epoch 3178/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4323 - val_loss: 6.3944\n",
      "Epoch 3179/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2576 - val_loss: 6.2978\n",
      "Epoch 3180/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.2985 - val_loss: 6.2718\n",
      "Epoch 3181/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3324 - val_loss: 6.5261\n",
      "Epoch 3182/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4030 - val_loss: 6.2976\n",
      "Epoch 3183/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2610 - val_loss: 6.6295\n",
      "Epoch 3184/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2642 - val_loss: 6.3283\n",
      "Epoch 3185/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4150 - val_loss: 6.2440\n",
      "Epoch 3186/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3623 - val_loss: 6.4249\n",
      "Epoch 3187/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6054 - val_loss: 6.4441\n",
      "Epoch 3188/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3737 - val_loss: 6.3061\n",
      "Epoch 3189/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2409 - val_loss: 6.2982\n",
      "Epoch 3190/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.2673 - val_loss: 6.2669\n",
      "Epoch 3191/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2255 - val_loss: 6.3131\n",
      "Epoch 3192/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3307 - val_loss: 6.2787\n",
      "Epoch 3193/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2849 - val_loss: 6.4497\n",
      "Epoch 3194/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3446 - val_loss: 6.2379\n",
      "Epoch 3195/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2011 - val_loss: 6.2365\n",
      "Epoch 3196/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2912 - val_loss: 6.6266\n",
      "Epoch 3197/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.2896 - val_loss: 6.2614\n",
      "Epoch 3198/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.2489 - val_loss: 6.4052\n",
      "Epoch 3199/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.4011 - val_loss: 6.3757\n",
      "Epoch 3200/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2580 - val_loss: 6.3117\n",
      "Epoch 3201/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2805 - val_loss: 6.3797\n",
      "Epoch 3202/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2706 - val_loss: 6.3052\n",
      "Epoch 3203/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2366 - val_loss: 6.5570\n",
      "Epoch 3204/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5588 - val_loss: 6.3660\n",
      "Epoch 3205/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5083 - val_loss: 6.2584\n",
      "Epoch 3206/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5102 - val_loss: 6.5768\n",
      "Epoch 3207/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4057 - val_loss: 6.2628\n",
      "Epoch 3208/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3760 - val_loss: 6.2918\n",
      "Epoch 3209/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2936 - val_loss: 6.2326\n",
      "Epoch 3210/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2545 - val_loss: 6.2740\n",
      "Epoch 3211/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3950 - val_loss: 6.4280\n",
      "Epoch 3212/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4457 - val_loss: 6.5540\n",
      "Epoch 3213/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3028 - val_loss: 6.3060\n",
      "Epoch 3214/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3698 - val_loss: 6.2927\n",
      "Epoch 3215/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.2881 - val_loss: 6.6068\n",
      "Epoch 3216/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3749 - val_loss: 6.6318\n",
      "Epoch 3217/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7705 - val_loss: 6.7961\n",
      "Epoch 3218/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5456 - val_loss: 6.2849\n",
      "Epoch 3219/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2583 - val_loss: 6.3645\n",
      "Epoch 3220/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2633 - val_loss: 6.4120\n",
      "Epoch 3221/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4452 - val_loss: 6.2785\n",
      "Epoch 3222/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2706 - val_loss: 6.7003\n",
      "Epoch 3223/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4622 - val_loss: 6.4443\n",
      "Epoch 3224/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2443 - val_loss: 6.2630\n",
      "Epoch 3225/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3287 - val_loss: 6.6463\n",
      "Epoch 3226/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2996 - val_loss: 6.4433\n",
      "Epoch 3227/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2611 - val_loss: 6.7132\n",
      "Epoch 3228/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4476 - val_loss: 6.3496\n",
      "Epoch 3229/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3410 - val_loss: 6.2653\n",
      "Epoch 3230/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4015 - val_loss: 6.2756\n",
      "Epoch 3231/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2417 - val_loss: 6.2430\n",
      "Epoch 3232/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2167 - val_loss: 6.3643\n",
      "Epoch 3233/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3354 - val_loss: 6.2881\n",
      "Epoch 3234/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.2610 - val_loss: 6.3286\n",
      "Epoch 3235/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2859 - val_loss: 6.3459\n",
      "Epoch 3236/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2563 - val_loss: 6.2653\n",
      "Epoch 3237/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.2236 - val_loss: 6.4645\n",
      "Epoch 3238/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3008 - val_loss: 6.3086\n",
      "Epoch 3239/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2142 - val_loss: 6.6019\n",
      "Epoch 3240/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4794 - val_loss: 6.2061\n",
      "Epoch 3241/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4246 - val_loss: 6.3626\n",
      "Epoch 3242/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4193 - val_loss: 6.3634\n",
      "Epoch 3243/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1771 - val_loss: 6.3275\n",
      "Epoch 3244/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4363 - val_loss: 6.3704\n",
      "Epoch 3245/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3750 - val_loss: 6.9225\n",
      "Epoch 3246/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.6070 - val_loss: 6.2357\n",
      "Epoch 3247/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2518 - val_loss: 6.3089\n",
      "Epoch 3248/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3264 - val_loss: 6.4383\n",
      "Epoch 3249/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.6312 - val_loss: 6.1615\n",
      "Epoch 3250/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1755 - val_loss: 6.1821\n",
      "Epoch 3251/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2327 - val_loss: 6.2266\n",
      "Epoch 3252/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2301 - val_loss: 6.2401\n",
      "Epoch 3253/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2344 - val_loss: 6.6202\n",
      "Epoch 3254/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4801 - val_loss: 6.1972\n",
      "Epoch 3255/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2185 - val_loss: 6.3567\n",
      "Epoch 3256/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2890 - val_loss: 6.2386\n",
      "Epoch 3257/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2014 - val_loss: 6.2059\n",
      "Epoch 3258/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2926 - val_loss: 6.1664\n",
      "Epoch 3259/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5010 - val_loss: 6.2039\n",
      "Epoch 3260/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3224 - val_loss: 6.6941\n",
      "Epoch 3261/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2428 - val_loss: 6.3704\n",
      "Epoch 3262/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.7174 - val_loss: 7.3387\n",
      "Epoch 3263/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4661 - val_loss: 6.2627\n",
      "Epoch 3264/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2418 - val_loss: 6.4217\n",
      "Epoch 3265/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1823 - val_loss: 6.2446\n",
      "Epoch 3266/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2464 - val_loss: 6.5498\n",
      "Epoch 3267/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2311 - val_loss: 6.2255\n",
      "Epoch 3268/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2094 - val_loss: 6.2522\n",
      "Epoch 3269/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3473 - val_loss: 6.3260\n",
      "Epoch 3270/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3171 - val_loss: 6.2054\n",
      "Epoch 3271/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2608 - val_loss: 6.2878\n",
      "Epoch 3272/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2930 - val_loss: 6.2545\n",
      "Epoch 3273/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 5.3273 - val_loss: 6.3087\n",
      "Epoch 3274/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2385 - val_loss: 6.2028\n",
      "Epoch 3275/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2311 - val_loss: 6.8308\n",
      "Epoch 3276/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3074 - val_loss: 6.4297\n",
      "Epoch 3277/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2484 - val_loss: 6.2783\n",
      "Epoch 3278/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2472 - val_loss: 6.2296\n",
      "Epoch 3279/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1522 - val_loss: 6.2097\n",
      "Epoch 3280/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2121 - val_loss: 6.1912\n",
      "Epoch 3281/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2124 - val_loss: 6.2220\n",
      "Epoch 3282/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2150 - val_loss: 6.2291\n",
      "Epoch 3283/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2824 - val_loss: 6.1855\n",
      "Epoch 3284/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3296 - val_loss: 6.2682\n",
      "Epoch 3285/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2269 - val_loss: 6.1976\n",
      "Epoch 3286/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2395 - val_loss: 6.5699\n",
      "Epoch 3287/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2685 - val_loss: 6.5816\n",
      "Epoch 3288/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4234 - val_loss: 7.3102\n",
      "Epoch 3289/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.5693 - val_loss: 6.2191\n",
      "Epoch 3290/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3192 - val_loss: 6.6256\n",
      "Epoch 3291/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2168 - val_loss: 6.2585\n",
      "Epoch 3292/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1894 - val_loss: 6.3364\n",
      "Epoch 3293/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1840 - val_loss: 6.2217\n",
      "Epoch 3294/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.7928 - val_loss: 6.5034\n",
      "Epoch 3295/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3456 - val_loss: 6.3244\n",
      "Epoch 3296/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2207 - val_loss: 6.2407\n",
      "Epoch 3297/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1952 - val_loss: 6.2039\n",
      "Epoch 3298/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5321 - val_loss: 6.5128\n",
      "Epoch 3299/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2490 - val_loss: 6.1785\n",
      "Epoch 3300/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2825 - val_loss: 6.2539\n",
      "Epoch 3301/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.2499 - val_loss: 6.3007\n",
      "Epoch 3302/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3287 - val_loss: 6.2415\n",
      "Epoch 3303/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2420 - val_loss: 6.3807\n",
      "Epoch 3304/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2039 - val_loss: 6.2209\n",
      "Epoch 3305/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1792 - val_loss: 6.1692\n",
      "Epoch 3306/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2094 - val_loss: 6.2628\n",
      "Epoch 3307/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2275 - val_loss: 6.2149\n",
      "Epoch 3308/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1525 - val_loss: 6.2176\n",
      "Epoch 3309/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1768 - val_loss: 6.2227\n",
      "Epoch 3310/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1963 - val_loss: 6.1886\n",
      "Epoch 3311/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1804 - val_loss: 6.1769\n",
      "Epoch 3312/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1639 - val_loss: 6.2965\n",
      "Epoch 3313/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1974 - val_loss: 6.2833\n",
      "Epoch 3314/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1455 - val_loss: 6.2253\n",
      "Epoch 3315/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2509 - val_loss: 6.2040\n",
      "Epoch 3316/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2199 - val_loss: 6.4331\n",
      "Epoch 3317/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3280 - val_loss: 6.1960\n",
      "Epoch 3318/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2998 - val_loss: 6.2928\n",
      "Epoch 3319/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3617 - val_loss: 6.3645\n",
      "Epoch 3320/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1826 - val_loss: 6.3422\n",
      "Epoch 3321/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2605 - val_loss: 6.1662\n",
      "Epoch 3322/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2439 - val_loss: 6.1737\n",
      "Epoch 3323/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2299 - val_loss: 6.1388\n",
      "Epoch 3324/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1652 - val_loss: 6.4754\n",
      "Epoch 3325/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5462 - val_loss: 6.4545\n",
      "Epoch 3326/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.2721 - val_loss: 6.2231\n",
      "Epoch 3327/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2149 - val_loss: 6.2041\n",
      "Epoch 3328/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3061 - val_loss: 6.1868\n",
      "Epoch 3329/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2771 - val_loss: 6.5375\n",
      "Epoch 3330/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3711 - val_loss: 6.5658\n",
      "Epoch 3331/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3262 - val_loss: 6.3568\n",
      "Epoch 3332/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.2148 - val_loss: 6.1622\n",
      "Epoch 3333/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2141 - val_loss: 6.5806\n",
      "Epoch 3334/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1971 - val_loss: 6.1851\n",
      "Epoch 3335/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1729 - val_loss: 6.3000\n",
      "Epoch 3336/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.3892 - val_loss: 6.1739\n",
      "Epoch 3337/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2957 - val_loss: 6.2177\n",
      "Epoch 3338/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3531 - val_loss: 6.3188\n",
      "Epoch 3339/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2301 - val_loss: 6.4978\n",
      "Epoch 3340/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4036 - val_loss: 6.3148\n",
      "Epoch 3341/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4257 - val_loss: 6.4045\n",
      "Epoch 3342/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1624 - val_loss: 6.2883\n",
      "Epoch 3343/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2640 - val_loss: 6.1982\n",
      "Epoch 3344/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2600 - val_loss: 6.4521\n",
      "Epoch 3345/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3503 - val_loss: 6.3008\n",
      "Epoch 3346/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1797 - val_loss: 6.2586\n",
      "Epoch 3347/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1986 - val_loss: 6.1112\n",
      "Epoch 3348/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1535 - val_loss: 6.5807\n",
      "Epoch 3349/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 5.2143 - val_loss: 6.1699\n",
      "Epoch 3350/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.1465 - val_loss: 6.2126\n",
      "Epoch 3351/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2874 - val_loss: 6.1314\n",
      "Epoch 3352/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1470 - val_loss: 6.1262\n",
      "Epoch 3353/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1472 - val_loss: 6.1867\n",
      "Epoch 3354/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3130 - val_loss: 6.1326\n",
      "Epoch 3355/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4366 - val_loss: 6.1366\n",
      "Epoch 3356/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1531 - val_loss: 6.1863\n",
      "Epoch 3357/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1412 - val_loss: 6.3511\n",
      "Epoch 3358/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1294 - val_loss: 6.1800\n",
      "Epoch 3359/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2805 - val_loss: 6.2285\n",
      "Epoch 3360/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1947 - val_loss: 6.1450\n",
      "Epoch 3361/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2066 - val_loss: 6.2034\n",
      "Epoch 3362/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2455 - val_loss: 6.4538\n",
      "Epoch 3363/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1920 - val_loss: 6.1806\n",
      "Epoch 3364/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2388 - val_loss: 7.1460\n",
      "Epoch 3365/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.4728 - val_loss: 6.2501\n",
      "Epoch 3366/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2074 - val_loss: 6.5266\n",
      "Epoch 3367/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1875 - val_loss: 6.1955\n",
      "Epoch 3368/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3148 - val_loss: 6.6240\n",
      "Epoch 3369/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2962 - val_loss: 6.2623\n",
      "Epoch 3370/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3123 - val_loss: 6.3825\n",
      "Epoch 3371/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1071 - val_loss: 6.1037\n",
      "Epoch 3372/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1615 - val_loss: 6.0982\n",
      "Epoch 3373/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.1598 - val_loss: 6.1972\n",
      "Epoch 3374/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.1249 - val_loss: 6.1975\n",
      "Epoch 3375/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2247 - val_loss: 6.1339\n",
      "Epoch 3376/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0985 - val_loss: 6.3518\n",
      "Epoch 3377/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2616 - val_loss: 6.3637\n",
      "Epoch 3378/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2007 - val_loss: 6.1911\n",
      "Epoch 3379/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1696 - val_loss: 6.2379\n",
      "Epoch 3380/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1414 - val_loss: 6.4004\n",
      "Epoch 3381/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1880 - val_loss: 6.1467\n",
      "Epoch 3382/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1262 - val_loss: 6.1984\n",
      "Epoch 3383/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1403 - val_loss: 6.1793\n",
      "Epoch 3384/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3411 - val_loss: 6.3739\n",
      "Epoch 3385/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3243 - val_loss: 6.6697\n",
      "Epoch 3386/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5301 - val_loss: 6.7308\n",
      "Epoch 3387/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2415 - val_loss: 6.3542\n",
      "Epoch 3388/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2231 - val_loss: 6.1244\n",
      "Epoch 3389/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1498 - val_loss: 6.1617\n",
      "Epoch 3390/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1430 - val_loss: 6.4720\n",
      "Epoch 3391/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2283 - val_loss: 6.1573\n",
      "Epoch 3392/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2510 - val_loss: 6.2813\n",
      "Epoch 3393/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1883 - val_loss: 6.1834\n",
      "Epoch 3394/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1402 - val_loss: 6.1444\n",
      "Epoch 3395/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2657 - val_loss: 6.5698\n",
      "Epoch 3396/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3003 - val_loss: 6.1852\n",
      "Epoch 3397/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.2785 - val_loss: 6.1960\n",
      "Epoch 3398/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2579 - val_loss: 6.9413\n",
      "Epoch 3399/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3249 - val_loss: 6.1439\n",
      "Epoch 3400/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.1701 - val_loss: 6.1910\n",
      "Epoch 3401/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1710 - val_loss: 6.1987\n",
      "Epoch 3402/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2909 - val_loss: 6.2216\n",
      "Epoch 3403/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1779 - val_loss: 6.1486\n",
      "Epoch 3404/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1439 - val_loss: 6.1173\n",
      "Epoch 3405/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4137 - val_loss: 6.0727\n",
      "Epoch 3406/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1354 - val_loss: 6.1023\n",
      "Epoch 3407/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0910 - val_loss: 6.5246\n",
      "Epoch 3408/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1608 - val_loss: 6.0867\n",
      "Epoch 3409/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3350 - val_loss: 6.2385\n",
      "Epoch 3410/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2894 - val_loss: 6.1058\n",
      "Epoch 3411/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3633 - val_loss: 6.3365\n",
      "Epoch 3412/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2783 - val_loss: 6.4819\n",
      "Epoch 3413/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0604 - val_loss: 6.3233\n",
      "Epoch 3414/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1099 - val_loss: 6.2015\n",
      "Epoch 3415/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.5336 - val_loss: 6.9320\n",
      "Epoch 3416/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3695 - val_loss: 6.1819\n",
      "Epoch 3417/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1460 - val_loss: 6.2386\n",
      "Epoch 3418/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2026 - val_loss: 6.1401\n",
      "Epoch 3419/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.3867 - val_loss: 6.2441\n",
      "Epoch 3420/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1075 - val_loss: 6.1589\n",
      "Epoch 3421/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2190 - val_loss: 6.6654\n",
      "Epoch 3422/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1950 - val_loss: 6.1672\n",
      "Epoch 3423/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1156 - val_loss: 6.2179\n",
      "Epoch 3424/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3566 - val_loss: 6.3226\n",
      "Epoch 3425/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 5.2596 - val_loss: 6.0887\n",
      "Epoch 3426/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2137 - val_loss: 6.3685\n",
      "Epoch 3427/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2247 - val_loss: 6.1579\n",
      "Epoch 3428/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1974 - val_loss: 6.0949\n",
      "Epoch 3429/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1089 - val_loss: 6.1239\n",
      "Epoch 3430/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2548 - val_loss: 6.4167\n",
      "Epoch 3431/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2995 - val_loss: 6.0803\n",
      "Epoch 3432/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1396 - val_loss: 6.4435\n",
      "Epoch 3433/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2398 - val_loss: 6.2339\n",
      "Epoch 3434/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3131 - val_loss: 6.3415\n",
      "Epoch 3435/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4613 - val_loss: 6.4439\n",
      "Epoch 3436/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2342 - val_loss: 6.3463\n",
      "Epoch 3437/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3747 - val_loss: 6.3791\n",
      "Epoch 3438/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1577 - val_loss: 6.1632\n",
      "Epoch 3439/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0912 - val_loss: 6.1532\n",
      "Epoch 3440/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1138 - val_loss: 6.1139\n",
      "Epoch 3441/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3327 - val_loss: 6.1413\n",
      "Epoch 3442/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1272 - val_loss: 6.1211\n",
      "Epoch 3443/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 5.1573 - val_loss: 6.2067\n",
      "Epoch 3444/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1977 - val_loss: 6.1828\n",
      "Epoch 3445/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1488 - val_loss: 6.2184\n",
      "Epoch 3446/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1895 - val_loss: 6.0958\n",
      "Epoch 3447/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1543 - val_loss: 6.0686\n",
      "Epoch 3448/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1744 - val_loss: 6.1053\n",
      "Epoch 3449/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0791 - val_loss: 6.2010\n",
      "Epoch 3450/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2028 - val_loss: 6.1232\n",
      "Epoch 3451/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1814 - val_loss: 6.1229\n",
      "Epoch 3452/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1382 - val_loss: 6.1871\n",
      "Epoch 3453/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0877 - val_loss: 6.2224\n",
      "Epoch 3454/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1535 - val_loss: 6.1306\n",
      "Epoch 3455/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1267 - val_loss: 6.2688\n",
      "Epoch 3456/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1689 - val_loss: 6.0916\n",
      "Epoch 3457/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5596 - val_loss: 6.3192\n",
      "Epoch 3458/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2583 - val_loss: 6.2595\n",
      "Epoch 3459/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1087 - val_loss: 6.0638\n",
      "Epoch 3460/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0958 - val_loss: 6.2114\n",
      "Epoch 3461/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2101 - val_loss: 6.0948\n",
      "Epoch 3462/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1761 - val_loss: 6.1477\n",
      "Epoch 3463/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1436 - val_loss: 6.2036\n",
      "Epoch 3464/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5015 - val_loss: 6.3401\n",
      "Epoch 3465/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1637 - val_loss: 6.1423\n",
      "Epoch 3466/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0837 - val_loss: 6.0448\n",
      "Epoch 3467/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0692 - val_loss: 6.4824\n",
      "Epoch 3468/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3230 - val_loss: 6.0906\n",
      "Epoch 3469/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1250 - val_loss: 6.5641\n",
      "Epoch 3470/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2990 - val_loss: 6.1004\n",
      "Epoch 3471/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1159 - val_loss: 6.1600\n",
      "Epoch 3472/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1063 - val_loss: 6.1900\n",
      "Epoch 3473/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1498 - val_loss: 6.1123\n",
      "Epoch 3474/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1367 - val_loss: 6.1727\n",
      "Epoch 3475/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1199 - val_loss: 6.1193\n",
      "Epoch 3476/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1487 - val_loss: 6.0925\n",
      "Epoch 3477/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0731 - val_loss: 6.1563\n",
      "Epoch 3478/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0498 - val_loss: 6.0947\n",
      "Epoch 3479/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0826 - val_loss: 6.1049\n",
      "Epoch 3480/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1515 - val_loss: 6.4209\n",
      "Epoch 3481/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2371 - val_loss: 6.0928\n",
      "Epoch 3482/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3481 - val_loss: 6.2177\n",
      "Epoch 3483/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1012 - val_loss: 6.1978\n",
      "Epoch 3484/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.0896 - val_loss: 6.2044\n",
      "Epoch 3485/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.1782 - val_loss: 6.1818\n",
      "Epoch 3486/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2260 - val_loss: 6.2489\n",
      "Epoch 3487/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.1339 - val_loss: 6.0960\n",
      "Epoch 3488/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.0724 - val_loss: 6.0494\n",
      "Epoch 3489/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2479 - val_loss: 6.0768\n",
      "Epoch 3490/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1985 - val_loss: 6.1211\n",
      "Epoch 3491/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1048 - val_loss: 6.1023\n",
      "Epoch 3492/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0566 - val_loss: 6.1018\n",
      "Epoch 3493/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0889 - val_loss: 6.3254\n",
      "Epoch 3494/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3067 - val_loss: 6.1164\n",
      "Epoch 3495/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0377 - val_loss: 6.2062\n",
      "Epoch 3496/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1413 - val_loss: 6.2263\n",
      "Epoch 3497/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0880 - val_loss: 6.3858\n",
      "Epoch 3498/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3575 - val_loss: 6.4093\n",
      "Epoch 3499/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1236 - val_loss: 6.0858\n",
      "Epoch 3500/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3160 - val_loss: 6.4536\n",
      "Epoch 3501/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 56us/step - loss: 5.1778 - val_loss: 6.1799\n",
      "Epoch 3502/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1148 - val_loss: 6.2208\n",
      "Epoch 3503/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0930 - val_loss: 6.0687\n",
      "Epoch 3504/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0790 - val_loss: 6.0514\n",
      "Epoch 3505/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0918 - val_loss: 6.1831\n",
      "Epoch 3506/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1495 - val_loss: 6.1436\n",
      "Epoch 3507/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3502 - val_loss: 6.0430\n",
      "Epoch 3508/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0624 - val_loss: 6.0362\n",
      "Epoch 3509/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0852 - val_loss: 6.0894\n",
      "Epoch 3510/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1372 - val_loss: 6.0514\n",
      "Epoch 3511/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0993 - val_loss: 6.1318\n",
      "Epoch 3512/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0845 - val_loss: 6.0690\n",
      "Epoch 3513/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0859 - val_loss: 6.0934\n",
      "Epoch 3514/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0354 - val_loss: 6.0429\n",
      "Epoch 3515/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0944 - val_loss: 6.5305\n",
      "Epoch 3516/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3853 - val_loss: 7.2186\n",
      "Epoch 3517/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5898 - val_loss: 6.4201\n",
      "Epoch 3518/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1281 - val_loss: 6.0386\n",
      "Epoch 3519/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0881 - val_loss: 6.3416\n",
      "Epoch 3520/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4862 - val_loss: 6.7608\n",
      "Epoch 3521/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1643 - val_loss: 6.1404\n",
      "Epoch 3522/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1788 - val_loss: 6.0973\n",
      "Epoch 3523/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0702 - val_loss: 6.2974\n",
      "Epoch 3524/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1010 - val_loss: 6.0633\n",
      "Epoch 3525/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1172 - val_loss: 6.1617\n",
      "Epoch 3526/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1193 - val_loss: 6.0904\n",
      "Epoch 3527/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.2079 - val_loss: 6.2025\n",
      "Epoch 3528/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0601 - val_loss: 6.3320\n",
      "Epoch 3529/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1060 - val_loss: 6.4026\n",
      "Epoch 3530/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2156 - val_loss: 6.0590\n",
      "Epoch 3531/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1114 - val_loss: 6.0550\n",
      "Epoch 3532/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0730 - val_loss: 6.0554\n",
      "Epoch 3533/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3415 - val_loss: 6.5320\n",
      "Epoch 3534/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2002 - val_loss: 6.2269\n",
      "Epoch 3535/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1636 - val_loss: 6.1624\n",
      "Epoch 3536/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0936 - val_loss: 6.3351\n",
      "Epoch 3537/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2207 - val_loss: 6.0618\n",
      "Epoch 3538/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1514 - val_loss: 6.0826\n",
      "Epoch 3539/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0740 - val_loss: 6.0511\n",
      "Epoch 3540/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1111 - val_loss: 6.7230\n",
      "Epoch 3541/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5579 - val_loss: 6.1246\n",
      "Epoch 3542/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1628 - val_loss: 6.5754\n",
      "Epoch 3543/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4913 - val_loss: 6.5167\n",
      "Epoch 3544/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1866 - val_loss: 6.3805\n",
      "Epoch 3545/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1813 - val_loss: 6.0959\n",
      "Epoch 3546/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1460 - val_loss: 6.8965\n",
      "Epoch 3547/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3027 - val_loss: 6.3353\n",
      "Epoch 3548/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2232 - val_loss: 6.5474\n",
      "Epoch 3549/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2642 - val_loss: 6.4823\n",
      "Epoch 3550/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4225 - val_loss: 6.0926\n",
      "Epoch 3551/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2138 - val_loss: 6.0663\n",
      "Epoch 3552/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1635 - val_loss: 6.2151\n",
      "Epoch 3553/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0671 - val_loss: 6.0614\n",
      "Epoch 3554/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0854 - val_loss: 6.1803\n",
      "Epoch 3555/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0917 - val_loss: 6.2239\n",
      "Epoch 3556/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2089 - val_loss: 6.0319\n",
      "Epoch 3557/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1081 - val_loss: 6.2039\n",
      "Epoch 3558/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1495 - val_loss: 6.4535\n",
      "Epoch 3559/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0706 - val_loss: 6.2736\n",
      "Epoch 3560/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2118 - val_loss: 6.2569\n",
      "Epoch 3561/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0604 - val_loss: 6.1122\n",
      "Epoch 3562/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0589 - val_loss: 6.2898\n",
      "Epoch 3563/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1055 - val_loss: 6.0424\n",
      "Epoch 3564/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0036 - val_loss: 6.2356\n",
      "Epoch 3565/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1827 - val_loss: 6.1748\n",
      "Epoch 3566/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1184 - val_loss: 6.1508\n",
      "Epoch 3567/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2120 - val_loss: 6.1033\n",
      "Epoch 3568/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 5.1795 - val_loss: 6.0690\n",
      "Epoch 3569/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0999 - val_loss: 6.2405\n",
      "Epoch 3570/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.1834 - val_loss: 6.4228\n",
      "Epoch 3571/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0768 - val_loss: 6.0675\n",
      "Epoch 3572/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1395 - val_loss: 5.9957\n",
      "Epoch 3573/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0886 - val_loss: 6.0114\n",
      "Epoch 3574/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1629 - val_loss: 6.0443\n",
      "Epoch 3575/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9879 - val_loss: 6.2754\n",
      "Epoch 3576/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3471 - val_loss: 6.0820\n",
      "Epoch 3577/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 5.1214 - val_loss: 5.9929\n",
      "Epoch 3578/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1084 - val_loss: 6.5538\n",
      "Epoch 3579/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1170 - val_loss: 6.1903\n",
      "Epoch 3580/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0715 - val_loss: 6.0318\n",
      "Epoch 3581/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0382 - val_loss: 6.0207\n",
      "Epoch 3582/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0779 - val_loss: 6.1412\n",
      "Epoch 3583/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1149 - val_loss: 6.0321\n",
      "Epoch 3584/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1422 - val_loss: 6.0209\n",
      "Epoch 3585/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1515 - val_loss: 6.6869\n",
      "Epoch 3586/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3137 - val_loss: 6.0407\n",
      "Epoch 3587/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1291 - val_loss: 6.1801\n",
      "Epoch 3588/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.2668 - val_loss: 6.2979\n",
      "Epoch 3589/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1181 - val_loss: 6.5723\n",
      "Epoch 3590/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3475 - val_loss: 6.0568\n",
      "Epoch 3591/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1135 - val_loss: 6.0904\n",
      "Epoch 3592/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0098 - val_loss: 6.0410\n",
      "Epoch 3593/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9955 - val_loss: 6.5941\n",
      "Epoch 3594/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2460 - val_loss: 5.9723\n",
      "Epoch 3595/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0557 - val_loss: 5.9698\n",
      "Epoch 3596/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.1725 - val_loss: 6.4139\n",
      "Epoch 3597/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0779 - val_loss: 5.9900\n",
      "Epoch 3598/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0326 - val_loss: 6.0074\n",
      "Epoch 3599/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0468 - val_loss: 6.2236\n",
      "Epoch 3600/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0875 - val_loss: 6.0652\n",
      "Epoch 3601/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0605 - val_loss: 6.2983\n",
      "Epoch 3602/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1773 - val_loss: 6.1770\n",
      "Epoch 3603/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.2378 - val_loss: 6.0463\n",
      "Epoch 3604/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0989 - val_loss: 6.0336\n",
      "Epoch 3605/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0991 - val_loss: 6.0610\n",
      "Epoch 3606/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1466 - val_loss: 6.3239\n",
      "Epoch 3607/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2328 - val_loss: 6.1453\n",
      "Epoch 3608/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1473 - val_loss: 6.4261\n",
      "Epoch 3609/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0442 - val_loss: 6.0832\n",
      "Epoch 3610/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2535 - val_loss: 6.2534\n",
      "Epoch 3611/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1029 - val_loss: 6.5535\n",
      "Epoch 3612/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3540 - val_loss: 6.3465\n",
      "Epoch 3613/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1433 - val_loss: 6.4860\n",
      "Epoch 3614/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3730 - val_loss: 6.3458\n",
      "Epoch 3615/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2210 - val_loss: 6.3855\n",
      "Epoch 3616/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1501 - val_loss: 6.0107\n",
      "Epoch 3617/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1568 - val_loss: 6.1881\n",
      "Epoch 3618/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2004 - val_loss: 6.0285\n",
      "Epoch 3619/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1717 - val_loss: 6.1799\n",
      "Epoch 3620/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0906 - val_loss: 6.2629\n",
      "Epoch 3621/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2579 - val_loss: 6.1031\n",
      "Epoch 3622/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0513 - val_loss: 6.1896\n",
      "Epoch 3623/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.1886 - val_loss: 5.9837\n",
      "Epoch 3624/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9866 - val_loss: 6.1262\n",
      "Epoch 3625/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0467 - val_loss: 6.1418\n",
      "Epoch 3626/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1033 - val_loss: 6.1676\n",
      "Epoch 3627/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1059 - val_loss: 6.0344\n",
      "Epoch 3628/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0530 - val_loss: 6.1172\n",
      "Epoch 3629/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0280 - val_loss: 6.1458\n",
      "Epoch 3630/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0522 - val_loss: 6.0332\n",
      "Epoch 3631/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9952 - val_loss: 6.1114\n",
      "Epoch 3632/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0171 - val_loss: 6.0596\n",
      "Epoch 3633/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4279 - val_loss: 6.2294\n",
      "Epoch 3634/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1803 - val_loss: 6.0327\n",
      "Epoch 3635/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1123 - val_loss: 6.0655\n",
      "Epoch 3636/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9736 - val_loss: 6.0795\n",
      "Epoch 3637/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0073 - val_loss: 6.0366\n",
      "Epoch 3638/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0958 - val_loss: 6.2111\n",
      "Epoch 3639/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1300 - val_loss: 5.9880\n",
      "Epoch 3640/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9995 - val_loss: 6.0312\n",
      "Epoch 3641/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0841 - val_loss: 6.1799\n",
      "Epoch 3642/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0264 - val_loss: 6.1493\n",
      "Epoch 3643/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1058 - val_loss: 6.0320\n",
      "Epoch 3644/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1619 - val_loss: 6.2467\n",
      "Epoch 3645/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2822 - val_loss: 6.0357\n",
      "Epoch 3646/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9858 - val_loss: 6.0513\n",
      "Epoch 3647/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0344 - val_loss: 6.0412\n",
      "Epoch 3648/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1396 - val_loss: 6.1440\n",
      "Epoch 3649/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0383 - val_loss: 6.1469\n",
      "Epoch 3650/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1608 - val_loss: 6.1259\n",
      "Epoch 3651/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0685 - val_loss: 6.1415\n",
      "Epoch 3652/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0581 - val_loss: 6.0224\n",
      "Epoch 3653/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.1307 - val_loss: 6.0956\n",
      "Epoch 3654/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.0151 - val_loss: 6.0250\n",
      "Epoch 3655/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0716 - val_loss: 6.0052\n",
      "Epoch 3656/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0574 - val_loss: 6.2630\n",
      "Epoch 3657/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2036 - val_loss: 6.0517\n",
      "Epoch 3658/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0089 - val_loss: 6.0257\n",
      "Epoch 3659/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9797 - val_loss: 6.0121\n",
      "Epoch 3660/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0614 - val_loss: 5.9711\n",
      "Epoch 3661/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0274 - val_loss: 6.0188\n",
      "Epoch 3662/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9919 - val_loss: 6.0005\n",
      "Epoch 3663/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1752 - val_loss: 7.2384\n",
      "Epoch 3664/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2902 - val_loss: 6.0177\n",
      "Epoch 3665/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9907 - val_loss: 6.3171\n",
      "Epoch 3666/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0860 - val_loss: 6.0871\n",
      "Epoch 3667/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0023 - val_loss: 6.0646\n",
      "Epoch 3668/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0545 - val_loss: 5.9503\n",
      "Epoch 3669/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0835 - val_loss: 5.9721\n",
      "Epoch 3670/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9557 - val_loss: 6.0186\n",
      "Epoch 3671/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0503 - val_loss: 6.1162\n",
      "Epoch 3672/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0297 - val_loss: 5.9570\n",
      "Epoch 3673/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0499 - val_loss: 6.0775\n",
      "Epoch 3674/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0520 - val_loss: 5.9605\n",
      "Epoch 3675/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0268 - val_loss: 6.3966\n",
      "Epoch 3676/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4562 - val_loss: 6.4472\n",
      "Epoch 3677/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0745 - val_loss: 5.9633\n",
      "Epoch 3678/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0262 - val_loss: 6.1452\n",
      "Epoch 3679/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2533 - val_loss: 5.9749\n",
      "Epoch 3680/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2076 - val_loss: 6.1502\n",
      "Epoch 3681/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0795 - val_loss: 6.0268\n",
      "Epoch 3682/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0289 - val_loss: 5.9683\n",
      "Epoch 3683/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.0715 - val_loss: 5.9777\n",
      "Epoch 3684/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0589 - val_loss: 6.1983\n",
      "Epoch 3685/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0310 - val_loss: 5.9875\n",
      "Epoch 3686/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1222 - val_loss: 5.9687\n",
      "Epoch 3687/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9931 - val_loss: 5.9906\n",
      "Epoch 3688/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0241 - val_loss: 5.9946\n",
      "Epoch 3689/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0556 - val_loss: 5.9984\n",
      "Epoch 3690/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0030 - val_loss: 6.1035\n",
      "Epoch 3691/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0261 - val_loss: 5.9580\n",
      "Epoch 3692/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9991 - val_loss: 6.1180\n",
      "Epoch 3693/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9996 - val_loss: 5.9705\n",
      "Epoch 3694/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2194 - val_loss: 6.1519\n",
      "Epoch 3695/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0572 - val_loss: 5.9525\n",
      "Epoch 3696/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0042 - val_loss: 5.9402\n",
      "Epoch 3697/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1658 - val_loss: 6.0684\n",
      "Epoch 3698/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0526 - val_loss: 6.0351\n",
      "Epoch 3699/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9899 - val_loss: 6.4209\n",
      "Epoch 3700/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0567 - val_loss: 6.4887\n",
      "Epoch 3701/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2184 - val_loss: 5.9516\n",
      "Epoch 3702/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0644 - val_loss: 6.0474\n",
      "Epoch 3703/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1518 - val_loss: 6.0404\n",
      "Epoch 3704/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1712 - val_loss: 5.9942\n",
      "Epoch 3705/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0313 - val_loss: 6.0185\n",
      "Epoch 3706/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4110 - val_loss: 6.6908\n",
      "Epoch 3707/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2046 - val_loss: 5.9372\n",
      "Epoch 3708/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0130 - val_loss: 6.1698\n",
      "Epoch 3709/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1079 - val_loss: 5.9827\n",
      "Epoch 3710/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0067 - val_loss: 6.0031\n",
      "Epoch 3711/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0285 - val_loss: 6.1231\n",
      "Epoch 3712/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0354 - val_loss: 5.9789\n",
      "Epoch 3713/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0027 - val_loss: 6.0090\n",
      "Epoch 3714/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9528 - val_loss: 6.1065\n",
      "Epoch 3715/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9830 - val_loss: 5.9877\n",
      "Epoch 3716/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0499 - val_loss: 6.0627\n",
      "Epoch 3717/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9594 - val_loss: 5.9590\n",
      "Epoch 3718/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1400 - val_loss: 6.3065\n",
      "Epoch 3719/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0316 - val_loss: 6.1765\n",
      "Epoch 3720/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9292 - val_loss: 6.4770\n",
      "Epoch 3721/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3710 - val_loss: 6.0132\n",
      "Epoch 3722/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1774 - val_loss: 6.0784\n",
      "Epoch 3723/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0848 - val_loss: 5.9751\n",
      "Epoch 3724/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0989 - val_loss: 5.9343\n",
      "Epoch 3725/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9930 - val_loss: 5.9952\n",
      "Epoch 3726/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1466 - val_loss: 6.3123\n",
      "Epoch 3727/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1208 - val_loss: 6.0883\n",
      "Epoch 3728/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1457 - val_loss: 5.9412\n",
      "Epoch 3729/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 5.0899 - val_loss: 6.0196\n",
      "Epoch 3730/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9880 - val_loss: 6.1445\n",
      "Epoch 3731/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0143 - val_loss: 6.0719\n",
      "Epoch 3732/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9935 - val_loss: 5.9942\n",
      "Epoch 3733/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9653 - val_loss: 6.2850\n",
      "Epoch 3734/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0250 - val_loss: 6.0243\n",
      "Epoch 3735/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9586 - val_loss: 5.9727\n",
      "Epoch 3736/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0415 - val_loss: 5.9663\n",
      "Epoch 3737/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0081 - val_loss: 6.0083\n",
      "Epoch 3738/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0180 - val_loss: 6.0780\n",
      "Epoch 3739/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0215 - val_loss: 5.9616\n",
      "Epoch 3740/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9923 - val_loss: 6.0184\n",
      "Epoch 3741/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1582 - val_loss: 6.4892\n",
      "Epoch 3742/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.5384 - val_loss: 6.1859\n",
      "Epoch 3743/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0677 - val_loss: 5.9789\n",
      "Epoch 3744/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0598 - val_loss: 5.9796\n",
      "Epoch 3745/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9877 - val_loss: 6.0712\n",
      "Epoch 3746/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0984 - val_loss: 6.0076\n",
      "Epoch 3747/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9642 - val_loss: 5.9332\n",
      "Epoch 3748/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9839 - val_loss: 6.5460\n",
      "Epoch 3749/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0489 - val_loss: 5.9592\n",
      "Epoch 3750/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9791 - val_loss: 6.1346\n",
      "Epoch 3751/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0067 - val_loss: 6.3831\n",
      "Epoch 3752/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1578 - val_loss: 5.9743\n",
      "Epoch 3753/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0048 - val_loss: 5.9653\n",
      "Epoch 3754/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9991 - val_loss: 5.9526\n",
      "Epoch 3755/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0382 - val_loss: 6.1609\n",
      "Epoch 3756/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9730 - val_loss: 6.1878\n",
      "Epoch 3757/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9920 - val_loss: 5.9398\n",
      "Epoch 3758/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9318 - val_loss: 6.0122\n",
      "Epoch 3759/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1700 - val_loss: 6.3606\n",
      "Epoch 3760/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0939 - val_loss: 6.3691\n",
      "Epoch 3761/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3859 - val_loss: 6.0519\n",
      "Epoch 3762/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9439 - val_loss: 5.9494\n",
      "Epoch 3763/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0014 - val_loss: 6.8001\n",
      "Epoch 3764/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2360 - val_loss: 5.9235\n",
      "Epoch 3765/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9708 - val_loss: 6.1811\n",
      "Epoch 3766/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0422 - val_loss: 5.9836\n",
      "Epoch 3767/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0747 - val_loss: 6.0112\n",
      "Epoch 3768/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0435 - val_loss: 6.3724\n",
      "Epoch 3769/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1369 - val_loss: 6.0610\n",
      "Epoch 3770/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0294 - val_loss: 5.9687\n",
      "Epoch 3771/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0613 - val_loss: 5.9436\n",
      "Epoch 3772/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0016 - val_loss: 5.9130\n",
      "Epoch 3773/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9561 - val_loss: 5.8892\n",
      "Epoch 3774/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9641 - val_loss: 5.9336\n",
      "Epoch 3775/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0793 - val_loss: 5.9753\n",
      "Epoch 3776/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0449 - val_loss: 5.9431\n",
      "Epoch 3777/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1555 - val_loss: 6.3257\n",
      "Epoch 3778/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0449 - val_loss: 5.9638\n",
      "Epoch 3779/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0970 - val_loss: 5.9360\n",
      "Epoch 3780/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0892 - val_loss: 5.9732\n",
      "Epoch 3781/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9749 - val_loss: 6.2298\n",
      "Epoch 3782/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2903 - val_loss: 6.1490\n",
      "Epoch 3783/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0532 - val_loss: 5.9569\n",
      "Epoch 3784/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0416 - val_loss: 5.9933\n",
      "Epoch 3785/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9436 - val_loss: 6.1722\n",
      "Epoch 3786/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0672 - val_loss: 5.9153\n",
      "Epoch 3787/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9579 - val_loss: 5.9070\n",
      "Epoch 3788/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1405 - val_loss: 6.2083\n",
      "Epoch 3789/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9691 - val_loss: 6.2898\n",
      "Epoch 3790/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1234 - val_loss: 5.9650\n",
      "Epoch 3791/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0858 - val_loss: 5.9775\n",
      "Epoch 3792/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9384 - val_loss: 5.9434\n",
      "Epoch 3793/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1004 - val_loss: 6.0259\n",
      "Epoch 3794/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9836 - val_loss: 5.9246\n",
      "Epoch 3795/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9699 - val_loss: 6.0155\n",
      "Epoch 3796/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1213 - val_loss: 5.9172\n",
      "Epoch 3797/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0009 - val_loss: 5.9396\n",
      "Epoch 3798/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4315 - val_loss: 5.9986\n",
      "Epoch 3799/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0883 - val_loss: 5.9849\n",
      "Epoch 3800/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1991 - val_loss: 5.9715\n",
      "Epoch 3801/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.4120 - val_loss: 5.9892\n",
      "Epoch 3802/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1079 - val_loss: 6.4519\n",
      "Epoch 3803/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0623 - val_loss: 6.3183\n",
      "Epoch 3804/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1787 - val_loss: 5.9375\n",
      "Epoch 3805/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 5.0204 - val_loss: 5.9564\n",
      "Epoch 3806/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9544 - val_loss: 6.0198\n",
      "Epoch 3807/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1027 - val_loss: 6.1656\n",
      "Epoch 3808/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0513 - val_loss: 5.9527\n",
      "Epoch 3809/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0108 - val_loss: 6.3575\n",
      "Epoch 3810/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.3278 - val_loss: 6.4674\n",
      "Epoch 3811/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1682 - val_loss: 6.0116\n",
      "Epoch 3812/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9327 - val_loss: 6.3569\n",
      "Epoch 3813/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2918 - val_loss: 6.4086\n",
      "Epoch 3814/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9689 - val_loss: 5.9211\n",
      "Epoch 3815/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9510 - val_loss: 5.9278\n",
      "Epoch 3816/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0297 - val_loss: 6.1315\n",
      "Epoch 3817/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0928 - val_loss: 6.0568\n",
      "Epoch 3818/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1513 - val_loss: 5.9390\n",
      "Epoch 3819/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1246 - val_loss: 6.0710\n",
      "Epoch 3820/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0697 - val_loss: 6.0503\n",
      "Epoch 3821/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0393 - val_loss: 5.9488\n",
      "Epoch 3822/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9555 - val_loss: 5.9606\n",
      "Epoch 3823/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9385 - val_loss: 5.9604\n",
      "Epoch 3824/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0173 - val_loss: 5.9000\n",
      "Epoch 3825/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1533 - val_loss: 6.2427\n",
      "Epoch 3826/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0133 - val_loss: 5.9929\n",
      "Epoch 3827/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9264 - val_loss: 5.9590\n",
      "Epoch 3828/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9484 - val_loss: 6.3375\n",
      "Epoch 3829/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9967 - val_loss: 5.9799\n",
      "Epoch 3830/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9714 - val_loss: 5.9797\n",
      "Epoch 3831/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0695 - val_loss: 6.2279\n",
      "Epoch 3832/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9749 - val_loss: 5.9767\n",
      "Epoch 3833/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1396 - val_loss: 6.4685\n",
      "Epoch 3834/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0316 - val_loss: 5.9663\n",
      "Epoch 3835/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9750 - val_loss: 5.9563\n",
      "Epoch 3836/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9404 - val_loss: 6.0066\n",
      "Epoch 3837/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9459 - val_loss: 5.9181\n",
      "Epoch 3838/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0790 - val_loss: 6.0742\n",
      "Epoch 3839/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0228 - val_loss: 5.9551\n",
      "Epoch 3840/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.0245 - val_loss: 6.2138\n",
      "Epoch 3841/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1886 - val_loss: 6.3799\n",
      "Epoch 3842/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1475 - val_loss: 6.5896\n",
      "Epoch 3843/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0786 - val_loss: 6.0259\n",
      "Epoch 3844/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3105 - val_loss: 6.2313\n",
      "Epoch 3845/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0942 - val_loss: 5.8832\n",
      "Epoch 3846/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9111 - val_loss: 6.2575\n",
      "Epoch 3847/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9957 - val_loss: 6.0324\n",
      "Epoch 3848/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9423 - val_loss: 5.9174\n",
      "Epoch 3849/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9946 - val_loss: 6.0275\n",
      "Epoch 3850/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9512 - val_loss: 5.9636\n",
      "Epoch 3851/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9232 - val_loss: 5.9215\n",
      "Epoch 3852/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8898 - val_loss: 6.2500\n",
      "Epoch 3853/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9633 - val_loss: 6.1403\n",
      "Epoch 3854/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3564 - val_loss: 5.9364\n",
      "Epoch 3855/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9965 - val_loss: 6.0420\n",
      "Epoch 3856/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1407 - val_loss: 6.1400\n",
      "Epoch 3857/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0333 - val_loss: 6.2709\n",
      "Epoch 3858/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.2283 - val_loss: 5.9764\n",
      "Epoch 3859/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9074 - val_loss: 5.9215\n",
      "Epoch 3860/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9093 - val_loss: 6.1284\n",
      "Epoch 3861/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9994 - val_loss: 5.8866\n",
      "Epoch 3862/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0020 - val_loss: 5.9064\n",
      "Epoch 3863/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9940 - val_loss: 6.1602\n",
      "Epoch 3864/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1574 - val_loss: 6.2941\n",
      "Epoch 3865/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 5.4050 - val_loss: 5.9223\n",
      "Epoch 3866/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.5545 - val_loss: 5.9549\n",
      "Epoch 3867/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9448 - val_loss: 5.9051\n",
      "Epoch 3868/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.9428 - val_loss: 5.8883\n",
      "Epoch 3869/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9709 - val_loss: 6.1074\n",
      "Epoch 3870/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9970 - val_loss: 5.9125\n",
      "Epoch 3871/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9446 - val_loss: 5.9812\n",
      "Epoch 3872/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9905 - val_loss: 5.9305\n",
      "Epoch 3873/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2723 - val_loss: 6.6812\n",
      "Epoch 3874/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4738 - val_loss: 6.3356\n",
      "Epoch 3875/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0210 - val_loss: 5.9268\n",
      "Epoch 3876/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9513 - val_loss: 6.0168\n",
      "Epoch 3877/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9673 - val_loss: 5.9110\n",
      "Epoch 3878/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1126 - val_loss: 6.0155\n",
      "Epoch 3879/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0036 - val_loss: 6.1635\n",
      "Epoch 3880/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0211 - val_loss: 6.0158\n",
      "Epoch 3881/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 5.1290 - val_loss: 5.8876\n",
      "Epoch 3882/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0368 - val_loss: 5.9667\n",
      "Epoch 3883/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9061 - val_loss: 6.0133\n",
      "Epoch 3884/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9497 - val_loss: 5.9160\n",
      "Epoch 3885/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9692 - val_loss: 6.0800\n",
      "Epoch 3886/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0441 - val_loss: 5.8622\n",
      "Epoch 3887/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9419 - val_loss: 5.9798\n",
      "Epoch 3888/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3040 - val_loss: 6.4187\n",
      "Epoch 3889/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1627 - val_loss: 5.8870\n",
      "Epoch 3890/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9895 - val_loss: 5.9942\n",
      "Epoch 3891/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9040 - val_loss: 5.9902\n",
      "Epoch 3892/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8774 - val_loss: 5.8875\n",
      "Epoch 3893/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8954 - val_loss: 5.9967\n",
      "Epoch 3894/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9251 - val_loss: 6.2510\n",
      "Epoch 3895/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0437 - val_loss: 6.5256\n",
      "Epoch 3896/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0066 - val_loss: 6.0167\n",
      "Epoch 3897/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9866 - val_loss: 6.0024\n",
      "Epoch 3898/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9084 - val_loss: 5.8808\n",
      "Epoch 3899/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9558 - val_loss: 5.9736\n",
      "Epoch 3900/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9029 - val_loss: 5.9163\n",
      "Epoch 3901/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9265 - val_loss: 5.9910\n",
      "Epoch 3902/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9062 - val_loss: 6.3313\n",
      "Epoch 3903/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9771 - val_loss: 5.9166\n",
      "Epoch 3904/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9602 - val_loss: 6.5315\n",
      "Epoch 3905/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0914 - val_loss: 5.8828\n",
      "Epoch 3906/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 5.0536 - val_loss: 6.2418\n",
      "Epoch 3907/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.0289 - val_loss: 6.0747\n",
      "Epoch 3908/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.9861 - val_loss: 6.2226\n",
      "Epoch 3909/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9932 - val_loss: 5.9430\n",
      "Epoch 3910/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9899 - val_loss: 5.9024\n",
      "Epoch 3911/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9379 - val_loss: 6.0578\n",
      "Epoch 3912/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8913 - val_loss: 6.0782\n",
      "Epoch 3913/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3418 - val_loss: 6.6588\n",
      "Epoch 3914/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0196 - val_loss: 5.9979\n",
      "Epoch 3915/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9765 - val_loss: 6.0525\n",
      "Epoch 3916/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9084 - val_loss: 6.0281\n",
      "Epoch 3917/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9123 - val_loss: 5.9257\n",
      "Epoch 3918/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8862 - val_loss: 6.0020\n",
      "Epoch 3919/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9227 - val_loss: 5.9543\n",
      "Epoch 3920/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9526 - val_loss: 5.8538\n",
      "Epoch 3921/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9442 - val_loss: 5.8630\n",
      "Epoch 3922/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9866 - val_loss: 5.8743\n",
      "Epoch 3923/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9195 - val_loss: 5.9280\n",
      "Epoch 3924/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0373 - val_loss: 6.0374\n",
      "Epoch 3925/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0143 - val_loss: 5.9889\n",
      "Epoch 3926/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9838 - val_loss: 6.3468\n",
      "Epoch 3927/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0599 - val_loss: 5.9153\n",
      "Epoch 3928/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1253 - val_loss: 6.0058\n",
      "Epoch 3929/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9586 - val_loss: 5.8550\n",
      "Epoch 3930/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9660 - val_loss: 5.8795\n",
      "Epoch 3931/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9471 - val_loss: 6.1152\n",
      "Epoch 3932/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1048 - val_loss: 6.2053\n",
      "Epoch 3933/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9283 - val_loss: 6.1009\n",
      "Epoch 3934/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8856 - val_loss: 5.9577\n",
      "Epoch 3935/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8992 - val_loss: 5.9384\n",
      "Epoch 3936/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9017 - val_loss: 6.0848\n",
      "Epoch 3937/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2389 - val_loss: 6.4552\n",
      "Epoch 3938/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9258 - val_loss: 5.9299\n",
      "Epoch 3939/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8836 - val_loss: 5.9144\n",
      "Epoch 3940/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9022 - val_loss: 5.9385\n",
      "Epoch 3941/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0775 - val_loss: 6.1556\n",
      "Epoch 3942/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9217 - val_loss: 6.0354\n",
      "Epoch 3943/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9291 - val_loss: 5.8978\n",
      "Epoch 3944/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8890 - val_loss: 5.8630\n",
      "Epoch 3945/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9001 - val_loss: 5.9530\n",
      "Epoch 3946/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8822 - val_loss: 5.8652\n",
      "Epoch 3947/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9597 - val_loss: 5.9949\n",
      "Epoch 3948/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9333 - val_loss: 5.9216\n",
      "Epoch 3949/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0318 - val_loss: 5.8459\n",
      "Epoch 3950/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9716 - val_loss: 6.2420\n",
      "Epoch 3951/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9206 - val_loss: 6.0963\n",
      "Epoch 3952/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.9341 - val_loss: 5.9739\n",
      "Epoch 3953/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8604 - val_loss: 5.9970\n",
      "Epoch 3954/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9538 - val_loss: 6.0246\n",
      "Epoch 3955/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0433 - val_loss: 6.2576\n",
      "Epoch 3956/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9604 - val_loss: 6.1024\n",
      "Epoch 3957/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 4.9917 - val_loss: 5.9085\n",
      "Epoch 3958/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9641 - val_loss: 5.9766\n",
      "Epoch 3959/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8783 - val_loss: 5.9008\n",
      "Epoch 3960/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9027 - val_loss: 6.4214\n",
      "Epoch 3961/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3367 - val_loss: 6.6116\n",
      "Epoch 3962/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1373 - val_loss: 6.0786\n",
      "Epoch 3963/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0123 - val_loss: 6.3443\n",
      "Epoch 3964/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9257 - val_loss: 6.0318\n",
      "Epoch 3965/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9486 - val_loss: 5.9416\n",
      "Epoch 3966/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0181 - val_loss: 5.9152\n",
      "Epoch 3967/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8734 - val_loss: 5.9080\n",
      "Epoch 3968/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0498 - val_loss: 5.9913\n",
      "Epoch 3969/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9255 - val_loss: 5.9989\n",
      "Epoch 3970/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8642 - val_loss: 5.8937\n",
      "Epoch 3971/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8825 - val_loss: 5.9436\n",
      "Epoch 3972/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0485 - val_loss: 6.4144\n",
      "Epoch 3973/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9419 - val_loss: 5.9286\n",
      "Epoch 3974/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9012 - val_loss: 5.9628\n",
      "Epoch 3975/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0237 - val_loss: 5.8761\n",
      "Epoch 3976/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9596 - val_loss: 5.9419\n",
      "Epoch 3977/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9468 - val_loss: 5.9374\n",
      "Epoch 3978/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0714 - val_loss: 5.8850\n",
      "Epoch 3979/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1849 - val_loss: 6.1552\n",
      "Epoch 3980/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9397 - val_loss: 5.9703\n",
      "Epoch 3981/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3187 - val_loss: 6.2033\n",
      "Epoch 3982/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9277 - val_loss: 5.9537\n",
      "Epoch 3983/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8839 - val_loss: 6.4062\n",
      "Epoch 3984/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0795 - val_loss: 5.9138\n",
      "Epoch 3985/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9014 - val_loss: 5.8679\n",
      "Epoch 3986/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0567 - val_loss: 6.2898\n",
      "Epoch 3987/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1312 - val_loss: 5.8991\n",
      "Epoch 3988/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8483 - val_loss: 5.9477\n",
      "Epoch 3989/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8628 - val_loss: 6.1367\n",
      "Epoch 3990/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9364 - val_loss: 5.8882\n",
      "Epoch 3991/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9302 - val_loss: 5.9202\n",
      "Epoch 3992/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9347 - val_loss: 6.2880\n",
      "Epoch 3993/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9246 - val_loss: 5.9011\n",
      "Epoch 3994/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9132 - val_loss: 5.8871\n",
      "Epoch 3995/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9032 - val_loss: 6.0754\n",
      "Epoch 3996/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0478 - val_loss: 5.8851\n",
      "Epoch 3997/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8645 - val_loss: 6.3040\n",
      "Epoch 3998/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0637 - val_loss: 6.0261\n",
      "Epoch 3999/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1138 - val_loss: 6.3182\n",
      "Epoch 4000/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9297 - val_loss: 6.1473\n",
      "Epoch 4001/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0014 - val_loss: 5.8890\n",
      "Epoch 4002/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.8648 - val_loss: 6.2092\n",
      "Epoch 4003/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.1728 - val_loss: 6.4550\n",
      "Epoch 4004/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0644 - val_loss: 6.3534\n",
      "Epoch 4005/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9533 - val_loss: 5.8791\n",
      "Epoch 4006/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9842 - val_loss: 5.8474\n",
      "Epoch 4007/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9267 - val_loss: 5.9611\n",
      "Epoch 4008/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8940 - val_loss: 6.0490\n",
      "Epoch 4009/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0785 - val_loss: 6.1031\n",
      "Epoch 4010/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9277 - val_loss: 5.8863\n",
      "Epoch 4011/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8966 - val_loss: 5.8452\n",
      "Epoch 4012/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1951 - val_loss: 6.0145\n",
      "Epoch 4013/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0275 - val_loss: 6.6460\n",
      "Epoch 4014/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0644 - val_loss: 5.8495\n",
      "Epoch 4015/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8553 - val_loss: 5.9143\n",
      "Epoch 4016/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9172 - val_loss: 5.9707\n",
      "Epoch 4017/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8604 - val_loss: 6.1983\n",
      "Epoch 4018/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8956 - val_loss: 5.8488\n",
      "Epoch 4019/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8561 - val_loss: 6.6233\n",
      "Epoch 4020/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2403 - val_loss: 5.9463\n",
      "Epoch 4021/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8582 - val_loss: 5.8861\n",
      "Epoch 4022/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8839 - val_loss: 5.8990\n",
      "Epoch 4023/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8788 - val_loss: 6.2504\n",
      "Epoch 4024/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0218 - val_loss: 5.8574\n",
      "Epoch 4025/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1181 - val_loss: 5.9802\n",
      "Epoch 4026/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8604 - val_loss: 5.8723\n",
      "Epoch 4027/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8309 - val_loss: 6.2732\n",
      "Epoch 4028/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9761 - val_loss: 5.9432\n",
      "Epoch 4029/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8675 - val_loss: 5.8617\n",
      "Epoch 4030/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0096 - val_loss: 5.8553\n",
      "Epoch 4031/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0216 - val_loss: 6.1615\n",
      "Epoch 4032/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9529 - val_loss: 5.9787\n",
      "Epoch 4033/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.9042 - val_loss: 5.8926\n",
      "Epoch 4034/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9717 - val_loss: 5.8771\n",
      "Epoch 4035/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9682 - val_loss: 6.1029\n",
      "Epoch 4036/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9477 - val_loss: 6.1041\n",
      "Epoch 4037/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9404 - val_loss: 6.5951\n",
      "Epoch 4038/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9577 - val_loss: 5.9024\n",
      "Epoch 4039/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8849 - val_loss: 5.9951\n",
      "Epoch 4040/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9681 - val_loss: 5.9601\n",
      "Epoch 4041/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8379 - val_loss: 6.1661\n",
      "Epoch 4042/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8802 - val_loss: 6.0211\n",
      "Epoch 4043/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9107 - val_loss: 5.9862\n",
      "Epoch 4044/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0715 - val_loss: 6.0445\n",
      "Epoch 4045/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0250 - val_loss: 6.1831\n",
      "Epoch 4046/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9756 - val_loss: 6.2423\n",
      "Epoch 4047/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9665 - val_loss: 5.8545\n",
      "Epoch 4048/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8491 - val_loss: 5.9066\n",
      "Epoch 4049/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9587 - val_loss: 6.2353\n",
      "Epoch 4050/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5354 - val_loss: 7.1163\n",
      "Epoch 4051/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2347 - val_loss: 6.0383\n",
      "Epoch 4052/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9209 - val_loss: 6.2153\n",
      "Epoch 4053/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1204 - val_loss: 5.9040\n",
      "Epoch 4054/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9427 - val_loss: 5.8596\n",
      "Epoch 4055/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8715 - val_loss: 5.9065\n",
      "Epoch 4056/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8334 - val_loss: 5.8493\n",
      "Epoch 4057/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8656 - val_loss: 6.5301\n",
      "Epoch 4058/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0634 - val_loss: 6.4341\n",
      "Epoch 4059/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0583 - val_loss: 5.8434\n",
      "Epoch 4060/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8749 - val_loss: 5.8540\n",
      "Epoch 4061/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9099 - val_loss: 5.8399\n",
      "Epoch 4062/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8192 - val_loss: 5.8931\n",
      "Epoch 4063/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8723 - val_loss: 5.9651\n",
      "Epoch 4064/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0123 - val_loss: 5.8739\n",
      "Epoch 4065/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8577 - val_loss: 5.8771\n",
      "Epoch 4066/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.9815 - val_loss: 5.9977\n",
      "Epoch 4067/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8970 - val_loss: 5.9395\n",
      "Epoch 4068/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8336 - val_loss: 5.8803\n",
      "Epoch 4069/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8132 - val_loss: 5.9559\n",
      "Epoch 4070/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1038 - val_loss: 5.9317\n",
      "Epoch 4071/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9214 - val_loss: 6.6306\n",
      "Epoch 4072/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0340 - val_loss: 6.1015\n",
      "Epoch 4073/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9321 - val_loss: 6.1820\n",
      "Epoch 4074/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9355 - val_loss: 5.9416\n",
      "Epoch 4075/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0289 - val_loss: 6.1298\n",
      "Epoch 4076/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1989 - val_loss: 6.1091\n",
      "Epoch 4077/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8768 - val_loss: 5.9522\n",
      "Epoch 4078/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0301 - val_loss: 5.9349\n",
      "Epoch 4079/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8555 - val_loss: 5.8414\n",
      "Epoch 4080/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8171 - val_loss: 5.9462\n",
      "Epoch 4081/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0225 - val_loss: 6.1855\n",
      "Epoch 4082/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0525 - val_loss: 5.8576\n",
      "Epoch 4083/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8080 - val_loss: 6.1997\n",
      "Epoch 4084/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9816 - val_loss: 5.9132\n",
      "Epoch 4085/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8636 - val_loss: 5.8444\n",
      "Epoch 4086/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8125 - val_loss: 5.8284\n",
      "Epoch 4087/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8299 - val_loss: 5.8299\n",
      "Epoch 4088/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9260 - val_loss: 5.8066\n",
      "Epoch 4089/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8306 - val_loss: 6.3351\n",
      "Epoch 4090/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0675 - val_loss: 6.0406\n",
      "Epoch 4091/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1464 - val_loss: 6.3620\n",
      "Epoch 4092/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1884 - val_loss: 5.9289\n",
      "Epoch 4093/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8685 - val_loss: 5.8668\n",
      "Epoch 4094/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9280 - val_loss: 6.0690\n",
      "Epoch 4095/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8660 - val_loss: 6.0074\n",
      "Epoch 4096/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8780 - val_loss: 5.9539\n",
      "Epoch 4097/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0285 - val_loss: 5.8331\n",
      "Epoch 4098/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9689 - val_loss: 5.8307\n",
      "Epoch 4099/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8178 - val_loss: 6.1054\n",
      "Epoch 4100/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8480 - val_loss: 5.9376\n",
      "Epoch 4101/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9469 - val_loss: 5.8678\n",
      "Epoch 4102/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9047 - val_loss: 5.8071\n",
      "Epoch 4103/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8232 - val_loss: 6.0463\n",
      "Epoch 4104/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1364 - val_loss: 5.8908\n",
      "Epoch 4105/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1306 - val_loss: 7.2083\n",
      "Epoch 4106/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.5218 - val_loss: 6.4154\n",
      "Epoch 4107/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2466 - val_loss: 6.3497\n",
      "Epoch 4108/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9412 - val_loss: 6.1168\n",
      "Epoch 4109/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.9183 - val_loss: 5.8781\n",
      "Epoch 4110/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8575 - val_loss: 5.8399\n",
      "Epoch 4111/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8315 - val_loss: 5.9775\n",
      "Epoch 4112/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8267 - val_loss: 5.8337\n",
      "Epoch 4113/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8156 - val_loss: 5.8449\n",
      "Epoch 4114/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8718 - val_loss: 5.8507\n",
      "Epoch 4115/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9308 - val_loss: 5.8562\n",
      "Epoch 4116/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8918 - val_loss: 5.9825\n",
      "Epoch 4117/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8916 - val_loss: 5.8708\n",
      "Epoch 4118/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8809 - val_loss: 5.9189\n",
      "Epoch 4119/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8633 - val_loss: 6.0954\n",
      "Epoch 4120/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9380 - val_loss: 5.8836\n",
      "Epoch 4121/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8719 - val_loss: 5.9481\n",
      "Epoch 4122/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8560 - val_loss: 6.0471\n",
      "Epoch 4123/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9798 - val_loss: 6.1472\n",
      "Epoch 4124/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0320 - val_loss: 6.0197\n",
      "Epoch 4125/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8739 - val_loss: 5.9594\n",
      "Epoch 4126/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9423 - val_loss: 5.8450\n",
      "Epoch 4127/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8032 - val_loss: 5.9432\n",
      "Epoch 4128/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0817 - val_loss: 6.9744\n",
      "Epoch 4129/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9541 - val_loss: 5.8903\n",
      "Epoch 4130/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8663 - val_loss: 5.9875\n",
      "Epoch 4131/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8174 - val_loss: 5.9010\n",
      "Epoch 4132/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8525 - val_loss: 5.9307\n",
      "Epoch 4133/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9379 - val_loss: 6.0283\n",
      "Epoch 4134/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8960 - val_loss: 5.9935\n",
      "Epoch 4135/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0439 - val_loss: 5.8160\n",
      "Epoch 4136/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8712 - val_loss: 5.8264\n",
      "Epoch 4137/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0235 - val_loss: 6.0478\n",
      "Epoch 4138/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9750 - val_loss: 5.8601\n",
      "Epoch 4139/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8461 - val_loss: 6.1816\n",
      "Epoch 4140/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.2752 - val_loss: 5.8399\n",
      "Epoch 4141/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8220 - val_loss: 5.8786\n",
      "Epoch 4142/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.8002 - val_loss: 5.8344\n",
      "Epoch 4143/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8350 - val_loss: 5.8164\n",
      "Epoch 4144/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0403 - val_loss: 5.8662\n",
      "Epoch 4145/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8017 - val_loss: 5.9757\n",
      "Epoch 4146/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9459 - val_loss: 5.8498\n",
      "Epoch 4147/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8968 - val_loss: 5.8274\n",
      "Epoch 4148/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8469 - val_loss: 6.0892\n",
      "Epoch 4149/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9010 - val_loss: 7.0218\n",
      "Epoch 4150/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1576 - val_loss: 6.4026\n",
      "Epoch 4151/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0616 - val_loss: 6.6036\n",
      "Epoch 4152/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0677 - val_loss: 6.0770\n",
      "Epoch 4153/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9304 - val_loss: 5.8498\n",
      "Epoch 4154/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8441 - val_loss: 6.0174\n",
      "Epoch 4155/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9342 - val_loss: 5.8299\n",
      "Epoch 4156/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7943 - val_loss: 5.9986\n",
      "Epoch 4157/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0705 - val_loss: 5.8535\n",
      "Epoch 4158/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8404 - val_loss: 5.9871\n",
      "Epoch 4159/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8302 - val_loss: 5.8411\n",
      "Epoch 4160/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8138 - val_loss: 5.9035\n",
      "Epoch 4161/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.8357 - val_loss: 5.8652\n",
      "Epoch 4162/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8539 - val_loss: 6.0548\n",
      "Epoch 4163/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8958 - val_loss: 5.8398\n",
      "Epoch 4164/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8747 - val_loss: 5.7808\n",
      "Epoch 4165/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8503 - val_loss: 5.9912\n",
      "Epoch 4166/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8998 - val_loss: 5.7910\n",
      "Epoch 4167/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8919 - val_loss: 5.8148\n",
      "Epoch 4168/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8336 - val_loss: 5.9810\n",
      "Epoch 4169/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8963 - val_loss: 5.9929\n",
      "Epoch 4170/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8671 - val_loss: 6.0197\n",
      "Epoch 4171/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.4051 - val_loss: 6.0484\n",
      "Epoch 4172/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8918 - val_loss: 5.9771\n",
      "Epoch 4173/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9422 - val_loss: 6.2807\n",
      "Epoch 4174/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9193 - val_loss: 5.8583\n",
      "Epoch 4175/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9187 - val_loss: 6.8478\n",
      "Epoch 4176/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3089 - val_loss: 5.8185\n",
      "Epoch 4177/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8739 - val_loss: 5.8000\n",
      "Epoch 4178/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7895 - val_loss: 5.8115\n",
      "Epoch 4179/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8545 - val_loss: 5.8336\n",
      "Epoch 4180/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8302 - val_loss: 5.9597\n",
      "Epoch 4181/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8900 - val_loss: 5.8472\n",
      "Epoch 4182/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8516 - val_loss: 5.8412\n",
      "Epoch 4183/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8562 - val_loss: 6.0937\n",
      "Epoch 4184/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0014 - val_loss: 6.1184\n",
      "Epoch 4185/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 5.0893 - val_loss: 6.2466\n",
      "Epoch 4186/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.9250 - val_loss: 6.0419\n",
      "Epoch 4187/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9238 - val_loss: 6.4644\n",
      "Epoch 4188/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1668 - val_loss: 5.8540\n",
      "Epoch 4189/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9455 - val_loss: 5.8157\n",
      "Epoch 4190/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8002 - val_loss: 5.9624\n",
      "Epoch 4191/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9610 - val_loss: 5.8442\n",
      "Epoch 4192/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8002 - val_loss: 5.8003\n",
      "Epoch 4193/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8107 - val_loss: 5.9501\n",
      "Epoch 4194/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3387 - val_loss: 5.9091\n",
      "Epoch 4195/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8339 - val_loss: 6.1876\n",
      "Epoch 4196/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8859 - val_loss: 5.9846\n",
      "Epoch 4197/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9367 - val_loss: 5.8792\n",
      "Epoch 4198/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8429 - val_loss: 5.9026\n",
      "Epoch 4199/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8164 - val_loss: 5.7906\n",
      "Epoch 4200/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8111 - val_loss: 5.9285\n",
      "Epoch 4201/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8059 - val_loss: 5.9823\n",
      "Epoch 4202/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7962 - val_loss: 6.1285\n",
      "Epoch 4203/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0017 - val_loss: 6.4493\n",
      "Epoch 4204/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8947 - val_loss: 5.8667\n",
      "Epoch 4205/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9426 - val_loss: 5.8197\n",
      "Epoch 4206/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7926 - val_loss: 5.7997\n",
      "Epoch 4207/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8764 - val_loss: 5.9775\n",
      "Epoch 4208/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8733 - val_loss: 5.8510\n",
      "Epoch 4209/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8274 - val_loss: 5.8285\n",
      "Epoch 4210/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9025 - val_loss: 5.8485\n",
      "Epoch 4211/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8921 - val_loss: 5.8402\n",
      "Epoch 4212/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8400 - val_loss: 5.8845\n",
      "Epoch 4213/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8453 - val_loss: 5.9350\n",
      "Epoch 4214/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8789 - val_loss: 5.7764\n",
      "Epoch 4215/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7901 - val_loss: 5.8375\n",
      "Epoch 4216/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8082 - val_loss: 5.8615\n",
      "Epoch 4217/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8063 - val_loss: 5.8834\n",
      "Epoch 4218/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7660 - val_loss: 5.7804\n",
      "Epoch 4219/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8819 - val_loss: 5.8353\n",
      "Epoch 4220/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7898 - val_loss: 6.1167\n",
      "Epoch 4221/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9722 - val_loss: 6.4231\n",
      "Epoch 4222/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0512 - val_loss: 5.8570\n",
      "Epoch 4223/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8934 - val_loss: 5.9316\n",
      "Epoch 4224/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8153 - val_loss: 5.8947\n",
      "Epoch 4225/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8529 - val_loss: 5.8307\n",
      "Epoch 4226/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8151 - val_loss: 6.0037\n",
      "Epoch 4227/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8691 - val_loss: 5.9182\n",
      "Epoch 4228/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9872 - val_loss: 5.8812\n",
      "Epoch 4229/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8154 - val_loss: 5.9169\n",
      "Epoch 4230/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1071 - val_loss: 6.2025\n",
      "Epoch 4231/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9537 - val_loss: 5.8268\n",
      "Epoch 4232/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9678 - val_loss: 5.8708\n",
      "Epoch 4233/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0005 - val_loss: 5.8623\n",
      "Epoch 4234/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8327 - val_loss: 5.8400\n",
      "Epoch 4235/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8168 - val_loss: 6.4344\n",
      "Epoch 4236/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0938 - val_loss: 6.1913\n",
      "Epoch 4237/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8711 - val_loss: 5.9269\n",
      "Epoch 4238/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8398 - val_loss: 5.9879\n",
      "Epoch 4239/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9809 - val_loss: 5.9955\n",
      "Epoch 4240/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9901 - val_loss: 6.2631\n",
      "Epoch 4241/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9035 - val_loss: 5.9157\n",
      "Epoch 4242/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8801 - val_loss: 5.8766\n",
      "Epoch 4243/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7878 - val_loss: 6.2051\n",
      "Epoch 4244/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8357 - val_loss: 6.2412\n",
      "Epoch 4245/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9643 - val_loss: 5.8139\n",
      "Epoch 4246/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9790 - val_loss: 5.9765\n",
      "Epoch 4247/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7619 - val_loss: 5.9398\n",
      "Epoch 4248/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9370 - val_loss: 6.0307\n",
      "Epoch 4249/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9385 - val_loss: 6.1858\n",
      "Epoch 4250/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0204 - val_loss: 5.8805\n",
      "Epoch 4251/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9222 - val_loss: 6.0657\n",
      "Epoch 4252/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8823 - val_loss: 5.9025\n",
      "Epoch 4253/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8014 - val_loss: 5.9342\n",
      "Epoch 4254/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8756 - val_loss: 5.8444\n",
      "Epoch 4255/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8976 - val_loss: 5.9174\n",
      "Epoch 4256/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0059 - val_loss: 6.2711\n",
      "Epoch 4257/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8943 - val_loss: 5.8314\n",
      "Epoch 4258/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9582 - val_loss: 6.7511\n",
      "Epoch 4259/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0658 - val_loss: 5.9660\n",
      "Epoch 4260/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8636 - val_loss: 5.8696\n",
      "Epoch 4261/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.7820 - val_loss: 5.8942\n",
      "Epoch 4262/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8011 - val_loss: 5.8541\n",
      "Epoch 4263/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8432 - val_loss: 5.9594\n",
      "Epoch 4264/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0801 - val_loss: 5.8797\n",
      "Epoch 4265/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9114 - val_loss: 5.7499\n",
      "Epoch 4266/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7681 - val_loss: 5.8154\n",
      "Epoch 4267/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0330 - val_loss: 5.8110\n",
      "Epoch 4268/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7866 - val_loss: 5.8112\n",
      "Epoch 4269/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7929 - val_loss: 6.0915\n",
      "Epoch 4270/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9837 - val_loss: 5.8559\n",
      "Epoch 4271/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8117 - val_loss: 5.8477\n",
      "Epoch 4272/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8239 - val_loss: 6.0239\n",
      "Epoch 4273/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0485 - val_loss: 5.8144\n",
      "Epoch 4274/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8149 - val_loss: 5.7787\n",
      "Epoch 4275/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8936 - val_loss: 5.8653\n",
      "Epoch 4276/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8041 - val_loss: 5.7492\n",
      "Epoch 4277/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9825 - val_loss: 5.7575\n",
      "Epoch 4278/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8257 - val_loss: 6.2313\n",
      "Epoch 4279/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9977 - val_loss: 6.0135\n",
      "Epoch 4280/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.2515 - val_loss: 5.8289\n",
      "Epoch 4281/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9916 - val_loss: 5.7958\n",
      "Epoch 4282/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9625 - val_loss: 5.8410\n",
      "Epoch 4283/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9983 - val_loss: 5.9013\n",
      "Epoch 4284/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7501 - val_loss: 5.9203\n",
      "Epoch 4285/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8604 - val_loss: 5.8725\n",
      "Epoch 4286/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7984 - val_loss: 5.9581\n",
      "Epoch 4287/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9380 - val_loss: 6.2620\n",
      "Epoch 4288/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8771 - val_loss: 5.9910\n",
      "Epoch 4289/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0028 - val_loss: 5.8844\n",
      "Epoch 4290/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8517 - val_loss: 5.9052\n",
      "Epoch 4291/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7666 - val_loss: 5.8177\n",
      "Epoch 4292/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8034 - val_loss: 6.0389\n",
      "Epoch 4293/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7756 - val_loss: 6.0706\n",
      "Epoch 4294/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8992 - val_loss: 5.9855\n",
      "Epoch 4295/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8067 - val_loss: 5.9284\n",
      "Epoch 4296/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8825 - val_loss: 6.0077\n",
      "Epoch 4297/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9938 - val_loss: 5.7800\n",
      "Epoch 4298/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8320 - val_loss: 5.8465\n",
      "Epoch 4299/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7692 - val_loss: 5.9320\n",
      "Epoch 4300/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7872 - val_loss: 6.0530\n",
      "Epoch 4301/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.0684 - val_loss: 6.0847\n",
      "Epoch 4302/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9057 - val_loss: 5.9458\n",
      "Epoch 4303/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8064 - val_loss: 5.8548\n",
      "Epoch 4304/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8623 - val_loss: 5.8274\n",
      "Epoch 4305/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8715 - val_loss: 5.9313\n",
      "Epoch 4306/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7860 - val_loss: 5.8094\n",
      "Epoch 4307/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7513 - val_loss: 5.8408\n",
      "Epoch 4308/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8224 - val_loss: 5.8163\n",
      "Epoch 4309/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8109 - val_loss: 5.8172\n",
      "Epoch 4310/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8176 - val_loss: 5.7505\n",
      "Epoch 4311/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7968 - val_loss: 5.8181\n",
      "Epoch 4312/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7719 - val_loss: 5.7870\n",
      "Epoch 4313/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8821 - val_loss: 5.7682\n",
      "Epoch 4314/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9103 - val_loss: 5.8976\n",
      "Epoch 4315/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8423 - val_loss: 5.8122\n",
      "Epoch 4316/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8128 - val_loss: 5.9211\n",
      "Epoch 4317/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9309 - val_loss: 5.8426\n",
      "Epoch 4318/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8664 - val_loss: 5.9435\n",
      "Epoch 4319/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7511 - val_loss: 5.7823\n",
      "Epoch 4320/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7642 - val_loss: 6.0429\n",
      "Epoch 4321/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7451 - val_loss: 6.1262\n",
      "Epoch 4322/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8821 - val_loss: 6.1365\n",
      "Epoch 4323/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9260 - val_loss: 5.9051\n",
      "Epoch 4324/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8793 - val_loss: 5.7482\n",
      "Epoch 4325/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7580 - val_loss: 5.8703\n",
      "Epoch 4326/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8387 - val_loss: 5.8139\n",
      "Epoch 4327/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7985 - val_loss: 5.8625\n",
      "Epoch 4328/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7386 - val_loss: 5.8022\n",
      "Epoch 4329/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7349 - val_loss: 5.9075\n",
      "Epoch 4330/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0059 - val_loss: 6.0674\n",
      "Epoch 4331/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8519 - val_loss: 5.8531\n",
      "Epoch 4332/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7809 - val_loss: 5.8111\n",
      "Epoch 4333/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8321 - val_loss: 5.7996\n",
      "Epoch 4334/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.8831 - val_loss: 5.9120\n",
      "Epoch 4335/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8392 - val_loss: 5.8418\n",
      "Epoch 4336/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8037 - val_loss: 5.8088\n",
      "Epoch 4337/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.8387 - val_loss: 5.8750\n",
      "Epoch 4338/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7949 - val_loss: 5.7954\n",
      "Epoch 4339/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8039 - val_loss: 5.7943\n",
      "Epoch 4340/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7931 - val_loss: 5.7402\n",
      "Epoch 4341/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8336 - val_loss: 5.7492\n",
      "Epoch 4342/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7649 - val_loss: 5.9197\n",
      "Epoch 4343/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7423 - val_loss: 5.7457\n",
      "Epoch 4344/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7806 - val_loss: 5.8946\n",
      "Epoch 4345/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7378 - val_loss: 5.8900\n",
      "Epoch 4346/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8769 - val_loss: 5.7772\n",
      "Epoch 4347/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8406 - val_loss: 5.8272\n",
      "Epoch 4348/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7941 - val_loss: 6.0927\n",
      "Epoch 4349/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9637 - val_loss: 6.4436\n",
      "Epoch 4350/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0375 - val_loss: 5.8938\n",
      "Epoch 4351/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7981 - val_loss: 5.8026\n",
      "Epoch 4352/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9523 - val_loss: 6.0063\n",
      "Epoch 4353/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.7589 - val_loss: 6.0136\n",
      "Epoch 4354/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9433 - val_loss: 6.1119\n",
      "Epoch 4355/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 5.2044 - val_loss: 5.7207\n",
      "Epoch 4356/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8046 - val_loss: 5.7972\n",
      "Epoch 4357/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7573 - val_loss: 6.0927\n",
      "Epoch 4358/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8853 - val_loss: 6.2913\n",
      "Epoch 4359/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1977 - val_loss: 5.8397\n",
      "Epoch 4360/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9703 - val_loss: 5.8121\n",
      "Epoch 4361/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8728 - val_loss: 5.8468\n",
      "Epoch 4362/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8218 - val_loss: 5.8199\n",
      "Epoch 4363/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7853 - val_loss: 5.8401\n",
      "Epoch 4364/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7447 - val_loss: 5.8118\n",
      "Epoch 4365/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7332 - val_loss: 5.8284\n",
      "Epoch 4366/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7544 - val_loss: 5.8162\n",
      "Epoch 4367/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.8156 - val_loss: 5.7536\n",
      "Epoch 4368/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7856 - val_loss: 5.8424\n",
      "Epoch 4369/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7697 - val_loss: 5.9405\n",
      "Epoch 4370/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0549 - val_loss: 5.9473\n",
      "Epoch 4371/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8540 - val_loss: 5.7793\n",
      "Epoch 4372/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8068 - val_loss: 5.8923\n",
      "Epoch 4373/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7995 - val_loss: 5.8582\n",
      "Epoch 4374/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8205 - val_loss: 5.7929\n",
      "Epoch 4375/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7599 - val_loss: 5.7631\n",
      "Epoch 4376/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8019 - val_loss: 5.7389\n",
      "Epoch 4377/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7613 - val_loss: 5.8436\n",
      "Epoch 4378/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8127 - val_loss: 5.7524\n",
      "Epoch 4379/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7113 - val_loss: 5.8364\n",
      "Epoch 4380/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8880 - val_loss: 6.1623\n",
      "Epoch 4381/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7331 - val_loss: 5.8927\n",
      "Epoch 4382/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8252 - val_loss: 5.8583\n",
      "Epoch 4383/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7275 - val_loss: 5.8998\n",
      "Epoch 4384/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7688 - val_loss: 5.7971\n",
      "Epoch 4385/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8607 - val_loss: 5.8670\n",
      "Epoch 4386/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7925 - val_loss: 5.7664\n",
      "Epoch 4387/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8451 - val_loss: 5.8272\n",
      "Epoch 4388/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8296 - val_loss: 5.8561\n",
      "Epoch 4389/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8120 - val_loss: 5.9100\n",
      "Epoch 4390/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9250 - val_loss: 6.2920\n",
      "Epoch 4391/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9283 - val_loss: 5.9113\n",
      "Epoch 4392/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8855 - val_loss: 5.7654\n",
      "Epoch 4393/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8324 - val_loss: 5.8497\n",
      "Epoch 4394/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9018 - val_loss: 6.0750\n",
      "Epoch 4395/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7843 - val_loss: 6.4891\n",
      "Epoch 4396/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9169 - val_loss: 5.9856\n",
      "Epoch 4397/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8853 - val_loss: 5.9624\n",
      "Epoch 4398/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7819 - val_loss: 5.8648\n",
      "Epoch 4399/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8229 - val_loss: 6.3815\n",
      "Epoch 4400/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3777 - val_loss: 5.7565\n",
      "Epoch 4401/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7286 - val_loss: 5.7634\n",
      "Epoch 4402/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7066 - val_loss: 5.9398\n",
      "Epoch 4403/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.8967 - val_loss: 5.8127\n",
      "Epoch 4404/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7972 - val_loss: 5.7906\n",
      "Epoch 4405/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8538 - val_loss: 5.9880\n",
      "Epoch 4406/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9384 - val_loss: 5.8740\n",
      "Epoch 4407/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8461 - val_loss: 5.8372\n",
      "Epoch 4408/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7530 - val_loss: 5.9106\n",
      "Epoch 4409/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7682 - val_loss: 5.9182\n",
      "Epoch 4410/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4401 - val_loss: 6.3485\n",
      "Epoch 4411/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.3167 - val_loss: 6.0575\n",
      "Epoch 4412/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7227 - val_loss: 5.8272\n",
      "Epoch 4413/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.9072 - val_loss: 6.3678\n",
      "Epoch 4414/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 5.0496 - val_loss: 5.8957\n",
      "Epoch 4415/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8897 - val_loss: 5.7721\n",
      "Epoch 4416/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8866 - val_loss: 6.2539\n",
      "Epoch 4417/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8249 - val_loss: 5.7755\n",
      "Epoch 4418/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7588 - val_loss: 5.8045\n",
      "Epoch 4419/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7185 - val_loss: 5.8351\n",
      "Epoch 4420/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7266 - val_loss: 5.7769\n",
      "Epoch 4421/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7994 - val_loss: 5.8124\n",
      "Epoch 4422/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7432 - val_loss: 5.8108\n",
      "Epoch 4423/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8634 - val_loss: 5.7795\n",
      "Epoch 4424/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8021 - val_loss: 6.1222\n",
      "Epoch 4425/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9724 - val_loss: 5.7800\n",
      "Epoch 4426/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7816 - val_loss: 5.7812\n",
      "Epoch 4427/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7501 - val_loss: 5.7427\n",
      "Epoch 4428/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8159 - val_loss: 6.1709\n",
      "Epoch 4429/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.4328 - val_loss: 6.0357\n",
      "Epoch 4430/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7628 - val_loss: 5.9489\n",
      "Epoch 4431/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8105 - val_loss: 5.7997\n",
      "Epoch 4432/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7142 - val_loss: 6.0112\n",
      "Epoch 4433/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9666 - val_loss: 5.8129\n",
      "Epoch 4434/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7991 - val_loss: 5.8762\n",
      "Epoch 4435/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9137 - val_loss: 5.9054\n",
      "Epoch 4436/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7256 - val_loss: 6.0152\n",
      "Epoch 4437/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0076 - val_loss: 5.9107\n",
      "Epoch 4438/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7819 - val_loss: 5.9232\n",
      "Epoch 4439/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8803 - val_loss: 6.0859\n",
      "Epoch 4440/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7473 - val_loss: 5.7865\n",
      "Epoch 4441/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7894 - val_loss: 5.7959\n",
      "Epoch 4442/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7507 - val_loss: 6.0289\n",
      "Epoch 4443/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9130 - val_loss: 5.7964\n",
      "Epoch 4444/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8867 - val_loss: 5.9732\n",
      "Epoch 4445/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8945 - val_loss: 6.0125\n",
      "Epoch 4446/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0849 - val_loss: 5.9798\n",
      "Epoch 4447/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7537 - val_loss: 5.7441\n",
      "Epoch 4448/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7055 - val_loss: 5.8335\n",
      "Epoch 4449/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7018 - val_loss: 6.8649\n",
      "Epoch 4450/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0845 - val_loss: 5.8739\n",
      "Epoch 4451/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7200 - val_loss: 5.9541\n",
      "Epoch 4452/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8269 - val_loss: 6.0756\n",
      "Epoch 4453/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8866 - val_loss: 5.8218\n",
      "Epoch 4454/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7557 - val_loss: 5.7813\n",
      "Epoch 4455/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7434 - val_loss: 5.9930\n",
      "Epoch 4456/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7798 - val_loss: 5.8479\n",
      "Epoch 4457/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8226 - val_loss: 5.8113\n",
      "Epoch 4458/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9222 - val_loss: 5.8055\n",
      "Epoch 4459/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7782 - val_loss: 5.8110\n",
      "Epoch 4460/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8337 - val_loss: 6.1028\n",
      "Epoch 4461/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9180 - val_loss: 5.9378\n",
      "Epoch 4462/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7400 - val_loss: 5.9883\n",
      "Epoch 4463/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8976 - val_loss: 5.8689\n",
      "Epoch 4464/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7594 - val_loss: 5.7662\n",
      "Epoch 4465/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8158 - val_loss: 6.0010\n",
      "Epoch 4466/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0820 - val_loss: 5.7259\n",
      "Epoch 4467/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7262 - val_loss: 5.7582\n",
      "Epoch 4468/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7391 - val_loss: 5.8003\n",
      "Epoch 4469/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7534 - val_loss: 5.9843\n",
      "Epoch 4470/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8099 - val_loss: 5.7485\n",
      "Epoch 4471/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7387 - val_loss: 5.7987\n",
      "Epoch 4472/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7589 - val_loss: 5.8372\n",
      "Epoch 4473/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7723 - val_loss: 5.8213\n",
      "Epoch 4474/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8037 - val_loss: 5.8320\n",
      "Epoch 4475/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8834 - val_loss: 5.7577\n",
      "Epoch 4476/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1063 - val_loss: 5.7113\n",
      "Epoch 4477/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7301 - val_loss: 5.7771\n",
      "Epoch 4478/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7154 - val_loss: 5.7749\n",
      "Epoch 4479/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6800 - val_loss: 6.1654\n",
      "Epoch 4480/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9628 - val_loss: 5.8993\n",
      "Epoch 4481/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9581 - val_loss: 5.7857\n",
      "Epoch 4482/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8617 - val_loss: 6.2782\n",
      "Epoch 4483/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0482 - val_loss: 5.8823\n",
      "Epoch 4484/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1211 - val_loss: 6.0161\n",
      "Epoch 4485/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8039 - val_loss: 5.7576\n",
      "Epoch 4486/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7543 - val_loss: 5.7862\n",
      "Epoch 4487/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.8016 - val_loss: 5.8603\n",
      "Epoch 4488/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7093 - val_loss: 5.7344\n",
      "Epoch 4489/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.7812 - val_loss: 5.7167\n",
      "Epoch 4490/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7158 - val_loss: 6.0331\n",
      "Epoch 4491/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7486 - val_loss: 5.7749\n",
      "Epoch 4492/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7367 - val_loss: 5.7498\n",
      "Epoch 4493/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7032 - val_loss: 5.9871\n",
      "Epoch 4494/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7429 - val_loss: 5.7905\n",
      "Epoch 4495/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7701 - val_loss: 5.7540\n",
      "Epoch 4496/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7676 - val_loss: 5.9427\n",
      "Epoch 4497/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8435 - val_loss: 5.8653\n",
      "Epoch 4498/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7378 - val_loss: 5.7668\n",
      "Epoch 4499/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7464 - val_loss: 5.7109\n",
      "Epoch 4500/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8928 - val_loss: 6.2089\n",
      "Epoch 4501/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1959 - val_loss: 5.8024\n",
      "Epoch 4502/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7453 - val_loss: 5.7574\n",
      "Epoch 4503/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7775 - val_loss: 5.7878\n",
      "Epoch 4504/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6985 - val_loss: 5.7412\n",
      "Epoch 4505/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7742 - val_loss: 5.7661\n",
      "Epoch 4506/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8307 - val_loss: 5.7146\n",
      "Epoch 4507/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8569 - val_loss: 5.7447\n",
      "Epoch 4508/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9840 - val_loss: 5.8272\n",
      "Epoch 4509/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8095 - val_loss: 5.7931\n",
      "Epoch 4510/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7707 - val_loss: 5.8050\n",
      "Epoch 4511/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7828 - val_loss: 5.9487\n",
      "Epoch 4512/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8073 - val_loss: 5.9139\n",
      "Epoch 4513/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8746 - val_loss: 5.7391\n",
      "Epoch 4514/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8316 - val_loss: 5.7213\n",
      "Epoch 4515/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7950 - val_loss: 5.7876\n",
      "Epoch 4516/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8654 - val_loss: 5.8058\n",
      "Epoch 4517/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7233 - val_loss: 5.8428\n",
      "Epoch 4518/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6897 - val_loss: 5.7778\n",
      "Epoch 4519/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7252 - val_loss: 5.9134\n",
      "Epoch 4520/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9331 - val_loss: 6.3261\n",
      "Epoch 4521/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9039 - val_loss: 5.8703\n",
      "Epoch 4522/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8881 - val_loss: 5.9224\n",
      "Epoch 4523/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8194 - val_loss: 5.8296\n",
      "Epoch 4524/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7545 - val_loss: 5.7577\n",
      "Epoch 4525/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7970 - val_loss: 5.7474\n",
      "Epoch 4526/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7322 - val_loss: 5.7298\n",
      "Epoch 4527/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7047 - val_loss: 5.7574\n",
      "Epoch 4528/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7303 - val_loss: 5.8573\n",
      "Epoch 4529/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7645 - val_loss: 5.8323\n",
      "Epoch 4530/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6882 - val_loss: 5.7883\n",
      "Epoch 4531/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7671 - val_loss: 5.7295\n",
      "Epoch 4532/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7802 - val_loss: 5.7428\n",
      "Epoch 4533/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7194 - val_loss: 5.7204\n",
      "Epoch 4534/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7454 - val_loss: 5.7452\n",
      "Epoch 4535/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7391 - val_loss: 5.9045\n",
      "Epoch 4536/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7012 - val_loss: 5.7690\n",
      "Epoch 4537/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7250 - val_loss: 6.0353\n",
      "Epoch 4538/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8274 - val_loss: 6.0390\n",
      "Epoch 4539/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7604 - val_loss: 5.8197\n",
      "Epoch 4540/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8156 - val_loss: 5.8904\n",
      "Epoch 4541/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7955 - val_loss: 5.9001\n",
      "Epoch 4542/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7606 - val_loss: 5.7314\n",
      "Epoch 4543/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6899 - val_loss: 6.2415\n",
      "Epoch 4544/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8860 - val_loss: 5.7969\n",
      "Epoch 4545/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6803 - val_loss: 5.7280\n",
      "Epoch 4546/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7675 - val_loss: 5.8234\n",
      "Epoch 4547/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7899 - val_loss: 5.8276\n",
      "Epoch 4548/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8397 - val_loss: 5.9499\n",
      "Epoch 4549/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0246 - val_loss: 5.7674\n",
      "Epoch 4550/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.8101 - val_loss: 6.0618\n",
      "Epoch 4551/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7555 - val_loss: 5.8107\n",
      "Epoch 4552/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7204 - val_loss: 5.7355\n",
      "Epoch 4553/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6898 - val_loss: 6.0981\n",
      "Epoch 4554/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7223 - val_loss: 5.7939\n",
      "Epoch 4555/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.6856 - val_loss: 5.8313\n",
      "Epoch 4556/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.8399 - val_loss: 5.8298\n",
      "Epoch 4557/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.8515 - val_loss: 5.7422\n",
      "Epoch 4558/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.7056 - val_loss: 5.9166\n",
      "Epoch 4559/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.7668 - val_loss: 5.9068\n",
      "Epoch 4560/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7877 - val_loss: 5.9027\n",
      "Epoch 4561/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.8279 - val_loss: 5.7500\n",
      "Epoch 4562/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6755 - val_loss: 5.8437\n",
      "Epoch 4563/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9000 - val_loss: 6.0037\n",
      "Epoch 4564/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6941 - val_loss: 5.7114\n",
      "Epoch 4565/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 4.7750 - val_loss: 5.6899\n",
      "Epoch 4566/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7324 - val_loss: 5.6977\n",
      "Epoch 4567/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6500 - val_loss: 5.8200\n",
      "Epoch 4568/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6844 - val_loss: 5.9469\n",
      "Epoch 4569/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7217 - val_loss: 5.8535\n",
      "Epoch 4570/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7873 - val_loss: 5.8293\n",
      "Epoch 4571/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7611 - val_loss: 5.7307\n",
      "Epoch 4572/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8185 - val_loss: 5.9922\n",
      "Epoch 4573/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6782 - val_loss: 6.1198\n",
      "Epoch 4574/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8588 - val_loss: 5.9722\n",
      "Epoch 4575/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8338 - val_loss: 5.7166\n",
      "Epoch 4576/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7423 - val_loss: 6.1480\n",
      "Epoch 4577/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9466 - val_loss: 5.8558\n",
      "Epoch 4578/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8367 - val_loss: 5.8316\n",
      "Epoch 4579/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9632 - val_loss: 6.4168\n",
      "Epoch 4580/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8903 - val_loss: 5.8584\n",
      "Epoch 4581/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7013 - val_loss: 6.0146\n",
      "Epoch 4582/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9304 - val_loss: 5.7375\n",
      "Epoch 4583/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6864 - val_loss: 5.7853\n",
      "Epoch 4584/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8017 - val_loss: 5.8616\n",
      "Epoch 4585/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7247 - val_loss: 5.7242\n",
      "Epoch 4586/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6772 - val_loss: 5.7452\n",
      "Epoch 4587/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7128 - val_loss: 6.0973\n",
      "Epoch 4588/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8458 - val_loss: 5.8119\n",
      "Epoch 4589/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9680 - val_loss: 6.2552\n",
      "Epoch 4590/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.1114 - val_loss: 6.1049\n",
      "Epoch 4591/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0648 - val_loss: 5.8088\n",
      "Epoch 4592/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7426 - val_loss: 5.7195\n",
      "Epoch 4593/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7618 - val_loss: 5.8074\n",
      "Epoch 4594/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7249 - val_loss: 5.8109\n",
      "Epoch 4595/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7156 - val_loss: 5.7268\n",
      "Epoch 4596/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7262 - val_loss: 5.7192\n",
      "Epoch 4597/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7090 - val_loss: 5.9691\n",
      "Epoch 4598/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7768 - val_loss: 5.7145\n",
      "Epoch 4599/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9762 - val_loss: 5.8248\n",
      "Epoch 4600/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7177 - val_loss: 5.7501\n",
      "Epoch 4601/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6786 - val_loss: 5.7269\n",
      "Epoch 4602/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6936 - val_loss: 5.9146\n",
      "Epoch 4603/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7106 - val_loss: 5.7620\n",
      "Epoch 4604/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7881 - val_loss: 5.7487\n",
      "Epoch 4605/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8978 - val_loss: 5.7703\n",
      "Epoch 4606/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7268 - val_loss: 5.7792\n",
      "Epoch 4607/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7920 - val_loss: 5.7739\n",
      "Epoch 4608/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6844 - val_loss: 6.0050\n",
      "Epoch 4609/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7798 - val_loss: 5.6727\n",
      "Epoch 4610/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9394 - val_loss: 5.9643\n",
      "Epoch 4611/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.9573 - val_loss: 6.0374\n",
      "Epoch 4612/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.3070 - val_loss: 5.7936\n",
      "Epoch 4613/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0144 - val_loss: 6.1035\n",
      "Epoch 4614/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8198 - val_loss: 5.7638\n",
      "Epoch 4615/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6914 - val_loss: 5.7753\n",
      "Epoch 4616/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6626 - val_loss: 5.7193\n",
      "Epoch 4617/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6881 - val_loss: 5.8001\n",
      "Epoch 4618/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6700 - val_loss: 5.9008\n",
      "Epoch 4619/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7479 - val_loss: 6.5055\n",
      "Epoch 4620/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.2534 - val_loss: 5.9788\n",
      "Epoch 4621/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7307 - val_loss: 5.6653\n",
      "Epoch 4622/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7941 - val_loss: 5.9349\n",
      "Epoch 4623/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8168 - val_loss: 5.7037\n",
      "Epoch 4624/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7281 - val_loss: 5.8171\n",
      "Epoch 4625/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9112 - val_loss: 5.9560\n",
      "Epoch 4626/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0308 - val_loss: 5.9328\n",
      "Epoch 4627/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7397 - val_loss: 5.7340\n",
      "Epoch 4628/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9312 - val_loss: 5.7791\n",
      "Epoch 4629/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7965 - val_loss: 5.8192\n",
      "Epoch 4630/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7509 - val_loss: 5.7619\n",
      "Epoch 4631/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8539 - val_loss: 5.8083\n",
      "Epoch 4632/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9326 - val_loss: 5.7096\n",
      "Epoch 4633/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7960 - val_loss: 5.9246\n",
      "Epoch 4634/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7889 - val_loss: 5.6881\n",
      "Epoch 4635/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6864 - val_loss: 5.8076\n",
      "Epoch 4636/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8193 - val_loss: 5.7052\n",
      "Epoch 4637/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6721 - val_loss: 5.7821\n",
      "Epoch 4638/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8557 - val_loss: 5.7008\n",
      "Epoch 4639/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7990 - val_loss: 5.7493\n",
      "Epoch 4640/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7252 - val_loss: 5.9283\n",
      "Epoch 4641/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.7341 - val_loss: 5.6876\n",
      "Epoch 4642/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6884 - val_loss: 5.7629\n",
      "Epoch 4643/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7102 - val_loss: 5.7962\n",
      "Epoch 4644/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7283 - val_loss: 6.1074\n",
      "Epoch 4645/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7612 - val_loss: 5.9424\n",
      "Epoch 4646/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6738 - val_loss: 5.7174\n",
      "Epoch 4647/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7461 - val_loss: 5.8607\n",
      "Epoch 4648/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7222 - val_loss: 5.6854\n",
      "Epoch 4649/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7153 - val_loss: 5.7093\n",
      "Epoch 4650/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0697 - val_loss: 6.2020\n",
      "Epoch 4651/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0675 - val_loss: 5.7020\n",
      "Epoch 4652/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7145 - val_loss: 5.6959\n",
      "Epoch 4653/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8028 - val_loss: 5.7062\n",
      "Epoch 4654/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6878 - val_loss: 5.6520\n",
      "Epoch 4655/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7874 - val_loss: 5.7133\n",
      "Epoch 4656/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7666 - val_loss: 5.7282\n",
      "Epoch 4657/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7412 - val_loss: 5.7687\n",
      "Epoch 4658/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0531 - val_loss: 5.7609\n",
      "Epoch 4659/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6566 - val_loss: 6.2094\n",
      "Epoch 4660/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9301 - val_loss: 5.7900\n",
      "Epoch 4661/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6524 - val_loss: 5.7517\n",
      "Epoch 4662/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6899 - val_loss: 5.7477\n",
      "Epoch 4663/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7507 - val_loss: 5.6676\n",
      "Epoch 4664/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6975 - val_loss: 5.7125\n",
      "Epoch 4665/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6504 - val_loss: 5.8576\n",
      "Epoch 4666/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7668 - val_loss: 5.8413\n",
      "Epoch 4667/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7862 - val_loss: 5.8277\n",
      "Epoch 4668/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7252 - val_loss: 5.9835\n",
      "Epoch 4669/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7046 - val_loss: 5.7249\n",
      "Epoch 4670/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9070 - val_loss: 5.8033\n",
      "Epoch 4671/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7689 - val_loss: 5.6526\n",
      "Epoch 4672/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6624 - val_loss: 5.7735\n",
      "Epoch 4673/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8979 - val_loss: 5.7538\n",
      "Epoch 4674/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8414 - val_loss: 6.0627\n",
      "Epoch 4675/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9837 - val_loss: 5.8318\n",
      "Epoch 4676/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7306 - val_loss: 5.7360\n",
      "Epoch 4677/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8110 - val_loss: 5.7417\n",
      "Epoch 4678/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7290 - val_loss: 6.0414\n",
      "Epoch 4679/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7350 - val_loss: 5.9059\n",
      "Epoch 4680/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6510 - val_loss: 5.7025\n",
      "Epoch 4681/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6954 - val_loss: 5.7231\n",
      "Epoch 4682/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6770 - val_loss: 5.7779\n",
      "Epoch 4683/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6623 - val_loss: 5.7824\n",
      "Epoch 4684/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7365 - val_loss: 5.7445\n",
      "Epoch 4685/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7646 - val_loss: 5.8147\n",
      "Epoch 4686/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8355 - val_loss: 5.8784\n",
      "Epoch 4687/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6896 - val_loss: 5.6541\n",
      "Epoch 4688/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6754 - val_loss: 6.4749\n",
      "Epoch 4689/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 5.0835 - val_loss: 5.7492\n",
      "Epoch 4690/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6960 - val_loss: 5.7480\n",
      "Epoch 4691/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7263 - val_loss: 6.1077\n",
      "Epoch 4692/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8166 - val_loss: 6.1563\n",
      "Epoch 4693/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8641 - val_loss: 5.7027\n",
      "Epoch 4694/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6779 - val_loss: 5.6989\n",
      "Epoch 4695/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6369 - val_loss: 5.7266\n",
      "Epoch 4696/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7698 - val_loss: 5.7396\n",
      "Epoch 4697/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6625 - val_loss: 6.1715\n",
      "Epoch 4698/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8773 - val_loss: 5.7871\n",
      "Epoch 4699/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8052 - val_loss: 5.7120\n",
      "Epoch 4700/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6835 - val_loss: 5.7570\n",
      "Epoch 4701/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6459 - val_loss: 5.7180\n",
      "Epoch 4702/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8616 - val_loss: 5.9542\n",
      "Epoch 4703/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9206 - val_loss: 5.7820\n",
      "Epoch 4704/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7187 - val_loss: 5.7340\n",
      "Epoch 4705/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6432 - val_loss: 5.9412\n",
      "Epoch 4706/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9686 - val_loss: 5.6915\n",
      "Epoch 4707/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6463 - val_loss: 5.8513\n",
      "Epoch 4708/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7174 - val_loss: 5.6816\n",
      "Epoch 4709/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6251 - val_loss: 5.7227\n",
      "Epoch 4710/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6930 - val_loss: 6.3120\n",
      "Epoch 4711/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8176 - val_loss: 5.9106\n",
      "Epoch 4712/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8941 - val_loss: 5.9709\n",
      "Epoch 4713/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9095 - val_loss: 5.9912\n",
      "Epoch 4714/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.4319 - val_loss: 5.9017\n",
      "Epoch 4715/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7878 - val_loss: 6.0870\n",
      "Epoch 4716/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8572 - val_loss: 5.9209\n",
      "Epoch 4717/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.7803 - val_loss: 5.6914\n",
      "Epoch 4718/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7123 - val_loss: 5.7984\n",
      "Epoch 4719/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9357 - val_loss: 5.7129\n",
      "Epoch 4720/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7022 - val_loss: 5.7565\n",
      "Epoch 4721/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6285 - val_loss: 5.7751\n",
      "Epoch 4722/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7530 - val_loss: 5.6946\n",
      "Epoch 4723/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8903 - val_loss: 5.6896\n",
      "Epoch 4724/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7417 - val_loss: 5.7699\n",
      "Epoch 4725/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6344 - val_loss: 5.6474\n",
      "Epoch 4726/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6917 - val_loss: 5.9735\n",
      "Epoch 4727/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7546 - val_loss: 5.7183\n",
      "Epoch 4728/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8433 - val_loss: 5.7954\n",
      "Epoch 4729/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6928 - val_loss: 5.8525\n",
      "Epoch 4730/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7393 - val_loss: 5.7457\n",
      "Epoch 4731/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7130 - val_loss: 5.7776\n",
      "Epoch 4732/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9207 - val_loss: 5.7502\n",
      "Epoch 4733/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6977 - val_loss: 5.7335\n",
      "Epoch 4734/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7149 - val_loss: 5.9039\n",
      "Epoch 4735/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8112 - val_loss: 5.7085\n",
      "Epoch 4736/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6726 - val_loss: 5.6868\n",
      "Epoch 4737/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6169 - val_loss: 5.7985\n",
      "Epoch 4738/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6909 - val_loss: 5.7797\n",
      "Epoch 4739/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7115 - val_loss: 5.7566\n",
      "Epoch 4740/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6872 - val_loss: 5.7353\n",
      "Epoch 4741/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6637 - val_loss: 6.1760\n",
      "Epoch 4742/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1401 - val_loss: 5.9443\n",
      "Epoch 4743/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7150 - val_loss: 5.7783\n",
      "Epoch 4744/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7508 - val_loss: 6.1739\n",
      "Epoch 4745/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8059 - val_loss: 5.7358\n",
      "Epoch 4746/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6560 - val_loss: 5.7838\n",
      "Epoch 4747/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6634 - val_loss: 5.7000\n",
      "Epoch 4748/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6399 - val_loss: 5.7844\n",
      "Epoch 4749/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7501 - val_loss: 5.6762\n",
      "Epoch 4750/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6898 - val_loss: 5.9821\n",
      "Epoch 4751/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8101 - val_loss: 5.9768\n",
      "Epoch 4752/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1562 - val_loss: 5.7089\n",
      "Epoch 4753/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6372 - val_loss: 5.6812\n",
      "Epoch 4754/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6627 - val_loss: 5.8359\n",
      "Epoch 4755/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0631 - val_loss: 6.0476\n",
      "Epoch 4756/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9125 - val_loss: 5.8320\n",
      "Epoch 4757/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7096 - val_loss: 5.7008\n",
      "Epoch 4758/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6510 - val_loss: 5.8977\n",
      "Epoch 4759/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7843 - val_loss: 5.9584\n",
      "Epoch 4760/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6486 - val_loss: 5.7672\n",
      "Epoch 4761/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6720 - val_loss: 5.6871\n",
      "Epoch 4762/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6480 - val_loss: 5.8679\n",
      "Epoch 4763/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6850 - val_loss: 5.6893\n",
      "Epoch 4764/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7061 - val_loss: 5.7739\n",
      "Epoch 4765/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7387 - val_loss: 5.7906\n",
      "Epoch 4766/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6384 - val_loss: 5.7838\n",
      "Epoch 4767/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6652 - val_loss: 5.7902\n",
      "Epoch 4768/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6610 - val_loss: 5.7289\n",
      "Epoch 4769/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6437 - val_loss: 5.7831\n",
      "Epoch 4770/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8260 - val_loss: 5.7071\n",
      "Epoch 4771/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6338 - val_loss: 5.7987\n",
      "Epoch 4772/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6534 - val_loss: 5.6955\n",
      "Epoch 4773/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8583 - val_loss: 5.8841\n",
      "Epoch 4774/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8943 - val_loss: 5.7189\n",
      "Epoch 4775/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6867 - val_loss: 5.7761\n",
      "Epoch 4776/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7543 - val_loss: 5.7440\n",
      "Epoch 4777/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6490 - val_loss: 5.6950\n",
      "Epoch 4778/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5984 - val_loss: 5.7440\n",
      "Epoch 4779/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6293 - val_loss: 6.2488\n",
      "Epoch 4780/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8616 - val_loss: 6.0523\n",
      "Epoch 4781/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6755 - val_loss: 5.6722\n",
      "Epoch 4782/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6214 - val_loss: 5.9480\n",
      "Epoch 4783/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.2164 - val_loss: 6.1436\n",
      "Epoch 4784/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7678 - val_loss: 5.7439\n",
      "Epoch 4785/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7375 - val_loss: 6.0679\n",
      "Epoch 4786/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9852 - val_loss: 5.7543\n",
      "Epoch 4787/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6877 - val_loss: 5.9087\n",
      "Epoch 4788/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9926 - val_loss: 5.6754\n",
      "Epoch 4789/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6881 - val_loss: 5.8119\n",
      "Epoch 4790/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6668 - val_loss: 5.7471\n",
      "Epoch 4791/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6403 - val_loss: 5.6969\n",
      "Epoch 4792/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7878 - val_loss: 5.7393\n",
      "Epoch 4793/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.7183 - val_loss: 5.7415\n",
      "Epoch 4794/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7906 - val_loss: 5.6885\n",
      "Epoch 4795/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7121 - val_loss: 5.8667\n",
      "Epoch 4796/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7084 - val_loss: 5.6724\n",
      "Epoch 4797/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6466 - val_loss: 5.8377\n",
      "Epoch 4798/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7043 - val_loss: 5.8360\n",
      "Epoch 4799/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7982 - val_loss: 5.8338\n",
      "Epoch 4800/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6278 - val_loss: 6.1172\n",
      "Epoch 4801/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8886 - val_loss: 5.8079\n",
      "Epoch 4802/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7327 - val_loss: 5.7249\n",
      "Epoch 4803/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7924 - val_loss: 5.6603\n",
      "Epoch 4804/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7780 - val_loss: 5.6683\n",
      "Epoch 4805/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6973 - val_loss: 5.9842\n",
      "Epoch 4806/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0174 - val_loss: 5.8567\n",
      "Epoch 4807/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0233 - val_loss: 5.8211\n",
      "Epoch 4808/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6239 - val_loss: 5.6945\n",
      "Epoch 4809/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6586 - val_loss: 5.7171\n",
      "Epoch 4810/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.6139 - val_loss: 5.7123\n",
      "Epoch 4811/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.7421 - val_loss: 5.6761\n",
      "Epoch 4812/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.9251 - val_loss: 5.7002\n",
      "Epoch 4813/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6672 - val_loss: 5.7227\n",
      "Epoch 4814/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7856 - val_loss: 5.6888\n",
      "Epoch 4815/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7122 - val_loss: 5.9006\n",
      "Epoch 4816/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7617 - val_loss: 5.7320\n",
      "Epoch 4817/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6923 - val_loss: 5.6797\n",
      "Epoch 4818/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6740 - val_loss: 5.7710\n",
      "Epoch 4819/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8215 - val_loss: 5.6862\n",
      "Epoch 4820/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6406 - val_loss: 5.7066\n",
      "Epoch 4821/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6647 - val_loss: 5.6635\n",
      "Epoch 4822/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6220 - val_loss: 5.7331\n",
      "Epoch 4823/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8539 - val_loss: 5.6880\n",
      "Epoch 4824/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6077 - val_loss: 5.7648\n",
      "Epoch 4825/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7872 - val_loss: 5.6337\n",
      "Epoch 4826/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7089 - val_loss: 5.7869\n",
      "Epoch 4827/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6750 - val_loss: 5.6976\n",
      "Epoch 4828/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6395 - val_loss: 5.7050\n",
      "Epoch 4829/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6030 - val_loss: 6.0726\n",
      "Epoch 4830/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6826 - val_loss: 5.7343\n",
      "Epoch 4831/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6969 - val_loss: 5.8688\n",
      "Epoch 4832/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9328 - val_loss: 5.7459\n",
      "Epoch 4833/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.1705 - val_loss: 6.2936\n",
      "Epoch 4834/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7540 - val_loss: 5.7268\n",
      "Epoch 4835/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7287 - val_loss: 5.7053\n",
      "Epoch 4836/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6094 - val_loss: 5.8172\n",
      "Epoch 4837/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7131 - val_loss: 5.6340\n",
      "Epoch 4838/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6317 - val_loss: 5.6101\n",
      "Epoch 4839/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6219 - val_loss: 6.0394\n",
      "Epoch 4840/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7424 - val_loss: 5.6978\n",
      "Epoch 4841/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6514 - val_loss: 5.9223\n",
      "Epoch 4842/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7043 - val_loss: 5.7809\n",
      "Epoch 4843/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8796 - val_loss: 6.2106\n",
      "Epoch 4844/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8710 - val_loss: 5.6758\n",
      "Epoch 4845/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6872 - val_loss: 5.6862\n",
      "Epoch 4846/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7486 - val_loss: 6.0114\n",
      "Epoch 4847/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7628 - val_loss: 5.7378\n",
      "Epoch 4848/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7276 - val_loss: 5.6658\n",
      "Epoch 4849/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6398 - val_loss: 5.8081\n",
      "Epoch 4850/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6271 - val_loss: 5.7389\n",
      "Epoch 4851/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5984 - val_loss: 6.1977\n",
      "Epoch 4852/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7834 - val_loss: 5.7599\n",
      "Epoch 4853/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6516 - val_loss: 5.7067\n",
      "Epoch 4854/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6158 - val_loss: 5.9190\n",
      "Epoch 4855/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7492 - val_loss: 5.8009\n",
      "Epoch 4856/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9640 - val_loss: 5.7289\n",
      "Epoch 4857/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6118 - val_loss: 5.7528\n",
      "Epoch 4858/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5963 - val_loss: 5.6960\n",
      "Epoch 4859/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6201 - val_loss: 5.9062\n",
      "Epoch 4860/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7433 - val_loss: 5.7472\n",
      "Epoch 4861/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6250 - val_loss: 5.8564\n",
      "Epoch 4862/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6320 - val_loss: 5.8535\n",
      "Epoch 4863/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7269 - val_loss: 5.7188\n",
      "Epoch 4864/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6356 - val_loss: 5.7282\n",
      "Epoch 4865/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8367 - val_loss: 5.8007\n",
      "Epoch 4866/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6296 - val_loss: 5.8126\n",
      "Epoch 4867/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6921 - val_loss: 5.6704\n",
      "Epoch 4868/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9441 - val_loss: 5.7364\n",
      "Epoch 4869/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.6270 - val_loss: 5.7926\n",
      "Epoch 4870/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6619 - val_loss: 6.0909\n",
      "Epoch 4871/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8280 - val_loss: 5.7599\n",
      "Epoch 4872/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6483 - val_loss: 5.6773\n",
      "Epoch 4873/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6237 - val_loss: 5.6569\n",
      "Epoch 4874/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7680 - val_loss: 6.3419\n",
      "Epoch 4875/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7536 - val_loss: 5.9636\n",
      "Epoch 4876/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8279 - val_loss: 5.6710\n",
      "Epoch 4877/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7291 - val_loss: 5.7534\n",
      "Epoch 4878/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6022 - val_loss: 5.7757\n",
      "Epoch 4879/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7545 - val_loss: 5.7170\n",
      "Epoch 4880/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6349 - val_loss: 5.6601\n",
      "Epoch 4881/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6198 - val_loss: 6.6011\n",
      "Epoch 4882/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1489 - val_loss: 5.9714\n",
      "Epoch 4883/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6879 - val_loss: 5.6447\n",
      "Epoch 4884/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6239 - val_loss: 6.2606\n",
      "Epoch 4885/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7355 - val_loss: 5.6593\n",
      "Epoch 4886/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6831 - val_loss: 5.6658\n",
      "Epoch 4887/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7309 - val_loss: 5.6176\n",
      "Epoch 4888/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6083 - val_loss: 5.6868\n",
      "Epoch 4889/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6036 - val_loss: 5.6369\n",
      "Epoch 4890/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6331 - val_loss: 5.6177\n",
      "Epoch 4891/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7203 - val_loss: 5.6517\n",
      "Epoch 4892/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7871 - val_loss: 5.7637\n",
      "Epoch 4893/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6071 - val_loss: 5.8336\n",
      "Epoch 4894/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6155 - val_loss: 5.7491\n",
      "Epoch 4895/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9061 - val_loss: 5.8023\n",
      "Epoch 4896/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8091 - val_loss: 6.1150\n",
      "Epoch 4897/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7283 - val_loss: 5.7648\n",
      "Epoch 4898/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6037 - val_loss: 5.6520\n",
      "Epoch 4899/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7411 - val_loss: 6.2006\n",
      "Epoch 4900/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6919 - val_loss: 5.6784\n",
      "Epoch 4901/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5940 - val_loss: 5.9377\n",
      "Epoch 4902/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9840 - val_loss: 5.6430\n",
      "Epoch 4903/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7269 - val_loss: 5.9730\n",
      "Epoch 4904/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7345 - val_loss: 5.6793\n",
      "Epoch 4905/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5944 - val_loss: 5.6768\n",
      "Epoch 4906/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7319 - val_loss: 5.8395\n",
      "Epoch 4907/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6985 - val_loss: 5.7105\n",
      "Epoch 4908/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5831 - val_loss: 5.8182\n",
      "Epoch 4909/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6186 - val_loss: 5.6473\n",
      "Epoch 4910/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5840 - val_loss: 5.6863\n",
      "Epoch 4911/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6579 - val_loss: 5.7425\n",
      "Epoch 4912/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6251 - val_loss: 5.7140\n",
      "Epoch 4913/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6314 - val_loss: 5.7593\n",
      "Epoch 4914/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6928 - val_loss: 5.7581\n",
      "Epoch 4915/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5909 - val_loss: 5.6912\n",
      "Epoch 4916/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5941 - val_loss: 6.0279\n",
      "Epoch 4917/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8816 - val_loss: 5.8289\n",
      "Epoch 4918/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6312 - val_loss: 6.0047\n",
      "Epoch 4919/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7177 - val_loss: 5.6786\n",
      "Epoch 4920/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7595 - val_loss: 5.6166\n",
      "Epoch 4921/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6930 - val_loss: 5.7005\n",
      "Epoch 4922/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8372 - val_loss: 5.9899\n",
      "Epoch 4923/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7374 - val_loss: 5.6723\n",
      "Epoch 4924/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8048 - val_loss: 5.7380\n",
      "Epoch 4925/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7383 - val_loss: 6.5825\n",
      "Epoch 4926/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.1002 - val_loss: 5.8335\n",
      "Epoch 4927/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6437 - val_loss: 5.6417\n",
      "Epoch 4928/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7802 - val_loss: 5.8054\n",
      "Epoch 4929/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7328 - val_loss: 5.6668\n",
      "Epoch 4930/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5605 - val_loss: 5.7008\n",
      "Epoch 4931/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6176 - val_loss: 5.6259\n",
      "Epoch 4932/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5948 - val_loss: 5.8284\n",
      "Epoch 4933/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7635 - val_loss: 5.7718\n",
      "Epoch 4934/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9591 - val_loss: 6.3417\n",
      "Epoch 4935/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 5.0722 - val_loss: 6.0084\n",
      "Epoch 4936/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6331 - val_loss: 5.6552\n",
      "Epoch 4937/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6048 - val_loss: 5.7660\n",
      "Epoch 4938/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8466 - val_loss: 6.0749\n",
      "Epoch 4939/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7270 - val_loss: 6.1533\n",
      "Epoch 4940/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0502 - val_loss: 5.8914\n",
      "Epoch 4941/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6696 - val_loss: 5.6913\n",
      "Epoch 4942/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7426 - val_loss: 5.7013\n",
      "Epoch 4943/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8099 - val_loss: 6.4754\n",
      "Epoch 4944/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8386 - val_loss: 5.7221\n",
      "Epoch 4945/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.7624 - val_loss: 6.2204\n",
      "Epoch 4946/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.9008 - val_loss: 5.6148\n",
      "Epoch 4947/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6055 - val_loss: 5.6317\n",
      "Epoch 4948/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5681 - val_loss: 5.6550\n",
      "Epoch 4949/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6664 - val_loss: 5.7364\n",
      "Epoch 4950/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6560 - val_loss: 5.8173\n",
      "Epoch 4951/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6849 - val_loss: 5.6077\n",
      "Epoch 4952/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6256 - val_loss: 5.8276\n",
      "Epoch 4953/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6219 - val_loss: 5.7957\n",
      "Epoch 4954/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7377 - val_loss: 6.0726\n",
      "Epoch 4955/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9705 - val_loss: 5.8161\n",
      "Epoch 4956/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6588 - val_loss: 5.6840\n",
      "Epoch 4957/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5992 - val_loss: 5.6706\n",
      "Epoch 4958/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6482 - val_loss: 5.6080\n",
      "Epoch 4959/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7561 - val_loss: 5.6366\n",
      "Epoch 4960/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7280 - val_loss: 5.6136\n",
      "Epoch 4961/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7113 - val_loss: 5.6706\n",
      "Epoch 4962/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.6471 - val_loss: 5.6173\n",
      "Epoch 4963/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.5741 - val_loss: 5.6009\n",
      "Epoch 4964/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5707 - val_loss: 5.7775\n",
      "Epoch 4965/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7161 - val_loss: 5.9435\n",
      "Epoch 4966/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8086 - val_loss: 5.7603\n",
      "Epoch 4967/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8648 - val_loss: 5.8188\n",
      "Epoch 4968/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6766 - val_loss: 5.7264\n",
      "Epoch 4969/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7495 - val_loss: 5.6942\n",
      "Epoch 4970/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6721 - val_loss: 5.6266\n",
      "Epoch 4971/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5883 - val_loss: 5.6664\n",
      "Epoch 4972/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6576 - val_loss: 5.6456\n",
      "Epoch 4973/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7083 - val_loss: 5.6443\n",
      "Epoch 4974/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5769 - val_loss: 5.6316\n",
      "Epoch 4975/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6019 - val_loss: 5.6731\n",
      "Epoch 4976/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5868 - val_loss: 5.8266\n",
      "Epoch 4977/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6191 - val_loss: 5.7095\n",
      "Epoch 4978/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5415 - val_loss: 5.5617\n",
      "Epoch 4979/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6511 - val_loss: 6.4095\n",
      "Epoch 4980/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8066 - val_loss: 5.6508\n",
      "Epoch 4981/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7250 - val_loss: 5.8815\n",
      "Epoch 4982/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6780 - val_loss: 5.7256\n",
      "Epoch 4983/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6643 - val_loss: 5.6898\n",
      "Epoch 4984/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7364 - val_loss: 5.7558\n",
      "Epoch 4985/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8503 - val_loss: 5.8382\n",
      "Epoch 4986/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6172 - val_loss: 5.7012\n",
      "Epoch 4987/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6126 - val_loss: 5.6492\n",
      "Epoch 4988/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6066 - val_loss: 5.6996\n",
      "Epoch 4989/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6930 - val_loss: 5.8875\n",
      "Epoch 4990/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0289 - val_loss: 5.6805\n",
      "Epoch 4991/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7154 - val_loss: 5.8024\n",
      "Epoch 4992/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7822 - val_loss: 5.7191\n",
      "Epoch 4993/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5878 - val_loss: 5.6342\n",
      "Epoch 4994/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6176 - val_loss: 5.6958\n",
      "Epoch 4995/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6666 - val_loss: 5.7145\n",
      "Epoch 4996/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6771 - val_loss: 5.8148\n",
      "Epoch 4997/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8064 - val_loss: 5.7448\n",
      "Epoch 4998/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.6314 - val_loss: 5.7332\n",
      "Epoch 4999/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6255 - val_loss: 5.7146\n",
      "Epoch 5000/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6515 - val_loss: 5.6391\n",
      "Epoch 5001/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5735 - val_loss: 5.6715\n",
      "Epoch 5002/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5973 - val_loss: 5.6516\n",
      "Epoch 5003/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8866 - val_loss: 5.9360\n",
      "Epoch 5004/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7560 - val_loss: 5.7178\n",
      "Epoch 5005/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5594 - val_loss: 5.8275\n",
      "Epoch 5006/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6070 - val_loss: 5.6151\n",
      "Epoch 5007/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6375 - val_loss: 5.7179\n",
      "Epoch 5008/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6978 - val_loss: 5.6542\n",
      "Epoch 5009/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5720 - val_loss: 5.6578\n",
      "Epoch 5010/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5502 - val_loss: 5.9285\n",
      "Epoch 5011/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6183 - val_loss: 5.6945\n",
      "Epoch 5012/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6473 - val_loss: 5.6112\n",
      "Epoch 5013/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5907 - val_loss: 5.6885\n",
      "Epoch 5014/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8625 - val_loss: 5.6419\n",
      "Epoch 5015/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6456 - val_loss: 5.8975\n",
      "Epoch 5016/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6898 - val_loss: 5.6827\n",
      "Epoch 5017/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7590 - val_loss: 5.6923\n",
      "Epoch 5018/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5859 - val_loss: 5.7905\n",
      "Epoch 5019/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8008 - val_loss: 5.7013\n",
      "Epoch 5020/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6123 - val_loss: 5.6297\n",
      "Epoch 5021/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.5568 - val_loss: 5.6015\n",
      "Epoch 5022/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6333 - val_loss: 5.6423\n",
      "Epoch 5023/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6465 - val_loss: 5.6442\n",
      "Epoch 5024/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6657 - val_loss: 5.7764\n",
      "Epoch 5025/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6643 - val_loss: 5.6853\n",
      "Epoch 5026/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5896 - val_loss: 5.8325\n",
      "Epoch 5027/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5786 - val_loss: 5.7708\n",
      "Epoch 5028/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8916 - val_loss: 5.6157\n",
      "Epoch 5029/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6591 - val_loss: 5.6427\n",
      "Epoch 5030/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5980 - val_loss: 5.8016\n",
      "Epoch 5031/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7422 - val_loss: 5.6536\n",
      "Epoch 5032/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5760 - val_loss: 5.9486\n",
      "Epoch 5033/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6032 - val_loss: 5.6875\n",
      "Epoch 5034/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6996 - val_loss: 5.8852\n",
      "Epoch 5035/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7018 - val_loss: 5.9827\n",
      "Epoch 5036/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8299 - val_loss: 5.6332\n",
      "Epoch 5037/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5452 - val_loss: 5.7242\n",
      "Epoch 5038/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6206 - val_loss: 5.6841\n",
      "Epoch 5039/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7334 - val_loss: 6.2637\n",
      "Epoch 5040/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7101 - val_loss: 5.6618\n",
      "Epoch 5041/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6899 - val_loss: 5.7374\n",
      "Epoch 5042/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6092 - val_loss: 5.7394\n",
      "Epoch 5043/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5922 - val_loss: 5.6184\n",
      "Epoch 5044/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6616 - val_loss: 5.9669\n",
      "Epoch 5045/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6790 - val_loss: 5.5920\n",
      "Epoch 5046/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6500 - val_loss: 5.6253\n",
      "Epoch 5047/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6682 - val_loss: 5.7979\n",
      "Epoch 5048/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6621 - val_loss: 5.6978\n",
      "Epoch 5049/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6166 - val_loss: 5.6513\n",
      "Epoch 5050/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6390 - val_loss: 5.6196\n",
      "Epoch 5051/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6271 - val_loss: 5.6287\n",
      "Epoch 5052/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5843 - val_loss: 6.1146\n",
      "Epoch 5053/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7071 - val_loss: 5.7077\n",
      "Epoch 5054/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5784 - val_loss: 5.6174\n",
      "Epoch 5055/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5905 - val_loss: 5.6939\n",
      "Epoch 5056/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6284 - val_loss: 5.9222\n",
      "Epoch 5057/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9589 - val_loss: 5.9411\n",
      "Epoch 5058/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.7191 - val_loss: 5.7394\n",
      "Epoch 5059/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.5488 - val_loss: 5.6628\n",
      "Epoch 5060/10000\n",
      "675/675 [==============================] - 0s 119us/step - loss: 4.5858 - val_loss: 5.7069\n",
      "Epoch 5061/10000\n",
      "675/675 [==============================] - 0s 161us/step - loss: 4.6596 - val_loss: 5.8646\n",
      "Epoch 5062/10000\n",
      "675/675 [==============================] - 0s 147us/step - loss: 4.5963 - val_loss: 5.6708\n",
      "Epoch 5063/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 4.7168 - val_loss: 5.6996\n",
      "Epoch 5064/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.7299 - val_loss: 5.6775\n",
      "Epoch 5065/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.6585 - val_loss: 5.6193\n",
      "Epoch 5066/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.6197 - val_loss: 6.2732\n",
      "Epoch 5067/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 5.2420 - val_loss: 5.8208\n",
      "Epoch 5068/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.6771 - val_loss: 5.6211\n",
      "Epoch 5069/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5797 - val_loss: 5.9158\n",
      "Epoch 5070/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 5.0431 - val_loss: 5.5995\n",
      "Epoch 5071/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.6348 - val_loss: 5.5922\n",
      "Epoch 5072/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.7409 - val_loss: 5.5853\n",
      "Epoch 5073/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.8294 - val_loss: 6.4536\n",
      "Epoch 5074/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.9209 - val_loss: 6.1461\n",
      "Epoch 5075/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.9233 - val_loss: 5.7199\n",
      "Epoch 5076/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.5679 - val_loss: 5.7603\n",
      "Epoch 5077/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9418 - val_loss: 5.7061\n",
      "Epoch 5078/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.6432 - val_loss: 5.5734\n",
      "Epoch 5079/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.8089 - val_loss: 5.6315\n",
      "Epoch 5080/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5913 - val_loss: 5.8263\n",
      "Epoch 5081/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.6350 - val_loss: 5.6547\n",
      "Epoch 5082/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.6348 - val_loss: 5.6591\n",
      "Epoch 5083/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.7065 - val_loss: 6.0728\n",
      "Epoch 5084/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 5.0696 - val_loss: 6.2617\n",
      "Epoch 5085/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.9222 - val_loss: 5.8647\n",
      "Epoch 5086/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.7290 - val_loss: 6.1550\n",
      "Epoch 5087/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.8160 - val_loss: 5.7759\n",
      "Epoch 5088/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.7663 - val_loss: 6.3722\n",
      "Epoch 5089/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6986 - val_loss: 5.6936\n",
      "Epoch 5090/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.7321 - val_loss: 6.1381\n",
      "Epoch 5091/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.8248 - val_loss: 5.6036\n",
      "Epoch 5092/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5912 - val_loss: 5.6846\n",
      "Epoch 5093/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5737 - val_loss: 5.6240\n",
      "Epoch 5094/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5452 - val_loss: 5.7226\n",
      "Epoch 5095/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.6705 - val_loss: 5.6265\n",
      "Epoch 5096/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6697 - val_loss: 6.0588\n",
      "Epoch 5097/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 59us/step - loss: 4.7762 - val_loss: 5.7143\n",
      "Epoch 5098/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.6749 - val_loss: 5.9544\n",
      "Epoch 5099/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.7890 - val_loss: 5.8310\n",
      "Epoch 5100/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.6711 - val_loss: 5.7355\n",
      "Epoch 5101/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6231 - val_loss: 5.7128\n",
      "Epoch 5102/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6328 - val_loss: 5.6658\n",
      "Epoch 5103/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.7265 - val_loss: 5.6767\n",
      "Epoch 5104/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6454 - val_loss: 5.6943\n",
      "Epoch 5105/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6826 - val_loss: 5.8801\n",
      "Epoch 5106/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6849 - val_loss: 5.6601\n",
      "Epoch 5107/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5902 - val_loss: 5.9719\n",
      "Epoch 5108/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7823 - val_loss: 5.6701\n",
      "Epoch 5109/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7539 - val_loss: 5.7344\n",
      "Epoch 5110/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7445 - val_loss: 5.8680\n",
      "Epoch 5111/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6204 - val_loss: 5.7926\n",
      "Epoch 5112/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7673 - val_loss: 5.6722\n",
      "Epoch 5113/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5627 - val_loss: 5.7027\n",
      "Epoch 5114/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6064 - val_loss: 5.6297\n",
      "Epoch 5115/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5980 - val_loss: 6.0445\n",
      "Epoch 5116/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6708 - val_loss: 5.7169\n",
      "Epoch 5117/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5496 - val_loss: 5.6060\n",
      "Epoch 5118/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5928 - val_loss: 5.7155\n",
      "Epoch 5119/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7859 - val_loss: 5.7480\n",
      "Epoch 5120/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7575 - val_loss: 5.6330\n",
      "Epoch 5121/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6306 - val_loss: 5.8012\n",
      "Epoch 5122/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5815 - val_loss: 5.7493\n",
      "Epoch 5123/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7787 - val_loss: 5.8713\n",
      "Epoch 5124/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5916 - val_loss: 6.0181\n",
      "Epoch 5125/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5221 - val_loss: 5.6374\n",
      "Epoch 5126/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6749 - val_loss: 5.6447\n",
      "Epoch 5127/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6040 - val_loss: 5.6561\n",
      "Epoch 5128/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5300 - val_loss: 5.6482\n",
      "Epoch 5129/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8222 - val_loss: 6.1411\n",
      "Epoch 5130/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6702 - val_loss: 5.6698\n",
      "Epoch 5131/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5440 - val_loss: 5.6255\n",
      "Epoch 5132/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5406 - val_loss: 5.6481\n",
      "Epoch 5133/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6087 - val_loss: 5.6311\n",
      "Epoch 5134/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6378 - val_loss: 5.7120\n",
      "Epoch 5135/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7782 - val_loss: 6.0150\n",
      "Epoch 5136/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6342 - val_loss: 6.0672\n",
      "Epoch 5137/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6721 - val_loss: 5.7992\n",
      "Epoch 5138/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5639 - val_loss: 5.7181\n",
      "Epoch 5139/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6978 - val_loss: 5.7777\n",
      "Epoch 5140/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6331 - val_loss: 5.6300\n",
      "Epoch 5141/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5212 - val_loss: 5.8572\n",
      "Epoch 5142/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6430 - val_loss: 5.6408\n",
      "Epoch 5143/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5708 - val_loss: 5.6214\n",
      "Epoch 5144/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5354 - val_loss: 5.6031\n",
      "Epoch 5145/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6454 - val_loss: 5.8608\n",
      "Epoch 5146/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5338 - val_loss: 5.6405\n",
      "Epoch 5147/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5285 - val_loss: 5.7200\n",
      "Epoch 5148/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5742 - val_loss: 5.9013\n",
      "Epoch 5149/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7644 - val_loss: 5.7026\n",
      "Epoch 5150/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7614 - val_loss: 5.6417\n",
      "Epoch 5151/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6489 - val_loss: 5.7588\n",
      "Epoch 5152/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7713 - val_loss: 5.7923\n",
      "Epoch 5153/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6161 - val_loss: 5.6980\n",
      "Epoch 5154/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6252 - val_loss: 5.6778\n",
      "Epoch 5155/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6783 - val_loss: 5.5991\n",
      "Epoch 5156/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6702 - val_loss: 5.7092\n",
      "Epoch 5157/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5513 - val_loss: 5.6302\n",
      "Epoch 5158/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9569 - val_loss: 5.9356\n",
      "Epoch 5159/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7484 - val_loss: 5.7978\n",
      "Epoch 5160/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7753 - val_loss: 5.9596\n",
      "Epoch 5161/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6793 - val_loss: 5.7132\n",
      "Epoch 5162/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5720 - val_loss: 5.6006\n",
      "Epoch 5163/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5450 - val_loss: 5.6550\n",
      "Epoch 5164/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6392 - val_loss: 6.2456\n",
      "Epoch 5165/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8758 - val_loss: 5.8290\n",
      "Epoch 5166/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6745 - val_loss: 5.7787\n",
      "Epoch 5167/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5288 - val_loss: 5.5984\n",
      "Epoch 5168/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5150 - val_loss: 5.6346\n",
      "Epoch 5169/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7256 - val_loss: 6.4211\n",
      "Epoch 5170/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.3058 - val_loss: 5.7710\n",
      "Epoch 5171/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6467 - val_loss: 5.6620\n",
      "Epoch 5172/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5593 - val_loss: 5.9994\n",
      "Epoch 5173/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.6555 - val_loss: 5.6329\n",
      "Epoch 5174/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6148 - val_loss: 5.6128\n",
      "Epoch 5175/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5205 - val_loss: 6.3421\n",
      "Epoch 5176/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9757 - val_loss: 5.6395\n",
      "Epoch 5177/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6890 - val_loss: 5.6160\n",
      "Epoch 5178/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6964 - val_loss: 5.6595\n",
      "Epoch 5179/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5204 - val_loss: 5.6062\n",
      "Epoch 5180/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.6387 - val_loss: 5.6036\n",
      "Epoch 5181/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6917 - val_loss: 5.7203\n",
      "Epoch 5182/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5758 - val_loss: 5.8635\n",
      "Epoch 5183/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5862 - val_loss: 5.6114\n",
      "Epoch 5184/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5727 - val_loss: 5.6944\n",
      "Epoch 5185/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6138 - val_loss: 5.6367\n",
      "Epoch 5186/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5476 - val_loss: 5.5875\n",
      "Epoch 5187/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7471 - val_loss: 5.8403\n",
      "Epoch 5188/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6694 - val_loss: 5.6446\n",
      "Epoch 5189/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6050 - val_loss: 5.6484\n",
      "Epoch 5190/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5160 - val_loss: 5.6810\n",
      "Epoch 5191/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5547 - val_loss: 5.8447\n",
      "Epoch 5192/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6489 - val_loss: 5.6074\n",
      "Epoch 5193/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6581 - val_loss: 5.6775\n",
      "Epoch 5194/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6576 - val_loss: 5.6987\n",
      "Epoch 5195/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5872 - val_loss: 5.7029\n",
      "Epoch 5196/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5747 - val_loss: 5.9072\n",
      "Epoch 5197/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6155 - val_loss: 5.6074\n",
      "Epoch 5198/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5380 - val_loss: 5.8150\n",
      "Epoch 5199/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6515 - val_loss: 5.8251\n",
      "Epoch 5200/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6179 - val_loss: 6.0545\n",
      "Epoch 5201/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6225 - val_loss: 5.6733\n",
      "Epoch 5202/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6786 - val_loss: 5.6130\n",
      "Epoch 5203/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6504 - val_loss: 5.5984\n",
      "Epoch 5204/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5082 - val_loss: 5.6020\n",
      "Epoch 5205/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5327 - val_loss: 5.6277\n",
      "Epoch 5206/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5302 - val_loss: 5.6819\n",
      "Epoch 5207/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5261 - val_loss: 5.6541\n",
      "Epoch 5208/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8730 - val_loss: 5.6021\n",
      "Epoch 5209/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5812 - val_loss: 6.0045\n",
      "Epoch 5210/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8711 - val_loss: 6.5088\n",
      "Epoch 5211/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7776 - val_loss: 6.0654\n",
      "Epoch 5212/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6553 - val_loss: 6.0770\n",
      "Epoch 5213/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8906 - val_loss: 5.6277\n",
      "Epoch 5214/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5108 - val_loss: 5.7239\n",
      "Epoch 5215/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6043 - val_loss: 5.6378\n",
      "Epoch 5216/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8420 - val_loss: 5.6402\n",
      "Epoch 5217/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5473 - val_loss: 5.6486\n",
      "Epoch 5218/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5892 - val_loss: 5.6889\n",
      "Epoch 5219/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6513 - val_loss: 5.6652\n",
      "Epoch 5220/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6443 - val_loss: 5.7454\n",
      "Epoch 5221/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5736 - val_loss: 5.9939\n",
      "Epoch 5222/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6039 - val_loss: 5.9025\n",
      "Epoch 5223/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7317 - val_loss: 5.7539\n",
      "Epoch 5224/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5518 - val_loss: 5.8769\n",
      "Epoch 5225/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6280 - val_loss: 5.6291\n",
      "Epoch 5226/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5903 - val_loss: 5.6380\n",
      "Epoch 5227/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5664 - val_loss: 5.5469\n",
      "Epoch 5228/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5336 - val_loss: 5.9028\n",
      "Epoch 5229/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6724 - val_loss: 6.0142\n",
      "Epoch 5230/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8571 - val_loss: 6.1276\n",
      "Epoch 5231/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6070 - val_loss: 5.7714\n",
      "Epoch 5232/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7869 - val_loss: 5.6589\n",
      "Epoch 5233/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5184 - val_loss: 5.6156\n",
      "Epoch 5234/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5323 - val_loss: 5.6332\n",
      "Epoch 5235/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6882 - val_loss: 5.6318\n",
      "Epoch 5236/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6470 - val_loss: 6.1044\n",
      "Epoch 5237/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8496 - val_loss: 5.7899\n",
      "Epoch 5238/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6710 - val_loss: 5.6368\n",
      "Epoch 5239/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5088 - val_loss: 5.7068\n",
      "Epoch 5240/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5432 - val_loss: 5.6249\n",
      "Epoch 5241/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5000 - val_loss: 5.5846\n",
      "Epoch 5242/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5271 - val_loss: 5.5800\n",
      "Epoch 5243/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5323 - val_loss: 5.6049\n",
      "Epoch 5244/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5197 - val_loss: 5.7813\n",
      "Epoch 5245/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6847 - val_loss: 5.7245\n",
      "Epoch 5246/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5276 - val_loss: 5.6185\n",
      "Epoch 5247/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5198 - val_loss: 5.7269\n",
      "Epoch 5248/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5615 - val_loss: 5.6222\n",
      "Epoch 5249/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.4886 - val_loss: 5.6956\n",
      "Epoch 5250/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6367 - val_loss: 5.8922\n",
      "Epoch 5251/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6025 - val_loss: 5.6944\n",
      "Epoch 5252/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4932 - val_loss: 5.7853\n",
      "Epoch 5253/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5467 - val_loss: 5.9651\n",
      "Epoch 5254/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8489 - val_loss: 5.7000\n",
      "Epoch 5255/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7560 - val_loss: 5.7582\n",
      "Epoch 5256/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7704 - val_loss: 5.5970\n",
      "Epoch 5257/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6409 - val_loss: 5.5753\n",
      "Epoch 5258/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5886 - val_loss: 5.6044\n",
      "Epoch 5259/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5379 - val_loss: 5.6744\n",
      "Epoch 5260/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6086 - val_loss: 6.0730\n",
      "Epoch 5261/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9768 - val_loss: 6.1009\n",
      "Epoch 5262/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.1630 - val_loss: 5.7959\n",
      "Epoch 5263/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5365 - val_loss: 5.7578\n",
      "Epoch 5264/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5872 - val_loss: 5.6465\n",
      "Epoch 5265/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7245 - val_loss: 5.6795\n",
      "Epoch 5266/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5894 - val_loss: 5.7925\n",
      "Epoch 5267/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6032 - val_loss: 5.5655\n",
      "Epoch 5268/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5042 - val_loss: 5.7736\n",
      "Epoch 5269/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6861 - val_loss: 5.6721\n",
      "Epoch 5270/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5151 - val_loss: 5.7141\n",
      "Epoch 5271/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5403 - val_loss: 5.6400\n",
      "Epoch 5272/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5141 - val_loss: 5.6901\n",
      "Epoch 5273/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5274 - val_loss: 5.6259\n",
      "Epoch 5274/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4910 - val_loss: 5.7003\n",
      "Epoch 5275/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5228 - val_loss: 5.7769\n",
      "Epoch 5276/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6432 - val_loss: 5.6453\n",
      "Epoch 5277/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5766 - val_loss: 5.5727\n",
      "Epoch 5278/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5210 - val_loss: 5.9380\n",
      "Epoch 5279/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7035 - val_loss: 5.6119\n",
      "Epoch 5280/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5544 - val_loss: 5.6491\n",
      "Epoch 5281/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7725 - val_loss: 6.1476\n",
      "Epoch 5282/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6482 - val_loss: 5.7797\n",
      "Epoch 5283/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5553 - val_loss: 5.9600\n",
      "Epoch 5284/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0518 - val_loss: 6.1109\n",
      "Epoch 5285/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7568 - val_loss: 5.8478\n",
      "Epoch 5286/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5826 - val_loss: 5.6538\n",
      "Epoch 5287/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5453 - val_loss: 5.6493\n",
      "Epoch 5288/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6278 - val_loss: 5.6636\n",
      "Epoch 5289/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8332 - val_loss: 5.9720\n",
      "Epoch 5290/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.8708 - val_loss: 5.6013\n",
      "Epoch 5291/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5784 - val_loss: 5.6590\n",
      "Epoch 5292/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7388 - val_loss: 5.6275\n",
      "Epoch 5293/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5257 - val_loss: 5.6529\n",
      "Epoch 5294/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6329 - val_loss: 5.6318\n",
      "Epoch 5295/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6185 - val_loss: 5.6136\n",
      "Epoch 5296/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5938 - val_loss: 5.6203\n",
      "Epoch 5297/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6008 - val_loss: 5.6505\n",
      "Epoch 5298/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7639 - val_loss: 5.6016\n",
      "Epoch 5299/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5274 - val_loss: 5.5634\n",
      "Epoch 5300/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5059 - val_loss: 5.6640\n",
      "Epoch 5301/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9159 - val_loss: 5.6549\n",
      "Epoch 5302/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.7193 - val_loss: 5.7632\n",
      "Epoch 5303/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7264 - val_loss: 5.8482\n",
      "Epoch 5304/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5907 - val_loss: 5.6124\n",
      "Epoch 5305/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4912 - val_loss: 5.6604\n",
      "Epoch 5306/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5280 - val_loss: 5.6027\n",
      "Epoch 5307/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4758 - val_loss: 5.5994\n",
      "Epoch 5308/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4983 - val_loss: 5.6465\n",
      "Epoch 5309/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5225 - val_loss: 5.6363\n",
      "Epoch 5310/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5577 - val_loss: 5.5609\n",
      "Epoch 5311/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5126 - val_loss: 5.6638\n",
      "Epoch 5312/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5563 - val_loss: 5.5741\n",
      "Epoch 5313/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5623 - val_loss: 5.6390\n",
      "Epoch 5314/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5183 - val_loss: 5.7198\n",
      "Epoch 5315/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5984 - val_loss: 5.9742\n",
      "Epoch 5316/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6441 - val_loss: 5.6157\n",
      "Epoch 5317/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5374 - val_loss: 5.6100\n",
      "Epoch 5318/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4859 - val_loss: 5.6485\n",
      "Epoch 5319/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5847 - val_loss: 5.5641\n",
      "Epoch 5320/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6163 - val_loss: 5.7052\n",
      "Epoch 5321/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6193 - val_loss: 5.6833\n",
      "Epoch 5322/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6781 - val_loss: 5.6004\n",
      "Epoch 5323/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7944 - val_loss: 5.7764\n",
      "Epoch 5324/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5366 - val_loss: 5.6416\n",
      "Epoch 5325/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.5675 - val_loss: 5.6301\n",
      "Epoch 5326/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5081 - val_loss: 5.6042\n",
      "Epoch 5327/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5065 - val_loss: 5.6373\n",
      "Epoch 5328/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5558 - val_loss: 5.6727\n",
      "Epoch 5329/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6522 - val_loss: 5.7174\n",
      "Epoch 5330/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5720 - val_loss: 5.7875\n",
      "Epoch 5331/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5539 - val_loss: 5.6828\n",
      "Epoch 5332/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5937 - val_loss: 6.0214\n",
      "Epoch 5333/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8185 - val_loss: 5.6396\n",
      "Epoch 5334/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7104 - val_loss: 5.6558\n",
      "Epoch 5335/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5483 - val_loss: 5.8052\n",
      "Epoch 5336/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8781 - val_loss: 5.6181\n",
      "Epoch 5337/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6397 - val_loss: 5.9119\n",
      "Epoch 5338/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7628 - val_loss: 5.7138\n",
      "Epoch 5339/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5232 - val_loss: 5.7155\n",
      "Epoch 5340/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5367 - val_loss: 5.8128\n",
      "Epoch 5341/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5117 - val_loss: 5.6576\n",
      "Epoch 5342/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5639 - val_loss: 5.7353\n",
      "Epoch 5343/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6181 - val_loss: 5.6293\n",
      "Epoch 5344/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5287 - val_loss: 5.6126\n",
      "Epoch 5345/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7201 - val_loss: 5.7223\n",
      "Epoch 5346/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5933 - val_loss: 5.9111\n",
      "Epoch 5347/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7489 - val_loss: 5.6386\n",
      "Epoch 5348/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5391 - val_loss: 5.6514\n",
      "Epoch 5349/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5086 - val_loss: 5.7028\n",
      "Epoch 5350/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6500 - val_loss: 5.6739\n",
      "Epoch 5351/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6321 - val_loss: 5.7326\n",
      "Epoch 5352/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4873 - val_loss: 5.5805\n",
      "Epoch 5353/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4724 - val_loss: 5.5755\n",
      "Epoch 5354/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6060 - val_loss: 5.7733\n",
      "Epoch 5355/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6190 - val_loss: 5.8689\n",
      "Epoch 5356/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7428 - val_loss: 5.9033\n",
      "Epoch 5357/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6132 - val_loss: 5.6340\n",
      "Epoch 5358/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5416 - val_loss: 5.8500\n",
      "Epoch 5359/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5927 - val_loss: 5.6381\n",
      "Epoch 5360/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5210 - val_loss: 5.7035\n",
      "Epoch 5361/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5880 - val_loss: 5.6652\n",
      "Epoch 5362/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5404 - val_loss: 5.6619\n",
      "Epoch 5363/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5075 - val_loss: 5.6313\n",
      "Epoch 5364/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6533 - val_loss: 5.6625\n",
      "Epoch 5365/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5017 - val_loss: 5.5726\n",
      "Epoch 5366/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5344 - val_loss: 5.7061\n",
      "Epoch 5367/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6214 - val_loss: 5.6360\n",
      "Epoch 5368/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5965 - val_loss: 5.6002\n",
      "Epoch 5369/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5659 - val_loss: 5.8607\n",
      "Epoch 5370/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5900 - val_loss: 5.6596\n",
      "Epoch 5371/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5094 - val_loss: 5.5507\n",
      "Epoch 5372/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5837 - val_loss: 5.7194\n",
      "Epoch 5373/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7549 - val_loss: 5.6828\n",
      "Epoch 5374/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5242 - val_loss: 5.6404\n",
      "Epoch 5375/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5117 - val_loss: 5.7552\n",
      "Epoch 5376/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5777 - val_loss: 5.7043\n",
      "Epoch 5377/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5024 - val_loss: 5.7385\n",
      "Epoch 5378/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5503 - val_loss: 5.7434\n",
      "Epoch 5379/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6231 - val_loss: 5.7249\n",
      "Epoch 5380/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6162 - val_loss: 5.6297\n",
      "Epoch 5381/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7494 - val_loss: 5.5840\n",
      "Epoch 5382/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6754 - val_loss: 6.0711\n",
      "Epoch 5383/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 5.0121 - val_loss: 5.7599\n",
      "Epoch 5384/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5558 - val_loss: 5.6264\n",
      "Epoch 5385/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5168 - val_loss: 5.5855\n",
      "Epoch 5386/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4834 - val_loss: 5.6964\n",
      "Epoch 5387/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5452 - val_loss: 5.5929\n",
      "Epoch 5388/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5613 - val_loss: 5.7570\n",
      "Epoch 5389/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.7750 - val_loss: 5.7078\n",
      "Epoch 5390/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.8867 - val_loss: 5.7720\n",
      "Epoch 5391/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7734 - val_loss: 5.6096\n",
      "Epoch 5392/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5588 - val_loss: 5.5411\n",
      "Epoch 5393/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7209 - val_loss: 5.7324\n",
      "Epoch 5394/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6019 - val_loss: 5.7070\n",
      "Epoch 5395/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5688 - val_loss: 5.6272\n",
      "Epoch 5396/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5690 - val_loss: 5.7422\n",
      "Epoch 5397/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5571 - val_loss: 5.6289\n",
      "Epoch 5398/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6892 - val_loss: 5.6885\n",
      "Epoch 5399/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6084 - val_loss: 5.6311\n",
      "Epoch 5400/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7201 - val_loss: 6.2200\n",
      "Epoch 5401/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 5.2027 - val_loss: 5.9753\n",
      "Epoch 5402/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6719 - val_loss: 5.5965\n",
      "Epoch 5403/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5841 - val_loss: 5.6201\n",
      "Epoch 5404/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5040 - val_loss: 5.5832\n",
      "Epoch 5405/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5055 - val_loss: 5.5707\n",
      "Epoch 5406/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6185 - val_loss: 5.6258\n",
      "Epoch 5407/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5315 - val_loss: 5.9848\n",
      "Epoch 5408/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6979 - val_loss: 5.6400\n",
      "Epoch 5409/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4819 - val_loss: 5.5735\n",
      "Epoch 5410/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5770 - val_loss: 5.7488\n",
      "Epoch 5411/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6381 - val_loss: 5.6556\n",
      "Epoch 5412/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5440 - val_loss: 5.6027\n",
      "Epoch 5413/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4832 - val_loss: 5.7154\n",
      "Epoch 5414/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0593 - val_loss: 6.6707\n",
      "Epoch 5415/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8143 - val_loss: 6.1951\n",
      "Epoch 5416/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8065 - val_loss: 5.6490\n",
      "Epoch 5417/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5241 - val_loss: 5.7225\n",
      "Epoch 5418/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5614 - val_loss: 5.6193\n",
      "Epoch 5419/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6569 - val_loss: 5.5667\n",
      "Epoch 5420/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5302 - val_loss: 5.6063\n",
      "Epoch 5421/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5887 - val_loss: 5.7854\n",
      "Epoch 5422/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7995 - val_loss: 5.7527\n",
      "Epoch 5423/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6470 - val_loss: 5.9409\n",
      "Epoch 5424/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6133 - val_loss: 6.0051\n",
      "Epoch 5425/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7056 - val_loss: 5.9991\n",
      "Epoch 5426/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7708 - val_loss: 5.5881\n",
      "Epoch 5427/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6182 - val_loss: 5.6382\n",
      "Epoch 5428/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5413 - val_loss: 5.5939\n",
      "Epoch 5429/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4993 - val_loss: 5.6597\n",
      "Epoch 5430/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6038 - val_loss: 5.6668\n",
      "Epoch 5431/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6865 - val_loss: 6.2022\n",
      "Epoch 5432/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6950 - val_loss: 5.8855\n",
      "Epoch 5433/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.7268 - val_loss: 5.6303\n",
      "Epoch 5434/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7507 - val_loss: 5.5475\n",
      "Epoch 5435/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5352 - val_loss: 5.6403\n",
      "Epoch 5436/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5751 - val_loss: 5.6978\n",
      "Epoch 5437/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7037 - val_loss: 5.5753\n",
      "Epoch 5438/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5410 - val_loss: 5.7629\n",
      "Epoch 5439/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5359 - val_loss: 5.8261\n",
      "Epoch 5440/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5658 - val_loss: 5.8201\n",
      "Epoch 5441/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.7233 - val_loss: 5.8370\n",
      "Epoch 5442/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5917 - val_loss: 5.6455\n",
      "Epoch 5443/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4742 - val_loss: 5.7360\n",
      "Epoch 5444/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5051 - val_loss: 5.7261\n",
      "Epoch 5445/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6246 - val_loss: 5.5750\n",
      "Epoch 5446/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5190 - val_loss: 5.6356\n",
      "Epoch 5447/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5804 - val_loss: 5.6341\n",
      "Epoch 5448/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4574 - val_loss: 5.5432\n",
      "Epoch 5449/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4819 - val_loss: 5.6632\n",
      "Epoch 5450/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5786 - val_loss: 5.6048\n",
      "Epoch 5451/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5753 - val_loss: 5.7158\n",
      "Epoch 5452/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5543 - val_loss: 5.5924\n",
      "Epoch 5453/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5210 - val_loss: 5.5896\n",
      "Epoch 5454/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6948 - val_loss: 5.5957\n",
      "Epoch 5455/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5475 - val_loss: 5.7747\n",
      "Epoch 5456/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5930 - val_loss: 5.8101\n",
      "Epoch 5457/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5801 - val_loss: 5.5542\n",
      "Epoch 5458/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4922 - val_loss: 5.6841\n",
      "Epoch 5459/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6380 - val_loss: 5.5870\n",
      "Epoch 5460/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4954 - val_loss: 5.6257\n",
      "Epoch 5461/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6521 - val_loss: 5.8545\n",
      "Epoch 5462/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7595 - val_loss: 5.6165\n",
      "Epoch 5463/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6806 - val_loss: 5.8405\n",
      "Epoch 5464/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5927 - val_loss: 5.5262\n",
      "Epoch 5465/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5275 - val_loss: 5.8254\n",
      "Epoch 5466/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5176 - val_loss: 5.5648\n",
      "Epoch 5467/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6139 - val_loss: 5.6077\n",
      "Epoch 5468/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5265 - val_loss: 5.6411\n",
      "Epoch 5469/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5554 - val_loss: 5.6563\n",
      "Epoch 5470/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4891 - val_loss: 5.6313\n",
      "Epoch 5471/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5615 - val_loss: 5.6904\n",
      "Epoch 5472/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4683 - val_loss: 5.5420\n",
      "Epoch 5473/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4637 - val_loss: 5.6233\n",
      "Epoch 5474/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5372 - val_loss: 5.5737\n",
      "Epoch 5475/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4533 - val_loss: 5.6455\n",
      "Epoch 5476/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5153 - val_loss: 5.6594\n",
      "Epoch 5477/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.5217 - val_loss: 5.6349\n",
      "Epoch 5478/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4981 - val_loss: 5.6519\n",
      "Epoch 5479/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5127 - val_loss: 5.8602\n",
      "Epoch 5480/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0692 - val_loss: 5.5925\n",
      "Epoch 5481/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6252 - val_loss: 5.5912\n",
      "Epoch 5482/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4792 - val_loss: 5.5893\n",
      "Epoch 5483/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4448 - val_loss: 5.5612\n",
      "Epoch 5484/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6305 - val_loss: 5.8863\n",
      "Epoch 5485/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6218 - val_loss: 5.5739\n",
      "Epoch 5486/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4848 - val_loss: 5.5853\n",
      "Epoch 5487/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4605 - val_loss: 5.6161\n",
      "Epoch 5488/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4839 - val_loss: 5.6279\n",
      "Epoch 5489/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5856 - val_loss: 5.7459\n",
      "Epoch 5490/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6068 - val_loss: 5.5856\n",
      "Epoch 5491/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5427 - val_loss: 5.6332\n",
      "Epoch 5492/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4938 - val_loss: 5.5916\n",
      "Epoch 5493/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4903 - val_loss: 5.7157\n",
      "Epoch 5494/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5223 - val_loss: 5.6609\n",
      "Epoch 5495/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5235 - val_loss: 5.6445\n",
      "Epoch 5496/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6698 - val_loss: 5.9246\n",
      "Epoch 5497/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6793 - val_loss: 5.6426\n",
      "Epoch 5498/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6028 - val_loss: 5.5501\n",
      "Epoch 5499/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5814 - val_loss: 5.8368\n",
      "Epoch 5500/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6729 - val_loss: 5.5448\n",
      "Epoch 5501/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5040 - val_loss: 5.5960\n",
      "Epoch 5502/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4756 - val_loss: 5.5379\n",
      "Epoch 5503/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4477 - val_loss: 5.7281\n",
      "Epoch 5504/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5336 - val_loss: 5.5242\n",
      "Epoch 5505/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5125 - val_loss: 5.7100\n",
      "Epoch 5506/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5606 - val_loss: 5.5639\n",
      "Epoch 5507/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4854 - val_loss: 5.7987\n",
      "Epoch 5508/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5053 - val_loss: 5.5426\n",
      "Epoch 5509/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4967 - val_loss: 6.1345\n",
      "Epoch 5510/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9113 - val_loss: 5.9009\n",
      "Epoch 5511/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5828 - val_loss: 5.7656\n",
      "Epoch 5512/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6428 - val_loss: 5.5455\n",
      "Epoch 5513/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4872 - val_loss: 5.5362\n",
      "Epoch 5514/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4720 - val_loss: 5.6183\n",
      "Epoch 5515/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4365 - val_loss: 5.8768\n",
      "Epoch 5516/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0134 - val_loss: 5.6451\n",
      "Epoch 5517/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4744 - val_loss: 5.7438\n",
      "Epoch 5518/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4754 - val_loss: 5.6351\n",
      "Epoch 5519/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6350 - val_loss: 5.6803\n",
      "Epoch 5520/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5989 - val_loss: 5.5676\n",
      "Epoch 5521/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5476 - val_loss: 5.9718\n",
      "Epoch 5522/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5858 - val_loss: 5.5625\n",
      "Epoch 5523/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4689 - val_loss: 5.8064\n",
      "Epoch 5524/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8010 - val_loss: 5.5709\n",
      "Epoch 5525/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7171 - val_loss: 5.6250\n",
      "Epoch 5526/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4755 - val_loss: 5.5984\n",
      "Epoch 5527/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4639 - val_loss: 5.8277\n",
      "Epoch 5528/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7210 - val_loss: 5.7934\n",
      "Epoch 5529/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6653 - val_loss: 5.5640\n",
      "Epoch 5530/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5343 - val_loss: 5.7336\n",
      "Epoch 5531/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5499 - val_loss: 5.6952\n",
      "Epoch 5532/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5883 - val_loss: 5.5610\n",
      "Epoch 5533/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4980 - val_loss: 5.6111\n",
      "Epoch 5534/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5601 - val_loss: 6.2433\n",
      "Epoch 5535/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6052 - val_loss: 5.6158\n",
      "Epoch 5536/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5721 - val_loss: 5.6113\n",
      "Epoch 5537/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5047 - val_loss: 5.5806\n",
      "Epoch 5538/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4726 - val_loss: 5.5599\n",
      "Epoch 5539/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4839 - val_loss: 5.5808\n",
      "Epoch 5540/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4972 - val_loss: 5.5357\n",
      "Epoch 5541/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5677 - val_loss: 5.5701\n",
      "Epoch 5542/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5793 - val_loss: 6.3533\n",
      "Epoch 5543/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5463 - val_loss: 5.5707\n",
      "Epoch 5544/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6749 - val_loss: 5.7573\n",
      "Epoch 5545/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5527 - val_loss: 5.5181\n",
      "Epoch 5546/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5746 - val_loss: 5.5654\n",
      "Epoch 5547/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5871 - val_loss: 5.5649\n",
      "Epoch 5548/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5084 - val_loss: 6.4573\n",
      "Epoch 5549/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9644 - val_loss: 5.6387\n",
      "Epoch 5550/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5669 - val_loss: 5.7016\n",
      "Epoch 5551/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5914 - val_loss: 5.5202\n",
      "Epoch 5552/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5282 - val_loss: 5.5431\n",
      "Epoch 5553/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.6070 - val_loss: 6.0969\n",
      "Epoch 5554/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7532 - val_loss: 5.7061\n",
      "Epoch 5555/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5880 - val_loss: 5.5965\n",
      "Epoch 5556/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4606 - val_loss: 5.5785\n",
      "Epoch 5557/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5688 - val_loss: 5.7928\n",
      "Epoch 5558/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5604 - val_loss: 5.6083\n",
      "Epoch 5559/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5537 - val_loss: 5.5846\n",
      "Epoch 5560/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4838 - val_loss: 5.6364\n",
      "Epoch 5561/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6119 - val_loss: 5.6044\n",
      "Epoch 5562/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4485 - val_loss: 5.7381\n",
      "Epoch 5563/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5107 - val_loss: 5.7176\n",
      "Epoch 5564/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6241 - val_loss: 5.5424\n",
      "Epoch 5565/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4922 - val_loss: 5.6039\n",
      "Epoch 5566/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5112 - val_loss: 5.5868\n",
      "Epoch 5567/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4926 - val_loss: 5.5673\n",
      "Epoch 5568/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4918 - val_loss: 5.5919\n",
      "Epoch 5569/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6076 - val_loss: 5.9547\n",
      "Epoch 5570/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6294 - val_loss: 5.4848\n",
      "Epoch 5571/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4883 - val_loss: 5.5601\n",
      "Epoch 5572/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6213 - val_loss: 5.8074\n",
      "Epoch 5573/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4991 - val_loss: 5.5723\n",
      "Epoch 5574/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4589 - val_loss: 5.5226\n",
      "Epoch 5575/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4798 - val_loss: 5.8104\n",
      "Epoch 5576/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5472 - val_loss: 5.7122\n",
      "Epoch 5577/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4972 - val_loss: 5.5854\n",
      "Epoch 5578/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5152 - val_loss: 5.5906\n",
      "Epoch 5579/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6275 - val_loss: 5.5698\n",
      "Epoch 5580/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4532 - val_loss: 5.5692\n",
      "Epoch 5581/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4305 - val_loss: 5.6529\n",
      "Epoch 5582/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4895 - val_loss: 5.5896\n",
      "Epoch 5583/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5109 - val_loss: 5.6075\n",
      "Epoch 5584/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5505 - val_loss: 5.5417\n",
      "Epoch 5585/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6964 - val_loss: 5.6399\n",
      "Epoch 5586/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4328 - val_loss: 5.6144\n",
      "Epoch 5587/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4410 - val_loss: 5.6635\n",
      "Epoch 5588/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6845 - val_loss: 5.5831\n",
      "Epoch 5589/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4777 - val_loss: 5.5209\n",
      "Epoch 5590/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4706 - val_loss: 5.9120\n",
      "Epoch 5591/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6016 - val_loss: 5.6884\n",
      "Epoch 5592/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6061 - val_loss: 5.6869\n",
      "Epoch 5593/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6054 - val_loss: 5.5463\n",
      "Epoch 5594/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6162 - val_loss: 5.9122\n",
      "Epoch 5595/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7612 - val_loss: 5.6196\n",
      "Epoch 5596/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5508 - val_loss: 5.5630\n",
      "Epoch 5597/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5563 - val_loss: 5.5771\n",
      "Epoch 5598/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5265 - val_loss: 5.8006\n",
      "Epoch 5599/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5248 - val_loss: 5.5679\n",
      "Epoch 5600/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7256 - val_loss: 5.8634\n",
      "Epoch 5601/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4528 - val_loss: 5.6186\n",
      "Epoch 5602/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5179 - val_loss: 5.5664\n",
      "Epoch 5603/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5089 - val_loss: 5.4984\n",
      "Epoch 5604/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4735 - val_loss: 5.6223\n",
      "Epoch 5605/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6435 - val_loss: 5.5843\n",
      "Epoch 5606/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4529 - val_loss: 5.7568\n",
      "Epoch 5607/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5191 - val_loss: 5.6017\n",
      "Epoch 5608/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5963 - val_loss: 6.1013\n",
      "Epoch 5609/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6737 - val_loss: 5.5925\n",
      "Epoch 5610/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5083 - val_loss: 5.9948\n",
      "Epoch 5611/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7945 - val_loss: 5.7259\n",
      "Epoch 5612/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5722 - val_loss: 5.5553\n",
      "Epoch 5613/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4737 - val_loss: 5.5248\n",
      "Epoch 5614/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5376 - val_loss: 5.7604\n",
      "Epoch 5615/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5716 - val_loss: 5.5496\n",
      "Epoch 5616/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5165 - val_loss: 5.7114\n",
      "Epoch 5617/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7553 - val_loss: 5.6964\n",
      "Epoch 5618/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5662 - val_loss: 5.5979\n",
      "Epoch 5619/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6589 - val_loss: 5.7319\n",
      "Epoch 5620/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5650 - val_loss: 5.5908\n",
      "Epoch 5621/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4427 - val_loss: 5.5261\n",
      "Epoch 5622/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5833 - val_loss: 6.1261\n",
      "Epoch 5623/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7932 - val_loss: 5.5388\n",
      "Epoch 5624/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8926 - val_loss: 5.7961\n",
      "Epoch 5625/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7125 - val_loss: 5.6552\n",
      "Epoch 5626/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6511 - val_loss: 5.7474\n",
      "Epoch 5627/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4727 - val_loss: 5.9729\n",
      "Epoch 5628/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5010 - val_loss: 5.5813\n",
      "Epoch 5629/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.4221 - val_loss: 5.5560\n",
      "Epoch 5630/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4626 - val_loss: 5.6027\n",
      "Epoch 5631/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4631 - val_loss: 6.3645\n",
      "Epoch 5632/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8311 - val_loss: 5.6171\n",
      "Epoch 5633/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4027 - val_loss: 5.5873\n",
      "Epoch 5634/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4480 - val_loss: 5.5900\n",
      "Epoch 5635/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4997 - val_loss: 5.7497\n",
      "Epoch 5636/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5689 - val_loss: 5.6107\n",
      "Epoch 5637/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5222 - val_loss: 5.5999\n",
      "Epoch 5638/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4490 - val_loss: 5.5016\n",
      "Epoch 5639/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5335 - val_loss: 5.7051\n",
      "Epoch 5640/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6690 - val_loss: 5.5980\n",
      "Epoch 5641/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4879 - val_loss: 5.5558\n",
      "Epoch 5642/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5397 - val_loss: 5.6211\n",
      "Epoch 5643/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4528 - val_loss: 5.6367\n",
      "Epoch 5644/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7559 - val_loss: 5.5285\n",
      "Epoch 5645/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5442 - val_loss: 5.5769\n",
      "Epoch 5646/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4181 - val_loss: 5.6828\n",
      "Epoch 5647/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5889 - val_loss: 5.5905\n",
      "Epoch 5648/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4902 - val_loss: 5.9712\n",
      "Epoch 5649/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7518 - val_loss: 5.8935\n",
      "Epoch 5650/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4388 - val_loss: 5.9078\n",
      "Epoch 5651/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4517 - val_loss: 5.5933\n",
      "Epoch 5652/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5369 - val_loss: 5.5373\n",
      "Epoch 5653/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4335 - val_loss: 5.5815\n",
      "Epoch 5654/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4520 - val_loss: 5.5652\n",
      "Epoch 5655/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4779 - val_loss: 5.6090\n",
      "Epoch 5656/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6850 - val_loss: 5.9185\n",
      "Epoch 5657/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6490 - val_loss: 5.6069\n",
      "Epoch 5658/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4629 - val_loss: 5.5849\n",
      "Epoch 5659/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5026 - val_loss: 6.0628\n",
      "Epoch 5660/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6772 - val_loss: 5.5266\n",
      "Epoch 5661/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4411 - val_loss: 5.7280\n",
      "Epoch 5662/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6988 - val_loss: 5.9186\n",
      "Epoch 5663/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6093 - val_loss: 5.6313\n",
      "Epoch 5664/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4716 - val_loss: 5.5525\n",
      "Epoch 5665/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5038 - val_loss: 5.5728\n",
      "Epoch 5666/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5347 - val_loss: 5.8084\n",
      "Epoch 5667/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4973 - val_loss: 5.5754\n",
      "Epoch 5668/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5539 - val_loss: 5.5817\n",
      "Epoch 5669/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4911 - val_loss: 5.5847\n",
      "Epoch 5670/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5141 - val_loss: 5.5227\n",
      "Epoch 5671/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4841 - val_loss: 5.5459\n",
      "Epoch 5672/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4621 - val_loss: 5.5693\n",
      "Epoch 5673/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5456 - val_loss: 5.7901\n",
      "Epoch 5674/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4562 - val_loss: 5.5245\n",
      "Epoch 5675/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5224 - val_loss: 5.5444\n",
      "Epoch 5676/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5413 - val_loss: 6.0298\n",
      "Epoch 5677/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5664 - val_loss: 5.6694\n",
      "Epoch 5678/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6079 - val_loss: 5.5439\n",
      "Epoch 5679/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4519 - val_loss: 5.6165\n",
      "Epoch 5680/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4348 - val_loss: 5.5267\n",
      "Epoch 5681/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4699 - val_loss: 5.5462\n",
      "Epoch 5682/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4284 - val_loss: 5.7739\n",
      "Epoch 5683/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5504 - val_loss: 5.5271\n",
      "Epoch 5684/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5149 - val_loss: 5.7362\n",
      "Epoch 5685/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4624 - val_loss: 5.5881\n",
      "Epoch 5686/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4991 - val_loss: 6.1118\n",
      "Epoch 5687/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6787 - val_loss: 5.5958\n",
      "Epoch 5688/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5902 - val_loss: 5.7667\n",
      "Epoch 5689/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.8706 - val_loss: 5.8120\n",
      "Epoch 5690/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5234 - val_loss: 5.5385\n",
      "Epoch 5691/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5020 - val_loss: 5.5998\n",
      "Epoch 5692/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5645 - val_loss: 5.5978\n",
      "Epoch 5693/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4658 - val_loss: 5.5782\n",
      "Epoch 5694/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4911 - val_loss: 5.5578\n",
      "Epoch 5695/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4680 - val_loss: 5.6061\n",
      "Epoch 5696/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6280 - val_loss: 5.6949\n",
      "Epoch 5697/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4938 - val_loss: 5.6420\n",
      "Epoch 5698/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5891 - val_loss: 5.5339\n",
      "Epoch 5699/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5290 - val_loss: 5.6412\n",
      "Epoch 5700/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6262 - val_loss: 5.5176\n",
      "Epoch 5701/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7124 - val_loss: 5.5827\n",
      "Epoch 5702/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4964 - val_loss: 5.5481\n",
      "Epoch 5703/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4911 - val_loss: 5.6000\n",
      "Epoch 5704/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4385 - val_loss: 5.5514\n",
      "Epoch 5705/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.5972 - val_loss: 5.5236\n",
      "Epoch 5706/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5198 - val_loss: 5.6569\n",
      "Epoch 5707/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5442 - val_loss: 6.0265\n",
      "Epoch 5708/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6147 - val_loss: 6.0893\n",
      "Epoch 5709/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5533 - val_loss: 5.6294\n",
      "Epoch 5710/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6037 - val_loss: 6.0574\n",
      "Epoch 5711/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6182 - val_loss: 5.5511\n",
      "Epoch 5712/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4845 - val_loss: 5.5779\n",
      "Epoch 5713/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5867 - val_loss: 5.8579\n",
      "Epoch 5714/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4486 - val_loss: 5.5721\n",
      "Epoch 5715/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4548 - val_loss: 5.6552\n",
      "Epoch 5716/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6075 - val_loss: 5.6429\n",
      "Epoch 5717/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4888 - val_loss: 5.5778\n",
      "Epoch 5718/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4232 - val_loss: 5.9674\n",
      "Epoch 5719/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7083 - val_loss: 5.6546\n",
      "Epoch 5720/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5055 - val_loss: 5.5253\n",
      "Epoch 5721/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4400 - val_loss: 5.5704\n",
      "Epoch 5722/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4827 - val_loss: 5.8193\n",
      "Epoch 5723/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7978 - val_loss: 6.0292\n",
      "Epoch 5724/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6280 - val_loss: 5.5575\n",
      "Epoch 5725/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4328 - val_loss: 5.5691\n",
      "Epoch 5726/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5360 - val_loss: 5.8041\n",
      "Epoch 5727/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6560 - val_loss: 5.7608\n",
      "Epoch 5728/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6963 - val_loss: 5.7145\n",
      "Epoch 5729/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5656 - val_loss: 5.5370\n",
      "Epoch 5730/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6203 - val_loss: 5.5241\n",
      "Epoch 5731/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5816 - val_loss: 5.8488\n",
      "Epoch 5732/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0237 - val_loss: 5.5778\n",
      "Epoch 5733/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6209 - val_loss: 5.4905\n",
      "Epoch 5734/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4770 - val_loss: 5.5682\n",
      "Epoch 5735/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4196 - val_loss: 5.5219\n",
      "Epoch 5736/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4576 - val_loss: 6.1264\n",
      "Epoch 5737/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0418 - val_loss: 5.7352\n",
      "Epoch 5738/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6124 - val_loss: 5.6405\n",
      "Epoch 5739/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4996 - val_loss: 5.5151\n",
      "Epoch 5740/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4987 - val_loss: 6.0441\n",
      "Epoch 5741/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6074 - val_loss: 5.7736\n",
      "Epoch 5742/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4993 - val_loss: 5.6022\n",
      "Epoch 5743/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5149 - val_loss: 5.6885\n",
      "Epoch 5744/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4599 - val_loss: 5.5396\n",
      "Epoch 5745/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5226 - val_loss: 5.7087\n",
      "Epoch 5746/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6774 - val_loss: 5.5539\n",
      "Epoch 5747/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4581 - val_loss: 5.6148\n",
      "Epoch 5748/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4556 - val_loss: 5.5680\n",
      "Epoch 5749/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5190 - val_loss: 5.6599\n",
      "Epoch 5750/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5728 - val_loss: 5.6063\n",
      "Epoch 5751/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4943 - val_loss: 5.5097\n",
      "Epoch 5752/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5437 - val_loss: 5.8214\n",
      "Epoch 5753/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5322 - val_loss: 5.5662\n",
      "Epoch 5754/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6089 - val_loss: 5.5474\n",
      "Epoch 5755/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6365 - val_loss: 5.5018\n",
      "Epoch 5756/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4843 - val_loss: 5.5910\n",
      "Epoch 5757/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4119 - val_loss: 5.8804\n",
      "Epoch 5758/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5533 - val_loss: 5.7922\n",
      "Epoch 5759/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4528 - val_loss: 5.5628\n",
      "Epoch 5760/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6726 - val_loss: 5.5954\n",
      "Epoch 5761/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6585 - val_loss: 5.7042\n",
      "Epoch 5762/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5691 - val_loss: 5.5015\n",
      "Epoch 5763/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4788 - val_loss: 5.6047\n",
      "Epoch 5764/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4569 - val_loss: 5.5544\n",
      "Epoch 5765/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5450 - val_loss: 5.7277\n",
      "Epoch 5766/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5044 - val_loss: 5.8538\n",
      "Epoch 5767/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7309 - val_loss: 5.7120\n",
      "Epoch 5768/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5773 - val_loss: 5.5230\n",
      "Epoch 5769/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6871 - val_loss: 5.6786\n",
      "Epoch 5770/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6503 - val_loss: 5.5349\n",
      "Epoch 5771/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7469 - val_loss: 5.4656\n",
      "Epoch 5772/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7166 - val_loss: 6.1234\n",
      "Epoch 5773/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7505 - val_loss: 5.8332\n",
      "Epoch 5774/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5496 - val_loss: 5.6641\n",
      "Epoch 5775/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5370 - val_loss: 5.8476\n",
      "Epoch 5776/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5351 - val_loss: 5.5126\n",
      "Epoch 5777/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4586 - val_loss: 5.5017\n",
      "Epoch 5778/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4106 - val_loss: 5.7928\n",
      "Epoch 5779/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7368 - val_loss: 5.5895\n",
      "Epoch 5780/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5265 - val_loss: 5.5602\n",
      "Epoch 5781/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.5656 - val_loss: 5.5684\n",
      "Epoch 5782/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4638 - val_loss: 5.5389\n",
      "Epoch 5783/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7335 - val_loss: 5.5737\n",
      "Epoch 5784/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4850 - val_loss: 5.6355\n",
      "Epoch 5785/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5495 - val_loss: 5.5941\n",
      "Epoch 5786/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4721 - val_loss: 5.5440\n",
      "Epoch 5787/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4169 - val_loss: 5.6387\n",
      "Epoch 5788/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6702 - val_loss: 5.9788\n",
      "Epoch 5789/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6303 - val_loss: 5.5766\n",
      "Epoch 5790/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5094 - val_loss: 5.5432\n",
      "Epoch 5791/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4744 - val_loss: 5.5686\n",
      "Epoch 5792/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4558 - val_loss: 5.5493\n",
      "Epoch 5793/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4554 - val_loss: 5.5883\n",
      "Epoch 5794/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4923 - val_loss: 5.6084\n",
      "Epoch 5795/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5300 - val_loss: 5.6575\n",
      "Epoch 5796/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4899 - val_loss: 5.5951\n",
      "Epoch 5797/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4065 - val_loss: 5.5276\n",
      "Epoch 5798/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4501 - val_loss: 5.5506\n",
      "Epoch 5799/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4669 - val_loss: 5.5363\n",
      "Epoch 5800/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5104 - val_loss: 5.8831\n",
      "Epoch 5801/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5174 - val_loss: 5.5466\n",
      "Epoch 5802/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4959 - val_loss: 5.6808\n",
      "Epoch 5803/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4280 - val_loss: 5.5206\n",
      "Epoch 5804/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4244 - val_loss: 5.6116\n",
      "Epoch 5805/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6857 - val_loss: 5.5397\n",
      "Epoch 5806/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4795 - val_loss: 5.5253\n",
      "Epoch 5807/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4270 - val_loss: 5.5874\n",
      "Epoch 5808/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5823 - val_loss: 5.9529\n",
      "Epoch 5809/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5729 - val_loss: 5.8846\n",
      "Epoch 5810/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6448 - val_loss: 5.5625\n",
      "Epoch 5811/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4920 - val_loss: 5.6233\n",
      "Epoch 5812/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4163 - val_loss: 5.6168\n",
      "Epoch 5813/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4874 - val_loss: 5.6970\n",
      "Epoch 5814/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7020 - val_loss: 5.5852\n",
      "Epoch 5815/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4137 - val_loss: 5.6280\n",
      "Epoch 5816/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5695 - val_loss: 5.5630\n",
      "Epoch 5817/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5436 - val_loss: 5.4996\n",
      "Epoch 5818/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4280 - val_loss: 5.5475\n",
      "Epoch 5819/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5527 - val_loss: 5.5606\n",
      "Epoch 5820/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5883 - val_loss: 5.8958\n",
      "Epoch 5821/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5977 - val_loss: 5.7779\n",
      "Epoch 5822/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5523 - val_loss: 5.4932\n",
      "Epoch 5823/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4899 - val_loss: 5.5474\n",
      "Epoch 5824/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5044 - val_loss: 5.5614\n",
      "Epoch 5825/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5611 - val_loss: 5.5928\n",
      "Epoch 5826/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4249 - val_loss: 5.6100\n",
      "Epoch 5827/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5105 - val_loss: 5.8618\n",
      "Epoch 5828/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5298 - val_loss: 5.6480\n",
      "Epoch 5829/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5409 - val_loss: 5.5585\n",
      "Epoch 5830/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5441 - val_loss: 5.8538\n",
      "Epoch 5831/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7559 - val_loss: 6.0683\n",
      "Epoch 5832/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5239 - val_loss: 5.5260\n",
      "Epoch 5833/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5783 - val_loss: 5.5173\n",
      "Epoch 5834/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4814 - val_loss: 5.5333\n",
      "Epoch 5835/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4033 - val_loss: 5.5193\n",
      "Epoch 5836/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4209 - val_loss: 5.5155\n",
      "Epoch 5837/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5343 - val_loss: 5.5742\n",
      "Epoch 5838/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4386 - val_loss: 5.5052\n",
      "Epoch 5839/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4310 - val_loss: 5.5561\n",
      "Epoch 5840/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4319 - val_loss: 5.5419\n",
      "Epoch 5841/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6851 - val_loss: 6.0792\n",
      "Epoch 5842/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5092 - val_loss: 5.7365\n",
      "Epoch 5843/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5276 - val_loss: 5.5983\n",
      "Epoch 5844/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6043 - val_loss: 5.5678\n",
      "Epoch 5845/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3824 - val_loss: 5.7134\n",
      "Epoch 5846/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6729 - val_loss: 5.7516\n",
      "Epoch 5847/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4207 - val_loss: 5.7903\n",
      "Epoch 5848/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6087 - val_loss: 5.8041\n",
      "Epoch 5849/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5522 - val_loss: 5.5145\n",
      "Epoch 5850/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6103 - val_loss: 5.6460\n",
      "Epoch 5851/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5159 - val_loss: 5.5648\n",
      "Epoch 5852/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5214 - val_loss: 5.7139\n",
      "Epoch 5853/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5420 - val_loss: 5.5658\n",
      "Epoch 5854/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4745 - val_loss: 5.5084\n",
      "Epoch 5855/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4193 - val_loss: 5.5530\n",
      "Epoch 5856/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3926 - val_loss: 5.5270\n",
      "Epoch 5857/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.4364 - val_loss: 5.5599\n",
      "Epoch 5858/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5128 - val_loss: 5.8603\n",
      "Epoch 5859/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5980 - val_loss: 5.9850\n",
      "Epoch 5860/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4827 - val_loss: 5.5815\n",
      "Epoch 5861/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4441 - val_loss: 5.5868\n",
      "Epoch 5862/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3824 - val_loss: 5.5488\n",
      "Epoch 5863/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4176 - val_loss: 5.6576\n",
      "Epoch 5864/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4366 - val_loss: 5.5636\n",
      "Epoch 5865/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4658 - val_loss: 5.5512\n",
      "Epoch 5866/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4285 - val_loss: 5.6966\n",
      "Epoch 5867/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5782 - val_loss: 5.6645\n",
      "Epoch 5868/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5142 - val_loss: 5.5840\n",
      "Epoch 5869/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5170 - val_loss: 5.6699\n",
      "Epoch 5870/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5696 - val_loss: 5.6658\n",
      "Epoch 5871/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7060 - val_loss: 5.6306\n",
      "Epoch 5872/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3802 - val_loss: 5.5923\n",
      "Epoch 5873/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4283 - val_loss: 5.5671\n",
      "Epoch 5874/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3850 - val_loss: 5.5541\n",
      "Epoch 5875/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6300 - val_loss: 5.8570\n",
      "Epoch 5876/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5675 - val_loss: 5.8669\n",
      "Epoch 5877/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 5.0854 - val_loss: 5.6151\n",
      "Epoch 5878/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5715 - val_loss: 5.5837\n",
      "Epoch 5879/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4339 - val_loss: 5.6249\n",
      "Epoch 5880/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4081 - val_loss: 5.5015\n",
      "Epoch 5881/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5283 - val_loss: 5.6641\n",
      "Epoch 5882/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4295 - val_loss: 5.5568\n",
      "Epoch 5883/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4035 - val_loss: 5.5264\n",
      "Epoch 5884/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4916 - val_loss: 5.7893\n",
      "Epoch 5885/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4546 - val_loss: 5.5499\n",
      "Epoch 5886/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3835 - val_loss: 5.5342\n",
      "Epoch 5887/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4679 - val_loss: 5.6630\n",
      "Epoch 5888/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5278 - val_loss: 5.5448\n",
      "Epoch 5889/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4973 - val_loss: 5.5259\n",
      "Epoch 5890/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5009 - val_loss: 5.5321\n",
      "Epoch 5891/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6191 - val_loss: 6.2449\n",
      "Epoch 5892/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7263 - val_loss: 5.4986\n",
      "Epoch 5893/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4314 - val_loss: 5.6940\n",
      "Epoch 5894/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5106 - val_loss: 5.6612\n",
      "Epoch 5895/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5471 - val_loss: 5.8425\n",
      "Epoch 5896/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6888 - val_loss: 5.5443\n",
      "Epoch 5897/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5120 - val_loss: 6.0084\n",
      "Epoch 5898/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4967 - val_loss: 5.5439\n",
      "Epoch 5899/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4205 - val_loss: 5.6379\n",
      "Epoch 5900/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4934 - val_loss: 5.6088\n",
      "Epoch 5901/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5623 - val_loss: 5.7777\n",
      "Epoch 5902/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.9180 - val_loss: 6.4448\n",
      "Epoch 5903/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5411 - val_loss: 5.5162\n",
      "Epoch 5904/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4265 - val_loss: 5.6611\n",
      "Epoch 5905/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5364 - val_loss: 5.5415\n",
      "Epoch 5906/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4370 - val_loss: 5.6421\n",
      "Epoch 5907/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4924 - val_loss: 5.5196\n",
      "Epoch 5908/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3964 - val_loss: 5.5425\n",
      "Epoch 5909/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4277 - val_loss: 5.5617\n",
      "Epoch 5910/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5547 - val_loss: 5.5529\n",
      "Epoch 5911/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5453 - val_loss: 5.5972\n",
      "Epoch 5912/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4261 - val_loss: 5.5721\n",
      "Epoch 5913/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5016 - val_loss: 5.5473\n",
      "Epoch 5914/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4805 - val_loss: 5.6318\n",
      "Epoch 5915/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4331 - val_loss: 5.4955\n",
      "Epoch 5916/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.9918 - val_loss: 5.7424\n",
      "Epoch 5917/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5190 - val_loss: 5.5246\n",
      "Epoch 5918/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5700 - val_loss: 6.1609\n",
      "Epoch 5919/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6118 - val_loss: 5.5620\n",
      "Epoch 5920/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4514 - val_loss: 5.5628\n",
      "Epoch 5921/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4905 - val_loss: 5.7087\n",
      "Epoch 5922/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6009 - val_loss: 5.5665\n",
      "Epoch 5923/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8206 - val_loss: 5.8277\n",
      "Epoch 5924/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4899 - val_loss: 5.5186\n",
      "Epoch 5925/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4076 - val_loss: 5.5840\n",
      "Epoch 5926/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4660 - val_loss: 5.8438\n",
      "Epoch 5927/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5931 - val_loss: 5.5484\n",
      "Epoch 5928/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4168 - val_loss: 5.5772\n",
      "Epoch 5929/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4145 - val_loss: 5.5471\n",
      "Epoch 5930/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5821 - val_loss: 5.6447\n",
      "Epoch 5931/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4713 - val_loss: 6.0263\n",
      "Epoch 5932/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6783 - val_loss: 5.6363\n",
      "Epoch 5933/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.3717 - val_loss: 5.5550\n",
      "Epoch 5934/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4417 - val_loss: 5.5015\n",
      "Epoch 5935/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3635 - val_loss: 5.5460\n",
      "Epoch 5936/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3867 - val_loss: 5.5991\n",
      "Epoch 5937/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5369 - val_loss: 5.5203\n",
      "Epoch 5938/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5399 - val_loss: 5.6128\n",
      "Epoch 5939/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5173 - val_loss: 5.5053\n",
      "Epoch 5940/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4541 - val_loss: 5.4666\n",
      "Epoch 5941/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4160 - val_loss: 5.5679\n",
      "Epoch 5942/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6270 - val_loss: 5.6113\n",
      "Epoch 5943/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4992 - val_loss: 5.7914\n",
      "Epoch 5944/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4815 - val_loss: 5.5354\n",
      "Epoch 5945/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4537 - val_loss: 5.5733\n",
      "Epoch 5946/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3632 - val_loss: 5.5764\n",
      "Epoch 5947/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4599 - val_loss: 5.5705\n",
      "Epoch 5948/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6640 - val_loss: 5.8413\n",
      "Epoch 5949/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4527 - val_loss: 5.7023\n",
      "Epoch 5950/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4745 - val_loss: 5.6476\n",
      "Epoch 5951/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6051 - val_loss: 5.6529\n",
      "Epoch 5952/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4812 - val_loss: 5.5372\n",
      "Epoch 5953/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7702 - val_loss: 5.7788\n",
      "Epoch 5954/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4685 - val_loss: 5.4761\n",
      "Epoch 5955/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5649 - val_loss: 5.6838\n",
      "Epoch 5956/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5842 - val_loss: 5.6281\n",
      "Epoch 5957/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4723 - val_loss: 5.6537\n",
      "Epoch 5958/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5611 - val_loss: 5.5356\n",
      "Epoch 5959/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4532 - val_loss: 5.5742\n",
      "Epoch 5960/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4008 - val_loss: 5.6809\n",
      "Epoch 5961/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4031 - val_loss: 5.5238\n",
      "Epoch 5962/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4677 - val_loss: 5.6010\n",
      "Epoch 5963/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5791 - val_loss: 5.5552\n",
      "Epoch 5964/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3652 - val_loss: 5.5979\n",
      "Epoch 5965/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3626 - val_loss: 5.5534\n",
      "Epoch 5966/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4591 - val_loss: 5.6230\n",
      "Epoch 5967/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5005 - val_loss: 5.5880\n",
      "Epoch 5968/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3805 - val_loss: 5.5790\n",
      "Epoch 5969/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6815 - val_loss: 5.5070\n",
      "Epoch 5970/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4380 - val_loss: 5.5598\n",
      "Epoch 5971/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4106 - val_loss: 5.5482\n",
      "Epoch 5972/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4210 - val_loss: 5.5733\n",
      "Epoch 5973/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3861 - val_loss: 5.6613\n",
      "Epoch 5974/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4101 - val_loss: 5.5903\n",
      "Epoch 5975/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4701 - val_loss: 5.5316\n",
      "Epoch 5976/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3998 - val_loss: 5.5986\n",
      "Epoch 5977/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4556 - val_loss: 5.6064\n",
      "Epoch 5978/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4693 - val_loss: 5.5708\n",
      "Epoch 5979/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4966 - val_loss: 5.7626\n",
      "Epoch 5980/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5183 - val_loss: 5.5344\n",
      "Epoch 5981/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5249 - val_loss: 5.5445\n",
      "Epoch 5982/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5664 - val_loss: 5.9393\n",
      "Epoch 5983/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6348 - val_loss: 5.7969\n",
      "Epoch 5984/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4951 - val_loss: 5.6280\n",
      "Epoch 5985/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4642 - val_loss: 5.6788\n",
      "Epoch 5986/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4678 - val_loss: 5.6040\n",
      "Epoch 5987/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3890 - val_loss: 5.6507\n",
      "Epoch 5988/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4112 - val_loss: 5.6045\n",
      "Epoch 5989/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5023 - val_loss: 5.5089\n",
      "Epoch 5990/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4275 - val_loss: 5.6900\n",
      "Epoch 5991/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6049 - val_loss: 5.5622\n",
      "Epoch 5992/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4609 - val_loss: 5.6846\n",
      "Epoch 5993/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4009 - val_loss: 5.8474\n",
      "Epoch 5994/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8028 - val_loss: 5.4679\n",
      "Epoch 5995/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7408 - val_loss: 5.8387\n",
      "Epoch 5996/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6893 - val_loss: 5.4692\n",
      "Epoch 5997/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4800 - val_loss: 5.7798\n",
      "Epoch 5998/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4833 - val_loss: 5.5881\n",
      "Epoch 5999/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4239 - val_loss: 5.5569\n",
      "Epoch 6000/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4150 - val_loss: 5.5257\n",
      "Epoch 6001/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4580 - val_loss: 5.8439\n",
      "Epoch 6002/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4797 - val_loss: 5.5110\n",
      "Epoch 6003/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3743 - val_loss: 5.7995\n",
      "Epoch 6004/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4590 - val_loss: 5.5590\n",
      "Epoch 6005/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5325 - val_loss: 5.5579\n",
      "Epoch 6006/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4903 - val_loss: 5.5197\n",
      "Epoch 6007/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3714 - val_loss: 5.5042\n",
      "Epoch 6008/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3919 - val_loss: 5.4819\n",
      "Epoch 6009/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.3797 - val_loss: 5.6083\n",
      "Epoch 6010/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4463 - val_loss: 5.5253\n",
      "Epoch 6011/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3991 - val_loss: 5.5328\n",
      "Epoch 6012/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3745 - val_loss: 5.5779\n",
      "Epoch 6013/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.4063 - val_loss: 5.6940\n",
      "Epoch 6014/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4374 - val_loss: 5.6461\n",
      "Epoch 6015/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 4.4940 - val_loss: 5.5017\n",
      "Epoch 6016/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3967 - val_loss: 5.5070\n",
      "Epoch 6017/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4799 - val_loss: 5.5329\n",
      "Epoch 6018/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.4256 - val_loss: 5.5648\n",
      "Epoch 6019/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.4651 - val_loss: 5.8164\n",
      "Epoch 6020/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6630 - val_loss: 5.4955\n",
      "Epoch 6021/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5431 - val_loss: 5.5392\n",
      "Epoch 6022/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4052 - val_loss: 5.6241\n",
      "Epoch 6023/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4300 - val_loss: 5.5998\n",
      "Epoch 6024/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5173 - val_loss: 5.5419\n",
      "Epoch 6025/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4570 - val_loss: 5.5351\n",
      "Epoch 6026/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4290 - val_loss: 5.5661\n",
      "Epoch 6027/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4689 - val_loss: 5.5248\n",
      "Epoch 6028/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3876 - val_loss: 5.5260\n",
      "Epoch 6029/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5969 - val_loss: 5.5601\n",
      "Epoch 6030/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5405 - val_loss: 5.5375\n",
      "Epoch 6031/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3791 - val_loss: 5.7640\n",
      "Epoch 6032/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4377 - val_loss: 5.6332\n",
      "Epoch 6033/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3720 - val_loss: 5.5354\n",
      "Epoch 6034/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5183 - val_loss: 5.5533\n",
      "Epoch 6035/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7923 - val_loss: 5.7119\n",
      "Epoch 6036/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5993 - val_loss: 5.9121\n",
      "Epoch 6037/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6528 - val_loss: 5.6375\n",
      "Epoch 6038/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3863 - val_loss: 5.7787\n",
      "Epoch 6039/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7931 - val_loss: 6.1468\n",
      "Epoch 6040/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6016 - val_loss: 5.6023\n",
      "Epoch 6041/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6512 - val_loss: 5.7036\n",
      "Epoch 6042/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4715 - val_loss: 5.5522\n",
      "Epoch 6043/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5093 - val_loss: 5.7189\n",
      "Epoch 6044/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3798 - val_loss: 5.5537\n",
      "Epoch 6045/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4126 - val_loss: 5.5536\n",
      "Epoch 6046/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3880 - val_loss: 5.7138\n",
      "Epoch 6047/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4260 - val_loss: 5.8335\n",
      "Epoch 6048/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5222 - val_loss: 5.6117\n",
      "Epoch 6049/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4127 - val_loss: 5.5181\n",
      "Epoch 6050/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5916 - val_loss: 5.6037\n",
      "Epoch 6051/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5973 - val_loss: 5.5210\n",
      "Epoch 6052/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5489 - val_loss: 5.5056\n",
      "Epoch 6053/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4032 - val_loss: 6.4469\n",
      "Epoch 6054/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 5.0086 - val_loss: 6.0691\n",
      "Epoch 6055/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6515 - val_loss: 5.5398\n",
      "Epoch 6056/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5039 - val_loss: 5.6128\n",
      "Epoch 6057/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3862 - val_loss: 5.5404\n",
      "Epoch 6058/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3796 - val_loss: 5.7605\n",
      "Epoch 6059/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5636 - val_loss: 5.6024\n",
      "Epoch 6060/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3723 - val_loss: 5.5840\n",
      "Epoch 6061/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3640 - val_loss: 5.5729\n",
      "Epoch 6062/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5190 - val_loss: 5.5749\n",
      "Epoch 6063/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6026 - val_loss: 5.5235\n",
      "Epoch 6064/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4265 - val_loss: 5.5182\n",
      "Epoch 6065/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4029 - val_loss: 5.5225\n",
      "Epoch 6066/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3782 - val_loss: 5.5815\n",
      "Epoch 6067/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3669 - val_loss: 5.6104\n",
      "Epoch 6068/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3983 - val_loss: 5.4595\n",
      "Epoch 6069/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3911 - val_loss: 5.4992\n",
      "Epoch 6070/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3911 - val_loss: 5.4674\n",
      "Epoch 6071/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6595 - val_loss: 6.4194\n",
      "Epoch 6072/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 5.0036 - val_loss: 5.5417\n",
      "Epoch 6073/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.5891 - val_loss: 5.5908\n",
      "Epoch 6074/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4672 - val_loss: 5.5072\n",
      "Epoch 6075/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3848 - val_loss: 5.5477\n",
      "Epoch 6076/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3999 - val_loss: 5.8932\n",
      "Epoch 6077/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6847 - val_loss: 5.6103\n",
      "Epoch 6078/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4343 - val_loss: 5.5863\n",
      "Epoch 6079/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4761 - val_loss: 5.9980\n",
      "Epoch 6080/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5984 - val_loss: 5.5904\n",
      "Epoch 6081/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5157 - val_loss: 5.6451\n",
      "Epoch 6082/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6361 - val_loss: 5.6067\n",
      "Epoch 6083/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6921 - val_loss: 5.8826\n",
      "Epoch 6084/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3814 - val_loss: 5.5301\n",
      "Epoch 6085/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.4199 - val_loss: 5.4539\n",
      "Epoch 6086/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4721 - val_loss: 5.6976\n",
      "Epoch 6087/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5709 - val_loss: 5.4969\n",
      "Epoch 6088/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3462 - val_loss: 5.5640\n",
      "Epoch 6089/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3657 - val_loss: 5.5109\n",
      "Epoch 6090/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4379 - val_loss: 5.5249\n",
      "Epoch 6091/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3712 - val_loss: 5.5254\n",
      "Epoch 6092/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3717 - val_loss: 5.5403\n",
      "Epoch 6093/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5483 - val_loss: 5.7952\n",
      "Epoch 6094/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4246 - val_loss: 5.5286\n",
      "Epoch 6095/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4142 - val_loss: 5.6920\n",
      "Epoch 6096/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4258 - val_loss: 5.5162\n",
      "Epoch 6097/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3988 - val_loss: 5.5484\n",
      "Epoch 6098/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4366 - val_loss: 5.5184\n",
      "Epoch 6099/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6834 - val_loss: 5.5455\n",
      "Epoch 6100/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5967 - val_loss: 5.5410\n",
      "Epoch 6101/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3678 - val_loss: 5.4985\n",
      "Epoch 6102/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4513 - val_loss: 5.5678\n",
      "Epoch 6103/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5467 - val_loss: 5.5559\n",
      "Epoch 6104/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5372 - val_loss: 5.4988\n",
      "Epoch 6105/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4426 - val_loss: 5.5086\n",
      "Epoch 6106/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5056 - val_loss: 5.7942\n",
      "Epoch 6107/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6869 - val_loss: 5.7769\n",
      "Epoch 6108/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6339 - val_loss: 5.6172\n",
      "Epoch 6109/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3960 - val_loss: 5.5614\n",
      "Epoch 6110/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4078 - val_loss: 5.5323\n",
      "Epoch 6111/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3471 - val_loss: 5.7064\n",
      "Epoch 6112/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4408 - val_loss: 5.7653\n",
      "Epoch 6113/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5782 - val_loss: 5.6160\n",
      "Epoch 6114/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4131 - val_loss: 5.4999\n",
      "Epoch 6115/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3505 - val_loss: 5.5411\n",
      "Epoch 6116/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5715 - val_loss: 5.6199\n",
      "Epoch 6117/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5374 - val_loss: 5.5079\n",
      "Epoch 6118/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3934 - val_loss: 5.5071\n",
      "Epoch 6119/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3662 - val_loss: 5.5725\n",
      "Epoch 6120/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4267 - val_loss: 5.6121\n",
      "Epoch 6121/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4290 - val_loss: 5.6136\n",
      "Epoch 6122/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4656 - val_loss: 5.5703\n",
      "Epoch 6123/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3698 - val_loss: 5.5749\n",
      "Epoch 6124/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3981 - val_loss: 5.5183\n",
      "Epoch 6125/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3709 - val_loss: 5.5893\n",
      "Epoch 6126/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3960 - val_loss: 5.5717\n",
      "Epoch 6127/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3552 - val_loss: 5.5309\n",
      "Epoch 6128/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3970 - val_loss: 5.5669\n",
      "Epoch 6129/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3657 - val_loss: 5.5214\n",
      "Epoch 6130/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5104 - val_loss: 5.5394\n",
      "Epoch 6131/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4189 - val_loss: 5.6298\n",
      "Epoch 6132/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5611 - val_loss: 5.5667\n",
      "Epoch 6133/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6489 - val_loss: 6.6004\n",
      "Epoch 6134/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8310 - val_loss: 5.5695\n",
      "Epoch 6135/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3826 - val_loss: 5.5710\n",
      "Epoch 6136/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4047 - val_loss: 5.7033\n",
      "Epoch 6137/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6975 - val_loss: 5.7492\n",
      "Epoch 6138/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7821 - val_loss: 5.4868\n",
      "Epoch 6139/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5203 - val_loss: 5.5020\n",
      "Epoch 6140/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3703 - val_loss: 5.6322\n",
      "Epoch 6141/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6558 - val_loss: 5.6530\n",
      "Epoch 6142/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4271 - val_loss: 5.4762\n",
      "Epoch 6143/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4085 - val_loss: 5.4783\n",
      "Epoch 6144/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4528 - val_loss: 5.6148\n",
      "Epoch 6145/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4418 - val_loss: 5.6016\n",
      "Epoch 6146/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3770 - val_loss: 5.5463\n",
      "Epoch 6147/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5069 - val_loss: 5.4899\n",
      "Epoch 6148/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4599 - val_loss: 5.5199\n",
      "Epoch 6149/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3665 - val_loss: 5.5113\n",
      "Epoch 6150/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4742 - val_loss: 5.6024\n",
      "Epoch 6151/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4562 - val_loss: 5.5744\n",
      "Epoch 6152/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3917 - val_loss: 5.6714\n",
      "Epoch 6153/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3532 - val_loss: 5.5956\n",
      "Epoch 6154/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4119 - val_loss: 5.6030\n",
      "Epoch 6155/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6047 - val_loss: 5.7053\n",
      "Epoch 6156/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3559 - val_loss: 5.5796\n",
      "Epoch 6157/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3970 - val_loss: 5.4684\n",
      "Epoch 6158/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3610 - val_loss: 5.5846\n",
      "Epoch 6159/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5788 - val_loss: 5.5496\n",
      "Epoch 6160/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.3705 - val_loss: 5.5142\n",
      "Epoch 6161/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 4.3882 - val_loss: 5.5554\n",
      "Epoch 6162/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4380 - val_loss: 5.8422\n",
      "Epoch 6163/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4494 - val_loss: 5.8027\n",
      "Epoch 6164/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5563 - val_loss: 5.5950\n",
      "Epoch 6165/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3892 - val_loss: 5.5618\n",
      "Epoch 6166/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4809 - val_loss: 5.5600\n",
      "Epoch 6167/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3786 - val_loss: 5.6012\n",
      "Epoch 6168/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4081 - val_loss: 5.5974\n",
      "Epoch 6169/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4949 - val_loss: 5.8646\n",
      "Epoch 6170/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4537 - val_loss: 5.4699\n",
      "Epoch 6171/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4842 - val_loss: 5.5133\n",
      "Epoch 6172/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3868 - val_loss: 5.5306\n",
      "Epoch 6173/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3952 - val_loss: 5.5052\n",
      "Epoch 6174/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5906 - val_loss: 5.4945\n",
      "Epoch 6175/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3769 - val_loss: 5.6057\n",
      "Epoch 6176/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6290 - val_loss: 5.7380\n",
      "Epoch 6177/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5167 - val_loss: 5.4684\n",
      "Epoch 6178/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3538 - val_loss: 5.5801\n",
      "Epoch 6179/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4303 - val_loss: 5.5931\n",
      "Epoch 6180/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3896 - val_loss: 5.5575\n",
      "Epoch 6181/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3800 - val_loss: 5.6765\n",
      "Epoch 6182/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4066 - val_loss: 5.4769\n",
      "Epoch 6183/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4207 - val_loss: 5.5186\n",
      "Epoch 6184/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5243 - val_loss: 5.6832\n",
      "Epoch 6185/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5048 - val_loss: 5.5100\n",
      "Epoch 6186/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3722 - val_loss: 5.6031\n",
      "Epoch 6187/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4149 - val_loss: 5.4758\n",
      "Epoch 6188/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5428 - val_loss: 5.5200\n",
      "Epoch 6189/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3850 - val_loss: 5.5845\n",
      "Epoch 6190/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3769 - val_loss: 5.5543\n",
      "Epoch 6191/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3576 - val_loss: 5.6772\n",
      "Epoch 6192/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4258 - val_loss: 5.5903\n",
      "Epoch 6193/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3730 - val_loss: 5.4756\n",
      "Epoch 6194/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4747 - val_loss: 6.0219\n",
      "Epoch 6195/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.8276 - val_loss: 5.5705\n",
      "Epoch 6196/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3783 - val_loss: 5.5385\n",
      "Epoch 6197/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4477 - val_loss: 5.5641\n",
      "Epoch 6198/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4058 - val_loss: 5.7834\n",
      "Epoch 6199/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3872 - val_loss: 5.8769\n",
      "Epoch 6200/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5298 - val_loss: 5.7045\n",
      "Epoch 6201/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3760 - val_loss: 5.7852\n",
      "Epoch 6202/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3801 - val_loss: 5.4964\n",
      "Epoch 6203/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5175 - val_loss: 6.0546\n",
      "Epoch 6204/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8036 - val_loss: 5.7843\n",
      "Epoch 6205/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4521 - val_loss: 5.6062\n",
      "Epoch 6206/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3845 - val_loss: 5.5371\n",
      "Epoch 6207/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3497 - val_loss: 5.5576\n",
      "Epoch 6208/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5214 - val_loss: 5.7278\n",
      "Epoch 6209/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7791 - val_loss: 5.6001\n",
      "Epoch 6210/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6294 - val_loss: 5.5672\n",
      "Epoch 6211/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4491 - val_loss: 5.7019\n",
      "Epoch 6212/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5721 - val_loss: 5.5100\n",
      "Epoch 6213/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4723 - val_loss: 5.4568\n",
      "Epoch 6214/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5730 - val_loss: 5.9559\n",
      "Epoch 6215/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4813 - val_loss: 5.5346\n",
      "Epoch 6216/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3692 - val_loss: 5.5562\n",
      "Epoch 6217/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3883 - val_loss: 5.6286\n",
      "Epoch 6218/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4505 - val_loss: 5.5370\n",
      "Epoch 6219/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3526 - val_loss: 5.5167\n",
      "Epoch 6220/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3481 - val_loss: 5.5652\n",
      "Epoch 6221/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4557 - val_loss: 5.5477\n",
      "Epoch 6222/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3561 - val_loss: 5.5347\n",
      "Epoch 6223/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4277 - val_loss: 5.8208\n",
      "Epoch 6224/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5466 - val_loss: 5.5104\n",
      "Epoch 6225/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4477 - val_loss: 5.5692\n",
      "Epoch 6226/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4062 - val_loss: 5.5071\n",
      "Epoch 6227/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6140 - val_loss: 5.6968\n",
      "Epoch 6228/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6647 - val_loss: 5.6810\n",
      "Epoch 6229/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5428 - val_loss: 5.4681\n",
      "Epoch 6230/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4163 - val_loss: 5.4647\n",
      "Epoch 6231/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4704 - val_loss: 5.6655\n",
      "Epoch 6232/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4894 - val_loss: 5.7621\n",
      "Epoch 6233/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4563 - val_loss: 5.5509\n",
      "Epoch 6234/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3920 - val_loss: 5.5955\n",
      "Epoch 6235/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3600 - val_loss: 5.5581\n",
      "Epoch 6236/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3841 - val_loss: 5.9159\n",
      "Epoch 6237/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.6165 - val_loss: 5.5911\n",
      "Epoch 6238/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4482 - val_loss: 5.5549\n",
      "Epoch 6239/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3973 - val_loss: 5.5681\n",
      "Epoch 6240/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4804 - val_loss: 5.5637\n",
      "Epoch 6241/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4605 - val_loss: 5.6421\n",
      "Epoch 6242/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4678 - val_loss: 5.4784\n",
      "Epoch 6243/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4553 - val_loss: 5.5667\n",
      "Epoch 6244/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3688 - val_loss: 5.6263\n",
      "Epoch 6245/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3654 - val_loss: 5.6132\n",
      "Epoch 6246/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4054 - val_loss: 5.5089\n",
      "Epoch 6247/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3156 - val_loss: 5.5582\n",
      "Epoch 6248/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3716 - val_loss: 5.4638\n",
      "Epoch 6249/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3915 - val_loss: 5.4800\n",
      "Epoch 6250/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5234 - val_loss: 5.6200\n",
      "Epoch 6251/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3816 - val_loss: 5.4900\n",
      "Epoch 6252/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3946 - val_loss: 5.6067\n",
      "Epoch 6253/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5412 - val_loss: 5.6017\n",
      "Epoch 6254/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3861 - val_loss: 5.6048\n",
      "Epoch 6255/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4657 - val_loss: 5.7475\n",
      "Epoch 6256/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4056 - val_loss: 5.6870\n",
      "Epoch 6257/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3927 - val_loss: 5.5046\n",
      "Epoch 6258/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3741 - val_loss: 5.5971\n",
      "Epoch 6259/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3620 - val_loss: 5.7576\n",
      "Epoch 6260/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6234 - val_loss: 5.6944\n",
      "Epoch 6261/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6568 - val_loss: 5.9654\n",
      "Epoch 6262/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4486 - val_loss: 5.5338\n",
      "Epoch 6263/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3864 - val_loss: 5.7097\n",
      "Epoch 6264/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3674 - val_loss: 5.4885\n",
      "Epoch 6265/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3716 - val_loss: 5.7267\n",
      "Epoch 6266/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5045 - val_loss: 5.5450\n",
      "Epoch 6267/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4348 - val_loss: 5.5055\n",
      "Epoch 6268/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4166 - val_loss: 5.5792\n",
      "Epoch 6269/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3908 - val_loss: 5.7363\n",
      "Epoch 6270/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5333 - val_loss: 5.5922\n",
      "Epoch 6271/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5586 - val_loss: 5.6007\n",
      "Epoch 6272/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3376 - val_loss: 5.8030\n",
      "Epoch 6273/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5031 - val_loss: 5.8055\n",
      "Epoch 6274/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5054 - val_loss: 5.5145\n",
      "Epoch 6275/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4087 - val_loss: 5.4760\n",
      "Epoch 6276/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5181 - val_loss: 5.5293\n",
      "Epoch 6277/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3619 - val_loss: 5.5498\n",
      "Epoch 6278/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5363 - val_loss: 5.4726\n",
      "Epoch 6279/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3815 - val_loss: 5.6630\n",
      "Epoch 6280/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5625 - val_loss: 5.5389\n",
      "Epoch 6281/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3333 - val_loss: 5.5919\n",
      "Epoch 6282/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4598 - val_loss: 5.5603\n",
      "Epoch 6283/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4162 - val_loss: 5.4970\n",
      "Epoch 6284/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3488 - val_loss: 5.7281\n",
      "Epoch 6285/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.7562 - val_loss: 5.8713\n",
      "Epoch 6286/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.6148 - val_loss: 5.5737\n",
      "Epoch 6287/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4481 - val_loss: 5.5051\n",
      "Epoch 6288/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3660 - val_loss: 5.6289\n",
      "Epoch 6289/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4769 - val_loss: 5.4957\n",
      "Epoch 6290/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4576 - val_loss: 5.4481\n",
      "Epoch 6291/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5044 - val_loss: 5.6803\n",
      "Epoch 6292/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5744 - val_loss: 5.5531\n",
      "Epoch 6293/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3408 - val_loss: 5.5448\n",
      "Epoch 6294/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3842 - val_loss: 5.5769\n",
      "Epoch 6295/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4019 - val_loss: 5.5061\n",
      "Epoch 6296/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6763 - val_loss: 6.2199\n",
      "Epoch 6297/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7624 - val_loss: 5.8940\n",
      "Epoch 6298/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4282 - val_loss: 5.4868\n",
      "Epoch 6299/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3707 - val_loss: 5.5441\n",
      "Epoch 6300/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4103 - val_loss: 5.5221\n",
      "Epoch 6301/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3886 - val_loss: 5.4936\n",
      "Epoch 6302/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3378 - val_loss: 5.4941\n",
      "Epoch 6303/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3270 - val_loss: 5.5815\n",
      "Epoch 6304/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3968 - val_loss: 5.7140\n",
      "Epoch 6305/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4913 - val_loss: 5.5106\n",
      "Epoch 6306/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3322 - val_loss: 6.0571\n",
      "Epoch 6307/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7309 - val_loss: 6.1617\n",
      "Epoch 6308/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6307 - val_loss: 5.6914\n",
      "Epoch 6309/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5090 - val_loss: 5.5826\n",
      "Epoch 6310/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3441 - val_loss: 5.5611\n",
      "Epoch 6311/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3597 - val_loss: 5.5287\n",
      "Epoch 6312/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3538 - val_loss: 5.8584\n",
      "Epoch 6313/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.4523 - val_loss: 5.5604\n",
      "Epoch 6314/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4093 - val_loss: 5.5635\n",
      "Epoch 6315/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4660 - val_loss: 5.6417\n",
      "Epoch 6316/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3205 - val_loss: 5.5178\n",
      "Epoch 6317/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3606 - val_loss: 5.6747\n",
      "Epoch 6318/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7816 - val_loss: 6.2828\n",
      "Epoch 6319/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5489 - val_loss: 5.5556\n",
      "Epoch 6320/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3331 - val_loss: 5.5032\n",
      "Epoch 6321/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3383 - val_loss: 5.6158\n",
      "Epoch 6322/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4221 - val_loss: 5.5020\n",
      "Epoch 6323/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5052 - val_loss: 5.4890\n",
      "Epoch 6324/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4042 - val_loss: 5.5139\n",
      "Epoch 6325/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3786 - val_loss: 5.6292\n",
      "Epoch 6326/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4943 - val_loss: 5.8204\n",
      "Epoch 6327/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7195 - val_loss: 5.9836\n",
      "Epoch 6328/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6391 - val_loss: 5.8654\n",
      "Epoch 6329/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4496 - val_loss: 5.8903\n",
      "Epoch 6330/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6149 - val_loss: 5.7382\n",
      "Epoch 6331/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4525 - val_loss: 5.5124\n",
      "Epoch 6332/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4803 - val_loss: 5.6423\n",
      "Epoch 6333/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3639 - val_loss: 5.5499\n",
      "Epoch 6334/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3948 - val_loss: 5.4518\n",
      "Epoch 6335/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3559 - val_loss: 5.6216\n",
      "Epoch 6336/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4104 - val_loss: 5.4972\n",
      "Epoch 6337/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4261 - val_loss: 5.7838\n",
      "Epoch 6338/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5311 - val_loss: 5.5046\n",
      "Epoch 6339/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3762 - val_loss: 5.5005\n",
      "Epoch 6340/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6472 - val_loss: 5.5538\n",
      "Epoch 6341/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4337 - val_loss: 5.7757\n",
      "Epoch 6342/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4454 - val_loss: 5.5563\n",
      "Epoch 6343/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4094 - val_loss: 5.4807\n",
      "Epoch 6344/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3417 - val_loss: 5.4438\n",
      "Epoch 6345/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3239 - val_loss: 5.4990\n",
      "Epoch 6346/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3617 - val_loss: 5.4844\n",
      "Epoch 6347/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3911 - val_loss: 5.5850\n",
      "Epoch 6348/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4386 - val_loss: 5.4297\n",
      "Epoch 6349/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3251 - val_loss: 5.5245\n",
      "Epoch 6350/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3843 - val_loss: 5.7668\n",
      "Epoch 6351/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5043 - val_loss: 6.0092\n",
      "Epoch 6352/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7855 - val_loss: 5.5047\n",
      "Epoch 6353/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3149 - val_loss: 5.4769\n",
      "Epoch 6354/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3411 - val_loss: 5.6087\n",
      "Epoch 6355/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3762 - val_loss: 5.6966\n",
      "Epoch 6356/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3845 - val_loss: 5.5535\n",
      "Epoch 6357/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3450 - val_loss: 5.5363\n",
      "Epoch 6358/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5349 - val_loss: 5.5177\n",
      "Epoch 6359/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4242 - val_loss: 5.5148\n",
      "Epoch 6360/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3519 - val_loss: 5.6787\n",
      "Epoch 6361/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6874 - val_loss: 5.6972\n",
      "Epoch 6362/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4369 - val_loss: 5.4900\n",
      "Epoch 6363/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3056 - val_loss: 5.7735\n",
      "Epoch 6364/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5267 - val_loss: 5.6724\n",
      "Epoch 6365/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5940 - val_loss: 5.6032\n",
      "Epoch 6366/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3613 - val_loss: 5.7480\n",
      "Epoch 6367/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4000 - val_loss: 5.5936\n",
      "Epoch 6368/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3250 - val_loss: 5.5282\n",
      "Epoch 6369/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3873 - val_loss: 5.5050\n",
      "Epoch 6370/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3457 - val_loss: 5.4672\n",
      "Epoch 6371/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3828 - val_loss: 5.7145\n",
      "Epoch 6372/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4429 - val_loss: 5.8548\n",
      "Epoch 6373/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5453 - val_loss: 5.4662\n",
      "Epoch 6374/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3765 - val_loss: 5.5964\n",
      "Epoch 6375/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4350 - val_loss: 5.5501\n",
      "Epoch 6376/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4155 - val_loss: 5.5937\n",
      "Epoch 6377/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4454 - val_loss: 5.6749\n",
      "Epoch 6378/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.7409 - val_loss: 5.4550\n",
      "Epoch 6379/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5188 - val_loss: 5.5915\n",
      "Epoch 6380/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5087 - val_loss: 5.5905\n",
      "Epoch 6381/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4943 - val_loss: 5.4598\n",
      "Epoch 6382/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5239 - val_loss: 5.7318\n",
      "Epoch 6383/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4209 - val_loss: 5.5820\n",
      "Epoch 6384/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3733 - val_loss: 5.5675\n",
      "Epoch 6385/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3380 - val_loss: 5.5654\n",
      "Epoch 6386/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3346 - val_loss: 5.4850\n",
      "Epoch 6387/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3534 - val_loss: 5.9402\n",
      "Epoch 6388/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4478 - val_loss: 5.4705\n",
      "Epoch 6389/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.4353 - val_loss: 6.6336\n",
      "Epoch 6390/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 5.0442 - val_loss: 5.4794\n",
      "Epoch 6391/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3518 - val_loss: 5.5244\n",
      "Epoch 6392/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3149 - val_loss: 5.5341\n",
      "Epoch 6393/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2839 - val_loss: 5.6781\n",
      "Epoch 6394/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3347 - val_loss: 5.5787\n",
      "Epoch 6395/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.3257 - val_loss: 5.4756\n",
      "Epoch 6396/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3158 - val_loss: 5.6022\n",
      "Epoch 6397/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3588 - val_loss: 5.4820\n",
      "Epoch 6398/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3955 - val_loss: 5.5908\n",
      "Epoch 6399/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3150 - val_loss: 5.6114\n",
      "Epoch 6400/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3419 - val_loss: 5.5075\n",
      "Epoch 6401/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4089 - val_loss: 5.4523\n",
      "Epoch 6402/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3829 - val_loss: 5.4910\n",
      "Epoch 6403/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3412 - val_loss: 5.5168\n",
      "Epoch 6404/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5441 - val_loss: 5.4918\n",
      "Epoch 6405/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4362 - val_loss: 5.5655\n",
      "Epoch 6406/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4994 - val_loss: 5.5158\n",
      "Epoch 6407/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3809 - val_loss: 5.5472\n",
      "Epoch 6408/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3936 - val_loss: 5.6938\n",
      "Epoch 6409/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4834 - val_loss: 5.7421\n",
      "Epoch 6410/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4072 - val_loss: 5.5277\n",
      "Epoch 6411/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3499 - val_loss: 5.6065\n",
      "Epoch 6412/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3216 - val_loss: 5.5465\n",
      "Epoch 6413/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3624 - val_loss: 6.2849\n",
      "Epoch 6414/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5527 - val_loss: 5.5084\n",
      "Epoch 6415/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4267 - val_loss: 5.6306\n",
      "Epoch 6416/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3549 - val_loss: 5.5674\n",
      "Epoch 6417/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4086 - val_loss: 5.4888\n",
      "Epoch 6418/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3766 - val_loss: 5.5274\n",
      "Epoch 6419/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3837 - val_loss: 5.4696\n",
      "Epoch 6420/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2958 - val_loss: 5.5266\n",
      "Epoch 6421/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4633 - val_loss: 5.5533\n",
      "Epoch 6422/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3261 - val_loss: 5.6200\n",
      "Epoch 6423/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5479 - val_loss: 5.5384\n",
      "Epoch 6424/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4164 - val_loss: 5.5636\n",
      "Epoch 6425/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.5257 - val_loss: 5.6123\n",
      "Epoch 6426/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4330 - val_loss: 5.5085\n",
      "Epoch 6427/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.3445 - val_loss: 5.5076\n",
      "Epoch 6428/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.4621 - val_loss: 5.7721\n",
      "Epoch 6429/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.5581 - val_loss: 5.5378\n",
      "Epoch 6430/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.4075 - val_loss: 5.5098\n",
      "Epoch 6431/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3225 - val_loss: 5.6444\n",
      "Epoch 6432/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5688 - val_loss: 5.7129\n",
      "Epoch 6433/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6729 - val_loss: 5.5398\n",
      "Epoch 6434/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3933 - val_loss: 5.5281\n",
      "Epoch 6435/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3767 - val_loss: 5.4788\n",
      "Epoch 6436/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4122 - val_loss: 5.4583\n",
      "Epoch 6437/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3312 - val_loss: 5.4736\n",
      "Epoch 6438/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3061 - val_loss: 5.4695\n",
      "Epoch 6439/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3883 - val_loss: 5.5190\n",
      "Epoch 6440/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3590 - val_loss: 5.5660\n",
      "Epoch 6441/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3163 - val_loss: 5.5535\n",
      "Epoch 6442/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3856 - val_loss: 5.4846\n",
      "Epoch 6443/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3326 - val_loss: 5.6607\n",
      "Epoch 6444/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4595 - val_loss: 5.5376\n",
      "Epoch 6445/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3816 - val_loss: 5.5038\n",
      "Epoch 6446/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.3454 - val_loss: 5.5303\n",
      "Epoch 6447/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 4.4155 - val_loss: 5.5863\n",
      "Epoch 6448/10000\n",
      "675/675 [==============================] - 0s 127us/step - loss: 4.3058 - val_loss: 5.5651\n",
      "Epoch 6449/10000\n",
      "675/675 [==============================] - 0s 107us/step - loss: 4.5569 - val_loss: 5.5844\n",
      "Epoch 6450/10000\n",
      "675/675 [==============================] - 0s 107us/step - loss: 4.5619 - val_loss: 5.6004\n",
      "Epoch 6451/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 4.3882 - val_loss: 5.9906\n",
      "Epoch 6452/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 4.6438 - val_loss: 5.4829\n",
      "Epoch 6453/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 4.3813 - val_loss: 5.4909\n",
      "Epoch 6454/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.3068 - val_loss: 5.5113\n",
      "Epoch 6455/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.3434 - val_loss: 5.7762\n",
      "Epoch 6456/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.5630 - val_loss: 5.4982\n",
      "Epoch 6457/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.5108 - val_loss: 5.5109\n",
      "Epoch 6458/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 4.3428 - val_loss: 5.5353\n",
      "Epoch 6459/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.4789 - val_loss: 5.5019\n",
      "Epoch 6460/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3371 - val_loss: 5.4575\n",
      "Epoch 6461/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.4391 - val_loss: 5.6174\n",
      "Epoch 6462/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.6313 - val_loss: 5.5112\n",
      "Epoch 6463/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3063 - val_loss: 5.5381\n",
      "Epoch 6464/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 4.3367 - val_loss: 5.4571\n",
      "Epoch 6465/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 63us/step - loss: 4.3162 - val_loss: 5.7076\n",
      "Epoch 6466/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4400 - val_loss: 5.5165\n",
      "Epoch 6467/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3729 - val_loss: 5.5026\n",
      "Epoch 6468/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3848 - val_loss: 5.4805\n",
      "Epoch 6469/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3071 - val_loss: 5.6989\n",
      "Epoch 6470/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5340 - val_loss: 6.2993\n",
      "Epoch 6471/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6368 - val_loss: 5.4685\n",
      "Epoch 6472/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4079 - val_loss: 5.4634\n",
      "Epoch 6473/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4514 - val_loss: 5.4958\n",
      "Epoch 6474/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6032 - val_loss: 5.5513\n",
      "Epoch 6475/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5298 - val_loss: 5.5032\n",
      "Epoch 6476/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3194 - val_loss: 5.5221\n",
      "Epoch 6477/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 4.3617 - val_loss: 5.4515\n",
      "Epoch 6478/10000\n",
      "675/675 [==============================] - 0s 116us/step - loss: 4.3818 - val_loss: 5.8478\n",
      "Epoch 6479/10000\n",
      "675/675 [==============================] - 0s 114us/step - loss: 4.4554 - val_loss: 5.6077\n",
      "Epoch 6480/10000\n",
      "675/675 [==============================] - 0s 100us/step - loss: 4.3290 - val_loss: 6.1697\n",
      "Epoch 6481/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 4.9474 - val_loss: 5.7441\n",
      "Epoch 6482/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 4.4085 - val_loss: 5.7795\n",
      "Epoch 6483/10000\n",
      "675/675 [==============================] - 0s 103us/step - loss: 4.5374 - val_loss: 5.4515\n",
      "Epoch 6484/10000\n",
      "675/675 [==============================] - 0s 122us/step - loss: 4.3845 - val_loss: 5.4995\n",
      "Epoch 6485/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 4.3770 - val_loss: 5.5099\n",
      "Epoch 6486/10000\n",
      "675/675 [==============================] - 0s 128us/step - loss: 4.5492 - val_loss: 5.5055\n",
      "Epoch 6487/10000\n",
      "675/675 [==============================] - 0s 118us/step - loss: 4.4170 - val_loss: 5.5785\n",
      "Epoch 6488/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 4.3391 - val_loss: 5.4696\n",
      "Epoch 6489/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.3381 - val_loss: 6.0009\n",
      "Epoch 6490/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5560 - val_loss: 5.4584\n",
      "Epoch 6491/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4977 - val_loss: 5.4797\n",
      "Epoch 6492/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3501 - val_loss: 5.4623\n",
      "Epoch 6493/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.3811 - val_loss: 5.6037\n",
      "Epoch 6494/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5229 - val_loss: 5.6290\n",
      "Epoch 6495/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3740 - val_loss: 5.4765\n",
      "Epoch 6496/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3766 - val_loss: 5.5521\n",
      "Epoch 6497/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3419 - val_loss: 5.4928\n",
      "Epoch 6498/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3714 - val_loss: 5.5038\n",
      "Epoch 6499/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3708 - val_loss: 5.5301\n",
      "Epoch 6500/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3011 - val_loss: 5.4881\n",
      "Epoch 6501/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2916 - val_loss: 5.4787\n",
      "Epoch 6502/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.3870 - val_loss: 5.5702\n",
      "Epoch 6503/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 4.3323 - val_loss: 5.5190\n",
      "Epoch 6504/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 4.3804 - val_loss: 5.5232\n",
      "Epoch 6505/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.3910 - val_loss: 5.4907\n",
      "Epoch 6506/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.3880 - val_loss: 5.7534\n",
      "Epoch 6507/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.3539 - val_loss: 5.5043\n",
      "Epoch 6508/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3809 - val_loss: 5.4674\n",
      "Epoch 6509/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3457 - val_loss: 5.5211\n",
      "Epoch 6510/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3546 - val_loss: 5.5161\n",
      "Epoch 6511/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3296 - val_loss: 5.7356\n",
      "Epoch 6512/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3960 - val_loss: 5.4942\n",
      "Epoch 6513/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3122 - val_loss: 5.6329\n",
      "Epoch 6514/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3307 - val_loss: 5.5462\n",
      "Epoch 6515/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3685 - val_loss: 5.6751\n",
      "Epoch 6516/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4267 - val_loss: 5.4934\n",
      "Epoch 6517/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5312 - val_loss: 5.9495\n",
      "Epoch 6518/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5106 - val_loss: 5.5548\n",
      "Epoch 6519/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3867 - val_loss: 5.4145\n",
      "Epoch 6520/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3948 - val_loss: 5.4710\n",
      "Epoch 6521/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2994 - val_loss: 5.4742\n",
      "Epoch 6522/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4244 - val_loss: 5.4937\n",
      "Epoch 6523/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3490 - val_loss: 5.5161\n",
      "Epoch 6524/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3381 - val_loss: 5.4716\n",
      "Epoch 6525/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3221 - val_loss: 5.5695\n",
      "Epoch 6526/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4367 - val_loss: 5.5743\n",
      "Epoch 6527/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5392 - val_loss: 5.5521\n",
      "Epoch 6528/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3566 - val_loss: 5.4502\n",
      "Epoch 6529/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4286 - val_loss: 5.6689\n",
      "Epoch 6530/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3899 - val_loss: 5.4934\n",
      "Epoch 6531/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.3622 - val_loss: 5.4593\n",
      "Epoch 6532/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.4217 - val_loss: 5.5229\n",
      "Epoch 6533/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3230 - val_loss: 5.4518\n",
      "Epoch 6534/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3414 - val_loss: 5.4798\n",
      "Epoch 6535/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6081 - val_loss: 5.4379\n",
      "Epoch 6536/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.6143 - val_loss: 5.7193\n",
      "Epoch 6537/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3195 - val_loss: 5.4565\n",
      "Epoch 6538/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4469 - val_loss: 5.4533\n",
      "Epoch 6539/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3035 - val_loss: 5.6426\n",
      "Epoch 6540/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3032 - val_loss: 5.4561\n",
      "Epoch 6541/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.4359 - val_loss: 5.4967\n",
      "Epoch 6542/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3738 - val_loss: 5.5426\n",
      "Epoch 6543/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3678 - val_loss: 5.4821\n",
      "Epoch 6544/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3261 - val_loss: 5.5300\n",
      "Epoch 6545/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.3621 - val_loss: 5.4970\n",
      "Epoch 6546/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2959 - val_loss: 5.5507\n",
      "Epoch 6547/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3500 - val_loss: 5.8357\n",
      "Epoch 6548/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4422 - val_loss: 5.5210\n",
      "Epoch 6549/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3351 - val_loss: 5.7546\n",
      "Epoch 6550/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4302 - val_loss: 5.7153\n",
      "Epoch 6551/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3481 - val_loss: 5.4627\n",
      "Epoch 6552/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3604 - val_loss: 5.6898\n",
      "Epoch 6553/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3681 - val_loss: 5.4857\n",
      "Epoch 6554/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3795 - val_loss: 5.4842\n",
      "Epoch 6555/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4216 - val_loss: 5.5761\n",
      "Epoch 6556/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4340 - val_loss: 5.6247\n",
      "Epoch 6557/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4368 - val_loss: 5.4855\n",
      "Epoch 6558/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3621 - val_loss: 5.5121\n",
      "Epoch 6559/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5417 - val_loss: 5.4620\n",
      "Epoch 6560/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3349 - val_loss: 5.4488\n",
      "Epoch 6561/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4092 - val_loss: 5.7857\n",
      "Epoch 6562/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5775 - val_loss: 5.5246\n",
      "Epoch 6563/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3525 - val_loss: 5.5165\n",
      "Epoch 6564/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3123 - val_loss: 5.4442\n",
      "Epoch 6565/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5080 - val_loss: 5.4855\n",
      "Epoch 6566/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3175 - val_loss: 5.5344\n",
      "Epoch 6567/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.2988 - val_loss: 5.4708\n",
      "Epoch 6568/10000\n",
      "675/675 [==============================] - 0s 94us/step - loss: 4.3036 - val_loss: 5.4715\n",
      "Epoch 6569/10000\n",
      "675/675 [==============================] - 0s 119us/step - loss: 4.3885 - val_loss: 5.4869\n",
      "Epoch 6570/10000\n",
      "675/675 [==============================] - 0s 98us/step - loss: 4.5002 - val_loss: 5.6381\n",
      "Epoch 6571/10000\n",
      "675/675 [==============================] - 0s 95us/step - loss: 4.2819 - val_loss: 5.5192\n",
      "Epoch 6572/10000\n",
      "675/675 [==============================] - 0s 107us/step - loss: 4.3297 - val_loss: 5.6690\n",
      "Epoch 6573/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.4249 - val_loss: 5.6150\n",
      "Epoch 6574/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2989 - val_loss: 5.4156\n",
      "Epoch 6575/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2868 - val_loss: 5.4712\n",
      "Epoch 6576/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2923 - val_loss: 5.5587\n",
      "Epoch 6577/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3003 - val_loss: 5.9024\n",
      "Epoch 6578/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.4815 - val_loss: 5.7608\n",
      "Epoch 6579/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5998 - val_loss: 5.6321\n",
      "Epoch 6580/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3865 - val_loss: 5.4600\n",
      "Epoch 6581/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3041 - val_loss: 5.5671\n",
      "Epoch 6582/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4313 - val_loss: 6.0698\n",
      "Epoch 6583/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4065 - val_loss: 5.6019\n",
      "Epoch 6584/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4860 - val_loss: 5.6229\n",
      "Epoch 6585/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3580 - val_loss: 5.6033\n",
      "Epoch 6586/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3548 - val_loss: 5.5253\n",
      "Epoch 6587/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4475 - val_loss: 5.5746\n",
      "Epoch 6588/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4137 - val_loss: 5.4803\n",
      "Epoch 6589/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3770 - val_loss: 5.6277\n",
      "Epoch 6590/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6775 - val_loss: 5.4458\n",
      "Epoch 6591/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2960 - val_loss: 5.5791\n",
      "Epoch 6592/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4333 - val_loss: 5.4695\n",
      "Epoch 6593/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3349 - val_loss: 5.6514\n",
      "Epoch 6594/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7595 - val_loss: 5.5298\n",
      "Epoch 6595/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4044 - val_loss: 5.6501\n",
      "Epoch 6596/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3821 - val_loss: 5.5306\n",
      "Epoch 6597/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3318 - val_loss: 5.5185\n",
      "Epoch 6598/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3160 - val_loss: 5.4297\n",
      "Epoch 6599/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4839 - val_loss: 5.4930\n",
      "Epoch 6600/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4721 - val_loss: 5.5021\n",
      "Epoch 6601/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3358 - val_loss: 5.4516\n",
      "Epoch 6602/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3591 - val_loss: 5.4660\n",
      "Epoch 6603/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3017 - val_loss: 5.4912\n",
      "Epoch 6604/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3203 - val_loss: 5.4979\n",
      "Epoch 6605/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3286 - val_loss: 5.5025\n",
      "Epoch 6606/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3190 - val_loss: 5.6774\n",
      "Epoch 6607/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4223 - val_loss: 5.4294\n",
      "Epoch 6608/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3595 - val_loss: 5.5249\n",
      "Epoch 6609/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5389 - val_loss: 5.5634\n",
      "Epoch 6610/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3966 - val_loss: 5.4486\n",
      "Epoch 6611/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7078 - val_loss: 5.5768\n",
      "Epoch 6612/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4056 - val_loss: 5.5563\n",
      "Epoch 6613/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3647 - val_loss: 5.4624\n",
      "Epoch 6614/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3249 - val_loss: 5.5412\n",
      "Epoch 6615/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3545 - val_loss: 5.4189\n",
      "Epoch 6616/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3700 - val_loss: 5.4976\n",
      "Epoch 6617/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.4415 - val_loss: 5.6411\n",
      "Epoch 6618/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5356 - val_loss: 5.4842\n",
      "Epoch 6619/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3178 - val_loss: 5.5084\n",
      "Epoch 6620/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3608 - val_loss: 5.4384\n",
      "Epoch 6621/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2889 - val_loss: 5.4970\n",
      "Epoch 6622/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3672 - val_loss: 5.5145\n",
      "Epoch 6623/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4043 - val_loss: 5.5673\n",
      "Epoch 6624/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3587 - val_loss: 5.6726\n",
      "Epoch 6625/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3994 - val_loss: 5.5055\n",
      "Epoch 6626/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3649 - val_loss: 5.5259\n",
      "Epoch 6627/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.3420 - val_loss: 5.6464\n",
      "Epoch 6628/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.5486 - val_loss: 5.4584\n",
      "Epoch 6629/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.2942 - val_loss: 5.4193\n",
      "Epoch 6630/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3474 - val_loss: 5.4412\n",
      "Epoch 6631/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.5020 - val_loss: 5.7013\n",
      "Epoch 6632/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.3246 - val_loss: 5.4847\n",
      "Epoch 6633/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.4044 - val_loss: 5.6122\n",
      "Epoch 6634/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4010 - val_loss: 5.4492\n",
      "Epoch 6635/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3691 - val_loss: 5.4848\n",
      "Epoch 6636/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.3880 - val_loss: 5.4976\n",
      "Epoch 6637/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.2853 - val_loss: 5.4721\n",
      "Epoch 6638/10000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 4.2855 - val_loss: 5.4760\n",
      "Epoch 6639/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4296 - val_loss: 5.7557\n",
      "Epoch 6640/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3577 - val_loss: 5.8574\n",
      "Epoch 6641/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4612 - val_loss: 5.5176\n",
      "Epoch 6642/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4483 - val_loss: 5.6174\n",
      "Epoch 6643/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3313 - val_loss: 5.4805\n",
      "Epoch 6644/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3839 - val_loss: 5.6883\n",
      "Epoch 6645/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.4169 - val_loss: 5.4660\n",
      "Epoch 6646/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2962 - val_loss: 5.4887\n",
      "Epoch 6647/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2999 - val_loss: 5.4196\n",
      "Epoch 6648/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2717 - val_loss: 5.4407\n",
      "Epoch 6649/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4266 - val_loss: 5.3858\n",
      "Epoch 6650/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3723 - val_loss: 5.6917\n",
      "Epoch 6651/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.3851 - val_loss: 5.4693\n",
      "Epoch 6652/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2942 - val_loss: 5.4525\n",
      "Epoch 6653/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2922 - val_loss: 5.4528\n",
      "Epoch 6654/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3052 - val_loss: 5.5076\n",
      "Epoch 6655/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3240 - val_loss: 5.6205\n",
      "Epoch 6656/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3267 - val_loss: 5.5148\n",
      "Epoch 6657/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3897 - val_loss: 5.4816\n",
      "Epoch 6658/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3676 - val_loss: 5.5400\n",
      "Epoch 6659/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.3329 - val_loss: 5.5012\n",
      "Epoch 6660/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 4.3091 - val_loss: 5.4165\n",
      "Epoch 6661/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2562 - val_loss: 5.5134\n",
      "Epoch 6662/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5319 - val_loss: 5.4813\n",
      "Epoch 6663/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2991 - val_loss: 5.4836\n",
      "Epoch 6664/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3132 - val_loss: 5.4389\n",
      "Epoch 6665/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4574 - val_loss: 5.4269\n",
      "Epoch 6666/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3078 - val_loss: 5.5242\n",
      "Epoch 6667/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.5064 - val_loss: 5.4123\n",
      "Epoch 6668/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3612 - val_loss: 5.4466\n",
      "Epoch 6669/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3473 - val_loss: 5.5417\n",
      "Epoch 6670/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3646 - val_loss: 5.6864\n",
      "Epoch 6671/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3793 - val_loss: 5.4493\n",
      "Epoch 6672/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.4826 - val_loss: 5.4823\n",
      "Epoch 6673/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3725 - val_loss: 5.5717\n",
      "Epoch 6674/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3368 - val_loss: 5.4500\n",
      "Epoch 6675/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2390 - val_loss: 5.4847\n",
      "Epoch 6676/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2955 - val_loss: 5.4924\n",
      "Epoch 6677/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2948 - val_loss: 5.4604\n",
      "Epoch 6678/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.5863 - val_loss: 5.5101\n",
      "Epoch 6679/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6549 - val_loss: 5.5016\n",
      "Epoch 6680/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3498 - val_loss: 5.8097\n",
      "Epoch 6681/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6037 - val_loss: 5.4674\n",
      "Epoch 6682/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3292 - val_loss: 5.4723\n",
      "Epoch 6683/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3621 - val_loss: 5.7194\n",
      "Epoch 6684/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5546 - val_loss: 5.5101\n",
      "Epoch 6685/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.4622 - val_loss: 5.7622\n",
      "Epoch 6686/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.5824 - val_loss: 5.4562\n",
      "Epoch 6687/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.4993 - val_loss: 5.4726\n",
      "Epoch 6688/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.5720 - val_loss: 5.7103\n",
      "Epoch 6689/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3401 - val_loss: 5.4892\n",
      "Epoch 6690/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3332 - val_loss: 5.7813\n",
      "Epoch 6691/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.5226 - val_loss: 5.4965\n",
      "Epoch 6692/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3210 - val_loss: 5.5340\n",
      "Epoch 6693/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 64us/step - loss: 4.5284 - val_loss: 5.8783\n",
      "Epoch 6694/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.5470 - val_loss: 5.5600\n",
      "Epoch 6695/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3733 - val_loss: 5.7252\n",
      "Epoch 6696/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.6297 - val_loss: 5.4133\n",
      "Epoch 6697/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3570 - val_loss: 5.4212\n",
      "Epoch 6698/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3455 - val_loss: 5.4101\n",
      "Epoch 6699/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2879 - val_loss: 5.4107\n",
      "Epoch 6700/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3283 - val_loss: 5.5313\n",
      "Epoch 6701/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2997 - val_loss: 5.4302\n",
      "Epoch 6702/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3155 - val_loss: 5.4828\n",
      "Epoch 6703/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3234 - val_loss: 5.9412\n",
      "Epoch 6704/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.6634 - val_loss: 5.5209\n",
      "Epoch 6705/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5571 - val_loss: 5.5089\n",
      "Epoch 6706/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3016 - val_loss: 5.4982\n",
      "Epoch 6707/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3067 - val_loss: 5.4689\n",
      "Epoch 6708/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2650 - val_loss: 5.6249\n",
      "Epoch 6709/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3268 - val_loss: 5.4864\n",
      "Epoch 6710/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3672 - val_loss: 5.5011\n",
      "Epoch 6711/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5920 - val_loss: 5.5402\n",
      "Epoch 6712/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3896 - val_loss: 5.9944\n",
      "Epoch 6713/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4808 - val_loss: 5.5170\n",
      "Epoch 6714/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3558 - val_loss: 5.7296\n",
      "Epoch 6715/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.4312 - val_loss: 5.7073\n",
      "Epoch 6716/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3697 - val_loss: 5.7012\n",
      "Epoch 6717/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4195 - val_loss: 5.4361\n",
      "Epoch 6718/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2740 - val_loss: 5.6155\n",
      "Epoch 6719/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3274 - val_loss: 5.5937\n",
      "Epoch 6720/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3908 - val_loss: 5.5021\n",
      "Epoch 6721/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3482 - val_loss: 5.5280\n",
      "Epoch 6722/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3312 - val_loss: 5.4677\n",
      "Epoch 6723/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3736 - val_loss: 5.5805\n",
      "Epoch 6724/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 4.2970 - val_loss: 5.4600\n",
      "Epoch 6725/10000\n",
      "675/675 [==============================] - 0s 111us/step - loss: 4.3650 - val_loss: 5.6765\n",
      "Epoch 6726/10000\n",
      "675/675 [==============================] - 0s 123us/step - loss: 4.5952 - val_loss: 5.4103\n",
      "Epoch 6727/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.3075 - val_loss: 5.4533\n",
      "Epoch 6728/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.5424 - val_loss: 5.5109\n",
      "Epoch 6729/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3985 - val_loss: 5.7382\n",
      "Epoch 6730/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3891 - val_loss: 5.5127\n",
      "Epoch 6731/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3235 - val_loss: 5.5521\n",
      "Epoch 6732/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2757 - val_loss: 5.4787\n",
      "Epoch 6733/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3110 - val_loss: 5.5590\n",
      "Epoch 6734/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4016 - val_loss: 5.4894\n",
      "Epoch 6735/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3803 - val_loss: 5.5579\n",
      "Epoch 6736/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3948 - val_loss: 5.4922\n",
      "Epoch 6737/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.2746 - val_loss: 5.4981\n",
      "Epoch 6738/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2996 - val_loss: 5.4913\n",
      "Epoch 6739/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4734 - val_loss: 5.5051\n",
      "Epoch 6740/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2799 - val_loss: 5.5961\n",
      "Epoch 6741/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3012 - val_loss: 5.6582\n",
      "Epoch 6742/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5999 - val_loss: 5.5008\n",
      "Epoch 6743/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3890 - val_loss: 5.4578\n",
      "Epoch 6744/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2926 - val_loss: 5.6688\n",
      "Epoch 6745/10000\n",
      "675/675 [==============================] - 0s 109us/step - loss: 4.5253 - val_loss: 5.4383\n",
      "Epoch 6746/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 4.5157 - val_loss: 5.5061\n",
      "Epoch 6747/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 4.3979 - val_loss: 5.6586\n",
      "Epoch 6748/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 4.3509 - val_loss: 5.6737\n",
      "Epoch 6749/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.3903 - val_loss: 5.4975\n",
      "Epoch 6750/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6240 - val_loss: 5.5651\n",
      "Epoch 6751/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2888 - val_loss: 5.6902\n",
      "Epoch 6752/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3656 - val_loss: 5.5678\n",
      "Epoch 6753/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3510 - val_loss: 5.4915\n",
      "Epoch 6754/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2578 - val_loss: 5.5592\n",
      "Epoch 6755/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5253 - val_loss: 5.4668\n",
      "Epoch 6756/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3790 - val_loss: 5.4392\n",
      "Epoch 6757/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3216 - val_loss: 5.4626\n",
      "Epoch 6758/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2949 - val_loss: 5.4451\n",
      "Epoch 6759/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2962 - val_loss: 5.5422\n",
      "Epoch 6760/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2832 - val_loss: 5.5008\n",
      "Epoch 6761/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3516 - val_loss: 5.5135\n",
      "Epoch 6762/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3763 - val_loss: 5.4978\n",
      "Epoch 6763/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4013 - val_loss: 5.4976\n",
      "Epoch 6764/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3062 - val_loss: 5.4630\n",
      "Epoch 6765/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2483 - val_loss: 5.4473\n",
      "Epoch 6766/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3028 - val_loss: 5.4356\n",
      "Epoch 6767/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2785 - val_loss: 5.5441\n",
      "Epoch 6768/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3561 - val_loss: 5.4269\n",
      "Epoch 6769/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 62us/step - loss: 4.3366 - val_loss: 5.4696\n",
      "Epoch 6770/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3438 - val_loss: 5.4312\n",
      "Epoch 6771/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4224 - val_loss: 5.4254\n",
      "Epoch 6772/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6550 - val_loss: 5.8495\n",
      "Epoch 6773/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4221 - val_loss: 5.6411\n",
      "Epoch 6774/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3787 - val_loss: 5.4937\n",
      "Epoch 6775/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2797 - val_loss: 5.6652\n",
      "Epoch 6776/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4159 - val_loss: 5.4860\n",
      "Epoch 6777/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2473 - val_loss: 5.3985\n",
      "Epoch 6778/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2865 - val_loss: 5.4283\n",
      "Epoch 6779/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2802 - val_loss: 5.4293\n",
      "Epoch 6780/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2537 - val_loss: 5.4224\n",
      "Epoch 6781/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2601 - val_loss: 5.6122\n",
      "Epoch 6782/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2946 - val_loss: 5.4718\n",
      "Epoch 6783/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3276 - val_loss: 5.8090\n",
      "Epoch 6784/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4834 - val_loss: 5.4441\n",
      "Epoch 6785/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2623 - val_loss: 5.5291\n",
      "Epoch 6786/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3755 - val_loss: 5.5078\n",
      "Epoch 6787/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3032 - val_loss: 5.4531\n",
      "Epoch 6788/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3482 - val_loss: 5.5308\n",
      "Epoch 6789/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4311 - val_loss: 5.7913\n",
      "Epoch 6790/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3275 - val_loss: 5.4618\n",
      "Epoch 6791/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3495 - val_loss: 5.3730\n",
      "Epoch 6792/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3384 - val_loss: 5.4107\n",
      "Epoch 6793/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2886 - val_loss: 5.4356\n",
      "Epoch 6794/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2614 - val_loss: 5.3994\n",
      "Epoch 6795/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2611 - val_loss: 5.3894\n",
      "Epoch 6796/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3285 - val_loss: 5.4719\n",
      "Epoch 6797/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2820 - val_loss: 5.5314\n",
      "Epoch 6798/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2662 - val_loss: 5.4713\n",
      "Epoch 6799/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3209 - val_loss: 5.7294\n",
      "Epoch 6800/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4916 - val_loss: 5.4092\n",
      "Epoch 6801/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2972 - val_loss: 5.5263\n",
      "Epoch 6802/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3072 - val_loss: 5.4352\n",
      "Epoch 6803/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3302 - val_loss: 5.4958\n",
      "Epoch 6804/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3897 - val_loss: 5.3933\n",
      "Epoch 6805/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3016 - val_loss: 5.8043\n",
      "Epoch 6806/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5183 - val_loss: 5.7214\n",
      "Epoch 6807/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6683 - val_loss: 6.1253\n",
      "Epoch 6808/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4609 - val_loss: 5.3538\n",
      "Epoch 6809/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3591 - val_loss: 5.3837\n",
      "Epoch 6810/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3621 - val_loss: 5.7644\n",
      "Epoch 6811/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4657 - val_loss: 5.5055\n",
      "Epoch 6812/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5495 - val_loss: 5.5134\n",
      "Epoch 6813/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2803 - val_loss: 5.6272\n",
      "Epoch 6814/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4311 - val_loss: 5.6419\n",
      "Epoch 6815/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3756 - val_loss: 5.5487\n",
      "Epoch 6816/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4894 - val_loss: 5.9055\n",
      "Epoch 6817/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5737 - val_loss: 5.5802\n",
      "Epoch 6818/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3138 - val_loss: 5.4832\n",
      "Epoch 6819/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2867 - val_loss: 5.4595\n",
      "Epoch 6820/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2843 - val_loss: 5.4357\n",
      "Epoch 6821/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3003 - val_loss: 5.4371\n",
      "Epoch 6822/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3410 - val_loss: 5.4707\n",
      "Epoch 6823/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3734 - val_loss: 5.4976\n",
      "Epoch 6824/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4113 - val_loss: 5.6340\n",
      "Epoch 6825/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3827 - val_loss: 5.4704\n",
      "Epoch 6826/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2769 - val_loss: 5.8669\n",
      "Epoch 6827/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5472 - val_loss: 5.5374\n",
      "Epoch 6828/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3897 - val_loss: 5.5096\n",
      "Epoch 6829/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2928 - val_loss: 5.5794\n",
      "Epoch 6830/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5471 - val_loss: 5.9470\n",
      "Epoch 6831/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.4073 - val_loss: 5.4404\n",
      "Epoch 6832/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3356 - val_loss: 5.7330\n",
      "Epoch 6833/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5020 - val_loss: 5.8344\n",
      "Epoch 6834/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.5442 - val_loss: 5.5639\n",
      "Epoch 6835/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3032 - val_loss: 5.5540\n",
      "Epoch 6836/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4268 - val_loss: 5.5161\n",
      "Epoch 6837/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2720 - val_loss: 5.4190\n",
      "Epoch 6838/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3039 - val_loss: 5.3948\n",
      "Epoch 6839/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3493 - val_loss: 5.4161\n",
      "Epoch 6840/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3656 - val_loss: 5.4602\n",
      "Epoch 6841/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4143 - val_loss: 5.7281\n",
      "Epoch 6842/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3261 - val_loss: 5.4803\n",
      "Epoch 6843/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4005 - val_loss: 6.0766\n",
      "Epoch 6844/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6054 - val_loss: 5.5085\n",
      "Epoch 6845/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 56us/step - loss: 4.6439 - val_loss: 6.4119\n",
      "Epoch 6846/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4368 - val_loss: 5.5168\n",
      "Epoch 6847/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2734 - val_loss: 5.6667\n",
      "Epoch 6848/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4644 - val_loss: 5.8262\n",
      "Epoch 6849/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.5985 - val_loss: 5.6266\n",
      "Epoch 6850/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.4026 - val_loss: 5.5351\n",
      "Epoch 6851/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2354 - val_loss: 5.5027\n",
      "Epoch 6852/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3452 - val_loss: 5.6150\n",
      "Epoch 6853/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3733 - val_loss: 5.4660\n",
      "Epoch 6854/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2875 - val_loss: 5.5179\n",
      "Epoch 6855/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.3185 - val_loss: 5.4269\n",
      "Epoch 6856/10000\n",
      "675/675 [==============================] - 0s 116us/step - loss: 4.2797 - val_loss: 5.4313\n",
      "Epoch 6857/10000\n",
      "675/675 [==============================] - 0s 110us/step - loss: 4.2373 - val_loss: 5.4414\n",
      "Epoch 6858/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 4.2417 - val_loss: 5.4283\n",
      "Epoch 6859/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.2507 - val_loss: 5.4903\n",
      "Epoch 6860/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2859 - val_loss: 5.4569\n",
      "Epoch 6861/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2710 - val_loss: 5.5151\n",
      "Epoch 6862/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2802 - val_loss: 5.4722\n",
      "Epoch 6863/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3309 - val_loss: 5.5842\n",
      "Epoch 6864/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2562 - val_loss: 5.4239\n",
      "Epoch 6865/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2708 - val_loss: 5.4698\n",
      "Epoch 6866/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2968 - val_loss: 5.5166\n",
      "Epoch 6867/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2803 - val_loss: 5.5390\n",
      "Epoch 6868/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3130 - val_loss: 5.4705\n",
      "Epoch 6869/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3415 - val_loss: 5.4622\n",
      "Epoch 6870/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3907 - val_loss: 5.4189\n",
      "Epoch 6871/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4746 - val_loss: 5.5182\n",
      "Epoch 6872/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3387 - val_loss: 5.6451\n",
      "Epoch 6873/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2838 - val_loss: 5.4906\n",
      "Epoch 6874/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3126 - val_loss: 5.4969\n",
      "Epoch 6875/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3108 - val_loss: 5.4387\n",
      "Epoch 6876/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3487 - val_loss: 5.5016\n",
      "Epoch 6877/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4118 - val_loss: 5.5160\n",
      "Epoch 6878/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4260 - val_loss: 5.4222\n",
      "Epoch 6879/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.4020 - val_loss: 5.5289\n",
      "Epoch 6880/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 5.0130 - val_loss: 5.5032\n",
      "Epoch 6881/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2763 - val_loss: 5.4760\n",
      "Epoch 6882/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3325 - val_loss: 5.4481\n",
      "Epoch 6883/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2951 - val_loss: 5.4274\n",
      "Epoch 6884/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3326 - val_loss: 5.4492\n",
      "Epoch 6885/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2785 - val_loss: 5.6346\n",
      "Epoch 6886/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5994 - val_loss: 5.4648\n",
      "Epoch 6887/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2620 - val_loss: 5.5673\n",
      "Epoch 6888/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2456 - val_loss: 5.4878\n",
      "Epoch 6889/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2551 - val_loss: 5.7902\n",
      "Epoch 6890/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3966 - val_loss: 5.6995\n",
      "Epoch 6891/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3245 - val_loss: 5.5539\n",
      "Epoch 6892/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4260 - val_loss: 5.3834\n",
      "Epoch 6893/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3254 - val_loss: 5.4651\n",
      "Epoch 6894/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3575 - val_loss: 5.4151\n",
      "Epoch 6895/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2716 - val_loss: 5.5116\n",
      "Epoch 6896/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2893 - val_loss: 5.4927\n",
      "Epoch 6897/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3681 - val_loss: 5.4140\n",
      "Epoch 6898/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2496 - val_loss: 5.8427\n",
      "Epoch 6899/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4485 - val_loss: 5.4182\n",
      "Epoch 6900/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2309 - val_loss: 5.4590\n",
      "Epoch 6901/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2637 - val_loss: 5.5158\n",
      "Epoch 6902/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3281 - val_loss: 5.4347\n",
      "Epoch 6903/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2484 - val_loss: 5.5149\n",
      "Epoch 6904/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3502 - val_loss: 5.5334\n",
      "Epoch 6905/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4291 - val_loss: 5.4561\n",
      "Epoch 6906/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5345 - val_loss: 5.7828\n",
      "Epoch 6907/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3681 - val_loss: 5.5991\n",
      "Epoch 6908/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3490 - val_loss: 5.4941\n",
      "Epoch 6909/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2950 - val_loss: 5.4454\n",
      "Epoch 6910/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2612 - val_loss: 5.5143\n",
      "Epoch 6911/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2593 - val_loss: 5.4546\n",
      "Epoch 6912/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2616 - val_loss: 5.9052\n",
      "Epoch 6913/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4793 - val_loss: 5.6039\n",
      "Epoch 6914/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3594 - val_loss: 5.4375\n",
      "Epoch 6915/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2871 - val_loss: 5.3969\n",
      "Epoch 6916/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3481 - val_loss: 5.4262\n",
      "Epoch 6917/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4413 - val_loss: 5.5033\n",
      "Epoch 6918/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2614 - val_loss: 5.5330\n",
      "Epoch 6919/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4556 - val_loss: 5.4747\n",
      "Epoch 6920/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2768 - val_loss: 5.7917\n",
      "Epoch 6921/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.4938 - val_loss: 5.4615\n",
      "Epoch 6922/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3276 - val_loss: 5.5373\n",
      "Epoch 6923/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3794 - val_loss: 5.6121\n",
      "Epoch 6924/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4438 - val_loss: 5.4815\n",
      "Epoch 6925/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3389 - val_loss: 5.6000\n",
      "Epoch 6926/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2770 - val_loss: 5.5416\n",
      "Epoch 6927/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3509 - val_loss: 5.4864\n",
      "Epoch 6928/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3519 - val_loss: 5.4112\n",
      "Epoch 6929/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4194 - val_loss: 5.7105\n",
      "Epoch 6930/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4769 - val_loss: 5.5093\n",
      "Epoch 6931/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.4333 - val_loss: 5.5372\n",
      "Epoch 6932/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3328 - val_loss: 5.7079\n",
      "Epoch 6933/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2617 - val_loss: 5.4475\n",
      "Epoch 6934/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.3214 - val_loss: 5.4806\n",
      "Epoch 6935/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.3010 - val_loss: 5.6119\n",
      "Epoch 6936/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4506 - val_loss: 5.3934\n",
      "Epoch 6937/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4461 - val_loss: 5.6916\n",
      "Epoch 6938/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3834 - val_loss: 5.4518\n",
      "Epoch 6939/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4635 - val_loss: 5.5317\n",
      "Epoch 6940/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3679 - val_loss: 5.4619\n",
      "Epoch 6941/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2907 - val_loss: 5.4682\n",
      "Epoch 6942/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3214 - val_loss: 5.5005\n",
      "Epoch 6943/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3974 - val_loss: 5.7543\n",
      "Epoch 6944/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5281 - val_loss: 5.4221\n",
      "Epoch 6945/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2185 - val_loss: 6.0270\n",
      "Epoch 6946/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6318 - val_loss: 5.5234\n",
      "Epoch 6947/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3863 - val_loss: 5.5256\n",
      "Epoch 6948/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3121 - val_loss: 5.4724\n",
      "Epoch 6949/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6654 - val_loss: 6.3137\n",
      "Epoch 6950/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6165 - val_loss: 5.3366\n",
      "Epoch 6951/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2819 - val_loss: 5.6432\n",
      "Epoch 6952/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4081 - val_loss: 5.5167\n",
      "Epoch 6953/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2723 - val_loss: 5.5326\n",
      "Epoch 6954/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2929 - val_loss: 5.4831\n",
      "Epoch 6955/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2810 - val_loss: 5.7973\n",
      "Epoch 6956/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6396 - val_loss: 5.4743\n",
      "Epoch 6957/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3092 - val_loss: 6.0467\n",
      "Epoch 6958/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3707 - val_loss: 5.7477\n",
      "Epoch 6959/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5703 - val_loss: 5.6443\n",
      "Epoch 6960/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4428 - val_loss: 5.4862\n",
      "Epoch 6961/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6079 - val_loss: 5.7814\n",
      "Epoch 6962/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4556 - val_loss: 5.4408\n",
      "Epoch 6963/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4746 - val_loss: 5.5651\n",
      "Epoch 6964/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3764 - val_loss: 5.3841\n",
      "Epoch 6965/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2838 - val_loss: 5.5598\n",
      "Epoch 6966/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3871 - val_loss: 5.4784\n",
      "Epoch 6967/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2200 - val_loss: 5.5688\n",
      "Epoch 6968/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4331 - val_loss: 5.3813\n",
      "Epoch 6969/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6802 - val_loss: 5.9416\n",
      "Epoch 6970/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3943 - val_loss: 5.3999\n",
      "Epoch 6971/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3090 - val_loss: 5.4203\n",
      "Epoch 6972/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3015 - val_loss: 6.2396\n",
      "Epoch 6973/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7173 - val_loss: 5.5825\n",
      "Epoch 6974/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3857 - val_loss: 5.4142\n",
      "Epoch 6975/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2465 - val_loss: 5.4911\n",
      "Epoch 6976/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3032 - val_loss: 5.6791\n",
      "Epoch 6977/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4370 - val_loss: 5.4180\n",
      "Epoch 6978/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2727 - val_loss: 5.6782\n",
      "Epoch 6979/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5208 - val_loss: 5.4028\n",
      "Epoch 6980/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2712 - val_loss: 5.5426\n",
      "Epoch 6981/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2802 - val_loss: 5.5155\n",
      "Epoch 6982/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4132 - val_loss: 5.6021\n",
      "Epoch 6983/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5582 - val_loss: 5.7405\n",
      "Epoch 6984/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5313 - val_loss: 5.4879\n",
      "Epoch 6985/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3369 - val_loss: 5.4477\n",
      "Epoch 6986/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2630 - val_loss: 5.4430\n",
      "Epoch 6987/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2712 - val_loss: 5.4651\n",
      "Epoch 6988/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2718 - val_loss: 5.6113\n",
      "Epoch 6989/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2926 - val_loss: 5.4457\n",
      "Epoch 6990/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2765 - val_loss: 5.4692\n",
      "Epoch 6991/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5366 - val_loss: 5.4364\n",
      "Epoch 6992/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2715 - val_loss: 5.5359\n",
      "Epoch 6993/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3126 - val_loss: 5.6323\n",
      "Epoch 6994/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4049 - val_loss: 5.8274\n",
      "Epoch 6995/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8714 - val_loss: 5.3754\n",
      "Epoch 6996/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2636 - val_loss: 5.3941\n",
      "Epoch 6997/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.2398 - val_loss: 5.4393\n",
      "Epoch 6998/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3075 - val_loss: 5.3634\n",
      "Epoch 6999/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3335 - val_loss: 5.4166\n",
      "Epoch 7000/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2932 - val_loss: 5.4511\n",
      "Epoch 7001/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4625 - val_loss: 5.6470\n",
      "Epoch 7002/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5348 - val_loss: 5.5915\n",
      "Epoch 7003/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2637 - val_loss: 5.4062\n",
      "Epoch 7004/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2092 - val_loss: 5.4515\n",
      "Epoch 7005/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2646 - val_loss: 5.7223\n",
      "Epoch 7006/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5011 - val_loss: 5.4527\n",
      "Epoch 7007/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2766 - val_loss: 5.5204\n",
      "Epoch 7008/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2805 - val_loss: 5.3965\n",
      "Epoch 7009/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2822 - val_loss: 5.8023\n",
      "Epoch 7010/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7962 - val_loss: 5.6687\n",
      "Epoch 7011/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5019 - val_loss: 5.4876\n",
      "Epoch 7012/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3960 - val_loss: 5.4874\n",
      "Epoch 7013/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2816 - val_loss: 5.5449\n",
      "Epoch 7014/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2883 - val_loss: 5.5735\n",
      "Epoch 7015/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3866 - val_loss: 5.7328\n",
      "Epoch 7016/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3561 - val_loss: 5.4403\n",
      "Epoch 7017/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2670 - val_loss: 5.4115\n",
      "Epoch 7018/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2502 - val_loss: 5.4480\n",
      "Epoch 7019/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2617 - val_loss: 5.4170\n",
      "Epoch 7020/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3035 - val_loss: 5.5238\n",
      "Epoch 7021/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4773 - val_loss: 5.4409\n",
      "Epoch 7022/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4250 - val_loss: 5.5204\n",
      "Epoch 7023/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3727 - val_loss: 5.4530\n",
      "Epoch 7024/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3139 - val_loss: 5.6158\n",
      "Epoch 7025/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4448 - val_loss: 5.5349\n",
      "Epoch 7026/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2744 - val_loss: 5.6118\n",
      "Epoch 7027/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2984 - val_loss: 5.4292\n",
      "Epoch 7028/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2445 - val_loss: 5.5032\n",
      "Epoch 7029/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3185 - val_loss: 5.4779\n",
      "Epoch 7030/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3018 - val_loss: 5.4470\n",
      "Epoch 7031/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2657 - val_loss: 5.6490\n",
      "Epoch 7032/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3604 - val_loss: 5.4491\n",
      "Epoch 7033/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3535 - val_loss: 5.5599\n",
      "Epoch 7034/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5177 - val_loss: 6.0866\n",
      "Epoch 7035/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5587 - val_loss: 5.4512\n",
      "Epoch 7036/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3397 - val_loss: 5.5327\n",
      "Epoch 7037/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2572 - val_loss: 5.4240\n",
      "Epoch 7038/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2276 - val_loss: 5.5859\n",
      "Epoch 7039/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3131 - val_loss: 5.9277\n",
      "Epoch 7040/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5077 - val_loss: 5.6356\n",
      "Epoch 7041/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4890 - val_loss: 5.4484\n",
      "Epoch 7042/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3031 - val_loss: 5.5890\n",
      "Epoch 7043/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4242 - val_loss: 5.5935\n",
      "Epoch 7044/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7844 - val_loss: 6.2894\n",
      "Epoch 7045/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6014 - val_loss: 5.5451\n",
      "Epoch 7046/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2797 - val_loss: 5.5137\n",
      "Epoch 7047/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3698 - val_loss: 5.4106\n",
      "Epoch 7048/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2441 - val_loss: 5.4361\n",
      "Epoch 7049/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5358 - val_loss: 5.4636\n",
      "Epoch 7050/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4078 - val_loss: 5.4460\n",
      "Epoch 7051/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2269 - val_loss: 5.3939\n",
      "Epoch 7052/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3905 - val_loss: 5.6234\n",
      "Epoch 7053/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3226 - val_loss: 5.6902\n",
      "Epoch 7054/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2997 - val_loss: 5.4246\n",
      "Epoch 7055/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3413 - val_loss: 5.4636\n",
      "Epoch 7056/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3078 - val_loss: 5.4399\n",
      "Epoch 7057/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2329 - val_loss: 5.4500\n",
      "Epoch 7058/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3855 - val_loss: 5.5717\n",
      "Epoch 7059/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3125 - val_loss: 5.5104\n",
      "Epoch 7060/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3079 - val_loss: 6.1120\n",
      "Epoch 7061/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8064 - val_loss: 5.4302\n",
      "Epoch 7062/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3467 - val_loss: 5.6316\n",
      "Epoch 7063/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3861 - val_loss: 5.4166\n",
      "Epoch 7064/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3005 - val_loss: 5.6939\n",
      "Epoch 7065/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4310 - val_loss: 5.4895\n",
      "Epoch 7066/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3385 - val_loss: 5.4640\n",
      "Epoch 7067/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4278 - val_loss: 5.4485\n",
      "Epoch 7068/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2305 - val_loss: 5.4080\n",
      "Epoch 7069/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2929 - val_loss: 5.5460\n",
      "Epoch 7070/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2942 - val_loss: 5.6380\n",
      "Epoch 7071/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2745 - val_loss: 5.6043\n",
      "Epoch 7072/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3935 - val_loss: 5.4430\n",
      "Epoch 7073/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 61us/step - loss: 4.3126 - val_loss: 5.6646\n",
      "Epoch 7074/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3773 - val_loss: 5.4552\n",
      "Epoch 7075/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2354 - val_loss: 5.4784\n",
      "Epoch 7076/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3993 - val_loss: 5.4908\n",
      "Epoch 7077/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2915 - val_loss: 5.4206\n",
      "Epoch 7078/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2616 - val_loss: 5.4313\n",
      "Epoch 7079/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3036 - val_loss: 5.5709\n",
      "Epoch 7080/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3184 - val_loss: 5.4189\n",
      "Epoch 7081/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3220 - val_loss: 5.3906\n",
      "Epoch 7082/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2982 - val_loss: 5.9873\n",
      "Epoch 7083/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.5331 - val_loss: 5.9747\n",
      "Epoch 7084/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3546 - val_loss: 5.5584\n",
      "Epoch 7085/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2958 - val_loss: 5.4232\n",
      "Epoch 7086/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3377 - val_loss: 5.6008\n",
      "Epoch 7087/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.4293 - val_loss: 5.3880\n",
      "Epoch 7088/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3237 - val_loss: 5.5356\n",
      "Epoch 7089/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2813 - val_loss: 5.7611\n",
      "Epoch 7090/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.3880 - val_loss: 5.5576\n",
      "Epoch 7091/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2591 - val_loss: 5.5161\n",
      "Epoch 7092/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2557 - val_loss: 5.4753\n",
      "Epoch 7093/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2623 - val_loss: 5.4757\n",
      "Epoch 7094/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2203 - val_loss: 5.6505\n",
      "Epoch 7095/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2540 - val_loss: 5.4411\n",
      "Epoch 7096/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3798 - val_loss: 5.6053\n",
      "Epoch 7097/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3405 - val_loss: 5.4153\n",
      "Epoch 7098/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2328 - val_loss: 5.4839\n",
      "Epoch 7099/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2775 - val_loss: 5.4212\n",
      "Epoch 7100/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2520 - val_loss: 5.4544\n",
      "Epoch 7101/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4491 - val_loss: 5.4845\n",
      "Epoch 7102/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4101 - val_loss: 5.9421\n",
      "Epoch 7103/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4314 - val_loss: 5.4057\n",
      "Epoch 7104/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2667 - val_loss: 5.4348\n",
      "Epoch 7105/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2525 - val_loss: 5.4863\n",
      "Epoch 7106/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3266 - val_loss: 5.5477\n",
      "Epoch 7107/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.2871 - val_loss: 5.3906\n",
      "Epoch 7108/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.2593 - val_loss: 5.4096\n",
      "Epoch 7109/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.2199 - val_loss: 5.5023\n",
      "Epoch 7110/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.2618 - val_loss: 5.5512\n",
      "Epoch 7111/10000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 4.4016 - val_loss: 5.5288\n",
      "Epoch 7112/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.3608 - val_loss: 5.4913\n",
      "Epoch 7113/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.2313 - val_loss: 5.7175\n",
      "Epoch 7114/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2897 - val_loss: 5.4385\n",
      "Epoch 7115/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.2653 - val_loss: 5.4048\n",
      "Epoch 7116/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.4061 - val_loss: 5.4749\n",
      "Epoch 7117/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.3056 - val_loss: 5.5129\n",
      "Epoch 7118/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.2467 - val_loss: 5.5567\n",
      "Epoch 7119/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.4041 - val_loss: 5.4705\n",
      "Epoch 7120/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2131 - val_loss: 5.7207\n",
      "Epoch 7121/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.4043 - val_loss: 5.4605\n",
      "Epoch 7122/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2809 - val_loss: 5.4328\n",
      "Epoch 7123/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.3380 - val_loss: 5.8249\n",
      "Epoch 7124/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4840 - val_loss: 5.3868\n",
      "Epoch 7125/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.2735 - val_loss: 5.4498\n",
      "Epoch 7126/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2585 - val_loss: 5.4796\n",
      "Epoch 7127/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3100 - val_loss: 5.6620\n",
      "Epoch 7128/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2529 - val_loss: 5.5092\n",
      "Epoch 7129/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3013 - val_loss: 5.4109\n",
      "Epoch 7130/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2277 - val_loss: 5.6395\n",
      "Epoch 7131/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3245 - val_loss: 5.4303\n",
      "Epoch 7132/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2889 - val_loss: 5.4505\n",
      "Epoch 7133/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2720 - val_loss: 5.4140\n",
      "Epoch 7134/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2255 - val_loss: 5.4438\n",
      "Epoch 7135/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3839 - val_loss: 5.6669\n",
      "Epoch 7136/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.6987 - val_loss: 5.5528\n",
      "Epoch 7137/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3311 - val_loss: 5.5256\n",
      "Epoch 7138/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2023 - val_loss: 5.4767\n",
      "Epoch 7139/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3079 - val_loss: 5.3726\n",
      "Epoch 7140/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3772 - val_loss: 5.7033\n",
      "Epoch 7141/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.4675 - val_loss: 5.4247\n",
      "Epoch 7142/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2829 - val_loss: 5.4126\n",
      "Epoch 7143/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2337 - val_loss: 5.4364\n",
      "Epoch 7144/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.3027 - val_loss: 5.4877\n",
      "Epoch 7145/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2913 - val_loss: 5.4185\n",
      "Epoch 7146/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2558 - val_loss: 5.3900\n",
      "Epoch 7147/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2040 - val_loss: 5.6667\n",
      "Epoch 7148/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.6909 - val_loss: 5.3783\n",
      "Epoch 7149/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.4993 - val_loss: 5.4095\n",
      "Epoch 7150/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3515 - val_loss: 5.7333\n",
      "Epoch 7151/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3521 - val_loss: 5.4039\n",
      "Epoch 7152/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3320 - val_loss: 5.4462\n",
      "Epoch 7153/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3323 - val_loss: 6.0295\n",
      "Epoch 7154/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7576 - val_loss: 5.5479\n",
      "Epoch 7155/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3193 - val_loss: 5.4729\n",
      "Epoch 7156/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2466 - val_loss: 5.3968\n",
      "Epoch 7157/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2691 - val_loss: 5.4020\n",
      "Epoch 7158/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2530 - val_loss: 5.7716\n",
      "Epoch 7159/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3457 - val_loss: 5.4529\n",
      "Epoch 7160/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2997 - val_loss: 5.3808\n",
      "Epoch 7161/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2717 - val_loss: 5.3943\n",
      "Epoch 7162/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2928 - val_loss: 5.6229\n",
      "Epoch 7163/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4476 - val_loss: 5.5036\n",
      "Epoch 7164/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3429 - val_loss: 5.6112\n",
      "Epoch 7165/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4509 - val_loss: 5.4190\n",
      "Epoch 7166/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3059 - val_loss: 5.5894\n",
      "Epoch 7167/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2450 - val_loss: 5.4566\n",
      "Epoch 7168/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2540 - val_loss: 5.3692\n",
      "Epoch 7169/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2441 - val_loss: 5.6641\n",
      "Epoch 7170/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4027 - val_loss: 5.5979\n",
      "Epoch 7171/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3735 - val_loss: 5.4946\n",
      "Epoch 7172/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2858 - val_loss: 5.3946\n",
      "Epoch 7173/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2297 - val_loss: 5.4793\n",
      "Epoch 7174/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2869 - val_loss: 5.5502\n",
      "Epoch 7175/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3667 - val_loss: 5.4309\n",
      "Epoch 7176/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2258 - val_loss: 5.4237\n",
      "Epoch 7177/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3080 - val_loss: 5.4350\n",
      "Epoch 7178/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2850 - val_loss: 5.4647\n",
      "Epoch 7179/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2328 - val_loss: 5.4197\n",
      "Epoch 7180/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2286 - val_loss: 5.3830\n",
      "Epoch 7181/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3627 - val_loss: 5.4947\n",
      "Epoch 7182/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4842 - val_loss: 5.4258\n",
      "Epoch 7183/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2125 - val_loss: 5.4829\n",
      "Epoch 7184/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2954 - val_loss: 5.4787\n",
      "Epoch 7185/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.2334 - val_loss: 5.5867\n",
      "Epoch 7186/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4554 - val_loss: 5.4050\n",
      "Epoch 7187/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2408 - val_loss: 5.3880\n",
      "Epoch 7188/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2835 - val_loss: 5.4091\n",
      "Epoch 7189/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2326 - val_loss: 5.6427\n",
      "Epoch 7190/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4327 - val_loss: 5.4418\n",
      "Epoch 7191/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3415 - val_loss: 5.6245\n",
      "Epoch 7192/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4694 - val_loss: 5.6974\n",
      "Epoch 7193/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3796 - val_loss: 5.5267\n",
      "Epoch 7194/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.8233 - val_loss: 5.8145\n",
      "Epoch 7195/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3752 - val_loss: 5.4443\n",
      "Epoch 7196/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2608 - val_loss: 5.4078\n",
      "Epoch 7197/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2471 - val_loss: 5.4631\n",
      "Epoch 7198/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3159 - val_loss: 5.6960\n",
      "Epoch 7199/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5282 - val_loss: 5.4340\n",
      "Epoch 7200/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3105 - val_loss: 5.5974\n",
      "Epoch 7201/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2385 - val_loss: 5.8757\n",
      "Epoch 7202/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6440 - val_loss: 5.5163\n",
      "Epoch 7203/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2953 - val_loss: 5.4532\n",
      "Epoch 7204/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4554 - val_loss: 5.9891\n",
      "Epoch 7205/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3861 - val_loss: 5.5117\n",
      "Epoch 7206/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2739 - val_loss: 5.5317\n",
      "Epoch 7207/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2340 - val_loss: 5.4048\n",
      "Epoch 7208/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.2284 - val_loss: 5.3930\n",
      "Epoch 7209/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2251 - val_loss: 5.4115\n",
      "Epoch 7210/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2694 - val_loss: 5.4962\n",
      "Epoch 7211/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4903 - val_loss: 5.5309\n",
      "Epoch 7212/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.9907 - val_loss: 5.4613\n",
      "Epoch 7213/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.2828 - val_loss: 5.6394\n",
      "Epoch 7214/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 4.4866 - val_loss: 5.5066\n",
      "Epoch 7215/10000\n",
      "675/675 [==============================] - 0s 114us/step - loss: 4.3777 - val_loss: 5.3449\n",
      "Epoch 7216/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 4.3995 - val_loss: 5.6394\n",
      "Epoch 7217/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.2401 - val_loss: 5.4119\n",
      "Epoch 7218/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.3662 - val_loss: 5.4658\n",
      "Epoch 7219/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.3383 - val_loss: 5.4026\n",
      "Epoch 7220/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4089 - val_loss: 5.4155\n",
      "Epoch 7221/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4415 - val_loss: 5.7006\n",
      "Epoch 7222/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3172 - val_loss: 5.4661\n",
      "Epoch 7223/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2708 - val_loss: 5.4729\n",
      "Epoch 7224/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4858 - val_loss: 5.4242\n",
      "Epoch 7225/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 51us/step - loss: 4.3069 - val_loss: 5.9635\n",
      "Epoch 7226/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5310 - val_loss: 5.4211\n",
      "Epoch 7227/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2543 - val_loss: 5.6410\n",
      "Epoch 7228/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3504 - val_loss: 5.9283\n",
      "Epoch 7229/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4437 - val_loss: 5.4192\n",
      "Epoch 7230/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2904 - val_loss: 5.3972\n",
      "Epoch 7231/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4833 - val_loss: 5.4102\n",
      "Epoch 7232/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3260 - val_loss: 5.4413\n",
      "Epoch 7233/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4130 - val_loss: 5.4218\n",
      "Epoch 7234/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2772 - val_loss: 5.4219\n",
      "Epoch 7235/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2848 - val_loss: 5.4630\n",
      "Epoch 7236/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3739 - val_loss: 5.4116\n",
      "Epoch 7237/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2416 - val_loss: 5.4022\n",
      "Epoch 7238/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1988 - val_loss: 5.5008\n",
      "Epoch 7239/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2805 - val_loss: 5.3793\n",
      "Epoch 7240/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2757 - val_loss: 5.4179\n",
      "Epoch 7241/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2142 - val_loss: 5.8138\n",
      "Epoch 7242/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4143 - val_loss: 5.4582\n",
      "Epoch 7243/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3124 - val_loss: 5.5868\n",
      "Epoch 7244/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2405 - val_loss: 5.4198\n",
      "Epoch 7245/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4199 - val_loss: 5.7371\n",
      "Epoch 7246/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3736 - val_loss: 5.3629\n",
      "Epoch 7247/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2647 - val_loss: 5.3991\n",
      "Epoch 7248/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3020 - val_loss: 5.7390\n",
      "Epoch 7249/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5049 - val_loss: 5.3989\n",
      "Epoch 7250/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2717 - val_loss: 5.5297\n",
      "Epoch 7251/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3188 - val_loss: 5.7123\n",
      "Epoch 7252/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4895 - val_loss: 5.4083\n",
      "Epoch 7253/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2581 - val_loss: 5.3716\n",
      "Epoch 7254/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2296 - val_loss: 5.4173\n",
      "Epoch 7255/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4445 - val_loss: 5.7436\n",
      "Epoch 7256/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3018 - val_loss: 5.3988\n",
      "Epoch 7257/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2390 - val_loss: 5.4424\n",
      "Epoch 7258/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 4.4064 - val_loss: 5.7301\n",
      "Epoch 7259/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.4443 - val_loss: 5.4213\n",
      "Epoch 7260/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3274 - val_loss: 5.3998\n",
      "Epoch 7261/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3371 - val_loss: 5.7826\n",
      "Epoch 7262/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7054 - val_loss: 5.5788\n",
      "Epoch 7263/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5117 - val_loss: 5.3473\n",
      "Epoch 7264/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2131 - val_loss: 5.5850\n",
      "Epoch 7265/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5260 - val_loss: 5.6644\n",
      "Epoch 7266/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2836 - val_loss: 5.4534\n",
      "Epoch 7267/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3560 - val_loss: 5.4739\n",
      "Epoch 7268/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2464 - val_loss: 5.3532\n",
      "Epoch 7269/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2731 - val_loss: 5.3800\n",
      "Epoch 7270/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3219 - val_loss: 5.5281\n",
      "Epoch 7271/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4070 - val_loss: 5.3608\n",
      "Epoch 7272/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1953 - val_loss: 5.4445\n",
      "Epoch 7273/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.3147 - val_loss: 5.4956\n",
      "Epoch 7274/10000\n",
      "675/675 [==============================] - 0s 114us/step - loss: 4.2043 - val_loss: 5.6624\n",
      "Epoch 7275/10000\n",
      "675/675 [==============================] - 0s 123us/step - loss: 4.3315 - val_loss: 5.5659\n",
      "Epoch 7276/10000\n",
      "675/675 [==============================] - 0s 113us/step - loss: 4.2593 - val_loss: 5.7501\n",
      "Epoch 7277/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.9081 - val_loss: 6.1612\n",
      "Epoch 7278/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4183 - val_loss: 5.5721\n",
      "Epoch 7279/10000\n",
      "675/675 [==============================] - ETA: 0s - loss: 4.599 - 0s 55us/step - loss: 4.3465 - val_loss: 5.3672\n",
      "Epoch 7280/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2591 - val_loss: 5.8172\n",
      "Epoch 7281/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5245 - val_loss: 5.3919\n",
      "Epoch 7282/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2456 - val_loss: 5.4432\n",
      "Epoch 7283/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3254 - val_loss: 5.3855\n",
      "Epoch 7284/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1898 - val_loss: 5.3883\n",
      "Epoch 7285/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2724 - val_loss: 5.4494\n",
      "Epoch 7286/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2517 - val_loss: 5.5279\n",
      "Epoch 7287/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3655 - val_loss: 5.4557\n",
      "Epoch 7288/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2321 - val_loss: 5.5180\n",
      "Epoch 7289/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2625 - val_loss: 5.5000\n",
      "Epoch 7290/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2503 - val_loss: 5.4656\n",
      "Epoch 7291/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2855 - val_loss: 5.4089\n",
      "Epoch 7292/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3277 - val_loss: 5.6868\n",
      "Epoch 7293/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.5007 - val_loss: 5.4387\n",
      "Epoch 7294/10000\n",
      "675/675 [==============================] - 0s 105us/step - loss: 4.3675 - val_loss: 5.4029\n",
      "Epoch 7295/10000\n",
      "675/675 [==============================] - 0s 134us/step - loss: 4.3334 - val_loss: 5.6505\n",
      "Epoch 7296/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 4.2425 - val_loss: 5.6117\n",
      "Epoch 7297/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2717 - val_loss: 5.4404\n",
      "Epoch 7298/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2595 - val_loss: 5.5188\n",
      "Epoch 7299/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5503 - val_loss: 5.5706\n",
      "Epoch 7300/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4131 - val_loss: 5.5647\n",
      "Epoch 7301/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 57us/step - loss: 4.3565 - val_loss: 5.3882\n",
      "Epoch 7302/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2360 - val_loss: 5.5475\n",
      "Epoch 7303/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2638 - val_loss: 5.6096\n",
      "Epoch 7304/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3134 - val_loss: 5.3691\n",
      "Epoch 7305/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3666 - val_loss: 5.4811\n",
      "Epoch 7306/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2382 - val_loss: 5.3330\n",
      "Epoch 7307/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3093 - val_loss: 5.4034\n",
      "Epoch 7308/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2031 - val_loss: 5.4035\n",
      "Epoch 7309/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2147 - val_loss: 5.5382\n",
      "Epoch 7310/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4715 - val_loss: 5.4021\n",
      "Epoch 7311/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2364 - val_loss: 5.3937\n",
      "Epoch 7312/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2930 - val_loss: 5.3700\n",
      "Epoch 7313/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2170 - val_loss: 5.3582\n",
      "Epoch 7314/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2202 - val_loss: 5.4759\n",
      "Epoch 7315/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2928 - val_loss: 5.4355\n",
      "Epoch 7316/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2552 - val_loss: 5.5387\n",
      "Epoch 7317/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2625 - val_loss: 5.4665\n",
      "Epoch 7318/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3224 - val_loss: 5.4010\n",
      "Epoch 7319/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3769 - val_loss: 5.4904\n",
      "Epoch 7320/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3208 - val_loss: 5.3866\n",
      "Epoch 7321/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2061 - val_loss: 5.4530\n",
      "Epoch 7322/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2253 - val_loss: 5.4016\n",
      "Epoch 7323/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3651 - val_loss: 5.3591\n",
      "Epoch 7324/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4206 - val_loss: 5.6892\n",
      "Epoch 7325/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3929 - val_loss: 5.5093\n",
      "Epoch 7326/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3900 - val_loss: 5.4756\n",
      "Epoch 7327/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2979 - val_loss: 5.3656\n",
      "Epoch 7328/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2269 - val_loss: 5.3688\n",
      "Epoch 7329/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2022 - val_loss: 5.3605\n",
      "Epoch 7330/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2474 - val_loss: 5.3738\n",
      "Epoch 7331/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2513 - val_loss: 5.6101\n",
      "Epoch 7332/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2946 - val_loss: 5.3690\n",
      "Epoch 7333/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2296 - val_loss: 5.4687\n",
      "Epoch 7334/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2273 - val_loss: 5.3874\n",
      "Epoch 7335/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1966 - val_loss: 5.4876\n",
      "Epoch 7336/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2796 - val_loss: 5.4005\n",
      "Epoch 7337/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3085 - val_loss: 5.5554\n",
      "Epoch 7338/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3459 - val_loss: 5.4271\n",
      "Epoch 7339/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3802 - val_loss: 5.3664\n",
      "Epoch 7340/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2498 - val_loss: 5.4319\n",
      "Epoch 7341/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2603 - val_loss: 5.3958\n",
      "Epoch 7342/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2393 - val_loss: 5.4540\n",
      "Epoch 7343/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2084 - val_loss: 5.3714\n",
      "Epoch 7344/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2778 - val_loss: 5.4095\n",
      "Epoch 7345/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3049 - val_loss: 5.5976\n",
      "Epoch 7346/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2570 - val_loss: 5.4395\n",
      "Epoch 7347/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2784 - val_loss: 5.4530\n",
      "Epoch 7348/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2582 - val_loss: 5.5158\n",
      "Epoch 7349/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6829 - val_loss: 5.6147\n",
      "Epoch 7350/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2729 - val_loss: 5.4060\n",
      "Epoch 7351/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2433 - val_loss: 5.4347\n",
      "Epoch 7352/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2232 - val_loss: 5.4084\n",
      "Epoch 7353/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3765 - val_loss: 5.4660\n",
      "Epoch 7354/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2758 - val_loss: 5.4360\n",
      "Epoch 7355/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2223 - val_loss: 5.4556\n",
      "Epoch 7356/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2323 - val_loss: 5.4171\n",
      "Epoch 7357/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2233 - val_loss: 5.8382\n",
      "Epoch 7358/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3402 - val_loss: 5.3981\n",
      "Epoch 7359/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2130 - val_loss: 5.4025\n",
      "Epoch 7360/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2806 - val_loss: 5.5685\n",
      "Epoch 7361/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3940 - val_loss: 5.4645\n",
      "Epoch 7362/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3176 - val_loss: 5.5835\n",
      "Epoch 7363/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.3334 - val_loss: 5.4795\n",
      "Epoch 7364/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2562 - val_loss: 5.4081\n",
      "Epoch 7365/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1825 - val_loss: 5.4626\n",
      "Epoch 7366/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3874 - val_loss: 5.5174\n",
      "Epoch 7367/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4105 - val_loss: 5.6956\n",
      "Epoch 7368/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3813 - val_loss: 5.4930\n",
      "Epoch 7369/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2313 - val_loss: 5.5852\n",
      "Epoch 7370/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2420 - val_loss: 5.4013\n",
      "Epoch 7371/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2132 - val_loss: 5.3973\n",
      "Epoch 7372/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1995 - val_loss: 5.3979\n",
      "Epoch 7373/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3013 - val_loss: 5.4723\n",
      "Epoch 7374/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3229 - val_loss: 5.3734\n",
      "Epoch 7375/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2713 - val_loss: 5.6560\n",
      "Epoch 7376/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4481 - val_loss: 5.3934\n",
      "Epoch 7377/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.2717 - val_loss: 5.4245\n",
      "Epoch 7378/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2607 - val_loss: 5.4290\n",
      "Epoch 7379/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2825 - val_loss: 5.4865\n",
      "Epoch 7380/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3486 - val_loss: 6.2286\n",
      "Epoch 7381/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4767 - val_loss: 5.4423\n",
      "Epoch 7382/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2562 - val_loss: 5.3786\n",
      "Epoch 7383/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2540 - val_loss: 5.4106\n",
      "Epoch 7384/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3024 - val_loss: 5.5939\n",
      "Epoch 7385/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3554 - val_loss: 5.5655\n",
      "Epoch 7386/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6041 - val_loss: 5.4407\n",
      "Epoch 7387/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5024 - val_loss: 5.8978\n",
      "Epoch 7388/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4313 - val_loss: 5.3897\n",
      "Epoch 7389/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1862 - val_loss: 5.6697\n",
      "Epoch 7390/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3169 - val_loss: 5.6314\n",
      "Epoch 7391/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3144 - val_loss: 5.5845\n",
      "Epoch 7392/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.7430 - val_loss: 5.3868\n",
      "Epoch 7393/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2906 - val_loss: 5.5742\n",
      "Epoch 7394/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3571 - val_loss: 5.6300\n",
      "Epoch 7395/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2821 - val_loss: 5.4665\n",
      "Epoch 7396/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2201 - val_loss: 5.4340\n",
      "Epoch 7397/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2896 - val_loss: 5.4055\n",
      "Epoch 7398/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2332 - val_loss: 5.4501\n",
      "Epoch 7399/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2468 - val_loss: 5.5322\n",
      "Epoch 7400/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3016 - val_loss: 5.4289\n",
      "Epoch 7401/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1983 - val_loss: 5.5041\n",
      "Epoch 7402/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3491 - val_loss: 5.4110\n",
      "Epoch 7403/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2589 - val_loss: 5.4109\n",
      "Epoch 7404/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2409 - val_loss: 5.4460\n",
      "Epoch 7405/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2253 - val_loss: 5.4276\n",
      "Epoch 7406/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2825 - val_loss: 5.3954\n",
      "Epoch 7407/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1937 - val_loss: 5.3852\n",
      "Epoch 7408/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3673 - val_loss: 5.5776\n",
      "Epoch 7409/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2663 - val_loss: 5.5365\n",
      "Epoch 7410/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3428 - val_loss: 5.4690\n",
      "Epoch 7411/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2013 - val_loss: 5.5433\n",
      "Epoch 7412/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2718 - val_loss: 5.4235\n",
      "Epoch 7413/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2316 - val_loss: 5.4247\n",
      "Epoch 7414/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1816 - val_loss: 5.5051\n",
      "Epoch 7415/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2993 - val_loss: 5.4717\n",
      "Epoch 7416/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3496 - val_loss: 5.3905\n",
      "Epoch 7417/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1725 - val_loss: 5.4192\n",
      "Epoch 7418/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1712 - val_loss: 5.5491\n",
      "Epoch 7419/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2233 - val_loss: 5.3596\n",
      "Epoch 7420/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3044 - val_loss: 5.4587\n",
      "Epoch 7421/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2575 - val_loss: 5.3777\n",
      "Epoch 7422/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2939 - val_loss: 5.4259\n",
      "Epoch 7423/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2073 - val_loss: 5.3674\n",
      "Epoch 7424/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2422 - val_loss: 5.4631\n",
      "Epoch 7425/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2207 - val_loss: 5.4385\n",
      "Epoch 7426/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3427 - val_loss: 5.5324\n",
      "Epoch 7427/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2242 - val_loss: 5.4638\n",
      "Epoch 7428/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3070 - val_loss: 5.4126\n",
      "Epoch 7429/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2640 - val_loss: 5.5746\n",
      "Epoch 7430/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2560 - val_loss: 5.4395\n",
      "Epoch 7431/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2904 - val_loss: 5.4066\n",
      "Epoch 7432/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2910 - val_loss: 5.5169\n",
      "Epoch 7433/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2823 - val_loss: 5.3485\n",
      "Epoch 7434/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2170 - val_loss: 5.3901\n",
      "Epoch 7435/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2266 - val_loss: 5.8072\n",
      "Epoch 7436/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5040 - val_loss: 5.5266\n",
      "Epoch 7437/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2223 - val_loss: 5.5382\n",
      "Epoch 7438/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3519 - val_loss: 5.4694\n",
      "Epoch 7439/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2234 - val_loss: 5.6829\n",
      "Epoch 7440/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3915 - val_loss: 5.4394\n",
      "Epoch 7441/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2711 - val_loss: 5.4707\n",
      "Epoch 7442/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2653 - val_loss: 5.5042\n",
      "Epoch 7443/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2139 - val_loss: 5.4352\n",
      "Epoch 7444/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2706 - val_loss: 5.3902\n",
      "Epoch 7445/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3319 - val_loss: 5.4630\n",
      "Epoch 7446/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3411 - val_loss: 5.3403\n",
      "Epoch 7447/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1792 - val_loss: 5.5948\n",
      "Epoch 7448/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2562 - val_loss: 5.5968\n",
      "Epoch 7449/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2259 - val_loss: 5.4130\n",
      "Epoch 7450/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2153 - val_loss: 5.4797\n",
      "Epoch 7451/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2629 - val_loss: 5.3575\n",
      "Epoch 7452/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2146 - val_loss: 5.3903\n",
      "Epoch 7453/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.2775 - val_loss: 5.4704\n",
      "Epoch 7454/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3036 - val_loss: 5.4285\n",
      "Epoch 7455/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2210 - val_loss: 5.6130\n",
      "Epoch 7456/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5492 - val_loss: 5.5900\n",
      "Epoch 7457/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3175 - val_loss: 5.4503\n",
      "Epoch 7458/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2228 - val_loss: 5.4229\n",
      "Epoch 7459/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2121 - val_loss: 5.4207\n",
      "Epoch 7460/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3307 - val_loss: 5.4970\n",
      "Epoch 7461/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3326 - val_loss: 5.4496\n",
      "Epoch 7462/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2392 - val_loss: 5.3530\n",
      "Epoch 7463/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2947 - val_loss: 5.5057\n",
      "Epoch 7464/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1999 - val_loss: 5.4702\n",
      "Epoch 7465/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3983 - val_loss: 5.4693\n",
      "Epoch 7466/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3802 - val_loss: 5.4345\n",
      "Epoch 7467/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2461 - val_loss: 5.4770\n",
      "Epoch 7468/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2630 - val_loss: 5.5092\n",
      "Epoch 7469/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4803 - val_loss: 5.4956\n",
      "Epoch 7470/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4208 - val_loss: 5.4112\n",
      "Epoch 7471/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2580 - val_loss: 5.4681\n",
      "Epoch 7472/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2468 - val_loss: 5.3998\n",
      "Epoch 7473/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2424 - val_loss: 5.4527\n",
      "Epoch 7474/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2404 - val_loss: 5.4076\n",
      "Epoch 7475/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3482 - val_loss: 5.3215\n",
      "Epoch 7476/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1850 - val_loss: 5.3299\n",
      "Epoch 7477/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2917 - val_loss: 5.3805\n",
      "Epoch 7478/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2162 - val_loss: 5.4171\n",
      "Epoch 7479/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2095 - val_loss: 5.4079\n",
      "Epoch 7480/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2839 - val_loss: 5.3797\n",
      "Epoch 7481/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2303 - val_loss: 5.6363\n",
      "Epoch 7482/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3051 - val_loss: 5.5668\n",
      "Epoch 7483/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4688 - val_loss: 5.4214\n",
      "Epoch 7484/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3686 - val_loss: 5.3761\n",
      "Epoch 7485/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1987 - val_loss: 5.4243\n",
      "Epoch 7486/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5385 - val_loss: 5.3594\n",
      "Epoch 7487/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2953 - val_loss: 5.3913\n",
      "Epoch 7488/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1768 - val_loss: 5.4506\n",
      "Epoch 7489/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3456 - val_loss: 5.4477\n",
      "Epoch 7490/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2565 - val_loss: 5.4720\n",
      "Epoch 7491/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2711 - val_loss: 5.3883\n",
      "Epoch 7492/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 4.2442 - val_loss: 5.3990\n",
      "Epoch 7493/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2344 - val_loss: 5.5071\n",
      "Epoch 7494/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2710 - val_loss: 5.4733\n",
      "Epoch 7495/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1798 - val_loss: 5.3724\n",
      "Epoch 7496/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2177 - val_loss: 5.3813\n",
      "Epoch 7497/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2312 - val_loss: 5.3622\n",
      "Epoch 7498/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3445 - val_loss: 5.5815\n",
      "Epoch 7499/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4140 - val_loss: 5.6796\n",
      "Epoch 7500/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4412 - val_loss: 6.3420\n",
      "Epoch 7501/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5526 - val_loss: 5.4509\n",
      "Epoch 7502/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3261 - val_loss: 5.4896\n",
      "Epoch 7503/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2588 - val_loss: 5.4192\n",
      "Epoch 7504/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4232 - val_loss: 5.4016\n",
      "Epoch 7505/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3222 - val_loss: 5.4188\n",
      "Epoch 7506/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2204 - val_loss: 5.5005\n",
      "Epoch 7507/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2512 - val_loss: 5.4541\n",
      "Epoch 7508/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1712 - val_loss: 5.4478\n",
      "Epoch 7509/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1641 - val_loss: 5.4323\n",
      "Epoch 7510/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3866 - val_loss: 5.6513\n",
      "Epoch 7511/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2901 - val_loss: 5.5094\n",
      "Epoch 7512/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2146 - val_loss: 5.4597\n",
      "Epoch 7513/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2499 - val_loss: 5.5499\n",
      "Epoch 7514/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2846 - val_loss: 5.4429\n",
      "Epoch 7515/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3017 - val_loss: 5.4511\n",
      "Epoch 7516/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2822 - val_loss: 5.5536\n",
      "Epoch 7517/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3693 - val_loss: 5.3781\n",
      "Epoch 7518/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2750 - val_loss: 5.5751\n",
      "Epoch 7519/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2444 - val_loss: 5.3671\n",
      "Epoch 7520/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1805 - val_loss: 5.7215\n",
      "Epoch 7521/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6185 - val_loss: 5.3834\n",
      "Epoch 7522/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6940 - val_loss: 5.5630\n",
      "Epoch 7523/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3583 - val_loss: 5.3241\n",
      "Epoch 7524/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1826 - val_loss: 5.4211\n",
      "Epoch 7525/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2936 - val_loss: 5.4267\n",
      "Epoch 7526/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4935 - val_loss: 5.4444\n",
      "Epoch 7527/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3221 - val_loss: 5.4384\n",
      "Epoch 7528/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1651 - val_loss: 5.3578\n",
      "Epoch 7529/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.2289 - val_loss: 5.6123\n",
      "Epoch 7530/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4551 - val_loss: 5.4731\n",
      "Epoch 7531/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1772 - val_loss: 5.4916\n",
      "Epoch 7532/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3556 - val_loss: 5.3928\n",
      "Epoch 7533/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2188 - val_loss: 5.4071\n",
      "Epoch 7534/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2477 - val_loss: 5.4150\n",
      "Epoch 7535/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2832 - val_loss: 5.4214\n",
      "Epoch 7536/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2039 - val_loss: 5.3748\n",
      "Epoch 7537/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2456 - val_loss: 5.3557\n",
      "Epoch 7538/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4745 - val_loss: 5.4297\n",
      "Epoch 7539/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2692 - val_loss: 5.5287\n",
      "Epoch 7540/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2778 - val_loss: 5.4428\n",
      "Epoch 7541/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3031 - val_loss: 5.5351\n",
      "Epoch 7542/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5179 - val_loss: 5.9010\n",
      "Epoch 7543/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3770 - val_loss: 5.4020\n",
      "Epoch 7544/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3850 - val_loss: 5.7452\n",
      "Epoch 7545/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6394 - val_loss: 5.4188\n",
      "Epoch 7546/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2890 - val_loss: 5.9098\n",
      "Epoch 7547/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3496 - val_loss: 5.4075\n",
      "Epoch 7548/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2885 - val_loss: 5.4847\n",
      "Epoch 7549/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2882 - val_loss: 5.6659\n",
      "Epoch 7550/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5354 - val_loss: 5.3841\n",
      "Epoch 7551/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2045 - val_loss: 5.3919\n",
      "Epoch 7552/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1747 - val_loss: 5.3659\n",
      "Epoch 7553/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2018 - val_loss: 5.3586\n",
      "Epoch 7554/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1820 - val_loss: 5.5522\n",
      "Epoch 7555/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4120 - val_loss: 5.6272\n",
      "Epoch 7556/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3547 - val_loss: 5.4154\n",
      "Epoch 7557/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3336 - val_loss: 5.3874\n",
      "Epoch 7558/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2059 - val_loss: 5.4894\n",
      "Epoch 7559/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2993 - val_loss: 5.4704\n",
      "Epoch 7560/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2235 - val_loss: 5.4456\n",
      "Epoch 7561/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1868 - val_loss: 5.6272\n",
      "Epoch 7562/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2942 - val_loss: 5.4294\n",
      "Epoch 7563/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2415 - val_loss: 5.5785\n",
      "Epoch 7564/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5059 - val_loss: 5.3668\n",
      "Epoch 7565/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2006 - val_loss: 5.3687\n",
      "Epoch 7566/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2130 - val_loss: 5.3585\n",
      "Epoch 7567/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1980 - val_loss: 5.3704\n",
      "Epoch 7568/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2124 - val_loss: 5.4268\n",
      "Epoch 7569/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2873 - val_loss: 5.3409\n",
      "Epoch 7570/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1831 - val_loss: 5.4203\n",
      "Epoch 7571/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2277 - val_loss: 5.3644\n",
      "Epoch 7572/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1606 - val_loss: 5.4549\n",
      "Epoch 7573/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2480 - val_loss: 5.3833\n",
      "Epoch 7574/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2615 - val_loss: 5.3718\n",
      "Epoch 7575/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2959 - val_loss: 5.4521\n",
      "Epoch 7576/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1836 - val_loss: 5.4627\n",
      "Epoch 7577/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3648 - val_loss: 5.4602\n",
      "Epoch 7578/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2530 - val_loss: 5.4958\n",
      "Epoch 7579/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2584 - val_loss: 5.3740\n",
      "Epoch 7580/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1760 - val_loss: 5.6144\n",
      "Epoch 7581/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2935 - val_loss: 5.6667\n",
      "Epoch 7582/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2309 - val_loss: 5.3959\n",
      "Epoch 7583/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1522 - val_loss: 5.7541\n",
      "Epoch 7584/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3973 - val_loss: 5.4060\n",
      "Epoch 7585/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2674 - val_loss: 5.4672\n",
      "Epoch 7586/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1685 - val_loss: 5.4025\n",
      "Epoch 7587/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1946 - val_loss: 6.0886\n",
      "Epoch 7588/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2745 - val_loss: 5.4163\n",
      "Epoch 7589/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1869 - val_loss: 5.4410\n",
      "Epoch 7590/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2135 - val_loss: 5.4860\n",
      "Epoch 7591/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2412 - val_loss: 5.4041\n",
      "Epoch 7592/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1689 - val_loss: 5.3479\n",
      "Epoch 7593/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3528 - val_loss: 5.3378\n",
      "Epoch 7594/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1865 - val_loss: 5.5405\n",
      "Epoch 7595/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2940 - val_loss: 5.3905\n",
      "Epoch 7596/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2065 - val_loss: 5.3955\n",
      "Epoch 7597/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3477 - val_loss: 5.9214\n",
      "Epoch 7598/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6675 - val_loss: 5.4364\n",
      "Epoch 7599/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2527 - val_loss: 5.4054\n",
      "Epoch 7600/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2180 - val_loss: 5.3944\n",
      "Epoch 7601/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1948 - val_loss: 5.3026\n",
      "Epoch 7602/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1947 - val_loss: 5.4209\n",
      "Epoch 7603/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2183 - val_loss: 5.4630\n",
      "Epoch 7604/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1871 - val_loss: 5.3924\n",
      "Epoch 7605/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 56us/step - loss: 4.2041 - val_loss: 5.3757\n",
      "Epoch 7606/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3213 - val_loss: 5.3988\n",
      "Epoch 7607/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2514 - val_loss: 5.4227\n",
      "Epoch 7608/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1997 - val_loss: 5.4113\n",
      "Epoch 7609/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2240 - val_loss: 5.4519\n",
      "Epoch 7610/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3306 - val_loss: 5.3531\n",
      "Epoch 7611/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5675 - val_loss: 5.3810\n",
      "Epoch 7612/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4796 - val_loss: 5.6256\n",
      "Epoch 7613/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2611 - val_loss: 5.5188\n",
      "Epoch 7614/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3475 - val_loss: 5.4656\n",
      "Epoch 7615/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5001 - val_loss: 5.4674\n",
      "Epoch 7616/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5321 - val_loss: 5.6294\n",
      "Epoch 7617/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3468 - val_loss: 5.3350\n",
      "Epoch 7618/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2771 - val_loss: 5.3805\n",
      "Epoch 7619/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2944 - val_loss: 5.4893\n",
      "Epoch 7620/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2109 - val_loss: 5.4066\n",
      "Epoch 7621/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2004 - val_loss: 5.3914\n",
      "Epoch 7622/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2232 - val_loss: 6.2024\n",
      "Epoch 7623/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6504 - val_loss: 6.0645\n",
      "Epoch 7624/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3637 - val_loss: 5.4219\n",
      "Epoch 7625/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2796 - val_loss: 5.3674\n",
      "Epoch 7626/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5440 - val_loss: 5.4391\n",
      "Epoch 7627/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1907 - val_loss: 5.4686\n",
      "Epoch 7628/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4236 - val_loss: 5.3991\n",
      "Epoch 7629/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5298 - val_loss: 5.3853\n",
      "Epoch 7630/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2322 - val_loss: 5.3787\n",
      "Epoch 7631/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2875 - val_loss: 5.4036\n",
      "Epoch 7632/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3116 - val_loss: 5.4480\n",
      "Epoch 7633/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2692 - val_loss: 5.6844\n",
      "Epoch 7634/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6863 - val_loss: 5.3417\n",
      "Epoch 7635/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2094 - val_loss: 5.4137\n",
      "Epoch 7636/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1564 - val_loss: 5.4462\n",
      "Epoch 7637/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2759 - val_loss: 5.4511\n",
      "Epoch 7638/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1656 - val_loss: 5.3976\n",
      "Epoch 7639/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2050 - val_loss: 5.4010\n",
      "Epoch 7640/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2475 - val_loss: 5.4084\n",
      "Epoch 7641/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2626 - val_loss: 5.6448\n",
      "Epoch 7642/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2985 - val_loss: 5.3943\n",
      "Epoch 7643/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3160 - val_loss: 5.4899\n",
      "Epoch 7644/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2032 - val_loss: 5.3377\n",
      "Epoch 7645/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1709 - val_loss: 5.4962\n",
      "Epoch 7646/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2406 - val_loss: 5.3814\n",
      "Epoch 7647/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1520 - val_loss: 5.4225\n",
      "Epoch 7648/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1683 - val_loss: 5.4173\n",
      "Epoch 7649/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3108 - val_loss: 5.6195\n",
      "Epoch 7650/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3046 - val_loss: 5.4226\n",
      "Epoch 7651/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2551 - val_loss: 5.3755\n",
      "Epoch 7652/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2313 - val_loss: 5.3353\n",
      "Epoch 7653/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1772 - val_loss: 5.4169\n",
      "Epoch 7654/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1889 - val_loss: 5.4090\n",
      "Epoch 7655/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2295 - val_loss: 5.4030\n",
      "Epoch 7656/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1889 - val_loss: 5.4662\n",
      "Epoch 7657/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1529 - val_loss: 5.3479\n",
      "Epoch 7658/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4944 - val_loss: 5.4454\n",
      "Epoch 7659/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5087 - val_loss: 5.3190\n",
      "Epoch 7660/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3199 - val_loss: 5.4474\n",
      "Epoch 7661/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4275 - val_loss: 5.7267\n",
      "Epoch 7662/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3194 - val_loss: 5.3604\n",
      "Epoch 7663/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1700 - val_loss: 5.3995\n",
      "Epoch 7664/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2184 - val_loss: 5.4166\n",
      "Epoch 7665/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2868 - val_loss: 5.4635\n",
      "Epoch 7666/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3370 - val_loss: 5.4728\n",
      "Epoch 7667/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3315 - val_loss: 5.4223\n",
      "Epoch 7668/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2747 - val_loss: 5.5004\n",
      "Epoch 7669/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3084 - val_loss: 5.3501\n",
      "Epoch 7670/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1755 - val_loss: 5.5782\n",
      "Epoch 7671/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2731 - val_loss: 5.5389\n",
      "Epoch 7672/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2593 - val_loss: 5.3758\n",
      "Epoch 7673/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2663 - val_loss: 5.3614\n",
      "Epoch 7674/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2804 - val_loss: 5.5120\n",
      "Epoch 7675/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2197 - val_loss: 5.6851\n",
      "Epoch 7676/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4455 - val_loss: 5.4249\n",
      "Epoch 7677/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3810 - val_loss: 5.3714\n",
      "Epoch 7678/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2325 - val_loss: 5.5494\n",
      "Epoch 7679/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3238 - val_loss: 5.3863\n",
      "Epoch 7680/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1981 - val_loss: 5.6971\n",
      "Epoch 7681/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.4282 - val_loss: 5.4858\n",
      "Epoch 7682/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3254 - val_loss: 5.6906\n",
      "Epoch 7683/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2979 - val_loss: 5.3922\n",
      "Epoch 7684/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2384 - val_loss: 5.4538\n",
      "Epoch 7685/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2372 - val_loss: 5.4265\n",
      "Epoch 7686/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4841 - val_loss: 5.4130\n",
      "Epoch 7687/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4383 - val_loss: 5.5368\n",
      "Epoch 7688/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2260 - val_loss: 5.3916\n",
      "Epoch 7689/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3192 - val_loss: 5.5642\n",
      "Epoch 7690/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3750 - val_loss: 5.3428\n",
      "Epoch 7691/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2166 - val_loss: 5.3531\n",
      "Epoch 7692/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2625 - val_loss: 5.3826\n",
      "Epoch 7693/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1967 - val_loss: 5.5593\n",
      "Epoch 7694/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2092 - val_loss: 5.4181\n",
      "Epoch 7695/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2142 - val_loss: 5.3926\n",
      "Epoch 7696/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1915 - val_loss: 5.3683\n",
      "Epoch 7697/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4008 - val_loss: 5.5471\n",
      "Epoch 7698/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2009 - val_loss: 5.4959\n",
      "Epoch 7699/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2674 - val_loss: 5.5227\n",
      "Epoch 7700/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2111 - val_loss: 5.4511\n",
      "Epoch 7701/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1537 - val_loss: 5.3546\n",
      "Epoch 7702/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1766 - val_loss: 5.3811\n",
      "Epoch 7703/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1805 - val_loss: 5.4796\n",
      "Epoch 7704/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2730 - val_loss: 5.8939\n",
      "Epoch 7705/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5410 - val_loss: 5.7221\n",
      "Epoch 7706/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4257 - val_loss: 5.4010\n",
      "Epoch 7707/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1880 - val_loss: 5.3964\n",
      "Epoch 7708/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1553 - val_loss: 5.3946\n",
      "Epoch 7709/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1833 - val_loss: 5.4164\n",
      "Epoch 7710/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4490 - val_loss: 5.9123\n",
      "Epoch 7711/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2935 - val_loss: 5.4175\n",
      "Epoch 7712/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2532 - val_loss: 5.3613\n",
      "Epoch 7713/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1946 - val_loss: 5.8662\n",
      "Epoch 7714/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6386 - val_loss: 5.6562\n",
      "Epoch 7715/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2660 - val_loss: 5.5833\n",
      "Epoch 7716/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3519 - val_loss: 5.6556\n",
      "Epoch 7717/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2570 - val_loss: 5.4467\n",
      "Epoch 7718/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1753 - val_loss: 5.3258\n",
      "Epoch 7719/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1687 - val_loss: 5.3496\n",
      "Epoch 7720/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2005 - val_loss: 5.3951\n",
      "Epoch 7721/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1786 - val_loss: 5.4474\n",
      "Epoch 7722/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3024 - val_loss: 5.3872\n",
      "Epoch 7723/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1817 - val_loss: 5.3455\n",
      "Epoch 7724/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1808 - val_loss: 5.3662\n",
      "Epoch 7725/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2920 - val_loss: 5.3497\n",
      "Epoch 7726/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1956 - val_loss: 5.4188\n",
      "Epoch 7727/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1795 - val_loss: 5.3504\n",
      "Epoch 7728/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1829 - val_loss: 5.4247\n",
      "Epoch 7729/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2576 - val_loss: 5.3811\n",
      "Epoch 7730/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2219 - val_loss: 5.6937\n",
      "Epoch 7731/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3717 - val_loss: 5.3962\n",
      "Epoch 7732/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2038 - val_loss: 5.3905\n",
      "Epoch 7733/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3758 - val_loss: 5.5455\n",
      "Epoch 7734/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5539 - val_loss: 5.4508\n",
      "Epoch 7735/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3255 - val_loss: 5.4577\n",
      "Epoch 7736/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2003 - val_loss: 5.3633\n",
      "Epoch 7737/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1620 - val_loss: 5.6333\n",
      "Epoch 7738/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3124 - val_loss: 5.4616\n",
      "Epoch 7739/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2559 - val_loss: 5.6488\n",
      "Epoch 7740/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2704 - val_loss: 5.5802\n",
      "Epoch 7741/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4619 - val_loss: 5.5011\n",
      "Epoch 7742/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4047 - val_loss: 5.3430\n",
      "Epoch 7743/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3053 - val_loss: 5.4063\n",
      "Epoch 7744/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2255 - val_loss: 5.3823\n",
      "Epoch 7745/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4638 - val_loss: 5.4950\n",
      "Epoch 7746/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1843 - val_loss: 5.4143\n",
      "Epoch 7747/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1920 - val_loss: 5.3651\n",
      "Epoch 7748/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1991 - val_loss: 5.3382\n",
      "Epoch 7749/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2280 - val_loss: 5.3823\n",
      "Epoch 7750/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3819 - val_loss: 5.3601\n",
      "Epoch 7751/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1532 - val_loss: 5.4057\n",
      "Epoch 7752/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1583 - val_loss: 5.4830\n",
      "Epoch 7753/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3416 - val_loss: 5.4102\n",
      "Epoch 7754/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5933 - val_loss: 5.4555\n",
      "Epoch 7755/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1595 - val_loss: 5.3887\n",
      "Epoch 7756/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1958 - val_loss: 5.6298\n",
      "Epoch 7757/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.3444 - val_loss: 5.5651\n",
      "Epoch 7758/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3672 - val_loss: 5.4491\n",
      "Epoch 7759/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1547 - val_loss: 5.4641\n",
      "Epoch 7760/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2816 - val_loss: 5.5408\n",
      "Epoch 7761/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2316 - val_loss: 5.4011\n",
      "Epoch 7762/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1643 - val_loss: 5.3664\n",
      "Epoch 7763/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2658 - val_loss: 5.7370\n",
      "Epoch 7764/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4407 - val_loss: 5.3741\n",
      "Epoch 7765/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2149 - val_loss: 5.5926\n",
      "Epoch 7766/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5380 - val_loss: 5.3999\n",
      "Epoch 7767/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1905 - val_loss: 5.4463\n",
      "Epoch 7768/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2264 - val_loss: 5.4539\n",
      "Epoch 7769/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4066 - val_loss: 5.3374\n",
      "Epoch 7770/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1607 - val_loss: 5.3850\n",
      "Epoch 7771/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3171 - val_loss: 5.3203\n",
      "Epoch 7772/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1564 - val_loss: 5.3551\n",
      "Epoch 7773/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3188 - val_loss: 5.5086\n",
      "Epoch 7774/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2578 - val_loss: 5.5014\n",
      "Epoch 7775/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1611 - val_loss: 5.3861\n",
      "Epoch 7776/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2636 - val_loss: 5.3875\n",
      "Epoch 7777/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2107 - val_loss: 5.3511\n",
      "Epoch 7778/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3283 - val_loss: 5.4262\n",
      "Epoch 7779/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4407 - val_loss: 5.5360\n",
      "Epoch 7780/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2166 - val_loss: 5.3868\n",
      "Epoch 7781/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1724 - val_loss: 5.3889\n",
      "Epoch 7782/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1904 - val_loss: 5.6540\n",
      "Epoch 7783/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5614 - val_loss: 5.4131\n",
      "Epoch 7784/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1991 - val_loss: 5.6568\n",
      "Epoch 7785/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3544 - val_loss: 5.4165\n",
      "Epoch 7786/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2035 - val_loss: 5.3952\n",
      "Epoch 7787/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3417 - val_loss: 5.4691\n",
      "Epoch 7788/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3971 - val_loss: 5.5647\n",
      "Epoch 7789/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1920 - val_loss: 5.4976\n",
      "Epoch 7790/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2107 - val_loss: 5.4011\n",
      "Epoch 7791/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2439 - val_loss: 5.5998\n",
      "Epoch 7792/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3124 - val_loss: 5.4028\n",
      "Epoch 7793/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2671 - val_loss: 6.0185\n",
      "Epoch 7794/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6535 - val_loss: 5.3673\n",
      "Epoch 7795/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2217 - val_loss: 5.4117\n",
      "Epoch 7796/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2110 - val_loss: 5.3566\n",
      "Epoch 7797/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1864 - val_loss: 5.6536\n",
      "Epoch 7798/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4119 - val_loss: 5.4197\n",
      "Epoch 7799/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2949 - val_loss: 5.5577\n",
      "Epoch 7800/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2056 - val_loss: 5.3996\n",
      "Epoch 7801/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2654 - val_loss: 5.3462\n",
      "Epoch 7802/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3689 - val_loss: 5.6151\n",
      "Epoch 7803/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2058 - val_loss: 5.9254\n",
      "Epoch 7804/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4300 - val_loss: 5.4452\n",
      "Epoch 7805/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2072 - val_loss: 5.4457\n",
      "Epoch 7806/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2120 - val_loss: 5.8095\n",
      "Epoch 7807/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4562 - val_loss: 5.5749\n",
      "Epoch 7808/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4650 - val_loss: 5.3715\n",
      "Epoch 7809/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2089 - val_loss: 5.3659\n",
      "Epoch 7810/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1679 - val_loss: 5.3540\n",
      "Epoch 7811/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1667 - val_loss: 5.4032\n",
      "Epoch 7812/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2697 - val_loss: 5.4200\n",
      "Epoch 7813/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1778 - val_loss: 5.3801\n",
      "Epoch 7814/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2281 - val_loss: 5.3821\n",
      "Epoch 7815/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3115 - val_loss: 5.3767\n",
      "Epoch 7816/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3361 - val_loss: 5.6976\n",
      "Epoch 7817/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5070 - val_loss: 5.6506\n",
      "Epoch 7818/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4599 - val_loss: 5.8293\n",
      "Epoch 7819/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3450 - val_loss: 5.3802\n",
      "Epoch 7820/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4449 - val_loss: 5.4428\n",
      "Epoch 7821/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2715 - val_loss: 5.3679\n",
      "Epoch 7822/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1513 - val_loss: 5.4263\n",
      "Epoch 7823/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1709 - val_loss: 5.3635\n",
      "Epoch 7824/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1293 - val_loss: 5.3619\n",
      "Epoch 7825/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1495 - val_loss: 5.4099\n",
      "Epoch 7826/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2154 - val_loss: 5.5414\n",
      "Epoch 7827/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1713 - val_loss: 5.3300\n",
      "Epoch 7828/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1389 - val_loss: 5.3073\n",
      "Epoch 7829/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2067 - val_loss: 5.4422\n",
      "Epoch 7830/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4589 - val_loss: 5.5925\n",
      "Epoch 7831/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2688 - val_loss: 5.4080\n",
      "Epoch 7832/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1786 - val_loss: 5.3813\n",
      "Epoch 7833/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.1946 - val_loss: 5.4188\n",
      "Epoch 7834/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2005 - val_loss: 5.3669\n",
      "Epoch 7835/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2002 - val_loss: 5.3368\n",
      "Epoch 7836/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1827 - val_loss: 5.3853\n",
      "Epoch 7837/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2378 - val_loss: 5.4644\n",
      "Epoch 7838/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1938 - val_loss: 5.3748\n",
      "Epoch 7839/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2057 - val_loss: 5.3794\n",
      "Epoch 7840/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5229 - val_loss: 5.8994\n",
      "Epoch 7841/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2742 - val_loss: 5.3252\n",
      "Epoch 7842/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1978 - val_loss: 5.3469\n",
      "Epoch 7843/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1809 - val_loss: 5.5711\n",
      "Epoch 7844/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2068 - val_loss: 5.4125\n",
      "Epoch 7845/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1873 - val_loss: 5.4762\n",
      "Epoch 7846/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3142 - val_loss: 5.3694\n",
      "Epoch 7847/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6820 - val_loss: 5.5669\n",
      "Epoch 7848/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2083 - val_loss: 5.4614\n",
      "Epoch 7849/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2146 - val_loss: 5.6988\n",
      "Epoch 7850/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6920 - val_loss: 6.6599\n",
      "Epoch 7851/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7186 - val_loss: 5.3776\n",
      "Epoch 7852/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2969 - val_loss: 5.3891\n",
      "Epoch 7853/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2461 - val_loss: 5.3303\n",
      "Epoch 7854/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1487 - val_loss: 5.3740\n",
      "Epoch 7855/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2139 - val_loss: 5.3527\n",
      "Epoch 7856/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1395 - val_loss: 5.3801\n",
      "Epoch 7857/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2738 - val_loss: 5.4772\n",
      "Epoch 7858/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1664 - val_loss: 5.3461\n",
      "Epoch 7859/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1834 - val_loss: 5.4985\n",
      "Epoch 7860/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2453 - val_loss: 5.3851\n",
      "Epoch 7861/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1534 - val_loss: 5.4604\n",
      "Epoch 7862/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2084 - val_loss: 5.6512\n",
      "Epoch 7863/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4105 - val_loss: 5.3662\n",
      "Epoch 7864/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1966 - val_loss: 5.3596\n",
      "Epoch 7865/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1698 - val_loss: 5.3995\n",
      "Epoch 7866/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1812 - val_loss: 5.3594\n",
      "Epoch 7867/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2113 - val_loss: 5.3861\n",
      "Epoch 7868/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6120 - val_loss: 5.7416\n",
      "Epoch 7869/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5138 - val_loss: 5.3846\n",
      "Epoch 7870/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4065 - val_loss: 5.4017\n",
      "Epoch 7871/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2524 - val_loss: 5.3658\n",
      "Epoch 7872/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2177 - val_loss: 5.4424\n",
      "Epoch 7873/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5027 - val_loss: 5.4710\n",
      "Epoch 7874/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1897 - val_loss: 5.4126\n",
      "Epoch 7875/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2422 - val_loss: 5.5391\n",
      "Epoch 7876/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2659 - val_loss: 5.3800\n",
      "Epoch 7877/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1604 - val_loss: 5.7197\n",
      "Epoch 7878/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4190 - val_loss: 5.4961\n",
      "Epoch 7879/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3039 - val_loss: 5.3468\n",
      "Epoch 7880/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2044 - val_loss: 5.5358\n",
      "Epoch 7881/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2239 - val_loss: 5.6453\n",
      "Epoch 7882/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3069 - val_loss: 5.3931\n",
      "Epoch 7883/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2851 - val_loss: 5.2870\n",
      "Epoch 7884/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2323 - val_loss: 5.4238\n",
      "Epoch 7885/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2002 - val_loss: 5.3328\n",
      "Epoch 7886/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3602 - val_loss: 5.3380\n",
      "Epoch 7887/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2799 - val_loss: 5.3235\n",
      "Epoch 7888/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1324 - val_loss: 5.3871\n",
      "Epoch 7889/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1895 - val_loss: 5.3729\n",
      "Epoch 7890/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1682 - val_loss: 5.3286\n",
      "Epoch 7891/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1449 - val_loss: 5.4326\n",
      "Epoch 7892/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3303 - val_loss: 6.1925\n",
      "Epoch 7893/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4505 - val_loss: 5.3836\n",
      "Epoch 7894/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2270 - val_loss: 5.3325\n",
      "Epoch 7895/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1392 - val_loss: 5.3993\n",
      "Epoch 7896/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2544 - val_loss: 5.3277\n",
      "Epoch 7897/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1985 - val_loss: 5.3161\n",
      "Epoch 7898/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2211 - val_loss: 5.3744\n",
      "Epoch 7899/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1637 - val_loss: 5.3917\n",
      "Epoch 7900/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2991 - val_loss: 5.3171\n",
      "Epoch 7901/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3016 - val_loss: 5.5235\n",
      "Epoch 7902/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1957 - val_loss: 5.3781\n",
      "Epoch 7903/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2751 - val_loss: 5.4069\n",
      "Epoch 7904/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1241 - val_loss: 5.4627\n",
      "Epoch 7905/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2120 - val_loss: 5.4490\n",
      "Epoch 7906/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3017 - val_loss: 5.4041\n",
      "Epoch 7907/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1764 - val_loss: 5.3417\n",
      "Epoch 7908/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1841 - val_loss: 5.3795\n",
      "Epoch 7909/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.2163 - val_loss: 5.3739\n",
      "Epoch 7910/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2077 - val_loss: 5.3252\n",
      "Epoch 7911/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1979 - val_loss: 5.3288\n",
      "Epoch 7912/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1678 - val_loss: 5.3744\n",
      "Epoch 7913/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1777 - val_loss: 5.8821\n",
      "Epoch 7914/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6539 - val_loss: 5.3315\n",
      "Epoch 7915/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3067 - val_loss: 5.3894\n",
      "Epoch 7916/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2971 - val_loss: 5.4112\n",
      "Epoch 7917/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1975 - val_loss: 5.3810\n",
      "Epoch 7918/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2820 - val_loss: 5.3581\n",
      "Epoch 7919/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2154 - val_loss: 5.5376\n",
      "Epoch 7920/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3280 - val_loss: 5.3836\n",
      "Epoch 7921/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2075 - val_loss: 5.6411\n",
      "Epoch 7922/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4456 - val_loss: 5.6081\n",
      "Epoch 7923/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1972 - val_loss: 5.3515\n",
      "Epoch 7924/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1959 - val_loss: 5.3512\n",
      "Epoch 7925/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1869 - val_loss: 5.3825\n",
      "Epoch 7926/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2001 - val_loss: 5.4263\n",
      "Epoch 7927/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1944 - val_loss: 5.3736\n",
      "Epoch 7928/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2259 - val_loss: 5.4822\n",
      "Epoch 7929/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2043 - val_loss: 5.3393\n",
      "Epoch 7930/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1707 - val_loss: 5.5019\n",
      "Epoch 7931/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2713 - val_loss: 5.4226\n",
      "Epoch 7932/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1889 - val_loss: 5.3880\n",
      "Epoch 7933/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2948 - val_loss: 5.3836\n",
      "Epoch 7934/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1624 - val_loss: 5.5306\n",
      "Epoch 7935/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3640 - val_loss: 6.4581\n",
      "Epoch 7936/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7078 - val_loss: 5.4120\n",
      "Epoch 7937/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2569 - val_loss: 5.3282\n",
      "Epoch 7938/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1703 - val_loss: 5.3427\n",
      "Epoch 7939/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 4.3334 - val_loss: 5.3488\n",
      "Epoch 7940/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2308 - val_loss: 5.6055\n",
      "Epoch 7941/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3871 - val_loss: 5.4301\n",
      "Epoch 7942/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3240 - val_loss: 5.3462\n",
      "Epoch 7943/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.3831 - val_loss: 5.3967\n",
      "Epoch 7944/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1690 - val_loss: 5.3407\n",
      "Epoch 7945/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2267 - val_loss: 5.3956\n",
      "Epoch 7946/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4050 - val_loss: 5.9631\n",
      "Epoch 7947/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3487 - val_loss: 5.2877\n",
      "Epoch 7948/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3943 - val_loss: 6.0226\n",
      "Epoch 7949/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4195 - val_loss: 5.4720\n",
      "Epoch 7950/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1911 - val_loss: 5.3436\n",
      "Epoch 7951/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2693 - val_loss: 5.7650\n",
      "Epoch 7952/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3495 - val_loss: 5.4977\n",
      "Epoch 7953/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2692 - val_loss: 5.3677\n",
      "Epoch 7954/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1268 - val_loss: 6.1584\n",
      "Epoch 7955/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8641 - val_loss: 5.4078\n",
      "Epoch 7956/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4512 - val_loss: 5.4070\n",
      "Epoch 7957/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2380 - val_loss: 5.4322\n",
      "Epoch 7958/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4027 - val_loss: 5.4044\n",
      "Epoch 7959/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1758 - val_loss: 5.3244\n",
      "Epoch 7960/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2825 - val_loss: 5.3212\n",
      "Epoch 7961/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1870 - val_loss: 5.3173\n",
      "Epoch 7962/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3277 - val_loss: 5.3973\n",
      "Epoch 7963/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3121 - val_loss: 5.5397\n",
      "Epoch 7964/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2989 - val_loss: 5.4183\n",
      "Epoch 7965/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1207 - val_loss: 5.3823\n",
      "Epoch 7966/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1359 - val_loss: 5.3918\n",
      "Epoch 7967/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1640 - val_loss: 5.3598\n",
      "Epoch 7968/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1316 - val_loss: 5.3823\n",
      "Epoch 7969/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2258 - val_loss: 5.4491\n",
      "Epoch 7970/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3523 - val_loss: 5.3780\n",
      "Epoch 7971/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2727 - val_loss: 5.5436\n",
      "Epoch 7972/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1958 - val_loss: 5.3798\n",
      "Epoch 7973/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3184 - val_loss: 5.6852\n",
      "Epoch 7974/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6171 - val_loss: 5.4888\n",
      "Epoch 7975/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2495 - val_loss: 5.3864\n",
      "Epoch 7976/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2295 - val_loss: 5.4821\n",
      "Epoch 7977/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7844 - val_loss: 5.9984\n",
      "Epoch 7978/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4612 - val_loss: 5.4157\n",
      "Epoch 7979/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2312 - val_loss: 5.4615\n",
      "Epoch 7980/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4606 - val_loss: 5.4043\n",
      "Epoch 7981/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2545 - val_loss: 5.3680\n",
      "Epoch 7982/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4210 - val_loss: 5.3418\n",
      "Epoch 7983/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1942 - val_loss: 5.6327\n",
      "Epoch 7984/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3226 - val_loss: 5.3664\n",
      "Epoch 7985/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.1889 - val_loss: 5.4896\n",
      "Epoch 7986/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.4109 - val_loss: 5.4739\n",
      "Epoch 7987/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1980 - val_loss: 5.3428\n",
      "Epoch 7988/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4297 - val_loss: 5.3987\n",
      "Epoch 7989/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1433 - val_loss: 5.4360\n",
      "Epoch 7990/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4370 - val_loss: 5.3268\n",
      "Epoch 7991/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1857 - val_loss: 5.6679\n",
      "Epoch 7992/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2716 - val_loss: 5.3687\n",
      "Epoch 7993/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2155 - val_loss: 5.4318\n",
      "Epoch 7994/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2282 - val_loss: 5.3931\n",
      "Epoch 7995/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2410 - val_loss: 5.3557\n",
      "Epoch 7996/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2028 - val_loss: 5.3822\n",
      "Epoch 7997/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1664 - val_loss: 5.6619\n",
      "Epoch 7998/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3070 - val_loss: 5.3627\n",
      "Epoch 7999/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3161 - val_loss: 5.3668\n",
      "Epoch 8000/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5943 - val_loss: 5.3535\n",
      "Epoch 8001/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7297 - val_loss: 5.4832\n",
      "Epoch 8002/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2046 - val_loss: 5.7786\n",
      "Epoch 8003/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2977 - val_loss: 5.5736\n",
      "Epoch 8004/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1554 - val_loss: 5.3942\n",
      "Epoch 8005/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4243 - val_loss: 5.3757\n",
      "Epoch 8006/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2103 - val_loss: 5.4327\n",
      "Epoch 8007/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3781 - val_loss: 5.3467\n",
      "Epoch 8008/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1780 - val_loss: 5.3772\n",
      "Epoch 8009/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1661 - val_loss: 5.3537\n",
      "Epoch 8010/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1971 - val_loss: 5.5022\n",
      "Epoch 8011/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2388 - val_loss: 5.5701\n",
      "Epoch 8012/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3831 - val_loss: 5.3706\n",
      "Epoch 8013/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2053 - val_loss: 5.3902\n",
      "Epoch 8014/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3529 - val_loss: 6.2122\n",
      "Epoch 8015/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4543 - val_loss: 5.3894\n",
      "Epoch 8016/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5937 - val_loss: 5.8815\n",
      "Epoch 8017/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2619 - val_loss: 5.4692\n",
      "Epoch 8018/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2003 - val_loss: 5.3164\n",
      "Epoch 8019/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2113 - val_loss: 5.4083\n",
      "Epoch 8020/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2586 - val_loss: 5.7836\n",
      "Epoch 8021/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5108 - val_loss: 5.6340\n",
      "Epoch 8022/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5174 - val_loss: 5.5587\n",
      "Epoch 8023/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2675 - val_loss: 5.3556\n",
      "Epoch 8024/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1656 - val_loss: 5.4343\n",
      "Epoch 8025/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1931 - val_loss: 5.4410\n",
      "Epoch 8026/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2502 - val_loss: 5.3460\n",
      "Epoch 8027/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1620 - val_loss: 5.4960\n",
      "Epoch 8028/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3032 - val_loss: 5.3967\n",
      "Epoch 8029/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1355 - val_loss: 5.3651\n",
      "Epoch 8030/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1287 - val_loss: 5.3374\n",
      "Epoch 8031/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1481 - val_loss: 5.3494\n",
      "Epoch 8032/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1285 - val_loss: 5.3656\n",
      "Epoch 8033/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1674 - val_loss: 5.3655\n",
      "Epoch 8034/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1656 - val_loss: 5.4121\n",
      "Epoch 8035/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1438 - val_loss: 5.3654\n",
      "Epoch 8036/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2065 - val_loss: 5.5771\n",
      "Epoch 8037/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2348 - val_loss: 5.3874\n",
      "Epoch 8038/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3728 - val_loss: 5.5210\n",
      "Epoch 8039/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1624 - val_loss: 5.3759\n",
      "Epoch 8040/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2484 - val_loss: 5.4756\n",
      "Epoch 8041/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4815 - val_loss: 5.4102\n",
      "Epoch 8042/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1361 - val_loss: 5.4830\n",
      "Epoch 8043/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5039 - val_loss: 5.4071\n",
      "Epoch 8044/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2160 - val_loss: 5.4322\n",
      "Epoch 8045/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2526 - val_loss: 5.3648\n",
      "Epoch 8046/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2936 - val_loss: 5.5034\n",
      "Epoch 8047/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2231 - val_loss: 5.5285\n",
      "Epoch 8048/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2103 - val_loss: 5.2878\n",
      "Epoch 8049/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1545 - val_loss: 5.3998\n",
      "Epoch 8050/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2271 - val_loss: 5.3234\n",
      "Epoch 8051/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2285 - val_loss: 5.3419\n",
      "Epoch 8052/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2230 - val_loss: 5.3382\n",
      "Epoch 8053/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2711 - val_loss: 5.3302\n",
      "Epoch 8054/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2633 - val_loss: 5.3456\n",
      "Epoch 8055/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1918 - val_loss: 5.3932\n",
      "Epoch 8056/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5766 - val_loss: 5.4026\n",
      "Epoch 8057/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3054 - val_loss: 5.4024\n",
      "Epoch 8058/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1773 - val_loss: 5.3964\n",
      "Epoch 8059/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1314 - val_loss: 5.4609\n",
      "Epoch 8060/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1604 - val_loss: 5.3668\n",
      "Epoch 8061/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.2461 - val_loss: 5.7454\n",
      "Epoch 8062/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4589 - val_loss: 5.5362\n",
      "Epoch 8063/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7231 - val_loss: 5.4528\n",
      "Epoch 8064/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2998 - val_loss: 5.5739\n",
      "Epoch 8065/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2377 - val_loss: 5.4909\n",
      "Epoch 8066/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2436 - val_loss: 5.3814\n",
      "Epoch 8067/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2775 - val_loss: 5.4353\n",
      "Epoch 8068/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2331 - val_loss: 5.3882\n",
      "Epoch 8069/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1958 - val_loss: 5.6705\n",
      "Epoch 8070/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2730 - val_loss: 5.6551\n",
      "Epoch 8071/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5520 - val_loss: 5.4070\n",
      "Epoch 8072/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2550 - val_loss: 5.4036\n",
      "Epoch 8073/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2112 - val_loss: 5.4898\n",
      "Epoch 8074/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3249 - val_loss: 5.5320\n",
      "Epoch 8075/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2409 - val_loss: 5.3355\n",
      "Epoch 8076/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1538 - val_loss: 5.3941\n",
      "Epoch 8077/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2146 - val_loss: 5.3897\n",
      "Epoch 8078/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1342 - val_loss: 5.3698\n",
      "Epoch 8079/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2571 - val_loss: 5.5594\n",
      "Epoch 8080/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2579 - val_loss: 5.3483\n",
      "Epoch 8081/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2728 - val_loss: 5.3224\n",
      "Epoch 8082/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1374 - val_loss: 5.3542\n",
      "Epoch 8083/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1675 - val_loss: 5.3588\n",
      "Epoch 8084/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1899 - val_loss: 5.7349\n",
      "Epoch 8085/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2799 - val_loss: 5.3308\n",
      "Epoch 8086/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1656 - val_loss: 5.5221\n",
      "Epoch 8087/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2243 - val_loss: 5.3422\n",
      "Epoch 8088/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2053 - val_loss: 5.2714\n",
      "Epoch 8089/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1779 - val_loss: 5.6971\n",
      "Epoch 8090/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4165 - val_loss: 5.3131\n",
      "Epoch 8091/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1178 - val_loss: 5.3474\n",
      "Epoch 8092/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1352 - val_loss: 5.5285\n",
      "Epoch 8093/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1375 - val_loss: 5.3859\n",
      "Epoch 8094/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2117 - val_loss: 5.8795\n",
      "Epoch 8095/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3869 - val_loss: 5.3403\n",
      "Epoch 8096/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1185 - val_loss: 5.3910\n",
      "Epoch 8097/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1815 - val_loss: 5.3300\n",
      "Epoch 8098/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1892 - val_loss: 5.3471\n",
      "Epoch 8099/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3383 - val_loss: 6.0253\n",
      "Epoch 8100/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7972 - val_loss: 5.3801\n",
      "Epoch 8101/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3915 - val_loss: 5.3577\n",
      "Epoch 8102/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2464 - val_loss: 5.3131\n",
      "Epoch 8103/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1987 - val_loss: 5.2673\n",
      "Epoch 8104/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1586 - val_loss: 5.5319\n",
      "Epoch 8105/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2670 - val_loss: 5.2887\n",
      "Epoch 8106/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1726 - val_loss: 5.3141\n",
      "Epoch 8107/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2854 - val_loss: 5.5013\n",
      "Epoch 8108/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3322 - val_loss: 5.3042\n",
      "Epoch 8109/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1482 - val_loss: 5.3869\n",
      "Epoch 8110/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2869 - val_loss: 5.6771\n",
      "Epoch 8111/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3268 - val_loss: 5.3502\n",
      "Epoch 8112/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1490 - val_loss: 5.3521\n",
      "Epoch 8113/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1650 - val_loss: 5.3506\n",
      "Epoch 8114/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1100 - val_loss: 5.3594\n",
      "Epoch 8115/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1364 - val_loss: 5.4282\n",
      "Epoch 8116/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3514 - val_loss: 5.5635\n",
      "Epoch 8117/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4855 - val_loss: 5.4892\n",
      "Epoch 8118/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2234 - val_loss: 5.4059\n",
      "Epoch 8119/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4354 - val_loss: 5.3923\n",
      "Epoch 8120/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2625 - val_loss: 5.6093\n",
      "Epoch 8121/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6084 - val_loss: 5.4066\n",
      "Epoch 8122/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3766 - val_loss: 5.4060\n",
      "Epoch 8123/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7587 - val_loss: 5.5358\n",
      "Epoch 8124/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2156 - val_loss: 5.4439\n",
      "Epoch 8125/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1921 - val_loss: 5.4936\n",
      "Epoch 8126/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3779 - val_loss: 5.3416\n",
      "Epoch 8127/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2499 - val_loss: 5.4101\n",
      "Epoch 8128/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2092 - val_loss: 5.3565\n",
      "Epoch 8129/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1655 - val_loss: 5.3507\n",
      "Epoch 8130/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1705 - val_loss: 5.3572\n",
      "Epoch 8131/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1682 - val_loss: 5.3182\n",
      "Epoch 8132/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1913 - val_loss: 5.5391\n",
      "Epoch 8133/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1757 - val_loss: 5.3775\n",
      "Epoch 8134/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1310 - val_loss: 5.3497\n",
      "Epoch 8135/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1518 - val_loss: 5.5772\n",
      "Epoch 8136/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1586 - val_loss: 5.3614\n",
      "Epoch 8137/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.1466 - val_loss: 5.3797\n",
      "Epoch 8138/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1431 - val_loss: 5.4416\n",
      "Epoch 8139/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2315 - val_loss: 5.5081\n",
      "Epoch 8140/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2794 - val_loss: 5.4527\n",
      "Epoch 8141/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2460 - val_loss: 5.3970\n",
      "Epoch 8142/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2422 - val_loss: 5.3182\n",
      "Epoch 8143/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1454 - val_loss: 5.3886\n",
      "Epoch 8144/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2585 - val_loss: 5.9620\n",
      "Epoch 8145/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4789 - val_loss: 5.3698\n",
      "Epoch 8146/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1813 - val_loss: 5.5047\n",
      "Epoch 8147/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2038 - val_loss: 5.3356\n",
      "Epoch 8148/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1693 - val_loss: 5.3334\n",
      "Epoch 8149/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2422 - val_loss: 5.4361\n",
      "Epoch 8150/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1783 - val_loss: 5.3663\n",
      "Epoch 8151/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1467 - val_loss: 5.3044\n",
      "Epoch 8152/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1944 - val_loss: 5.3768\n",
      "Epoch 8153/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1777 - val_loss: 5.3903\n",
      "Epoch 8154/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2496 - val_loss: 6.0550\n",
      "Epoch 8155/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6543 - val_loss: 5.9630\n",
      "Epoch 8156/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4531 - val_loss: 5.3568\n",
      "Epoch 8157/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1685 - val_loss: 5.3523\n",
      "Epoch 8158/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3389 - val_loss: 5.7512\n",
      "Epoch 8159/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3700 - val_loss: 5.3201\n",
      "Epoch 8160/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1620 - val_loss: 5.3115\n",
      "Epoch 8161/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2008 - val_loss: 5.4374\n",
      "Epoch 8162/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2177 - val_loss: 5.4106\n",
      "Epoch 8163/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3212 - val_loss: 5.5465\n",
      "Epoch 8164/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2043 - val_loss: 5.5315\n",
      "Epoch 8165/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2829 - val_loss: 5.5001\n",
      "Epoch 8166/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3140 - val_loss: 5.3213\n",
      "Epoch 8167/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3582 - val_loss: 5.4519\n",
      "Epoch 8168/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1855 - val_loss: 5.3395\n",
      "Epoch 8169/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2279 - val_loss: 5.4595\n",
      "Epoch 8170/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1484 - val_loss: 5.3475\n",
      "Epoch 8171/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1561 - val_loss: 5.3492\n",
      "Epoch 8172/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1562 - val_loss: 5.3504\n",
      "Epoch 8173/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4211 - val_loss: 5.7423\n",
      "Epoch 8174/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2913 - val_loss: 5.3144\n",
      "Epoch 8175/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1179 - val_loss: 5.3901\n",
      "Epoch 8176/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1272 - val_loss: 5.3265\n",
      "Epoch 8177/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1511 - val_loss: 5.3752\n",
      "Epoch 8178/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1841 - val_loss: 5.3168\n",
      "Epoch 8179/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1357 - val_loss: 5.3364\n",
      "Epoch 8180/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1242 - val_loss: 5.3512\n",
      "Epoch 8181/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3611 - val_loss: 5.6356\n",
      "Epoch 8182/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2168 - val_loss: 5.4062\n",
      "Epoch 8183/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1691 - val_loss: 5.3201\n",
      "Epoch 8184/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1924 - val_loss: 5.3860\n",
      "Epoch 8185/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2998 - val_loss: 5.3365\n",
      "Epoch 8186/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1795 - val_loss: 5.3910\n",
      "Epoch 8187/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1492 - val_loss: 5.3447\n",
      "Epoch 8188/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2344 - val_loss: 5.4127\n",
      "Epoch 8189/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2232 - val_loss: 5.4736\n",
      "Epoch 8190/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2839 - val_loss: 5.4186\n",
      "Epoch 8191/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2756 - val_loss: 5.3731\n",
      "Epoch 8192/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1551 - val_loss: 5.3392\n",
      "Epoch 8193/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1623 - val_loss: 5.5797\n",
      "Epoch 8194/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2448 - val_loss: 5.3076\n",
      "Epoch 8195/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3002 - val_loss: 5.3504\n",
      "Epoch 8196/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1512 - val_loss: 5.3083\n",
      "Epoch 8197/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2920 - val_loss: 5.3310\n",
      "Epoch 8198/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2939 - val_loss: 5.3517\n",
      "Epoch 8199/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5866 - val_loss: 5.2779\n",
      "Epoch 8200/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2191 - val_loss: 5.3783\n",
      "Epoch 8201/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3031 - val_loss: 5.3965\n",
      "Epoch 8202/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2423 - val_loss: 6.1539\n",
      "Epoch 8203/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5067 - val_loss: 5.3216\n",
      "Epoch 8204/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3712 - val_loss: 5.3077\n",
      "Epoch 8205/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2329 - val_loss: 5.4889\n",
      "Epoch 8206/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2196 - val_loss: 5.3928\n",
      "Epoch 8207/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1493 - val_loss: 5.4425\n",
      "Epoch 8208/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3244 - val_loss: 5.4716\n",
      "Epoch 8209/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3053 - val_loss: 5.3533\n",
      "Epoch 8210/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1678 - val_loss: 5.3364\n",
      "Epoch 8211/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2081 - val_loss: 5.4106\n",
      "Epoch 8212/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2839 - val_loss: 5.7514\n",
      "Epoch 8213/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.3875 - val_loss: 5.4923\n",
      "Epoch 8214/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1723 - val_loss: 5.3579\n",
      "Epoch 8215/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2851 - val_loss: 5.4169\n",
      "Epoch 8216/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2175 - val_loss: 5.4339\n",
      "Epoch 8217/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3356 - val_loss: 5.3869\n",
      "Epoch 8218/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3142 - val_loss: 5.3492\n",
      "Epoch 8219/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1292 - val_loss: 5.3532\n",
      "Epoch 8220/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1644 - val_loss: 5.3780\n",
      "Epoch 8221/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2019 - val_loss: 6.0146\n",
      "Epoch 8222/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5182 - val_loss: 5.5217\n",
      "Epoch 8223/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1645 - val_loss: 5.4018\n",
      "Epoch 8224/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4109 - val_loss: 5.3529\n",
      "Epoch 8225/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1193 - val_loss: 5.3243\n",
      "Epoch 8226/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1319 - val_loss: 5.3466\n",
      "Epoch 8227/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2121 - val_loss: 5.5205\n",
      "Epoch 8228/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2028 - val_loss: 5.2987\n",
      "Epoch 8229/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1452 - val_loss: 5.3315\n",
      "Epoch 8230/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1902 - val_loss: 5.3792\n",
      "Epoch 8231/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1992 - val_loss: 5.4164\n",
      "Epoch 8232/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2609 - val_loss: 5.4568\n",
      "Epoch 8233/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2815 - val_loss: 5.5692\n",
      "Epoch 8234/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3079 - val_loss: 5.4013\n",
      "Epoch 8235/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2486 - val_loss: 5.3614\n",
      "Epoch 8236/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1591 - val_loss: 5.4185\n",
      "Epoch 8237/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1551 - val_loss: 5.4515\n",
      "Epoch 8238/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2861 - val_loss: 5.6849\n",
      "Epoch 8239/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4259 - val_loss: 5.6513\n",
      "Epoch 8240/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7443 - val_loss: 5.6547\n",
      "Epoch 8241/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2971 - val_loss: 5.3062\n",
      "Epoch 8242/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2340 - val_loss: 5.3131\n",
      "Epoch 8243/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2453 - val_loss: 5.4843\n",
      "Epoch 8244/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1505 - val_loss: 5.3355\n",
      "Epoch 8245/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1790 - val_loss: 5.4242\n",
      "Epoch 8246/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2280 - val_loss: 5.6104\n",
      "Epoch 8247/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2745 - val_loss: 5.4003\n",
      "Epoch 8248/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3552 - val_loss: 5.3224\n",
      "Epoch 8249/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2332 - val_loss: 5.6785\n",
      "Epoch 8250/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4843 - val_loss: 5.2775\n",
      "Epoch 8251/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1745 - val_loss: 5.2891\n",
      "Epoch 8252/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1343 - val_loss: 5.3053\n",
      "Epoch 8253/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1233 - val_loss: 5.3590\n",
      "Epoch 8254/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2433 - val_loss: 5.4212\n",
      "Epoch 8255/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1238 - val_loss: 5.4056\n",
      "Epoch 8256/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2982 - val_loss: 5.7012\n",
      "Epoch 8257/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1679 - val_loss: 5.3220\n",
      "Epoch 8258/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1912 - val_loss: 5.4437\n",
      "Epoch 8259/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1846 - val_loss: 5.4034\n",
      "Epoch 8260/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1524 - val_loss: 5.4092\n",
      "Epoch 8261/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3317 - val_loss: 5.7349\n",
      "Epoch 8262/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.7504 - val_loss: 5.4592\n",
      "Epoch 8263/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3592 - val_loss: 5.4094\n",
      "Epoch 8264/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2259 - val_loss: 5.2513\n",
      "Epoch 8265/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2268 - val_loss: 5.3461\n",
      "Epoch 8266/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3939 - val_loss: 5.3803\n",
      "Epoch 8267/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2317 - val_loss: 5.3063\n",
      "Epoch 8268/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1218 - val_loss: 5.3540\n",
      "Epoch 8269/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1939 - val_loss: 5.5493\n",
      "Epoch 8270/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2661 - val_loss: 5.3314\n",
      "Epoch 8271/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1638 - val_loss: 5.3093\n",
      "Epoch 8272/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1062 - val_loss: 5.3453\n",
      "Epoch 8273/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1523 - val_loss: 5.3415\n",
      "Epoch 8274/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2120 - val_loss: 5.4377\n",
      "Epoch 8275/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1732 - val_loss: 5.4285\n",
      "Epoch 8276/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1952 - val_loss: 5.4489\n",
      "Epoch 8277/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2208 - val_loss: 5.3213\n",
      "Epoch 8278/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2014 - val_loss: 5.3362\n",
      "Epoch 8279/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1668 - val_loss: 5.3045\n",
      "Epoch 8280/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1424 - val_loss: 5.7000\n",
      "Epoch 8281/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5836 - val_loss: 5.3815\n",
      "Epoch 8282/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3943 - val_loss: 5.3825\n",
      "Epoch 8283/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1483 - val_loss: 5.3370\n",
      "Epoch 8284/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1476 - val_loss: 5.3401\n",
      "Epoch 8285/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2133 - val_loss: 5.3817\n",
      "Epoch 8286/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1471 - val_loss: 5.3786\n",
      "Epoch 8287/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1454 - val_loss: 5.4548\n",
      "Epoch 8288/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2282 - val_loss: 5.7052\n",
      "Epoch 8289/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.3197 - val_loss: 5.4276\n",
      "Epoch 8290/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2871 - val_loss: 5.3883\n",
      "Epoch 8291/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1894 - val_loss: 5.3627\n",
      "Epoch 8292/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2763 - val_loss: 5.3724\n",
      "Epoch 8293/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2768 - val_loss: 5.3080\n",
      "Epoch 8294/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1015 - val_loss: 5.5778\n",
      "Epoch 8295/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4100 - val_loss: 5.3717\n",
      "Epoch 8296/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1766 - val_loss: 5.3751\n",
      "Epoch 8297/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1634 - val_loss: 5.4320\n",
      "Epoch 8298/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2183 - val_loss: 5.5158\n",
      "Epoch 8299/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3699 - val_loss: 5.3539\n",
      "Epoch 8300/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4488 - val_loss: 5.2936\n",
      "Epoch 8301/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1229 - val_loss: 5.3679\n",
      "Epoch 8302/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2181 - val_loss: 5.3505\n",
      "Epoch 8303/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2068 - val_loss: 5.4669\n",
      "Epoch 8304/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2866 - val_loss: 5.3345\n",
      "Epoch 8305/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2590 - val_loss: 5.4287\n",
      "Epoch 8306/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1722 - val_loss: 5.3274\n",
      "Epoch 8307/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1720 - val_loss: 5.4079\n",
      "Epoch 8308/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1741 - val_loss: 5.4459\n",
      "Epoch 8309/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1373 - val_loss: 5.3473\n",
      "Epoch 8310/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1539 - val_loss: 5.3619\n",
      "Epoch 8311/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2967 - val_loss: 5.3157\n",
      "Epoch 8312/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1060 - val_loss: 5.2950\n",
      "Epoch 8313/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1309 - val_loss: 5.4494\n",
      "Epoch 8314/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2107 - val_loss: 5.4711\n",
      "Epoch 8315/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3526 - val_loss: 5.4065\n",
      "Epoch 8316/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1934 - val_loss: 5.4127\n",
      "Epoch 8317/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1553 - val_loss: 5.3721\n",
      "Epoch 8318/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1711 - val_loss: 5.4073\n",
      "Epoch 8319/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1063 - val_loss: 5.3309\n",
      "Epoch 8320/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1788 - val_loss: 5.4438\n",
      "Epoch 8321/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1821 - val_loss: 5.3668\n",
      "Epoch 8322/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2483 - val_loss: 5.6073\n",
      "Epoch 8323/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2245 - val_loss: 5.7605\n",
      "Epoch 8324/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3338 - val_loss: 5.4355\n",
      "Epoch 8325/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2643 - val_loss: 5.4073\n",
      "Epoch 8326/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4153 - val_loss: 5.5762\n",
      "Epoch 8327/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2887 - val_loss: 5.3950\n",
      "Epoch 8328/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1928 - val_loss: 5.2901\n",
      "Epoch 8329/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4999 - val_loss: 5.3444\n",
      "Epoch 8330/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1439 - val_loss: 5.3497\n",
      "Epoch 8331/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1976 - val_loss: 5.3860\n",
      "Epoch 8332/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2130 - val_loss: 5.7504\n",
      "Epoch 8333/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2600 - val_loss: 5.3990\n",
      "Epoch 8334/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2108 - val_loss: 6.1772\n",
      "Epoch 8335/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.8850 - val_loss: 6.4141\n",
      "Epoch 8336/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6094 - val_loss: 6.0517\n",
      "Epoch 8337/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6694 - val_loss: 5.4122\n",
      "Epoch 8338/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1320 - val_loss: 5.5187\n",
      "Epoch 8339/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2770 - val_loss: 5.4487\n",
      "Epoch 8340/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1531 - val_loss: 5.5259\n",
      "Epoch 8341/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3379 - val_loss: 5.4217\n",
      "Epoch 8342/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1690 - val_loss: 5.4176\n",
      "Epoch 8343/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1155 - val_loss: 5.3798\n",
      "Epoch 8344/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2584 - val_loss: 5.3833\n",
      "Epoch 8345/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2667 - val_loss: 5.4152\n",
      "Epoch 8346/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2799 - val_loss: 5.5490\n",
      "Epoch 8347/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2965 - val_loss: 5.3785\n",
      "Epoch 8348/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1251 - val_loss: 5.4138\n",
      "Epoch 8349/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1062 - val_loss: 5.4301\n",
      "Epoch 8350/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2443 - val_loss: 5.3657\n",
      "Epoch 8351/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0840 - val_loss: 5.3259\n",
      "Epoch 8352/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1056 - val_loss: 5.3313\n",
      "Epoch 8353/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2536 - val_loss: 5.3410\n",
      "Epoch 8354/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1523 - val_loss: 5.4001\n",
      "Epoch 8355/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1009 - val_loss: 5.5488\n",
      "Epoch 8356/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1383 - val_loss: 5.3786\n",
      "Epoch 8357/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1408 - val_loss: 5.3640\n",
      "Epoch 8358/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1876 - val_loss: 5.3543\n",
      "Epoch 8359/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2098 - val_loss: 5.6982\n",
      "Epoch 8360/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3814 - val_loss: 5.5541\n",
      "Epoch 8361/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2688 - val_loss: 5.3920\n",
      "Epoch 8362/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3866 - val_loss: 5.7031\n",
      "Epoch 8363/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4227 - val_loss: 5.4679\n",
      "Epoch 8364/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2167 - val_loss: 5.8944\n",
      "Epoch 8365/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 4.4450 - val_loss: 5.4010\n",
      "Epoch 8366/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2765 - val_loss: 5.3314\n",
      "Epoch 8367/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1387 - val_loss: 5.2959\n",
      "Epoch 8368/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1292 - val_loss: 5.3835\n",
      "Epoch 8369/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1029 - val_loss: 5.3221\n",
      "Epoch 8370/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1286 - val_loss: 5.3225\n",
      "Epoch 8371/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2871 - val_loss: 5.3110\n",
      "Epoch 8372/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1759 - val_loss: 5.3520\n",
      "Epoch 8373/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2802 - val_loss: 6.0089\n",
      "Epoch 8374/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5080 - val_loss: 5.3711\n",
      "Epoch 8375/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3660 - val_loss: 5.2271\n",
      "Epoch 8376/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2723 - val_loss: 5.3616\n",
      "Epoch 8377/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2712 - val_loss: 5.3314\n",
      "Epoch 8378/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1968 - val_loss: 5.3300\n",
      "Epoch 8379/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2022 - val_loss: 5.3627\n",
      "Epoch 8380/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2558 - val_loss: 5.3314\n",
      "Epoch 8381/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1219 - val_loss: 5.3178\n",
      "Epoch 8382/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1010 - val_loss: 5.3148\n",
      "Epoch 8383/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1773 - val_loss: 5.3578\n",
      "Epoch 8384/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2456 - val_loss: 5.4220\n",
      "Epoch 8385/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1582 - val_loss: 5.3503\n",
      "Epoch 8386/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1446 - val_loss: 5.3432\n",
      "Epoch 8387/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3033 - val_loss: 5.2947\n",
      "Epoch 8388/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2014 - val_loss: 5.3986\n",
      "Epoch 8389/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1327 - val_loss: 5.4560\n",
      "Epoch 8390/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2348 - val_loss: 5.3480\n",
      "Epoch 8391/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1250 - val_loss: 5.5389\n",
      "Epoch 8392/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2137 - val_loss: 5.2970\n",
      "Epoch 8393/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1684 - val_loss: 5.4028\n",
      "Epoch 8394/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1615 - val_loss: 5.3627\n",
      "Epoch 8395/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1268 - val_loss: 5.3565\n",
      "Epoch 8396/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3738 - val_loss: 5.2893\n",
      "Epoch 8397/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1832 - val_loss: 5.3704\n",
      "Epoch 8398/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1265 - val_loss: 5.3265\n",
      "Epoch 8399/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2483 - val_loss: 5.4428\n",
      "Epoch 8400/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1675 - val_loss: 5.3587\n",
      "Epoch 8401/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1519 - val_loss: 5.4013\n",
      "Epoch 8402/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2125 - val_loss: 5.3594\n",
      "Epoch 8403/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2723 - val_loss: 5.3463\n",
      "Epoch 8404/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2174 - val_loss: 5.6921\n",
      "Epoch 8405/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4873 - val_loss: 5.3835\n",
      "Epoch 8406/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3020 - val_loss: 5.5060\n",
      "Epoch 8407/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4168 - val_loss: 5.3915\n",
      "Epoch 8408/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1002 - val_loss: 5.4472\n",
      "Epoch 8409/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3774 - val_loss: 5.3303\n",
      "Epoch 8410/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1516 - val_loss: 5.3226\n",
      "Epoch 8411/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2345 - val_loss: 5.5684\n",
      "Epoch 8412/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2117 - val_loss: 5.3493\n",
      "Epoch 8413/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1884 - val_loss: 5.3817\n",
      "Epoch 8414/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1087 - val_loss: 5.3580\n",
      "Epoch 8415/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1445 - val_loss: 5.4431\n",
      "Epoch 8416/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2185 - val_loss: 5.3266\n",
      "Epoch 8417/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2270 - val_loss: 5.3236\n",
      "Epoch 8418/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1430 - val_loss: 5.3501\n",
      "Epoch 8419/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1462 - val_loss: 5.4108\n",
      "Epoch 8420/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1391 - val_loss: 5.4035\n",
      "Epoch 8421/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2223 - val_loss: 5.6148\n",
      "Epoch 8422/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1578 - val_loss: 5.3536\n",
      "Epoch 8423/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1211 - val_loss: 5.4041\n",
      "Epoch 8424/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1712 - val_loss: 5.3128\n",
      "Epoch 8425/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1488 - val_loss: 5.3440\n",
      "Epoch 8426/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1989 - val_loss: 5.4581\n",
      "Epoch 8427/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2226 - val_loss: 5.3816\n",
      "Epoch 8428/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2228 - val_loss: 5.3466\n",
      "Epoch 8429/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2733 - val_loss: 5.5312\n",
      "Epoch 8430/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2258 - val_loss: 5.3120\n",
      "Epoch 8431/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1202 - val_loss: 5.3392\n",
      "Epoch 8432/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1274 - val_loss: 5.3417\n",
      "Epoch 8433/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1216 - val_loss: 5.3714\n",
      "Epoch 8434/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1714 - val_loss: 5.3238\n",
      "Epoch 8435/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1952 - val_loss: 5.6298\n",
      "Epoch 8436/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2880 - val_loss: 5.3099\n",
      "Epoch 8437/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1270 - val_loss: 5.7214\n",
      "Epoch 8438/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4140 - val_loss: 5.6163\n",
      "Epoch 8439/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1878 - val_loss: 5.3976\n",
      "Epoch 8440/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2111 - val_loss: 5.4743\n",
      "Epoch 8441/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.1423 - val_loss: 5.3076\n",
      "Epoch 8442/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1231 - val_loss: 5.3610\n",
      "Epoch 8443/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3296 - val_loss: 5.5971\n",
      "Epoch 8444/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1787 - val_loss: 5.3211\n",
      "Epoch 8445/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1376 - val_loss: 5.3427\n",
      "Epoch 8446/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1680 - val_loss: 5.4107\n",
      "Epoch 8447/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1886 - val_loss: 5.3752\n",
      "Epoch 8448/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2517 - val_loss: 5.3363\n",
      "Epoch 8449/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1132 - val_loss: 5.3771\n",
      "Epoch 8450/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1370 - val_loss: 5.3214\n",
      "Epoch 8451/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2190 - val_loss: 5.3916\n",
      "Epoch 8452/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1772 - val_loss: 5.3876\n",
      "Epoch 8453/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1960 - val_loss: 5.3099\n",
      "Epoch 8454/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1763 - val_loss: 5.3321\n",
      "Epoch 8455/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1429 - val_loss: 5.2785\n",
      "Epoch 8456/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1215 - val_loss: 5.2766\n",
      "Epoch 8457/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1310 - val_loss: 5.3725\n",
      "Epoch 8458/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5310 - val_loss: 5.3827\n",
      "Epoch 8459/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1676 - val_loss: 5.4586\n",
      "Epoch 8460/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3490 - val_loss: 5.3301\n",
      "Epoch 8461/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1234 - val_loss: 5.4017\n",
      "Epoch 8462/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1240 - val_loss: 5.2839\n",
      "Epoch 8463/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0973 - val_loss: 5.3274\n",
      "Epoch 8464/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1532 - val_loss: 5.4196\n",
      "Epoch 8465/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1647 - val_loss: 5.3400\n",
      "Epoch 8466/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1038 - val_loss: 5.3778\n",
      "Epoch 8467/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2755 - val_loss: 5.5343\n",
      "Epoch 8468/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3112 - val_loss: 5.3090\n",
      "Epoch 8469/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2461 - val_loss: 5.3462\n",
      "Epoch 8470/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2964 - val_loss: 5.4336\n",
      "Epoch 8471/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1190 - val_loss: 5.4725\n",
      "Epoch 8472/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2471 - val_loss: 5.3563\n",
      "Epoch 8473/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1295 - val_loss: 5.2933\n",
      "Epoch 8474/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1432 - val_loss: 5.3367\n",
      "Epoch 8475/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0973 - val_loss: 5.3856\n",
      "Epoch 8476/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1460 - val_loss: 5.3704\n",
      "Epoch 8477/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1829 - val_loss: 5.3046\n",
      "Epoch 8478/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1329 - val_loss: 5.5664\n",
      "Epoch 8479/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2975 - val_loss: 5.3898\n",
      "Epoch 8480/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1881 - val_loss: 5.3759\n",
      "Epoch 8481/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1128 - val_loss: 5.4477\n",
      "Epoch 8482/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2242 - val_loss: 5.3389\n",
      "Epoch 8483/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1157 - val_loss: 5.6177\n",
      "Epoch 8484/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4002 - val_loss: 5.4561\n",
      "Epoch 8485/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1733 - val_loss: 5.4027\n",
      "Epoch 8486/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2240 - val_loss: 5.5567\n",
      "Epoch 8487/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5322 - val_loss: 5.7658\n",
      "Epoch 8488/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2766 - val_loss: 5.3498\n",
      "Epoch 8489/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1490 - val_loss: 5.3149\n",
      "Epoch 8490/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0973 - val_loss: 5.5794\n",
      "Epoch 8491/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4341 - val_loss: 5.3552\n",
      "Epoch 8492/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4149 - val_loss: 6.2305\n",
      "Epoch 8493/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3800 - val_loss: 5.3488\n",
      "Epoch 8494/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1051 - val_loss: 5.3284\n",
      "Epoch 8495/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2501 - val_loss: 5.3881\n",
      "Epoch 8496/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2343 - val_loss: 5.4352\n",
      "Epoch 8497/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1597 - val_loss: 5.3773\n",
      "Epoch 8498/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1840 - val_loss: 5.3126\n",
      "Epoch 8499/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1299 - val_loss: 5.3626\n",
      "Epoch 8500/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1823 - val_loss: 5.5685\n",
      "Epoch 8501/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2067 - val_loss: 5.3025\n",
      "Epoch 8502/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1289 - val_loss: 5.3132\n",
      "Epoch 8503/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2131 - val_loss: 5.3451\n",
      "Epoch 8504/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2246 - val_loss: 5.5209\n",
      "Epoch 8505/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1247 - val_loss: 5.4348\n",
      "Epoch 8506/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2196 - val_loss: 5.2837\n",
      "Epoch 8507/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1996 - val_loss: 5.2929\n",
      "Epoch 8508/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1094 - val_loss: 5.3539\n",
      "Epoch 8509/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1829 - val_loss: 5.3343\n",
      "Epoch 8510/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1711 - val_loss: 5.5067\n",
      "Epoch 8511/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3140 - val_loss: 5.5842\n",
      "Epoch 8512/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1423 - val_loss: 5.3748\n",
      "Epoch 8513/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1543 - val_loss: 5.3374\n",
      "Epoch 8514/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3721 - val_loss: 5.3807\n",
      "Epoch 8515/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3686 - val_loss: 5.4460\n",
      "Epoch 8516/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5584 - val_loss: 5.7422\n",
      "Epoch 8517/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.2329 - val_loss: 5.2783\n",
      "Epoch 8518/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1709 - val_loss: 5.3382\n",
      "Epoch 8519/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1537 - val_loss: 5.4114\n",
      "Epoch 8520/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1520 - val_loss: 5.3150\n",
      "Epoch 8521/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1318 - val_loss: 5.3398\n",
      "Epoch 8522/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1201 - val_loss: 5.3371\n",
      "Epoch 8523/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1708 - val_loss: 5.3254\n",
      "Epoch 8524/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1482 - val_loss: 5.7770\n",
      "Epoch 8525/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6470 - val_loss: 5.6394\n",
      "Epoch 8526/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2052 - val_loss: 5.2776\n",
      "Epoch 8527/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1653 - val_loss: 5.2747\n",
      "Epoch 8528/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1344 - val_loss: 5.3854\n",
      "Epoch 8529/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2241 - val_loss: 5.3154\n",
      "Epoch 8530/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1289 - val_loss: 5.3202\n",
      "Epoch 8531/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1530 - val_loss: 5.4919\n",
      "Epoch 8532/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1547 - val_loss: 5.4251\n",
      "Epoch 8533/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2469 - val_loss: 5.3845\n",
      "Epoch 8534/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1257 - val_loss: 5.5023\n",
      "Epoch 8535/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2004 - val_loss: 5.3046\n",
      "Epoch 8536/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1275 - val_loss: 5.2733\n",
      "Epoch 8537/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1425 - val_loss: 5.4016\n",
      "Epoch 8538/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1742 - val_loss: 5.2953\n",
      "Epoch 8539/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1053 - val_loss: 5.3418\n",
      "Epoch 8540/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2540 - val_loss: 5.3108\n",
      "Epoch 8541/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1282 - val_loss: 5.2928\n",
      "Epoch 8542/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1467 - val_loss: 5.3174\n",
      "Epoch 8543/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2644 - val_loss: 5.3140\n",
      "Epoch 8544/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1165 - val_loss: 5.4071\n",
      "Epoch 8545/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1332 - val_loss: 5.4495\n",
      "Epoch 8546/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2063 - val_loss: 5.6165\n",
      "Epoch 8547/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3125 - val_loss: 5.4134\n",
      "Epoch 8548/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2260 - val_loss: 5.5104\n",
      "Epoch 8549/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3088 - val_loss: 5.3641\n",
      "Epoch 8550/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2409 - val_loss: 5.4011\n",
      "Epoch 8551/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2728 - val_loss: 5.4400\n",
      "Epoch 8552/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2166 - val_loss: 5.3731\n",
      "Epoch 8553/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1449 - val_loss: 5.4119\n",
      "Epoch 8554/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0933 - val_loss: 5.3506\n",
      "Epoch 8555/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1254 - val_loss: 5.4565\n",
      "Epoch 8556/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1748 - val_loss: 5.7005\n",
      "Epoch 8557/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3646 - val_loss: 5.3913\n",
      "Epoch 8558/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1453 - val_loss: 5.4155\n",
      "Epoch 8559/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1139 - val_loss: 5.3524\n",
      "Epoch 8560/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1341 - val_loss: 5.4671\n",
      "Epoch 8561/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2632 - val_loss: 5.4613\n",
      "Epoch 8562/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1068 - val_loss: 5.3305\n",
      "Epoch 8563/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4338 - val_loss: 5.3220\n",
      "Epoch 8564/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1868 - val_loss: 5.3027\n",
      "Epoch 8565/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1081 - val_loss: 5.3574\n",
      "Epoch 8566/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2168 - val_loss: 5.4561\n",
      "Epoch 8567/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2783 - val_loss: 5.4937\n",
      "Epoch 8568/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1351 - val_loss: 5.3486\n",
      "Epoch 8569/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1301 - val_loss: 5.5960\n",
      "Epoch 8570/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.6365 - val_loss: 5.2868\n",
      "Epoch 8571/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2539 - val_loss: 5.3373\n",
      "Epoch 8572/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2331 - val_loss: 5.6125\n",
      "Epoch 8573/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3242 - val_loss: 5.7215\n",
      "Epoch 8574/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7078 - val_loss: 5.4290\n",
      "Epoch 8575/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2003 - val_loss: 5.5835\n",
      "Epoch 8576/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6071 - val_loss: 5.8636\n",
      "Epoch 8577/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4900 - val_loss: 5.3594\n",
      "Epoch 8578/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1467 - val_loss: 5.4836\n",
      "Epoch 8579/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1118 - val_loss: 5.4901\n",
      "Epoch 8580/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2831 - val_loss: 5.3642\n",
      "Epoch 8581/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1548 - val_loss: 5.2927\n",
      "Epoch 8582/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1157 - val_loss: 5.3372\n",
      "Epoch 8583/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1173 - val_loss: 5.3420\n",
      "Epoch 8584/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1229 - val_loss: 5.3558\n",
      "Epoch 8585/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1386 - val_loss: 5.3833\n",
      "Epoch 8586/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1672 - val_loss: 5.3938\n",
      "Epoch 8587/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1321 - val_loss: 5.3885\n",
      "Epoch 8588/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2028 - val_loss: 5.3625\n",
      "Epoch 8589/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1507 - val_loss: 5.3776\n",
      "Epoch 8590/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1248 - val_loss: 5.3881\n",
      "Epoch 8591/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2742 - val_loss: 5.3070\n",
      "Epoch 8592/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1359 - val_loss: 5.3656\n",
      "Epoch 8593/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 58us/step - loss: 4.0698 - val_loss: 5.3564\n",
      "Epoch 8594/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1411 - val_loss: 5.4837\n",
      "Epoch 8595/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2277 - val_loss: 5.3214\n",
      "Epoch 8596/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1005 - val_loss: 5.3189\n",
      "Epoch 8597/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1026 - val_loss: 5.3898\n",
      "Epoch 8598/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1548 - val_loss: 5.3622\n",
      "Epoch 8599/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0909 - val_loss: 5.3495\n",
      "Epoch 8600/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2800 - val_loss: 5.3522\n",
      "Epoch 8601/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3437 - val_loss: 5.3650\n",
      "Epoch 8602/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2199 - val_loss: 5.7579\n",
      "Epoch 8603/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3260 - val_loss: 5.4255\n",
      "Epoch 8604/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1853 - val_loss: 5.3479\n",
      "Epoch 8605/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2338 - val_loss: 5.5050\n",
      "Epoch 8606/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1953 - val_loss: 5.5319\n",
      "Epoch 8607/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2373 - val_loss: 5.4278\n",
      "Epoch 8608/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1491 - val_loss: 5.5417\n",
      "Epoch 8609/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2672 - val_loss: 5.3127\n",
      "Epoch 8610/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2705 - val_loss: 5.5397\n",
      "Epoch 8611/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1577 - val_loss: 5.2661\n",
      "Epoch 8612/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0875 - val_loss: 5.3087\n",
      "Epoch 8613/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1400 - val_loss: 5.3854\n",
      "Epoch 8614/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2226 - val_loss: 5.7437\n",
      "Epoch 8615/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1950 - val_loss: 5.3320\n",
      "Epoch 8616/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0988 - val_loss: 5.5324\n",
      "Epoch 8617/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1709 - val_loss: 5.3204\n",
      "Epoch 8618/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1825 - val_loss: 5.3091\n",
      "Epoch 8619/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.2585 - val_loss: 5.4288\n",
      "Epoch 8620/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2923 - val_loss: 5.2950\n",
      "Epoch 8621/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1727 - val_loss: 6.0436\n",
      "Epoch 8622/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.8290 - val_loss: 6.3109\n",
      "Epoch 8623/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.5162 - val_loss: 5.8155\n",
      "Epoch 8624/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2990 - val_loss: 5.6636\n",
      "Epoch 8625/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.4300 - val_loss: 5.3058\n",
      "Epoch 8626/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.1002 - val_loss: 5.3399\n",
      "Epoch 8627/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2795 - val_loss: 5.4864\n",
      "Epoch 8628/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3752 - val_loss: 5.3383\n",
      "Epoch 8629/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0993 - val_loss: 5.2891\n",
      "Epoch 8630/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1460 - val_loss: 5.3509\n",
      "Epoch 8631/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2555 - val_loss: 5.4456\n",
      "Epoch 8632/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1087 - val_loss: 5.3170\n",
      "Epoch 8633/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2384 - val_loss: 5.3641\n",
      "Epoch 8634/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2407 - val_loss: 5.6601\n",
      "Epoch 8635/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2598 - val_loss: 5.6896\n",
      "Epoch 8636/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4652 - val_loss: 5.3768\n",
      "Epoch 8637/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2191 - val_loss: 5.4124\n",
      "Epoch 8638/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1165 - val_loss: 5.3365\n",
      "Epoch 8639/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1352 - val_loss: 5.3992\n",
      "Epoch 8640/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3703 - val_loss: 5.9268\n",
      "Epoch 8641/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3931 - val_loss: 5.3877\n",
      "Epoch 8642/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0844 - val_loss: 5.4469\n",
      "Epoch 8643/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4251 - val_loss: 5.3857\n",
      "Epoch 8644/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1030 - val_loss: 5.3165\n",
      "Epoch 8645/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0834 - val_loss: 5.3922\n",
      "Epoch 8646/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1405 - val_loss: 5.3203\n",
      "Epoch 8647/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1290 - val_loss: 5.4141\n",
      "Epoch 8648/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1387 - val_loss: 5.3561\n",
      "Epoch 8649/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1933 - val_loss: 5.4747\n",
      "Epoch 8650/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2511 - val_loss: 5.2996\n",
      "Epoch 8651/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1958 - val_loss: 5.2351\n",
      "Epoch 8652/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1130 - val_loss: 5.2751\n",
      "Epoch 8653/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.1040 - val_loss: 5.2763\n",
      "Epoch 8654/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1645 - val_loss: 5.2508\n",
      "Epoch 8655/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.1221 - val_loss: 5.3727\n",
      "Epoch 8656/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1543 - val_loss: 5.3591\n",
      "Epoch 8657/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3099 - val_loss: 5.6202\n",
      "Epoch 8658/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1581 - val_loss: 5.3538\n",
      "Epoch 8659/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1330 - val_loss: 5.4963\n",
      "Epoch 8660/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2162 - val_loss: 5.3256\n",
      "Epoch 8661/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1568 - val_loss: 5.2781\n",
      "Epoch 8662/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0992 - val_loss: 5.3164\n",
      "Epoch 8663/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1908 - val_loss: 5.4391\n",
      "Epoch 8664/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1099 - val_loss: 5.4757\n",
      "Epoch 8665/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2655 - val_loss: 5.3424\n",
      "Epoch 8666/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1366 - val_loss: 5.9163\n",
      "Epoch 8667/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3536 - val_loss: 5.3132\n",
      "Epoch 8668/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2992 - val_loss: 5.3377\n",
      "Epoch 8669/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.1360 - val_loss: 5.3555\n",
      "Epoch 8670/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1646 - val_loss: 5.4164\n",
      "Epoch 8671/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5172 - val_loss: 5.5201\n",
      "Epoch 8672/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1842 - val_loss: 5.5387\n",
      "Epoch 8673/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2418 - val_loss: 5.3408\n",
      "Epoch 8674/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1246 - val_loss: 5.4331\n",
      "Epoch 8675/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1265 - val_loss: 5.4942\n",
      "Epoch 8676/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2215 - val_loss: 5.5176\n",
      "Epoch 8677/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2323 - val_loss: 5.4136\n",
      "Epoch 8678/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1435 - val_loss: 5.3621\n",
      "Epoch 8679/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1083 - val_loss: 5.3348\n",
      "Epoch 8680/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1412 - val_loss: 5.3597\n",
      "Epoch 8681/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1058 - val_loss: 5.3391\n",
      "Epoch 8682/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1561 - val_loss: 5.4569\n",
      "Epoch 8683/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3159 - val_loss: 5.3622\n",
      "Epoch 8684/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1440 - val_loss: 5.3179\n",
      "Epoch 8685/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1105 - val_loss: 5.2902\n",
      "Epoch 8686/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0974 - val_loss: 5.3100\n",
      "Epoch 8687/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2105 - val_loss: 5.2613\n",
      "Epoch 8688/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2074 - val_loss: 5.2731\n",
      "Epoch 8689/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1297 - val_loss: 5.3633\n",
      "Epoch 8690/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1234 - val_loss: 5.2837\n",
      "Epoch 8691/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1368 - val_loss: 5.5486\n",
      "Epoch 8692/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2030 - val_loss: 5.3401\n",
      "Epoch 8693/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1065 - val_loss: 5.2904\n",
      "Epoch 8694/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1348 - val_loss: 5.3436\n",
      "Epoch 8695/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3470 - val_loss: 5.2708\n",
      "Epoch 8696/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2340 - val_loss: 5.3060\n",
      "Epoch 8697/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0957 - val_loss: 5.2844\n",
      "Epoch 8698/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0901 - val_loss: 5.4514\n",
      "Epoch 8699/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2511 - val_loss: 5.3195\n",
      "Epoch 8700/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1606 - val_loss: 5.3649\n",
      "Epoch 8701/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2634 - val_loss: 5.3913\n",
      "Epoch 8702/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0927 - val_loss: 5.2880\n",
      "Epoch 8703/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1049 - val_loss: 5.3237\n",
      "Epoch 8704/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1280 - val_loss: 5.3420\n",
      "Epoch 8705/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0834 - val_loss: 5.3195\n",
      "Epoch 8706/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1198 - val_loss: 5.3616\n",
      "Epoch 8707/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2720 - val_loss: 5.9532\n",
      "Epoch 8708/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6153 - val_loss: 5.2524\n",
      "Epoch 8709/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2843 - val_loss: 5.4905\n",
      "Epoch 8710/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2408 - val_loss: 5.3331\n",
      "Epoch 8711/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1348 - val_loss: 5.3013\n",
      "Epoch 8712/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1135 - val_loss: 5.3265\n",
      "Epoch 8713/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2502 - val_loss: 5.3603\n",
      "Epoch 8714/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1724 - val_loss: 5.3267\n",
      "Epoch 8715/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1348 - val_loss: 5.3301\n",
      "Epoch 8716/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3613 - val_loss: 5.5123\n",
      "Epoch 8717/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3387 - val_loss: 5.7670\n",
      "Epoch 8718/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5218 - val_loss: 5.3908\n",
      "Epoch 8719/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2106 - val_loss: 5.3733\n",
      "Epoch 8720/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2049 - val_loss: 5.3931\n",
      "Epoch 8721/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1092 - val_loss: 5.2810\n",
      "Epoch 8722/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0868 - val_loss: 5.3408\n",
      "Epoch 8723/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2272 - val_loss: 5.8425\n",
      "Epoch 8724/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.6043 - val_loss: 5.3082\n",
      "Epoch 8725/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2419 - val_loss: 5.7588\n",
      "Epoch 8726/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3472 - val_loss: 5.3951\n",
      "Epoch 8727/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3069 - val_loss: 5.3412\n",
      "Epoch 8728/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1654 - val_loss: 5.2976\n",
      "Epoch 8729/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1525 - val_loss: 5.4706\n",
      "Epoch 8730/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2232 - val_loss: 5.3647\n",
      "Epoch 8731/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1296 - val_loss: 5.5988\n",
      "Epoch 8732/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2134 - val_loss: 5.4023\n",
      "Epoch 8733/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2461 - val_loss: 5.3730\n",
      "Epoch 8734/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0941 - val_loss: 5.3094\n",
      "Epoch 8735/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1148 - val_loss: 5.4616\n",
      "Epoch 8736/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1704 - val_loss: 5.5560\n",
      "Epoch 8737/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4373 - val_loss: 5.3180\n",
      "Epoch 8738/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3309 - val_loss: 5.3512\n",
      "Epoch 8739/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2595 - val_loss: 5.3593\n",
      "Epoch 8740/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1070 - val_loss: 5.3657\n",
      "Epoch 8741/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2160 - val_loss: 5.4335\n",
      "Epoch 8742/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1664 - val_loss: 5.3016\n",
      "Epoch 8743/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0988 - val_loss: 5.3692\n",
      "Epoch 8744/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1666 - val_loss: 5.3200\n",
      "Epoch 8745/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 56us/step - loss: 4.0893 - val_loss: 5.3730\n",
      "Epoch 8746/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1594 - val_loss: 5.2906\n",
      "Epoch 8747/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1131 - val_loss: 5.2865\n",
      "Epoch 8748/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1337 - val_loss: 5.5804\n",
      "Epoch 8749/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.7071 - val_loss: 5.3938\n",
      "Epoch 8750/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2114 - val_loss: 5.3565\n",
      "Epoch 8751/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2606 - val_loss: 5.3029\n",
      "Epoch 8752/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0985 - val_loss: 5.3101\n",
      "Epoch 8753/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1002 - val_loss: 6.1005\n",
      "Epoch 8754/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4143 - val_loss: 5.3236\n",
      "Epoch 8755/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2748 - val_loss: 5.3104\n",
      "Epoch 8756/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3157 - val_loss: 5.8080\n",
      "Epoch 8757/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4890 - val_loss: 5.2710\n",
      "Epoch 8758/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1447 - val_loss: 5.3262\n",
      "Epoch 8759/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0981 - val_loss: 5.3277\n",
      "Epoch 8760/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1206 - val_loss: 5.2777\n",
      "Epoch 8761/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1282 - val_loss: 5.2839\n",
      "Epoch 8762/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1735 - val_loss: 5.4972\n",
      "Epoch 8763/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1771 - val_loss: 5.3507\n",
      "Epoch 8764/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2050 - val_loss: 5.2972\n",
      "Epoch 8765/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0734 - val_loss: 5.3539\n",
      "Epoch 8766/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1154 - val_loss: 5.2919\n",
      "Epoch 8767/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1369 - val_loss: 5.3308\n",
      "Epoch 8768/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3494 - val_loss: 6.0799\n",
      "Epoch 8769/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3234 - val_loss: 5.4051\n",
      "Epoch 8770/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1168 - val_loss: 5.3422\n",
      "Epoch 8771/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1584 - val_loss: 5.2838\n",
      "Epoch 8772/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2684 - val_loss: 5.5255\n",
      "Epoch 8773/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5580 - val_loss: 5.4390\n",
      "Epoch 8774/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3758 - val_loss: 5.6086\n",
      "Epoch 8775/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2230 - val_loss: 5.4615\n",
      "Epoch 8776/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1098 - val_loss: 5.2774\n",
      "Epoch 8777/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2051 - val_loss: 5.3842\n",
      "Epoch 8778/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2639 - val_loss: 5.2980\n",
      "Epoch 8779/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1109 - val_loss: 5.3499\n",
      "Epoch 8780/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1138 - val_loss: 5.4006\n",
      "Epoch 8781/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1581 - val_loss: 5.3709\n",
      "Epoch 8782/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2496 - val_loss: 5.6494\n",
      "Epoch 8783/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2579 - val_loss: 5.3386\n",
      "Epoch 8784/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0906 - val_loss: 5.3408\n",
      "Epoch 8785/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1284 - val_loss: 5.7260\n",
      "Epoch 8786/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3777 - val_loss: 5.3290\n",
      "Epoch 8787/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1999 - val_loss: 5.6003\n",
      "Epoch 8788/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2337 - val_loss: 5.2605\n",
      "Epoch 8789/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1678 - val_loss: 5.2596\n",
      "Epoch 8790/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2253 - val_loss: 5.2761\n",
      "Epoch 8791/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1121 - val_loss: 5.3397\n",
      "Epoch 8792/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1141 - val_loss: 5.3263\n",
      "Epoch 8793/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0647 - val_loss: 5.3340\n",
      "Epoch 8794/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0956 - val_loss: 5.3197\n",
      "Epoch 8795/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1817 - val_loss: 5.3557\n",
      "Epoch 8796/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2092 - val_loss: 5.3294\n",
      "Epoch 8797/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1543 - val_loss: 5.3322\n",
      "Epoch 8798/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2174 - val_loss: 5.8247\n",
      "Epoch 8799/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5594 - val_loss: 5.3234\n",
      "Epoch 8800/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0804 - val_loss: 5.6415\n",
      "Epoch 8801/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3628 - val_loss: 5.2438\n",
      "Epoch 8802/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1772 - val_loss: 5.2633\n",
      "Epoch 8803/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1871 - val_loss: 5.3664\n",
      "Epoch 8804/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2064 - val_loss: 5.2829\n",
      "Epoch 8805/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1599 - val_loss: 5.3286\n",
      "Epoch 8806/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2005 - val_loss: 5.3805\n",
      "Epoch 8807/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1708 - val_loss: 5.3277\n",
      "Epoch 8808/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0960 - val_loss: 5.4188\n",
      "Epoch 8809/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1716 - val_loss: 5.5038\n",
      "Epoch 8810/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1943 - val_loss: 5.2616\n",
      "Epoch 8811/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0659 - val_loss: 5.3210\n",
      "Epoch 8812/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0975 - val_loss: 5.2815\n",
      "Epoch 8813/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0622 - val_loss: 5.2884\n",
      "Epoch 8814/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1593 - val_loss: 5.9084\n",
      "Epoch 8815/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4112 - val_loss: 5.3148\n",
      "Epoch 8816/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2434 - val_loss: 5.6920\n",
      "Epoch 8817/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3276 - val_loss: 5.6393\n",
      "Epoch 8818/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3035 - val_loss: 5.3541\n",
      "Epoch 8819/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2183 - val_loss: 5.2914\n",
      "Epoch 8820/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0973 - val_loss: 5.7876\n",
      "Epoch 8821/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 56us/step - loss: 4.4252 - val_loss: 5.2841\n",
      "Epoch 8822/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1632 - val_loss: 5.4389\n",
      "Epoch 8823/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2868 - val_loss: 5.3447\n",
      "Epoch 8824/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3100 - val_loss: 5.2971\n",
      "Epoch 8825/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2740 - val_loss: 6.1864\n",
      "Epoch 8826/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.7706 - val_loss: 5.2691\n",
      "Epoch 8827/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2134 - val_loss: 5.3990\n",
      "Epoch 8828/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.1645 - val_loss: 5.3067\n",
      "Epoch 8829/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 4.1193 - val_loss: 5.3575\n",
      "Epoch 8830/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.1990 - val_loss: 5.4351\n",
      "Epoch 8831/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2689 - val_loss: 5.3204\n",
      "Epoch 8832/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1981 - val_loss: 5.4641\n",
      "Epoch 8833/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3288 - val_loss: 5.3540\n",
      "Epoch 8834/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1506 - val_loss: 5.3256\n",
      "Epoch 8835/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0896 - val_loss: 5.3862\n",
      "Epoch 8836/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2307 - val_loss: 5.2985\n",
      "Epoch 8837/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0836 - val_loss: 5.3192\n",
      "Epoch 8838/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1217 - val_loss: 5.2896\n",
      "Epoch 8839/10000\n",
      "675/675 [==============================] - 0s 107us/step - loss: 4.0530 - val_loss: 5.3693\n",
      "Epoch 8840/10000\n",
      "675/675 [==============================] - 0s 130us/step - loss: 4.3046 - val_loss: 5.3292\n",
      "Epoch 8841/10000\n",
      "675/675 [==============================] - 0s 124us/step - loss: 4.1062 - val_loss: 5.3426\n",
      "Epoch 8842/10000\n",
      "675/675 [==============================] - 0s 120us/step - loss: 4.1016 - val_loss: 5.3319\n",
      "Epoch 8843/10000\n",
      "675/675 [==============================] - 0s 115us/step - loss: 4.2210 - val_loss: 5.3381\n",
      "Epoch 8844/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 4.2182 - val_loss: 5.3301\n",
      "Epoch 8845/10000\n",
      "675/675 [==============================] - 0s 119us/step - loss: 4.1443 - val_loss: 5.3287\n",
      "Epoch 8846/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 4.1283 - val_loss: 5.3056\n",
      "Epoch 8847/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2072 - val_loss: 5.6442\n",
      "Epoch 8848/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4066 - val_loss: 5.3341\n",
      "Epoch 8849/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0984 - val_loss: 5.4237\n",
      "Epoch 8850/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1125 - val_loss: 5.9051\n",
      "Epoch 8851/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6338 - val_loss: 5.8499\n",
      "Epoch 8852/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2046 - val_loss: 5.3154\n",
      "Epoch 8853/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0800 - val_loss: 5.3168\n",
      "Epoch 8854/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4178 - val_loss: 5.7234\n",
      "Epoch 8855/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1586 - val_loss: 5.2882\n",
      "Epoch 8856/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0986 - val_loss: 5.3619\n",
      "Epoch 8857/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1076 - val_loss: 5.3046\n",
      "Epoch 8858/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0868 - val_loss: 5.3238\n",
      "Epoch 8859/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2379 - val_loss: 5.2891\n",
      "Epoch 8860/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3924 - val_loss: 5.2777\n",
      "Epoch 8861/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.2095 - val_loss: 5.3804\n",
      "Epoch 8862/10000\n",
      "675/675 [==============================] - 0s 110us/step - loss: 4.1787 - val_loss: 5.5655\n",
      "Epoch 8863/10000\n",
      "675/675 [==============================] - 0s 111us/step - loss: 4.5672 - val_loss: 5.6774\n",
      "Epoch 8864/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 4.2620 - val_loss: 5.3537\n",
      "Epoch 8865/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.1146 - val_loss: 5.3376\n",
      "Epoch 8866/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.1226 - val_loss: 5.2686\n",
      "Epoch 8867/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0529 - val_loss: 5.3696\n",
      "Epoch 8868/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0837 - val_loss: 5.4013\n",
      "Epoch 8869/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1238 - val_loss: 5.4208\n",
      "Epoch 8870/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2228 - val_loss: 5.7248\n",
      "Epoch 8871/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.7909 - val_loss: 5.4168\n",
      "Epoch 8872/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1714 - val_loss: 5.3284\n",
      "Epoch 8873/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0874 - val_loss: 5.3162\n",
      "Epoch 8874/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3363 - val_loss: 5.3060\n",
      "Epoch 8875/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1806 - val_loss: 5.3285\n",
      "Epoch 8876/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0979 - val_loss: 5.8339\n",
      "Epoch 8877/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6877 - val_loss: 5.3534\n",
      "Epoch 8878/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1452 - val_loss: 5.5403\n",
      "Epoch 8879/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1607 - val_loss: 5.2870\n",
      "Epoch 8880/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3135 - val_loss: 5.2659\n",
      "Epoch 8881/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1603 - val_loss: 5.2634\n",
      "Epoch 8882/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2164 - val_loss: 5.4691\n",
      "Epoch 8883/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1964 - val_loss: 5.3868\n",
      "Epoch 8884/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1759 - val_loss: 5.7594\n",
      "Epoch 8885/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.6059 - val_loss: 5.3846\n",
      "Epoch 8886/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1525 - val_loss: 6.0702\n",
      "Epoch 8887/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.4445 - val_loss: 5.2818\n",
      "Epoch 8888/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.1521 - val_loss: 5.3728\n",
      "Epoch 8889/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2772 - val_loss: 5.2577\n",
      "Epoch 8890/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1575 - val_loss: 5.8085\n",
      "Epoch 8891/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4645 - val_loss: 5.2702\n",
      "Epoch 8892/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1174 - val_loss: 5.4104\n",
      "Epoch 8893/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2910 - val_loss: 5.2834\n",
      "Epoch 8894/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1223 - val_loss: 5.3594\n",
      "Epoch 8895/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2794 - val_loss: 5.2767\n",
      "Epoch 8896/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1982 - val_loss: 5.2823\n",
      "Epoch 8897/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.1908 - val_loss: 5.5019\n",
      "Epoch 8898/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2157 - val_loss: 5.4528\n",
      "Epoch 8899/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3480 - val_loss: 5.2997\n",
      "Epoch 8900/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0956 - val_loss: 5.3316\n",
      "Epoch 8901/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1371 - val_loss: 5.3523\n",
      "Epoch 8902/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1133 - val_loss: 5.4201\n",
      "Epoch 8903/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1297 - val_loss: 5.2812\n",
      "Epoch 8904/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0950 - val_loss: 5.3730\n",
      "Epoch 8905/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2432 - val_loss: 5.3523\n",
      "Epoch 8906/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2940 - val_loss: 5.5473\n",
      "Epoch 8907/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3047 - val_loss: 5.4228\n",
      "Epoch 8908/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2622 - val_loss: 5.3715\n",
      "Epoch 8909/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.4804 - val_loss: 5.2890\n",
      "Epoch 8910/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1344 - val_loss: 5.2655\n",
      "Epoch 8911/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0948 - val_loss: 5.3021\n",
      "Epoch 8912/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1147 - val_loss: 5.3558\n",
      "Epoch 8913/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2275 - val_loss: 5.2817\n",
      "Epoch 8914/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1540 - val_loss: 5.2423\n",
      "Epoch 8915/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2753 - val_loss: 5.3573\n",
      "Epoch 8916/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1882 - val_loss: 5.2755\n",
      "Epoch 8917/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2224 - val_loss: 5.2961\n",
      "Epoch 8918/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2525 - val_loss: 5.3155\n",
      "Epoch 8919/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0793 - val_loss: 5.2995\n",
      "Epoch 8920/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1794 - val_loss: 5.3142\n",
      "Epoch 8921/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3268 - val_loss: 5.3442\n",
      "Epoch 8922/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0823 - val_loss: 5.2918\n",
      "Epoch 8923/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0982 - val_loss: 5.6315\n",
      "Epoch 8924/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1997 - val_loss: 5.7043\n",
      "Epoch 8925/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3300 - val_loss: 5.3210\n",
      "Epoch 8926/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1609 - val_loss: 5.2991\n",
      "Epoch 8927/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1128 - val_loss: 5.3440\n",
      "Epoch 8928/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4105 - val_loss: 5.6730\n",
      "Epoch 8929/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2356 - val_loss: 5.4760\n",
      "Epoch 8930/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3880 - val_loss: 5.4793\n",
      "Epoch 8931/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1592 - val_loss: 5.3317\n",
      "Epoch 8932/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2122 - val_loss: 5.3379\n",
      "Epoch 8933/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1831 - val_loss: 5.2809\n",
      "Epoch 8934/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0605 - val_loss: 5.7909\n",
      "Epoch 8935/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4286 - val_loss: 5.3397\n",
      "Epoch 8936/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2293 - val_loss: 5.2787\n",
      "Epoch 8937/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1081 - val_loss: 5.2783\n",
      "Epoch 8938/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1151 - val_loss: 5.8637\n",
      "Epoch 8939/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4281 - val_loss: 5.3074\n",
      "Epoch 8940/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1808 - val_loss: 5.3371\n",
      "Epoch 8941/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1229 - val_loss: 5.2595\n",
      "Epoch 8942/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1337 - val_loss: 5.2867\n",
      "Epoch 8943/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2084 - val_loss: 5.3232\n",
      "Epoch 8944/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0613 - val_loss: 5.3646\n",
      "Epoch 8945/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1256 - val_loss: 5.2737\n",
      "Epoch 8946/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4331 - val_loss: 5.2577\n",
      "Epoch 8947/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1328 - val_loss: 5.3206\n",
      "Epoch 8948/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1972 - val_loss: 5.3256\n",
      "Epoch 8949/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2163 - val_loss: 5.3325\n",
      "Epoch 8950/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1578 - val_loss: 5.4250\n",
      "Epoch 8951/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2524 - val_loss: 5.2881\n",
      "Epoch 8952/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3862 - val_loss: 5.3865\n",
      "Epoch 8953/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1663 - val_loss: 5.3414\n",
      "Epoch 8954/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1253 - val_loss: 5.3192\n",
      "Epoch 8955/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0593 - val_loss: 5.3451\n",
      "Epoch 8956/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1126 - val_loss: 5.4204\n",
      "Epoch 8957/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.1483 - val_loss: 5.2994\n",
      "Epoch 8958/10000\n",
      "675/675 [==============================] - 0s 95us/step - loss: 4.0667 - val_loss: 5.4230\n",
      "Epoch 8959/10000\n",
      "675/675 [==============================] - 0s 105us/step - loss: 4.1640 - val_loss: 5.3690\n",
      "Epoch 8960/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 4.2092 - val_loss: 5.4259\n",
      "Epoch 8961/10000\n",
      "675/675 [==============================] - 0s 100us/step - loss: 4.3005 - val_loss: 5.3694\n",
      "Epoch 8962/10000\n",
      "675/675 [==============================] - 0s 123us/step - loss: 4.2291 - val_loss: 5.2687\n",
      "Epoch 8963/10000\n",
      "675/675 [==============================] - 0s 115us/step - loss: 4.1358 - val_loss: 5.3178\n",
      "Epoch 8964/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.1453 - val_loss: 5.3795\n",
      "Epoch 8965/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.0751 - val_loss: 5.2623\n",
      "Epoch 8966/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0765 - val_loss: 5.3344\n",
      "Epoch 8967/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1655 - val_loss: 5.5334\n",
      "Epoch 8968/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.8221 - val_loss: 5.3320\n",
      "Epoch 8969/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2732 - val_loss: 5.3032\n",
      "Epoch 8970/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1955 - val_loss: 5.5113\n",
      "Epoch 8971/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4969 - val_loss: 5.5510\n",
      "Epoch 8972/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1511 - val_loss: 5.4425\n",
      "Epoch 8973/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 56us/step - loss: 4.1490 - val_loss: 5.2844\n",
      "Epoch 8974/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1145 - val_loss: 5.3115\n",
      "Epoch 8975/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1485 - val_loss: 5.2986\n",
      "Epoch 8976/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0960 - val_loss: 5.2719\n",
      "Epoch 8977/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0875 - val_loss: 5.2666\n",
      "Epoch 8978/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0905 - val_loss: 5.3563\n",
      "Epoch 8979/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2449 - val_loss: 5.2689\n",
      "Epoch 8980/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2971 - val_loss: 5.4558\n",
      "Epoch 8981/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1207 - val_loss: 5.3374\n",
      "Epoch 8982/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.0925 - val_loss: 5.4694\n",
      "Epoch 8983/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.3814 - val_loss: 5.2940\n",
      "Epoch 8984/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2281 - val_loss: 5.3400\n",
      "Epoch 8985/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2858 - val_loss: 5.3608\n",
      "Epoch 8986/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1046 - val_loss: 5.2756\n",
      "Epoch 8987/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1214 - val_loss: 5.3322\n",
      "Epoch 8988/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1466 - val_loss: 5.3206\n",
      "Epoch 8989/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1034 - val_loss: 5.4339\n",
      "Epoch 8990/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2191 - val_loss: 5.3362\n",
      "Epoch 8991/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1838 - val_loss: 5.5169\n",
      "Epoch 8992/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2991 - val_loss: 5.4012\n",
      "Epoch 8993/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5630 - val_loss: 5.2759\n",
      "Epoch 8994/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2040 - val_loss: 5.3642\n",
      "Epoch 8995/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1244 - val_loss: 5.3779\n",
      "Epoch 8996/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.1154 - val_loss: 5.3383\n",
      "Epoch 8997/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.2178 - val_loss: 5.2614\n",
      "Epoch 8998/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.4584 - val_loss: 5.6724\n",
      "Epoch 8999/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.5165 - val_loss: 5.3967\n",
      "Epoch 9000/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1185 - val_loss: 5.6202\n",
      "Epoch 9001/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3858 - val_loss: 5.3244\n",
      "Epoch 9002/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.0569 - val_loss: 5.6698\n",
      "Epoch 9003/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.7749 - val_loss: 6.3611\n",
      "Epoch 9004/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.5953 - val_loss: 5.6200\n",
      "Epoch 9005/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2878 - val_loss: 5.4444\n",
      "Epoch 9006/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.1207 - val_loss: 5.2913\n",
      "Epoch 9007/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2458 - val_loss: 5.4187\n",
      "Epoch 9008/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1745 - val_loss: 5.3848\n",
      "Epoch 9009/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1483 - val_loss: 5.2989\n",
      "Epoch 9010/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3989 - val_loss: 5.3107\n",
      "Epoch 9011/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2100 - val_loss: 5.2617\n",
      "Epoch 9012/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.0805 - val_loss: 5.2940\n",
      "Epoch 9013/10000\n",
      "675/675 [==============================] - 0s 99us/step - loss: 4.1483 - val_loss: 5.2713\n",
      "Epoch 9014/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1039 - val_loss: 5.3483\n",
      "Epoch 9015/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1377 - val_loss: 5.5702\n",
      "Epoch 9016/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1105 - val_loss: 5.2941\n",
      "Epoch 9017/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0952 - val_loss: 5.3616\n",
      "Epoch 9018/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1282 - val_loss: 5.3161\n",
      "Epoch 9019/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0874 - val_loss: 5.3285\n",
      "Epoch 9020/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1222 - val_loss: 5.4234\n",
      "Epoch 9021/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4538 - val_loss: 5.3212\n",
      "Epoch 9022/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4124 - val_loss: 5.4393\n",
      "Epoch 9023/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3063 - val_loss: 5.2592\n",
      "Epoch 9024/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0981 - val_loss: 5.3286\n",
      "Epoch 9025/10000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 4.1368 - val_loss: 5.3295\n",
      "Epoch 9026/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.1268 - val_loss: 5.4012\n",
      "Epoch 9027/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3338 - val_loss: 5.6130\n",
      "Epoch 9028/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1287 - val_loss: 5.3631\n",
      "Epoch 9029/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4436 - val_loss: 5.5755\n",
      "Epoch 9030/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2951 - val_loss: 5.3645\n",
      "Epoch 9031/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1393 - val_loss: 5.2860\n",
      "Epoch 9032/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.0666 - val_loss: 5.3036\n",
      "Epoch 9033/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0760 - val_loss: 5.3094\n",
      "Epoch 9034/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.2066 - val_loss: 5.3814\n",
      "Epoch 9035/10000\n",
      "675/675 [==============================] - 0s 117us/step - loss: 4.1582 - val_loss: 5.2866\n",
      "Epoch 9036/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.0662 - val_loss: 5.2880\n",
      "Epoch 9037/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0884 - val_loss: 5.2960\n",
      "Epoch 9038/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0677 - val_loss: 5.3428\n",
      "Epoch 9039/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1646 - val_loss: 5.4847\n",
      "Epoch 9040/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3713 - val_loss: 5.3433\n",
      "Epoch 9041/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1339 - val_loss: 5.3642\n",
      "Epoch 9042/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2287 - val_loss: 5.2835\n",
      "Epoch 9043/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1685 - val_loss: 5.2790\n",
      "Epoch 9044/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0672 - val_loss: 5.2937\n",
      "Epoch 9045/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2138 - val_loss: 5.3715\n",
      "Epoch 9046/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 4.1483 - val_loss: 5.3254\n",
      "Epoch 9047/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.0809 - val_loss: 5.2500\n",
      "Epoch 9048/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1842 - val_loss: 5.2894\n",
      "Epoch 9049/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.4045 - val_loss: 5.4404\n",
      "Epoch 9050/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0615 - val_loss: 5.3633\n",
      "Epoch 9051/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0700 - val_loss: 5.2986\n",
      "Epoch 9052/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0668 - val_loss: 5.5233\n",
      "Epoch 9053/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4922 - val_loss: 5.6249\n",
      "Epoch 9054/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2000 - val_loss: 5.3266\n",
      "Epoch 9055/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1670 - val_loss: 5.4415\n",
      "Epoch 9056/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3125 - val_loss: 5.4047\n",
      "Epoch 9057/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.2348 - val_loss: 5.3587\n",
      "Epoch 9058/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.1801 - val_loss: 5.4939\n",
      "Epoch 9059/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 4.1184 - val_loss: 5.3818\n",
      "Epoch 9060/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0897 - val_loss: 5.2674\n",
      "Epoch 9061/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0633 - val_loss: 5.4287\n",
      "Epoch 9062/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1727 - val_loss: 5.4284\n",
      "Epoch 9063/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2832 - val_loss: 5.6415\n",
      "Epoch 9064/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2270 - val_loss: 5.3781\n",
      "Epoch 9065/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1367 - val_loss: 5.2872\n",
      "Epoch 9066/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1175 - val_loss: 5.2763\n",
      "Epoch 9067/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0832 - val_loss: 5.7807\n",
      "Epoch 9068/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3754 - val_loss: 5.3405\n",
      "Epoch 9069/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0628 - val_loss: 5.3832\n",
      "Epoch 9070/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.1075 - val_loss: 5.3541\n",
      "Epoch 9071/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.1966 - val_loss: 5.3540\n",
      "Epoch 9072/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2273 - val_loss: 5.5422\n",
      "Epoch 9073/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2038 - val_loss: 5.3132\n",
      "Epoch 9074/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1005 - val_loss: 5.2505\n",
      "Epoch 9075/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2055 - val_loss: 5.3093\n",
      "Epoch 9076/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0844 - val_loss: 5.3363\n",
      "Epoch 9077/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1842 - val_loss: 5.5066\n",
      "Epoch 9078/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2596 - val_loss: 5.3110\n",
      "Epoch 9079/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0913 - val_loss: 5.3513\n",
      "Epoch 9080/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2147 - val_loss: 5.2841\n",
      "Epoch 9081/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0683 - val_loss: 5.3172\n",
      "Epoch 9082/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1436 - val_loss: 5.8296\n",
      "Epoch 9083/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.2161 - val_loss: 5.3536\n",
      "Epoch 9084/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 4.1593 - val_loss: 5.7018\n",
      "Epoch 9085/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.2494 - val_loss: 5.3616\n",
      "Epoch 9086/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0749 - val_loss: 5.3419\n",
      "Epoch 9087/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1418 - val_loss: 5.2797\n",
      "Epoch 9088/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2334 - val_loss: 5.3557\n",
      "Epoch 9089/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2258 - val_loss: 5.8151\n",
      "Epoch 9090/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3303 - val_loss: 5.3751\n",
      "Epoch 9091/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0685 - val_loss: 5.2780\n",
      "Epoch 9092/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1651 - val_loss: 5.3212\n",
      "Epoch 9093/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0875 - val_loss: 5.3747\n",
      "Epoch 9094/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2323 - val_loss: 5.5631\n",
      "Epoch 9095/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1576 - val_loss: 5.3049\n",
      "Epoch 9096/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.0836 - val_loss: 5.3938\n",
      "Epoch 9097/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2103 - val_loss: 5.5556\n",
      "Epoch 9098/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.2137 - val_loss: 5.3327\n",
      "Epoch 9099/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.1467 - val_loss: 5.3663\n",
      "Epoch 9100/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.3820 - val_loss: 5.4038\n",
      "Epoch 9101/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.3282 - val_loss: 5.4362\n",
      "Epoch 9102/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.1236 - val_loss: 5.3335\n",
      "Epoch 9103/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.0842 - val_loss: 5.3635\n",
      "Epoch 9104/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3070 - val_loss: 5.6098\n",
      "Epoch 9105/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2903 - val_loss: 5.3785\n",
      "Epoch 9106/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.1387 - val_loss: 5.3111\n",
      "Epoch 9107/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1376 - val_loss: 5.3626\n",
      "Epoch 9108/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1035 - val_loss: 5.4682\n",
      "Epoch 9109/10000\n",
      "675/675 [==============================] - 0s 46us/step - loss: 4.2409 - val_loss: 5.6433\n",
      "Epoch 9110/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0736 - val_loss: 5.3014\n",
      "Epoch 9111/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1678 - val_loss: 5.6104\n",
      "Epoch 9112/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1208 - val_loss: 5.3242\n",
      "Epoch 9113/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.2687 - val_loss: 5.2933\n",
      "Epoch 9114/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1993 - val_loss: 5.5300\n",
      "Epoch 9115/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1955 - val_loss: 5.3371\n",
      "Epoch 9116/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2227 - val_loss: 5.2792\n",
      "Epoch 9117/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0831 - val_loss: 5.7425\n",
      "Epoch 9118/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1981 - val_loss: 5.3854\n",
      "Epoch 9119/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1720 - val_loss: 5.3141\n",
      "Epoch 9120/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1238 - val_loss: 5.4652\n",
      "Epoch 9121/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0961 - val_loss: 5.7807\n",
      "Epoch 9122/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5524 - val_loss: 5.3121\n",
      "Epoch 9123/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2317 - val_loss: 5.3421\n",
      "Epoch 9124/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1109 - val_loss: 5.5856\n",
      "Epoch 9125/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 52us/step - loss: 4.1996 - val_loss: 5.5112\n",
      "Epoch 9126/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.1240 - val_loss: 5.3154\n",
      "Epoch 9127/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1022 - val_loss: 5.3960\n",
      "Epoch 9128/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1095 - val_loss: 5.3350\n",
      "Epoch 9129/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0804 - val_loss: 6.0750\n",
      "Epoch 9130/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.5101 - val_loss: 5.3459\n",
      "Epoch 9131/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3549 - val_loss: 5.3907\n",
      "Epoch 9132/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0666 - val_loss: 5.4549\n",
      "Epoch 9133/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2157 - val_loss: 5.3800\n",
      "Epoch 9134/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3181 - val_loss: 5.3245\n",
      "Epoch 9135/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0522 - val_loss: 5.2550\n",
      "Epoch 9136/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0428 - val_loss: 5.5817\n",
      "Epoch 9137/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4262 - val_loss: 5.3720\n",
      "Epoch 9138/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2660 - val_loss: 5.2609\n",
      "Epoch 9139/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 4.0684 - val_loss: 5.2833\n",
      "Epoch 9140/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4485 - val_loss: 5.4931\n",
      "Epoch 9141/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1602 - val_loss: 5.3751\n",
      "Epoch 9142/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3160 - val_loss: 5.2512\n",
      "Epoch 9143/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1424 - val_loss: 5.2691\n",
      "Epoch 9144/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1155 - val_loss: 5.2907\n",
      "Epoch 9145/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0712 - val_loss: 5.2839\n",
      "Epoch 9146/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1071 - val_loss: 5.2599\n",
      "Epoch 9147/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0663 - val_loss: 5.2924\n",
      "Epoch 9148/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1138 - val_loss: 5.2688\n",
      "Epoch 9149/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0556 - val_loss: 5.3434\n",
      "Epoch 9150/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2392 - val_loss: 5.4341\n",
      "Epoch 9151/10000\n",
      "675/675 [==============================] - 0s 129us/step - loss: 4.4195 - val_loss: 5.5033\n",
      "Epoch 9152/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1878 - val_loss: 5.3054\n",
      "Epoch 9153/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1386 - val_loss: 5.3040\n",
      "Epoch 9154/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1427 - val_loss: 5.3185\n",
      "Epoch 9155/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1050 - val_loss: 5.3215\n",
      "Epoch 9156/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1352 - val_loss: 5.3273\n",
      "Epoch 9157/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.1174 - val_loss: 5.2825\n",
      "Epoch 9158/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.0633 - val_loss: 5.3790\n",
      "Epoch 9159/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.1832 - val_loss: 5.5535\n",
      "Epoch 9160/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3334 - val_loss: 5.2772\n",
      "Epoch 9161/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2197 - val_loss: 5.4295\n",
      "Epoch 9162/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0887 - val_loss: 5.5284\n",
      "Epoch 9163/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1335 - val_loss: 5.4564\n",
      "Epoch 9164/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1374 - val_loss: 5.2827\n",
      "Epoch 9165/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1792 - val_loss: 5.3372\n",
      "Epoch 9166/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1753 - val_loss: 5.2789\n",
      "Epoch 9167/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2833 - val_loss: 5.3485\n",
      "Epoch 9168/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1538 - val_loss: 5.3325\n",
      "Epoch 9169/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2049 - val_loss: 5.4942\n",
      "Epoch 9170/10000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 4.2294 - val_loss: 5.3212\n",
      "Epoch 9171/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2476 - val_loss: 5.2775\n",
      "Epoch 9172/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 4.1972 - val_loss: 5.3588\n",
      "Epoch 9173/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 4.1362 - val_loss: 5.2927\n",
      "Epoch 9174/10000\n",
      "675/675 [==============================] - 0s 47us/step - loss: 4.1601 - val_loss: 5.3244\n",
      "Epoch 9175/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 4.0806 - val_loss: 5.2962\n",
      "Epoch 9176/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1069 - val_loss: 5.3566\n",
      "Epoch 9177/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1444 - val_loss: 5.3036\n",
      "Epoch 9178/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.1817 - val_loss: 5.5944\n",
      "Epoch 9179/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.5511 - val_loss: 5.4134\n",
      "Epoch 9180/10000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 4.1570 - val_loss: 5.3004\n",
      "Epoch 9181/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.1098 - val_loss: 5.2999\n",
      "Epoch 9182/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.2475 - val_loss: 5.5974\n",
      "Epoch 9183/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1933 - val_loss: 5.4428\n",
      "Epoch 9184/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2903 - val_loss: 5.2818\n",
      "Epoch 9185/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2644 - val_loss: 5.3102\n",
      "Epoch 9186/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0691 - val_loss: 5.2928\n",
      "Epoch 9187/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3566 - val_loss: 5.2984\n",
      "Epoch 9188/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1788 - val_loss: 5.3031\n",
      "Epoch 9189/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0835 - val_loss: 5.2661\n",
      "Epoch 9190/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.1629 - val_loss: 5.3914\n",
      "Epoch 9191/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1321 - val_loss: 5.9785\n",
      "Epoch 9192/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2373 - val_loss: 5.2691\n",
      "Epoch 9193/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0967 - val_loss: 5.3030\n",
      "Epoch 9194/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0643 - val_loss: 5.2675\n",
      "Epoch 9195/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1146 - val_loss: 5.3262\n",
      "Epoch 9196/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1184 - val_loss: 5.2902\n",
      "Epoch 9197/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0809 - val_loss: 5.3463\n",
      "Epoch 9198/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2037 - val_loss: 5.6880\n",
      "Epoch 9199/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2420 - val_loss: 5.3485\n",
      "Epoch 9200/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.0806 - val_loss: 5.4066\n",
      "Epoch 9201/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 59us/step - loss: 4.2738 - val_loss: 5.2656\n",
      "Epoch 9202/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.0858 - val_loss: 5.4156\n",
      "Epoch 9203/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.2762 - val_loss: 5.4913\n",
      "Epoch 9204/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3666 - val_loss: 5.6872\n",
      "Epoch 9205/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1588 - val_loss: 5.2807\n",
      "Epoch 9206/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0751 - val_loss: 5.2933\n",
      "Epoch 9207/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1609 - val_loss: 5.3026\n",
      "Epoch 9208/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.2431 - val_loss: 5.3934\n",
      "Epoch 9209/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.0838 - val_loss: 5.3451\n",
      "Epoch 9210/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2318 - val_loss: 5.3189\n",
      "Epoch 9211/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0528 - val_loss: 5.2751\n",
      "Epoch 9212/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1032 - val_loss: 5.4054\n",
      "Epoch 9213/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2669 - val_loss: 5.4866\n",
      "Epoch 9214/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2360 - val_loss: 5.5241\n",
      "Epoch 9215/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0861 - val_loss: 5.3685\n",
      "Epoch 9216/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1046 - val_loss: 5.3644\n",
      "Epoch 9217/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3896 - val_loss: 5.5776\n",
      "Epoch 9218/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0846 - val_loss: 5.2556\n",
      "Epoch 9219/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1416 - val_loss: 5.3072\n",
      "Epoch 9220/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1108 - val_loss: 5.2795\n",
      "Epoch 9221/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1885 - val_loss: 5.2992\n",
      "Epoch 9222/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1745 - val_loss: 5.2989\n",
      "Epoch 9223/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1477 - val_loss: 5.8822\n",
      "Epoch 9224/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.6020 - val_loss: 5.2459\n",
      "Epoch 9225/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2239 - val_loss: 5.4153\n",
      "Epoch 9226/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.4988 - val_loss: 5.2894\n",
      "Epoch 9227/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1309 - val_loss: 5.3177\n",
      "Epoch 9228/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3059 - val_loss: 5.3296\n",
      "Epoch 9229/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1630 - val_loss: 5.2863\n",
      "Epoch 9230/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1495 - val_loss: 5.2909\n",
      "Epoch 9231/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1254 - val_loss: 5.3333\n",
      "Epoch 9232/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0748 - val_loss: 5.4595\n",
      "Epoch 9233/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1692 - val_loss: 5.3187\n",
      "Epoch 9234/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0770 - val_loss: 5.3196\n",
      "Epoch 9235/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1585 - val_loss: 5.5005\n",
      "Epoch 9236/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1420 - val_loss: 5.3477\n",
      "Epoch 9237/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0647 - val_loss: 5.3341\n",
      "Epoch 9238/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0962 - val_loss: 5.3078\n",
      "Epoch 9239/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0540 - val_loss: 5.3835\n",
      "Epoch 9240/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1655 - val_loss: 5.5759\n",
      "Epoch 9241/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2052 - val_loss: 5.3253\n",
      "Epoch 9242/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1222 - val_loss: 5.3069\n",
      "Epoch 9243/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1620 - val_loss: 5.3486\n",
      "Epoch 9244/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2237 - val_loss: 5.3597\n",
      "Epoch 9245/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1772 - val_loss: 5.3670\n",
      "Epoch 9246/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1002 - val_loss: 5.3482\n",
      "Epoch 9247/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2124 - val_loss: 5.8606\n",
      "Epoch 9248/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.5003 - val_loss: 5.3838\n",
      "Epoch 9249/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2902 - val_loss: 5.3791\n",
      "Epoch 9250/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1659 - val_loss: 5.3159\n",
      "Epoch 9251/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0451 - val_loss: 5.4305\n",
      "Epoch 9252/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0626 - val_loss: 5.2901\n",
      "Epoch 9253/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2322 - val_loss: 5.3842\n",
      "Epoch 9254/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1181 - val_loss: 5.2963\n",
      "Epoch 9255/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2141 - val_loss: 5.2485\n",
      "Epoch 9256/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.3516 - val_loss: 5.3051\n",
      "Epoch 9257/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.1096 - val_loss: 5.3058\n",
      "Epoch 9258/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.1359 - val_loss: 5.2418\n",
      "Epoch 9259/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2006 - val_loss: 5.2804\n",
      "Epoch 9260/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2117 - val_loss: 5.4503\n",
      "Epoch 9261/10000\n",
      "675/675 [==============================] - 0s 46us/step - loss: 4.1447 - val_loss: 5.3380\n",
      "Epoch 9262/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.0827 - val_loss: 5.3702\n",
      "Epoch 9263/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1125 - val_loss: 5.2414\n",
      "Epoch 9264/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0982 - val_loss: 5.3332\n",
      "Epoch 9265/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0712 - val_loss: 5.2746\n",
      "Epoch 9266/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2459 - val_loss: 5.2916\n",
      "Epoch 9267/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.4001 - val_loss: 5.2545\n",
      "Epoch 9268/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0675 - val_loss: 5.3070\n",
      "Epoch 9269/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1441 - val_loss: 5.2582\n",
      "Epoch 9270/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2154 - val_loss: 5.2725\n",
      "Epoch 9271/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.3710 - val_loss: 5.2991\n",
      "Epoch 9272/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.1891 - val_loss: 5.3172\n",
      "Epoch 9273/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0966 - val_loss: 5.2829\n",
      "Epoch 9274/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.0625 - val_loss: 5.3249\n",
      "Epoch 9275/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.0904 - val_loss: 5.2546\n",
      "Epoch 9276/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1619 - val_loss: 5.5011\n",
      "Epoch 9277/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 50us/step - loss: 4.1416 - val_loss: 5.2763\n",
      "Epoch 9278/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1044 - val_loss: 5.8484\n",
      "Epoch 9279/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 4.5614 - val_loss: 5.2655\n",
      "Epoch 9280/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.2393 - val_loss: 5.5953\n",
      "Epoch 9281/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.5896 - val_loss: 5.3179\n",
      "Epoch 9282/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1834 - val_loss: 5.3418\n",
      "Epoch 9283/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0935 - val_loss: 5.4714\n",
      "Epoch 9284/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1407 - val_loss: 5.2902\n",
      "Epoch 9285/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1133 - val_loss: 5.3801\n",
      "Epoch 9286/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1363 - val_loss: 5.3594\n",
      "Epoch 9287/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0535 - val_loss: 5.3568\n",
      "Epoch 9288/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1981 - val_loss: 5.2761\n",
      "Epoch 9289/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2939 - val_loss: 5.4459\n",
      "Epoch 9290/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.3023 - val_loss: 5.3913\n",
      "Epoch 9291/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0949 - val_loss: 5.3977\n",
      "Epoch 9292/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1165 - val_loss: 5.5624\n",
      "Epoch 9293/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1904 - val_loss: 5.3138\n",
      "Epoch 9294/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0514 - val_loss: 5.2945\n",
      "Epoch 9295/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1947 - val_loss: 5.2814\n",
      "Epoch 9296/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1701 - val_loss: 5.4085\n",
      "Epoch 9297/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.1569 - val_loss: 5.3339\n",
      "Epoch 9298/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.0782 - val_loss: 5.2493\n",
      "Epoch 9299/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.0615 - val_loss: 5.3478\n",
      "Epoch 9300/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0715 - val_loss: 5.3535\n",
      "Epoch 9301/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1970 - val_loss: 5.3894\n",
      "Epoch 9302/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1307 - val_loss: 5.4122\n",
      "Epoch 9303/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2620 - val_loss: 5.2906\n",
      "Epoch 9304/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5330 - val_loss: 6.4570\n",
      "Epoch 9305/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6841 - val_loss: 5.3340\n",
      "Epoch 9306/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5585 - val_loss: 5.4736\n",
      "Epoch 9307/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1302 - val_loss: 5.2774\n",
      "Epoch 9308/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1022 - val_loss: 5.3318\n",
      "Epoch 9309/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.1606 - val_loss: 5.2997\n",
      "Epoch 9310/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0742 - val_loss: 5.4567\n",
      "Epoch 9311/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1277 - val_loss: 5.4601\n",
      "Epoch 9312/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1929 - val_loss: 5.2715\n",
      "Epoch 9313/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1004 - val_loss: 5.3048\n",
      "Epoch 9314/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2060 - val_loss: 5.2970\n",
      "Epoch 9315/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0547 - val_loss: 5.5446\n",
      "Epoch 9316/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.4491 - val_loss: 5.2799\n",
      "Epoch 9317/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4623 - val_loss: 5.2896\n",
      "Epoch 9318/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1789 - val_loss: 5.2310\n",
      "Epoch 9319/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1226 - val_loss: 6.3085\n",
      "Epoch 9320/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.9254 - val_loss: 5.7121\n",
      "Epoch 9321/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1504 - val_loss: 5.3006\n",
      "Epoch 9322/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0887 - val_loss: 5.2714\n",
      "Epoch 9323/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1045 - val_loss: 5.3392\n",
      "Epoch 9324/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5149 - val_loss: 5.7417\n",
      "Epoch 9325/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.4455 - val_loss: 5.2902\n",
      "Epoch 9326/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1031 - val_loss: 5.3144\n",
      "Epoch 9327/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2570 - val_loss: 5.2962\n",
      "Epoch 9328/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1254 - val_loss: 5.3897\n",
      "Epoch 9329/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1628 - val_loss: 5.2623\n",
      "Epoch 9330/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1538 - val_loss: 5.2553\n",
      "Epoch 9331/10000\n",
      "675/675 [==============================] - 0s 49us/step - loss: 4.2312 - val_loss: 5.1981\n",
      "Epoch 9332/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 4.1254 - val_loss: 5.2239\n",
      "Epoch 9333/10000\n",
      "675/675 [==============================] - 0s 48us/step - loss: 4.0682 - val_loss: 5.4204\n",
      "Epoch 9334/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2833 - val_loss: 5.2682\n",
      "Epoch 9335/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.2653 - val_loss: 5.6024\n",
      "Epoch 9336/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1597 - val_loss: 5.2683\n",
      "Epoch 9337/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0519 - val_loss: 5.2678\n",
      "Epoch 9338/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0438 - val_loss: 5.3251\n",
      "Epoch 9339/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.1643 - val_loss: 5.5995\n",
      "Epoch 9340/10000\n",
      "675/675 [==============================] - 0s 68us/step - loss: 4.0819 - val_loss: 5.2379\n",
      "Epoch 9341/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.1107 - val_loss: 5.3045\n",
      "Epoch 9342/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.1820 - val_loss: 5.2938\n",
      "Epoch 9343/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0921 - val_loss: 5.3954\n",
      "Epoch 9344/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1769 - val_loss: 5.2578\n",
      "Epoch 9345/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0748 - val_loss: 5.2676\n",
      "Epoch 9346/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0822 - val_loss: 5.3003\n",
      "Epoch 9347/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1205 - val_loss: 5.5337\n",
      "Epoch 9348/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2847 - val_loss: 5.2611\n",
      "Epoch 9349/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0913 - val_loss: 5.2478\n",
      "Epoch 9350/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0907 - val_loss: 5.2971\n",
      "Epoch 9351/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1273 - val_loss: 5.4597\n",
      "Epoch 9352/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2820 - val_loss: 5.2872\n",
      "Epoch 9353/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 81us/step - loss: 4.0783 - val_loss: 5.2670\n",
      "Epoch 9354/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.1029 - val_loss: 5.3815\n",
      "Epoch 9355/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.1745 - val_loss: 5.2678\n",
      "Epoch 9356/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.1790 - val_loss: 5.3229\n",
      "Epoch 9357/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.1064 - val_loss: 5.3046\n",
      "Epoch 9358/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0817 - val_loss: 5.3190\n",
      "Epoch 9359/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0866 - val_loss: 5.2778\n",
      "Epoch 9360/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1155 - val_loss: 5.2905\n",
      "Epoch 9361/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0897 - val_loss: 5.4537\n",
      "Epoch 9362/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4757 - val_loss: 5.3834\n",
      "Epoch 9363/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2386 - val_loss: 5.3833\n",
      "Epoch 9364/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0886 - val_loss: 5.2888\n",
      "Epoch 9365/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1755 - val_loss: 5.2650\n",
      "Epoch 9366/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1200 - val_loss: 5.3783\n",
      "Epoch 9367/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1693 - val_loss: 5.3870\n",
      "Epoch 9368/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2537 - val_loss: 5.3299\n",
      "Epoch 9369/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.3102 - val_loss: 5.3920\n",
      "Epoch 9370/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1971 - val_loss: 5.6279\n",
      "Epoch 9371/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2003 - val_loss: 5.2650\n",
      "Epoch 9372/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1805 - val_loss: 5.2702\n",
      "Epoch 9373/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2098 - val_loss: 5.4227\n",
      "Epoch 9374/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3335 - val_loss: 5.3003\n",
      "Epoch 9375/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2714 - val_loss: 5.2745\n",
      "Epoch 9376/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0679 - val_loss: 5.2503\n",
      "Epoch 9377/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2340 - val_loss: 5.6704\n",
      "Epoch 9378/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2192 - val_loss: 5.3057\n",
      "Epoch 9379/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0666 - val_loss: 5.2819\n",
      "Epoch 9380/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1007 - val_loss: 5.3032\n",
      "Epoch 9381/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0490 - val_loss: 5.3095\n",
      "Epoch 9382/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2800 - val_loss: 5.2921\n",
      "Epoch 9383/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1149 - val_loss: 5.2385\n",
      "Epoch 9384/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0715 - val_loss: 5.3891\n",
      "Epoch 9385/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1705 - val_loss: 5.2993\n",
      "Epoch 9386/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1848 - val_loss: 5.2565\n",
      "Epoch 9387/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2448 - val_loss: 5.2376\n",
      "Epoch 9388/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1524 - val_loss: 5.2825\n",
      "Epoch 9389/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0870 - val_loss: 5.2153\n",
      "Epoch 9390/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0886 - val_loss: 5.3478\n",
      "Epoch 9391/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.5389 - val_loss: 5.5453\n",
      "Epoch 9392/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1176 - val_loss: 5.4018\n",
      "Epoch 9393/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2571 - val_loss: 5.3945\n",
      "Epoch 9394/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0326 - val_loss: 5.2394\n",
      "Epoch 9395/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0407 - val_loss: 5.3352\n",
      "Epoch 9396/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1242 - val_loss: 5.2535\n",
      "Epoch 9397/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1025 - val_loss: 5.3214\n",
      "Epoch 9398/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3066 - val_loss: 5.3028\n",
      "Epoch 9399/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2278 - val_loss: 5.2818\n",
      "Epoch 9400/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1349 - val_loss: 5.2492\n",
      "Epoch 9401/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0913 - val_loss: 5.2849\n",
      "Epoch 9402/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3197 - val_loss: 5.3250\n",
      "Epoch 9403/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0851 - val_loss: 5.2563\n",
      "Epoch 9404/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2237 - val_loss: 5.2505\n",
      "Epoch 9405/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0356 - val_loss: 5.2537\n",
      "Epoch 9406/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0984 - val_loss: 5.2661\n",
      "Epoch 9407/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0565 - val_loss: 5.2626\n",
      "Epoch 9408/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0771 - val_loss: 5.2561\n",
      "Epoch 9409/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3016 - val_loss: 5.5084\n",
      "Epoch 9410/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1673 - val_loss: 5.3339\n",
      "Epoch 9411/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0584 - val_loss: 5.2738\n",
      "Epoch 9412/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1032 - val_loss: 5.3223\n",
      "Epoch 9413/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1339 - val_loss: 5.2898\n",
      "Epoch 9414/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1247 - val_loss: 5.4177\n",
      "Epoch 9415/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2272 - val_loss: 5.3693\n",
      "Epoch 9416/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0761 - val_loss: 5.3258\n",
      "Epoch 9417/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1253 - val_loss: 5.3451\n",
      "Epoch 9418/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0537 - val_loss: 5.3206\n",
      "Epoch 9419/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1755 - val_loss: 5.4365\n",
      "Epoch 9420/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1526 - val_loss: 5.2552\n",
      "Epoch 9421/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0757 - val_loss: 5.2386\n",
      "Epoch 9422/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0588 - val_loss: 5.2547\n",
      "Epoch 9423/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0755 - val_loss: 5.2650\n",
      "Epoch 9424/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0682 - val_loss: 5.3312\n",
      "Epoch 9425/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0622 - val_loss: 5.5031\n",
      "Epoch 9426/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3825 - val_loss: 5.3623\n",
      "Epoch 9427/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2745 - val_loss: 5.3134\n",
      "Epoch 9428/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1311 - val_loss: 5.2298\n",
      "Epoch 9429/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 53us/step - loss: 4.1392 - val_loss: 5.2293\n",
      "Epoch 9430/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0598 - val_loss: 5.3095\n",
      "Epoch 9431/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0908 - val_loss: 5.5588\n",
      "Epoch 9432/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2363 - val_loss: 5.4828\n",
      "Epoch 9433/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2953 - val_loss: 5.2857\n",
      "Epoch 9434/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3119 - val_loss: 5.3074\n",
      "Epoch 9435/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1248 - val_loss: 5.3908\n",
      "Epoch 9436/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0885 - val_loss: 5.2755\n",
      "Epoch 9437/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0825 - val_loss: 5.2399\n",
      "Epoch 9438/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0599 - val_loss: 5.6200\n",
      "Epoch 9439/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3861 - val_loss: 5.3148\n",
      "Epoch 9440/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1765 - val_loss: 5.2738\n",
      "Epoch 9441/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0930 - val_loss: 5.2157\n",
      "Epoch 9442/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1122 - val_loss: 5.4885\n",
      "Epoch 9443/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1717 - val_loss: 5.3398\n",
      "Epoch 9444/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2919 - val_loss: 5.3352\n",
      "Epoch 9445/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1588 - val_loss: 5.3665\n",
      "Epoch 9446/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0894 - val_loss: 5.3704\n",
      "Epoch 9447/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1403 - val_loss: 5.2677\n",
      "Epoch 9448/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1751 - val_loss: 5.2497\n",
      "Epoch 9449/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0760 - val_loss: 5.2527\n",
      "Epoch 9450/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1141 - val_loss: 5.5907\n",
      "Epoch 9451/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.4872 - val_loss: 5.3000\n",
      "Epoch 9452/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3018 - val_loss: 5.4509\n",
      "Epoch 9453/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1685 - val_loss: 5.2474\n",
      "Epoch 9454/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0734 - val_loss: 5.4216\n",
      "Epoch 9455/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1438 - val_loss: 5.2515\n",
      "Epoch 9456/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0463 - val_loss: 5.2480\n",
      "Epoch 9457/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1543 - val_loss: 5.2611\n",
      "Epoch 9458/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1441 - val_loss: 5.5273\n",
      "Epoch 9459/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1083 - val_loss: 5.2879\n",
      "Epoch 9460/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4083 - val_loss: 5.2316\n",
      "Epoch 9461/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0501 - val_loss: 5.3745\n",
      "Epoch 9462/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2496 - val_loss: 5.2868\n",
      "Epoch 9463/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2903 - val_loss: 5.5278\n",
      "Epoch 9464/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2166 - val_loss: 5.2507\n",
      "Epoch 9465/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0640 - val_loss: 5.4177\n",
      "Epoch 9466/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2109 - val_loss: 5.3028\n",
      "Epoch 9467/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1553 - val_loss: 5.3811\n",
      "Epoch 9468/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0726 - val_loss: 5.2736\n",
      "Epoch 9469/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1107 - val_loss: 5.4202\n",
      "Epoch 9470/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1487 - val_loss: 5.2853\n",
      "Epoch 9471/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1693 - val_loss: 5.2593\n",
      "Epoch 9472/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0751 - val_loss: 5.2645\n",
      "Epoch 9473/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0895 - val_loss: 5.2661\n",
      "Epoch 9474/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1128 - val_loss: 5.2364\n",
      "Epoch 9475/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1127 - val_loss: 5.4288\n",
      "Epoch 9476/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1424 - val_loss: 5.3351\n",
      "Epoch 9477/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1877 - val_loss: 5.2812\n",
      "Epoch 9478/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0903 - val_loss: 5.2462\n",
      "Epoch 9479/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0437 - val_loss: 5.2937\n",
      "Epoch 9480/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0555 - val_loss: 5.4382\n",
      "Epoch 9481/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3372 - val_loss: 5.2958\n",
      "Epoch 9482/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0598 - val_loss: 5.4458\n",
      "Epoch 9483/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1985 - val_loss: 5.2458\n",
      "Epoch 9484/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0920 - val_loss: 5.2609\n",
      "Epoch 9485/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0357 - val_loss: 5.3051\n",
      "Epoch 9486/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0546 - val_loss: 5.2977\n",
      "Epoch 9487/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0616 - val_loss: 5.4173\n",
      "Epoch 9488/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1174 - val_loss: 5.2544\n",
      "Epoch 9489/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0877 - val_loss: 5.6098\n",
      "Epoch 9490/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.2441 - val_loss: 5.2764\n",
      "Epoch 9491/10000\n",
      "675/675 [==============================] - 0s 66us/step - loss: 4.1100 - val_loss: 5.2826\n",
      "Epoch 9492/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1435 - val_loss: 5.2977\n",
      "Epoch 9493/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0956 - val_loss: 5.4952\n",
      "Epoch 9494/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.3029 - val_loss: 5.4087\n",
      "Epoch 9495/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4180 - val_loss: 5.2646\n",
      "Epoch 9496/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0941 - val_loss: 5.3275\n",
      "Epoch 9497/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1518 - val_loss: 5.3392\n",
      "Epoch 9498/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1439 - val_loss: 5.3330\n",
      "Epoch 9499/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2022 - val_loss: 5.2700\n",
      "Epoch 9500/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2992 - val_loss: 5.2451\n",
      "Epoch 9501/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2100 - val_loss: 5.2580\n",
      "Epoch 9502/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0687 - val_loss: 5.3735\n",
      "Epoch 9503/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4207 - val_loss: 5.5259\n",
      "Epoch 9504/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0434 - val_loss: 5.2681\n",
      "Epoch 9505/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 55us/step - loss: 4.0821 - val_loss: 5.5438\n",
      "Epoch 9506/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.3602 - val_loss: 5.2744\n",
      "Epoch 9507/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3837 - val_loss: 5.3426\n",
      "Epoch 9508/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0898 - val_loss: 5.4005\n",
      "Epoch 9509/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2798 - val_loss: 5.3955\n",
      "Epoch 9510/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1032 - val_loss: 5.2728\n",
      "Epoch 9511/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1358 - val_loss: 5.2687\n",
      "Epoch 9512/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1532 - val_loss: 5.4076\n",
      "Epoch 9513/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1421 - val_loss: 5.3262\n",
      "Epoch 9514/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0348 - val_loss: 5.3573\n",
      "Epoch 9515/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1057 - val_loss: 5.4372\n",
      "Epoch 9516/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1699 - val_loss: 5.3195\n",
      "Epoch 9517/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1789 - val_loss: 5.2650\n",
      "Epoch 9518/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0733 - val_loss: 5.2217\n",
      "Epoch 9519/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0759 - val_loss: 5.2698\n",
      "Epoch 9520/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0637 - val_loss: 5.2847\n",
      "Epoch 9521/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1853 - val_loss: 5.2533\n",
      "Epoch 9522/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2798 - val_loss: 5.3050\n",
      "Epoch 9523/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0661 - val_loss: 5.2891\n",
      "Epoch 9524/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1468 - val_loss: 5.2554\n",
      "Epoch 9525/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2426 - val_loss: 5.2333\n",
      "Epoch 9526/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1642 - val_loss: 5.4585\n",
      "Epoch 9527/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2407 - val_loss: 5.2847\n",
      "Epoch 9528/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0364 - val_loss: 5.3307\n",
      "Epoch 9529/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1555 - val_loss: 5.2378\n",
      "Epoch 9530/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0529 - val_loss: 5.3492\n",
      "Epoch 9531/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1680 - val_loss: 5.2699\n",
      "Epoch 9532/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1350 - val_loss: 5.5773\n",
      "Epoch 9533/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2159 - val_loss: 5.4891\n",
      "Epoch 9534/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1917 - val_loss: 5.4467\n",
      "Epoch 9535/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1082 - val_loss: 5.2938\n",
      "Epoch 9536/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0421 - val_loss: 5.3134\n",
      "Epoch 9537/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1388 - val_loss: 5.2399\n",
      "Epoch 9538/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0737 - val_loss: 5.3049\n",
      "Epoch 9539/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1036 - val_loss: 5.2420\n",
      "Epoch 9540/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0951 - val_loss: 5.3036\n",
      "Epoch 9541/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0791 - val_loss: 5.2898\n",
      "Epoch 9542/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0888 - val_loss: 5.4506\n",
      "Epoch 9543/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1115 - val_loss: 5.3030\n",
      "Epoch 9544/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0480 - val_loss: 5.3377\n",
      "Epoch 9545/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0825 - val_loss: 5.3544\n",
      "Epoch 9546/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0989 - val_loss: 5.2589\n",
      "Epoch 9547/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0714 - val_loss: 5.2868\n",
      "Epoch 9548/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1087 - val_loss: 5.2360\n",
      "Epoch 9549/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1826 - val_loss: 5.3851\n",
      "Epoch 9550/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1972 - val_loss: 5.2819\n",
      "Epoch 9551/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1758 - val_loss: 5.3486\n",
      "Epoch 9552/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.0389 - val_loss: 5.3543\n",
      "Epoch 9553/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 4.2047 - val_loss: 5.3038\n",
      "Epoch 9554/10000\n",
      "675/675 [==============================] - 0s 143us/step - loss: 4.2270 - val_loss: 5.7167\n",
      "Epoch 9555/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 4.2846 - val_loss: 5.5859\n",
      "Epoch 9556/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.8563 - val_loss: 5.7107\n",
      "Epoch 9557/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2173 - val_loss: 5.2685\n",
      "Epoch 9558/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0613 - val_loss: 5.3786\n",
      "Epoch 9559/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0747 - val_loss: 5.2815\n",
      "Epoch 9560/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0734 - val_loss: 5.2449\n",
      "Epoch 9561/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1557 - val_loss: 5.3361\n",
      "Epoch 9562/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1352 - val_loss: 5.3015\n",
      "Epoch 9563/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1886 - val_loss: 5.4180\n",
      "Epoch 9564/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0734 - val_loss: 5.2603\n",
      "Epoch 9565/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 4.0224 - val_loss: 5.4057\n",
      "Epoch 9566/10000\n",
      "675/675 [==============================] - 0s 105us/step - loss: 4.1325 - val_loss: 5.2764\n",
      "Epoch 9567/10000\n",
      "675/675 [==============================] - 0s 124us/step - loss: 4.0140 - val_loss: 5.5023\n",
      "Epoch 9568/10000\n",
      "675/675 [==============================] - 0s 95us/step - loss: 4.1900 - val_loss: 5.2148\n",
      "Epoch 9569/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.0588 - val_loss: 5.4314\n",
      "Epoch 9570/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.2921 - val_loss: 5.2493\n",
      "Epoch 9571/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.0284 - val_loss: 5.3296\n",
      "Epoch 9572/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 4.0869 - val_loss: 5.2738\n",
      "Epoch 9573/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 4.0454 - val_loss: 5.2736\n",
      "Epoch 9574/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.0689 - val_loss: 5.2455\n",
      "Epoch 9575/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 4.1347 - val_loss: 5.2567\n",
      "Epoch 9576/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.0651 - val_loss: 5.3041\n",
      "Epoch 9577/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.0786 - val_loss: 5.2477\n",
      "Epoch 9578/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.1262 - val_loss: 5.3037\n",
      "Epoch 9579/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 4.1573 - val_loss: 5.2914\n",
      "Epoch 9580/10000\n",
      "675/675 [==============================] - 0s 99us/step - loss: 4.0281 - val_loss: 5.3489\n",
      "Epoch 9581/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 86us/step - loss: 4.2021 - val_loss: 5.4268\n",
      "Epoch 9582/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 4.0557 - val_loss: 5.3303\n",
      "Epoch 9583/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 4.2850 - val_loss: 5.2775\n",
      "Epoch 9584/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.0713 - val_loss: 5.4172\n",
      "Epoch 9585/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.2482 - val_loss: 5.2388\n",
      "Epoch 9586/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1875 - val_loss: 5.2779\n",
      "Epoch 9587/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.3201 - val_loss: 5.9922\n",
      "Epoch 9588/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.5254 - val_loss: 5.3404\n",
      "Epoch 9589/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 4.6320 - val_loss: 5.4542\n",
      "Epoch 9590/10000\n",
      "675/675 [==============================] - 0s 71us/step - loss: 4.2016 - val_loss: 5.3375\n",
      "Epoch 9591/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.0708 - val_loss: 5.3440\n",
      "Epoch 9592/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.1006 - val_loss: 5.5369\n",
      "Epoch 9593/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.1388 - val_loss: 5.2945\n",
      "Epoch 9594/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.1601 - val_loss: 5.3409\n",
      "Epoch 9595/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.2659 - val_loss: 5.6363\n",
      "Epoch 9596/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.2416 - val_loss: 5.2311\n",
      "Epoch 9597/10000\n",
      "675/675 [==============================] - 0s 108us/step - loss: 4.0796 - val_loss: 5.3236\n",
      "Epoch 9598/10000\n",
      "675/675 [==============================] - 0s 105us/step - loss: 4.1737 - val_loss: 5.4900\n",
      "Epoch 9599/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 4.2146 - val_loss: 5.2403\n",
      "Epoch 9600/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.0829 - val_loss: 5.2869\n",
      "Epoch 9601/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.0841 - val_loss: 5.2934\n",
      "Epoch 9602/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 4.0707 - val_loss: 5.4825\n",
      "Epoch 9603/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.1312 - val_loss: 5.3454\n",
      "Epoch 9604/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.2167 - val_loss: 5.3961\n",
      "Epoch 9605/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.4445 - val_loss: 6.4151\n",
      "Epoch 9606/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 4.7306 - val_loss: 5.4095\n",
      "Epoch 9607/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.1006 - val_loss: 5.2676\n",
      "Epoch 9608/10000\n",
      "675/675 [==============================] - 0s 101us/step - loss: 4.2245 - val_loss: 5.2935\n",
      "Epoch 9609/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 4.1210 - val_loss: 5.2408\n",
      "Epoch 9610/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.1222 - val_loss: 5.2611\n",
      "Epoch 9611/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1465 - val_loss: 5.2911\n",
      "Epoch 9612/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.0718 - val_loss: 5.2684\n",
      "Epoch 9613/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.0300 - val_loss: 5.2605\n",
      "Epoch 9614/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.1453 - val_loss: 5.2301\n",
      "Epoch 9615/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1711 - val_loss: 5.2538\n",
      "Epoch 9616/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1043 - val_loss: 5.2535\n",
      "Epoch 9617/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1130 - val_loss: 5.2889\n",
      "Epoch 9618/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.0993 - val_loss: 5.2564\n",
      "Epoch 9619/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.1030 - val_loss: 5.2543\n",
      "Epoch 9620/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.0634 - val_loss: 5.8675\n",
      "Epoch 9621/10000\n",
      "675/675 [==============================] - 0s 65us/step - loss: 4.8219 - val_loss: 5.9897\n",
      "Epoch 9622/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 4.2739 - val_loss: 5.2274\n",
      "Epoch 9623/10000\n",
      "675/675 [==============================] - 0s 94us/step - loss: 4.0377 - val_loss: 5.2437\n",
      "Epoch 9624/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 4.0859 - val_loss: 5.2696\n",
      "Epoch 9625/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.0164 - val_loss: 5.4742\n",
      "Epoch 9626/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.1629 - val_loss: 5.3156\n",
      "Epoch 9627/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.1146 - val_loss: 5.2506\n",
      "Epoch 9628/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.0674 - val_loss: 5.3131\n",
      "Epoch 9629/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.1768 - val_loss: 5.4527\n",
      "Epoch 9630/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.2212 - val_loss: 5.2725\n",
      "Epoch 9631/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.0454 - val_loss: 5.3631\n",
      "Epoch 9632/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.0488 - val_loss: 5.3776\n",
      "Epoch 9633/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.2537 - val_loss: 5.3617\n",
      "Epoch 9634/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 4.2486 - val_loss: 5.5872\n",
      "Epoch 9635/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1823 - val_loss: 5.3476\n",
      "Epoch 9636/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2122 - val_loss: 6.1001\n",
      "Epoch 9637/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.2460 - val_loss: 5.3554\n",
      "Epoch 9638/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1338 - val_loss: 5.3047\n",
      "Epoch 9639/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.0814 - val_loss: 5.3001\n",
      "Epoch 9640/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0488 - val_loss: 5.4891\n",
      "Epoch 9641/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.0925 - val_loss: 5.3493\n",
      "Epoch 9642/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.1012 - val_loss: 5.3281\n",
      "Epoch 9643/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0459 - val_loss: 5.5303\n",
      "Epoch 9644/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0751 - val_loss: 5.2734\n",
      "Epoch 9645/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0383 - val_loss: 5.2673\n",
      "Epoch 9646/10000\n",
      "675/675 [==============================] - 0s 61us/step - loss: 4.1522 - val_loss: 5.2385\n",
      "Epoch 9647/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.0748 - val_loss: 5.3770\n",
      "Epoch 9648/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 4.2458 - val_loss: 5.2570\n",
      "Epoch 9649/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.4351 - val_loss: 5.3833\n",
      "Epoch 9650/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.1035 - val_loss: 5.2311\n",
      "Epoch 9651/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 4.0615 - val_loss: 5.3498\n",
      "Epoch 9652/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.1248 - val_loss: 5.3146\n",
      "Epoch 9653/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.1494 - val_loss: 5.2563\n",
      "Epoch 9654/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.0481 - val_loss: 5.2857\n",
      "Epoch 9655/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.0917 - val_loss: 5.6787\n",
      "Epoch 9656/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.3113 - val_loss: 5.2389\n",
      "Epoch 9657/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 77us/step - loss: 4.0979 - val_loss: 5.2646\n",
      "Epoch 9658/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.0391 - val_loss: 5.2712\n",
      "Epoch 9659/10000\n",
      "675/675 [==============================] - 0s 101us/step - loss: 4.1360 - val_loss: 5.7302\n",
      "Epoch 9660/10000\n",
      "675/675 [==============================] - 0s 103us/step - loss: 4.3299 - val_loss: 5.3230\n",
      "Epoch 9661/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 4.1982 - val_loss: 5.3513\n",
      "Epoch 9662/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 4.1993 - val_loss: 5.5613\n",
      "Epoch 9663/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.1532 - val_loss: 5.3404\n",
      "Epoch 9664/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.4197 - val_loss: 5.2354\n",
      "Epoch 9665/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.0761 - val_loss: 5.5672\n",
      "Epoch 9666/10000\n",
      "675/675 [==============================] - 0s 72us/step - loss: 4.4315 - val_loss: 5.3150\n",
      "Epoch 9667/10000\n",
      "675/675 [==============================] - 0s 73us/step - loss: 4.1799 - val_loss: 5.2811\n",
      "Epoch 9668/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 4.1293 - val_loss: 5.2844\n",
      "Epoch 9669/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 4.1278 - val_loss: 5.5212\n",
      "Epoch 9670/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.1438 - val_loss: 5.2812\n",
      "Epoch 9671/10000\n",
      "675/675 [==============================] - 0s 116us/step - loss: 4.0847 - val_loss: 5.2621\n",
      "Epoch 9672/10000\n",
      "675/675 [==============================] - 0s 111us/step - loss: 4.0426 - val_loss: 5.4559\n",
      "Epoch 9673/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 4.1003 - val_loss: 5.8935\n",
      "Epoch 9674/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 4.1899 - val_loss: 5.3031\n",
      "Epoch 9675/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.1929 - val_loss: 5.5598\n",
      "Epoch 9676/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1272 - val_loss: 5.1897\n",
      "Epoch 9677/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.1275 - val_loss: 5.2575\n",
      "Epoch 9678/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.4437 - val_loss: 5.6205\n",
      "Epoch 9679/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.1496 - val_loss: 5.3284\n",
      "Epoch 9680/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.0775 - val_loss: 5.2282\n",
      "Epoch 9681/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.2141 - val_loss: 5.2766\n",
      "Epoch 9682/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 4.0699 - val_loss: 5.2410\n",
      "Epoch 9683/10000\n",
      "675/675 [==============================] - 0s 98us/step - loss: 4.1612 - val_loss: 5.6097\n",
      "Epoch 9684/10000\n",
      "675/675 [==============================] - 0s 94us/step - loss: 4.2569 - val_loss: 5.3532\n",
      "Epoch 9685/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 4.3209 - val_loss: 5.6930\n",
      "Epoch 9686/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 4.1435 - val_loss: 5.3271\n",
      "Epoch 9687/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.1084 - val_loss: 5.2777\n",
      "Epoch 9688/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 4.1155 - val_loss: 5.8133\n",
      "Epoch 9689/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.0848 - val_loss: 5.4004\n",
      "Epoch 9690/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.1605 - val_loss: 5.2743\n",
      "Epoch 9691/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.0243 - val_loss: 5.3142\n",
      "Epoch 9692/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 4.1722 - val_loss: 5.3993\n",
      "Epoch 9693/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 4.0722 - val_loss: 5.2991\n",
      "Epoch 9694/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 4.0496 - val_loss: 5.3452\n",
      "Epoch 9695/10000\n",
      "675/675 [==============================] - ETA: 0s - loss: 3.406 - 0s 100us/step - loss: 4.1723 - val_loss: 5.2789\n",
      "Epoch 9696/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.0964 - val_loss: 5.2926\n",
      "Epoch 9697/10000\n",
      "675/675 [==============================] - 0s 62us/step - loss: 4.0523 - val_loss: 5.2537\n",
      "Epoch 9698/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0453 - val_loss: 5.2341\n",
      "Epoch 9699/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0582 - val_loss: 5.3153\n",
      "Epoch 9700/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0656 - val_loss: 5.2576\n",
      "Epoch 9701/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0334 - val_loss: 5.3420\n",
      "Epoch 9702/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.4337 - val_loss: 5.3803\n",
      "Epoch 9703/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1527 - val_loss: 5.3040\n",
      "Epoch 9704/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.1224 - val_loss: 5.4091\n",
      "Epoch 9705/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 4.5015 - val_loss: 5.3594\n",
      "Epoch 9706/10000\n",
      "675/675 [==============================] - 0s 149us/step - loss: 4.1864 - val_loss: 5.2776\n",
      "Epoch 9707/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 4.1128 - val_loss: 5.2952\n",
      "Epoch 9708/10000\n",
      "675/675 [==============================] - 0s 70us/step - loss: 4.1375 - val_loss: 5.2691\n",
      "Epoch 9709/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.0394 - val_loss: 5.2951\n",
      "Epoch 9710/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0599 - val_loss: 5.2310\n",
      "Epoch 9711/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0444 - val_loss: 5.2652\n",
      "Epoch 9712/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1948 - val_loss: 5.2942\n",
      "Epoch 9713/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 4.0951 - val_loss: 5.3744\n",
      "Epoch 9714/10000\n",
      "675/675 [==============================] - 0s 118us/step - loss: 4.1162 - val_loss: 5.2200\n",
      "Epoch 9715/10000\n",
      "675/675 [==============================] - 0s 113us/step - loss: 4.0292 - val_loss: 5.6899\n",
      "Epoch 9716/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 4.2633 - val_loss: 5.4281\n",
      "Epoch 9717/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.2914 - val_loss: 5.5136\n",
      "Epoch 9718/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1425 - val_loss: 5.2805\n",
      "Epoch 9719/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0653 - val_loss: 5.4896\n",
      "Epoch 9720/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0649 - val_loss: 5.4072\n",
      "Epoch 9721/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1811 - val_loss: 5.4927\n",
      "Epoch 9722/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2215 - val_loss: 5.2729\n",
      "Epoch 9723/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0905 - val_loss: 5.3877\n",
      "Epoch 9724/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0987 - val_loss: 5.3199\n",
      "Epoch 9725/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1908 - val_loss: 5.2674\n",
      "Epoch 9726/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 4.0311 - val_loss: 5.2652\n",
      "Epoch 9727/10000\n",
      "675/675 [==============================] - 0s 125us/step - loss: 4.0286 - val_loss: 5.2911\n",
      "Epoch 9728/10000\n",
      "675/675 [==============================] - 0s 94us/step - loss: 4.1067 - val_loss: 5.3212\n",
      "Epoch 9729/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0875 - val_loss: 5.2466\n",
      "Epoch 9730/10000\n",
      "675/675 [==============================] - 0s 112us/step - loss: 4.1019 - val_loss: 5.2423\n",
      "Epoch 9731/10000\n",
      "675/675 [==============================] - 0s 113us/step - loss: 4.0680 - val_loss: 5.3353\n",
      "Epoch 9732/10000\n",
      "675/675 [==============================] - 0s 114us/step - loss: 4.2879 - val_loss: 5.2674\n",
      "Epoch 9733/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 91us/step - loss: 4.0193 - val_loss: 5.3163\n",
      "Epoch 9734/10000\n",
      "675/675 [==============================] - 0s 64us/step - loss: 4.1125 - val_loss: 5.3658\n",
      "Epoch 9735/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2148 - val_loss: 5.2152\n",
      "Epoch 9736/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0688 - val_loss: 5.2354\n",
      "Epoch 9737/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0401 - val_loss: 5.2206\n",
      "Epoch 9738/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1669 - val_loss: 5.2165\n",
      "Epoch 9739/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0527 - val_loss: 5.2406\n",
      "Epoch 9740/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2133 - val_loss: 5.2362\n",
      "Epoch 9741/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2214 - val_loss: 5.3717\n",
      "Epoch 9742/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4347 - val_loss: 5.4819\n",
      "Epoch 9743/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1752 - val_loss: 5.1825\n",
      "Epoch 9744/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1354 - val_loss: 5.4269\n",
      "Epoch 9745/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0963 - val_loss: 5.4674\n",
      "Epoch 9746/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2297 - val_loss: 5.3633\n",
      "Epoch 9747/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1019 - val_loss: 5.2290\n",
      "Epoch 9748/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0540 - val_loss: 5.2629\n",
      "Epoch 9749/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.3565 - val_loss: 5.4497\n",
      "Epoch 9750/10000\n",
      "675/675 [==============================] - 0s 108us/step - loss: 4.0348 - val_loss: 5.2716\n",
      "Epoch 9751/10000\n",
      "675/675 [==============================] - 0s 110us/step - loss: 4.0817 - val_loss: 5.3424\n",
      "Epoch 9752/10000\n",
      "675/675 [==============================] - 0s 115us/step - loss: 4.2901 - val_loss: 5.5696\n",
      "Epoch 9753/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 4.2430 - val_loss: 5.4796\n",
      "Epoch 9754/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.2134 - val_loss: 5.2625\n",
      "Epoch 9755/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1302 - val_loss: 5.2626\n",
      "Epoch 9756/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2857 - val_loss: 5.6089\n",
      "Epoch 9757/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.3805 - val_loss: 5.3211\n",
      "Epoch 9758/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1209 - val_loss: 5.3264\n",
      "Epoch 9759/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2666 - val_loss: 5.2524\n",
      "Epoch 9760/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.3250 - val_loss: 6.0858\n",
      "Epoch 9761/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.3995 - val_loss: 5.3853\n",
      "Epoch 9762/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.1524 - val_loss: 5.3969\n",
      "Epoch 9763/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2648 - val_loss: 5.8042\n",
      "Epoch 9764/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1437 - val_loss: 5.3212\n",
      "Epoch 9765/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1650 - val_loss: 5.2947\n",
      "Epoch 9766/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0633 - val_loss: 5.3307\n",
      "Epoch 9767/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1363 - val_loss: 5.2440\n",
      "Epoch 9768/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.3577 - val_loss: 5.3781\n",
      "Epoch 9769/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 4.1579 - val_loss: 5.2963\n",
      "Epoch 9770/10000\n",
      "675/675 [==============================] - 0s 129us/step - loss: 4.3588 - val_loss: 5.5778\n",
      "Epoch 9771/10000\n",
      "675/675 [==============================] - 0s 108us/step - loss: 4.1464 - val_loss: 5.2401\n",
      "Epoch 9772/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 4.1694 - val_loss: 5.5529\n",
      "Epoch 9773/10000\n",
      "675/675 [==============================] - 0s 106us/step - loss: 4.2318 - val_loss: 5.2447\n",
      "Epoch 9774/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 4.1071 - val_loss: 5.3982\n",
      "Epoch 9775/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.5130 - val_loss: 5.2938\n",
      "Epoch 9776/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0324 - val_loss: 5.3098\n",
      "Epoch 9777/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1417 - val_loss: 5.2911\n",
      "Epoch 9778/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2010 - val_loss: 5.3443\n",
      "Epoch 9779/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0888 - val_loss: 5.2126\n",
      "Epoch 9780/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2912 - val_loss: 5.5559\n",
      "Epoch 9781/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1256 - val_loss: 5.2910\n",
      "Epoch 9782/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.0923 - val_loss: 5.3151\n",
      "Epoch 9783/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0761 - val_loss: 5.2378\n",
      "Epoch 9784/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1602 - val_loss: 5.2352\n",
      "Epoch 9785/10000\n",
      "675/675 [==============================] - 0s 98us/step - loss: 4.0493 - val_loss: 5.3838\n",
      "Epoch 9786/10000\n",
      "675/675 [==============================] - 0s 126us/step - loss: 4.0892 - val_loss: 5.4976\n",
      "Epoch 9787/10000\n",
      "675/675 [==============================] - 0s 122us/step - loss: 4.1001 - val_loss: 5.4231\n",
      "Epoch 9788/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.2556 - val_loss: 5.2587\n",
      "Epoch 9789/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.2110 - val_loss: 5.3712\n",
      "Epoch 9790/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0751 - val_loss: 5.2337\n",
      "Epoch 9791/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0795 - val_loss: 5.2137\n",
      "Epoch 9792/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1023 - val_loss: 5.5024\n",
      "Epoch 9793/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3582 - val_loss: 5.3530\n",
      "Epoch 9794/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2910 - val_loss: 5.2926\n",
      "Epoch 9795/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1337 - val_loss: 5.2436\n",
      "Epoch 9796/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0751 - val_loss: 5.3013\n",
      "Epoch 9797/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2171 - val_loss: 5.3768\n",
      "Epoch 9798/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 4.1078 - val_loss: 5.2644\n",
      "Epoch 9799/10000\n",
      "675/675 [==============================] - 0s 109us/step - loss: 4.1675 - val_loss: 5.2474\n",
      "Epoch 9800/10000\n",
      "675/675 [==============================] - 0s 122us/step - loss: 4.1022 - val_loss: 5.6286\n",
      "Epoch 9801/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 4.1574 - val_loss: 5.4657\n",
      "Epoch 9802/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.0949 - val_loss: 5.2914\n",
      "Epoch 9803/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 4.0947 - val_loss: 5.2075\n",
      "Epoch 9804/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.0620 - val_loss: 5.4973\n",
      "Epoch 9805/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 4.1219 - val_loss: 5.3269\n",
      "Epoch 9806/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 4.1908 - val_loss: 5.3190\n",
      "Epoch 9807/10000\n",
      "675/675 [==============================] - 0s 63us/step - loss: 4.1525 - val_loss: 5.2839\n",
      "Epoch 9808/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.1366 - val_loss: 5.3826\n",
      "Epoch 9809/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 85us/step - loss: 4.0393 - val_loss: 5.3039\n",
      "Epoch 9810/10000\n",
      "675/675 [==============================] - 0s 142us/step - loss: 4.0406 - val_loss: 5.3208\n",
      "Epoch 9811/10000\n",
      "675/675 [==============================] - 0s 136us/step - loss: 4.0275 - val_loss: 5.4382\n",
      "Epoch 9812/10000\n",
      "675/675 [==============================] - 0s 107us/step - loss: 4.2393 - val_loss: 5.3046\n",
      "Epoch 9813/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 4.0795 - val_loss: 5.4528\n",
      "Epoch 9814/10000\n",
      "675/675 [==============================] - 0s 69us/step - loss: 4.2792 - val_loss: 5.4435\n",
      "Epoch 9815/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.1343 - val_loss: 5.3074\n",
      "Epoch 9816/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0470 - val_loss: 5.2583\n",
      "Epoch 9817/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0286 - val_loss: 5.2533\n",
      "Epoch 9818/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1513 - val_loss: 5.2563\n",
      "Epoch 9819/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0595 - val_loss: 5.2643\n",
      "Epoch 9820/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1250 - val_loss: 5.5181\n",
      "Epoch 9821/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.6467 - val_loss: 5.3612\n",
      "Epoch 9822/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0546 - val_loss: 5.3107\n",
      "Epoch 9823/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2184 - val_loss: 5.6848\n",
      "Epoch 9824/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.1887 - val_loss: 5.3636\n",
      "Epoch 9825/10000\n",
      "675/675 [==============================] - 0s 115us/step - loss: 4.1272 - val_loss: 5.4531\n",
      "Epoch 9826/10000\n",
      "675/675 [==============================] - 0s 120us/step - loss: 4.1759 - val_loss: 5.8385\n",
      "Epoch 9827/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 4.5547 - val_loss: 5.5093\n",
      "Epoch 9828/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.0964 - val_loss: 5.2967\n",
      "Epoch 9829/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0584 - val_loss: 5.8188\n",
      "Epoch 9830/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2510 - val_loss: 5.2846\n",
      "Epoch 9831/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0816 - val_loss: 5.4662\n",
      "Epoch 9832/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2947 - val_loss: 5.4543\n",
      "Epoch 9833/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1943 - val_loss: 5.8972\n",
      "Epoch 9834/10000\n",
      "675/675 [==============================] - 0s 74us/step - loss: 4.3357 - val_loss: 5.3670\n",
      "Epoch 9835/10000\n",
      "675/675 [==============================] - 0s 112us/step - loss: 4.2757 - val_loss: 5.6254\n",
      "Epoch 9836/10000\n",
      "675/675 [==============================] - 0s 116us/step - loss: 4.2314 - val_loss: 5.2133\n",
      "Epoch 9837/10000\n",
      "675/675 [==============================] - 0s 75us/step - loss: 4.0520 - val_loss: 5.2220\n",
      "Epoch 9838/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1921 - val_loss: 5.2777\n",
      "Epoch 9839/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0738 - val_loss: 5.3816\n",
      "Epoch 9840/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0866 - val_loss: 5.2678\n",
      "Epoch 9841/10000\n",
      "675/675 [==============================] - 0s 50us/step - loss: 4.0883 - val_loss: 5.4595\n",
      "Epoch 9842/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1892 - val_loss: 5.4295\n",
      "Epoch 9843/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1865 - val_loss: 5.3087\n",
      "Epoch 9844/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0684 - val_loss: 5.3028\n",
      "Epoch 9845/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0934 - val_loss: 5.3429\n",
      "Epoch 9846/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.5210 - val_loss: 5.9000\n",
      "Epoch 9847/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4202 - val_loss: 5.5555\n",
      "Epoch 9848/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3247 - val_loss: 5.3360\n",
      "Epoch 9849/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1653 - val_loss: 5.5852\n",
      "Epoch 9850/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4922 - val_loss: 5.2185\n",
      "Epoch 9851/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2507 - val_loss: 5.2613\n",
      "Epoch 9852/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1151 - val_loss: 6.0631\n",
      "Epoch 9853/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.8015 - val_loss: 6.0286\n",
      "Epoch 9854/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4298 - val_loss: 5.2170\n",
      "Epoch 9855/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1154 - val_loss: 5.2888\n",
      "Epoch 9856/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.0261 - val_loss: 5.1966\n",
      "Epoch 9857/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.0399 - val_loss: 5.2694\n",
      "Epoch 9858/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1322 - val_loss: 5.2875\n",
      "Epoch 9859/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.0575 - val_loss: 5.2291\n",
      "Epoch 9860/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0355 - val_loss: 5.3307\n",
      "Epoch 9861/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.0761 - val_loss: 5.2557\n",
      "Epoch 9862/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0929 - val_loss: 5.3855\n",
      "Epoch 9863/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0829 - val_loss: 5.2887\n",
      "Epoch 9864/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2624 - val_loss: 5.2564\n",
      "Epoch 9865/10000\n",
      "675/675 [==============================] - 0s 67us/step - loss: 4.2277 - val_loss: 5.3621\n",
      "Epoch 9866/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1389 - val_loss: 5.4270\n",
      "Epoch 9867/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1509 - val_loss: 5.2730\n",
      "Epoch 9868/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0416 - val_loss: 5.3977\n",
      "Epoch 9869/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2621 - val_loss: 5.4293\n",
      "Epoch 9870/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1840 - val_loss: 5.2900\n",
      "Epoch 9871/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2542 - val_loss: 5.2523\n",
      "Epoch 9872/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2794 - val_loss: 5.3765\n",
      "Epoch 9873/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0316 - val_loss: 5.8280\n",
      "Epoch 9874/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.4659 - val_loss: 6.0631\n",
      "Epoch 9875/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.5126 - val_loss: 5.4050\n",
      "Epoch 9876/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0648 - val_loss: 5.2382\n",
      "Epoch 9877/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1858 - val_loss: 5.2535\n",
      "Epoch 9878/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0676 - val_loss: 5.2247\n",
      "Epoch 9879/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2378 - val_loss: 5.4340\n",
      "Epoch 9880/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2225 - val_loss: 5.2817\n",
      "Epoch 9881/10000\n",
      "675/675 [==============================] - 0s 57us/step - loss: 4.2490 - val_loss: 5.3760\n",
      "Epoch 9882/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0355 - val_loss: 5.3956\n",
      "Epoch 9883/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2461 - val_loss: 5.7705\n",
      "Epoch 9884/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1921 - val_loss: 5.2645\n",
      "Epoch 9885/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 54us/step - loss: 4.0113 - val_loss: 5.2834\n",
      "Epoch 9886/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0184 - val_loss: 5.6334\n",
      "Epoch 9887/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4584 - val_loss: 6.0308\n",
      "Epoch 9888/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2441 - val_loss: 5.3498\n",
      "Epoch 9889/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0973 - val_loss: 5.3678\n",
      "Epoch 9890/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1509 - val_loss: 5.2383\n",
      "Epoch 9891/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1655 - val_loss: 5.2517\n",
      "Epoch 9892/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1626 - val_loss: 5.3334\n",
      "Epoch 9893/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0279 - val_loss: 5.3643\n",
      "Epoch 9894/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1126 - val_loss: 5.2240\n",
      "Epoch 9895/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0253 - val_loss: 5.6533\n",
      "Epoch 9896/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.4497 - val_loss: 5.2615\n",
      "Epoch 9897/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3407 - val_loss: 5.4848\n",
      "Epoch 9898/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1160 - val_loss: 5.2465\n",
      "Epoch 9899/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1949 - val_loss: 5.3150\n",
      "Epoch 9900/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0365 - val_loss: 5.4470\n",
      "Epoch 9901/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1723 - val_loss: 5.7557\n",
      "Epoch 9902/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2619 - val_loss: 5.3132\n",
      "Epoch 9903/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1273 - val_loss: 5.3143\n",
      "Epoch 9904/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1228 - val_loss: 5.3180\n",
      "Epoch 9905/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0749 - val_loss: 5.2861\n",
      "Epoch 9906/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1445 - val_loss: 5.2768\n",
      "Epoch 9907/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2776 - val_loss: 5.3481\n",
      "Epoch 9908/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0134 - val_loss: 5.4230\n",
      "Epoch 9909/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2445 - val_loss: 5.3323\n",
      "Epoch 9910/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0580 - val_loss: 5.3605\n",
      "Epoch 9911/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0921 - val_loss: 5.2949\n",
      "Epoch 9912/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0194 - val_loss: 5.3028\n",
      "Epoch 9913/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0733 - val_loss: 5.5045\n",
      "Epoch 9914/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1523 - val_loss: 5.5039\n",
      "Epoch 9915/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.5104 - val_loss: 5.6666\n",
      "Epoch 9916/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1355 - val_loss: 5.2543\n",
      "Epoch 9917/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0388 - val_loss: 5.2934\n",
      "Epoch 9918/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0873 - val_loss: 5.3228\n",
      "Epoch 9919/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1297 - val_loss: 5.2603\n",
      "Epoch 9920/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2335 - val_loss: 5.2669\n",
      "Epoch 9921/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1169 - val_loss: 5.2574\n",
      "Epoch 9922/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1328 - val_loss: 5.2978\n",
      "Epoch 9923/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1828 - val_loss: 5.5316\n",
      "Epoch 9924/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0692 - val_loss: 5.2863\n",
      "Epoch 9925/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0905 - val_loss: 5.2992\n",
      "Epoch 9926/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1109 - val_loss: 5.2850\n",
      "Epoch 9927/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 3.9993 - val_loss: 5.2411\n",
      "Epoch 9928/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0732 - val_loss: 5.2505\n",
      "Epoch 9929/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0961 - val_loss: 5.4183\n",
      "Epoch 9930/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1995 - val_loss: 5.2757\n",
      "Epoch 9931/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1124 - val_loss: 5.2517\n",
      "Epoch 9932/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0692 - val_loss: 5.2450\n",
      "Epoch 9933/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0141 - val_loss: 5.3330\n",
      "Epoch 9934/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.5135 - val_loss: 5.3220\n",
      "Epoch 9935/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1720 - val_loss: 5.2384\n",
      "Epoch 9936/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1516 - val_loss: 5.4093\n",
      "Epoch 9937/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1002 - val_loss: 5.4659\n",
      "Epoch 9938/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0876 - val_loss: 5.2749\n",
      "Epoch 9939/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0662 - val_loss: 5.3439\n",
      "Epoch 9940/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1369 - val_loss: 5.4734\n",
      "Epoch 9941/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0979 - val_loss: 5.2504\n",
      "Epoch 9942/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1468 - val_loss: 5.3151\n",
      "Epoch 9943/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1861 - val_loss: 5.2532\n",
      "Epoch 9944/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0237 - val_loss: 5.2346\n",
      "Epoch 9945/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2504 - val_loss: 5.3474\n",
      "Epoch 9946/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0806 - val_loss: 5.4583\n",
      "Epoch 9947/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2453 - val_loss: 5.2763\n",
      "Epoch 9948/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1357 - val_loss: 5.3884\n",
      "Epoch 9949/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2031 - val_loss: 5.2600\n",
      "Epoch 9950/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0569 - val_loss: 5.3334\n",
      "Epoch 9951/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1844 - val_loss: 5.2440\n",
      "Epoch 9952/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0822 - val_loss: 5.2068\n",
      "Epoch 9953/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0831 - val_loss: 5.3240\n",
      "Epoch 9954/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2050 - val_loss: 5.3912\n",
      "Epoch 9955/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1711 - val_loss: 5.2624\n",
      "Epoch 9956/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0318 - val_loss: 5.3422\n",
      "Epoch 9957/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1645 - val_loss: 5.5044\n",
      "Epoch 9958/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1779 - val_loss: 5.2923\n",
      "Epoch 9959/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1492 - val_loss: 5.4105\n",
      "Epoch 9960/10000\n",
      "675/675 [==============================] - 0s 58us/step - loss: 4.0761 - val_loss: 5.5285\n",
      "Epoch 9961/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 56us/step - loss: 4.0887 - val_loss: 5.3536\n",
      "Epoch 9962/10000\n",
      "675/675 [==============================] - 0s 60us/step - loss: 4.1746 - val_loss: 5.2638\n",
      "Epoch 9963/10000\n",
      "675/675 [==============================] - 0s 59us/step - loss: 4.0480 - val_loss: 5.6174\n",
      "Epoch 9964/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2362 - val_loss: 5.2026\n",
      "Epoch 9965/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.0578 - val_loss: 5.2262\n",
      "Epoch 9966/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2144 - val_loss: 5.2139\n",
      "Epoch 9967/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3581 - val_loss: 5.3060\n",
      "Epoch 9968/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0213 - val_loss: 5.2660\n",
      "Epoch 9969/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1844 - val_loss: 5.3529\n",
      "Epoch 9970/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.4024 - val_loss: 5.3122\n",
      "Epoch 9971/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.2319 - val_loss: 5.2649\n",
      "Epoch 9972/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0336 - val_loss: 5.2951\n",
      "Epoch 9973/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2337 - val_loss: 5.7499\n",
      "Epoch 9974/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1671 - val_loss: 5.2465\n",
      "Epoch 9975/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0959 - val_loss: 5.1884\n",
      "Epoch 9976/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.1010 - val_loss: 5.1782\n",
      "Epoch 9977/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.0997 - val_loss: 5.2576\n",
      "Epoch 9978/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0880 - val_loss: 5.5170\n",
      "Epoch 9979/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2206 - val_loss: 5.2320\n",
      "Epoch 9980/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0480 - val_loss: 5.3597\n",
      "Epoch 9981/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1954 - val_loss: 5.4196\n",
      "Epoch 9982/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.2245 - val_loss: 5.5361\n",
      "Epoch 9983/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.3850 - val_loss: 5.1891\n",
      "Epoch 9984/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1835 - val_loss: 5.2841\n",
      "Epoch 9985/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.2073 - val_loss: 5.2932\n",
      "Epoch 9986/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.1144 - val_loss: 5.4130\n",
      "Epoch 9987/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1875 - val_loss: 5.2617\n",
      "Epoch 9988/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.2555 - val_loss: 5.5044\n",
      "Epoch 9989/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.0713 - val_loss: 5.2362\n",
      "Epoch 9990/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1869 - val_loss: 5.1776\n",
      "Epoch 9991/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0575 - val_loss: 5.2187\n",
      "Epoch 9992/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.3054 - val_loss: 5.4722\n",
      "Epoch 9993/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.2175 - val_loss: 5.2335\n",
      "Epoch 9994/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.0521 - val_loss: 5.2596\n",
      "Epoch 9995/10000\n",
      "675/675 [==============================] - 0s 53us/step - loss: 4.1164 - val_loss: 5.8722\n",
      "Epoch 9996/10000\n",
      "675/675 [==============================] - 0s 54us/step - loss: 4.6483 - val_loss: 5.2274\n",
      "Epoch 9997/10000\n",
      "675/675 [==============================] - 0s 52us/step - loss: 4.1908 - val_loss: 5.2766\n",
      "Epoch 9998/10000\n",
      "675/675 [==============================] - 0s 55us/step - loss: 4.0750 - val_loss: 5.2120\n",
      "Epoch 9999/10000\n",
      "675/675 [==============================] - 0s 51us/step - loss: 4.1284 - val_loss: 5.4988\n",
      "Epoch 10000/10000\n",
      "675/675 [==============================] - 0s 56us/step - loss: 4.1170 - val_loss: 5.2686\n",
      "675/675 [==============================] - 0s 20us/step\n",
      "Error for the test set:  3.99022804401539\n"
     ]
    }
   ],
   "source": [
    "# !! fitting the model can take significant amount of time and computation power !!\n",
    "\n",
    "model = index_model()\n",
    "\n",
    "# traininging the neural network, including validation set to check if overfitting occurs\n",
    "history = model.fit(x_train, y_train, epochs=10000, validation_data=(x_valid, y_valid))\n",
    "\n",
    "# getting the performance by using the validaiton set \n",
    "Error = model.evaluate(x_train, y_train)\n",
    "print(\"Error for the test set: \", Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting predicted Index prices against realised Index prices\n",
    "predicted_level = model.predict(x_test)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.plot(predicted_level, color = 'orangered', label = 'Prediction')\n",
    "plt.plot(y_test[0:,], color = 'royalblue', label = 'TecDAX')\n",
    "plt.title('TecDAX Index Prediction', fontweight='bold',fontsize=22)\n",
    "plt.xlabel('Predicted days',fontsize=18,color='black')\n",
    "plt.ylabel('Price',fontsize=18,color='black')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualising overfitting\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss_values = np.log(history_dict['val_loss'])\n",
    "acc = history_dict['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.plot(epochs, loss_values, color = 'orangered', label = 'Logarithmic Loss')\n",
    "plt.title('Adam - log losses', fontweight='bold',fontsize=22)\n",
    "plt.xlabel('Epochs',fontsize=18,color='black')\n",
    "plt.ylabel('Loss',fontsize=18,color='black')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Additive Model Forecasting\n",
    "---\n",
    "\n",
    "In the scope of this report, forecasting is the attempt to predict how future prices will develop. We can distinguish between two major approaches in this matter, namely explanatory and time-series forcasting. While explanatory approaches aim to extract cause and effect relationships between input factors and output factors, time-series forecasting does not concentrate on understanding these relationships reather than finding patterns in past price realisations. There are two reasons for undertaking time-series forcasting. First, the system is too complex to be completely understood or the relationships that affect the output are to difficult to measure. Second, the main concern is not to understand why the output develops in a certain way but how it develops.\n",
    "\n",
    "Not all forecasting problems can be solved by the same procedure. Prophet is optimized for business forecasting tasks, which have the following characteristics: hourly, daily, or weekly observations with at least a few months (preferably multiple years) of history. At its core, the Prophet procedure is an additive regression model with three main components:\n",
    "\n",
    " - A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data.\n",
    " - A yearly seasonal component modeled using Fourier series.\n",
    " - A weekly seasonal component using dummy variables.\n",
    " \n",
    "We use the TecDAX Index dataset to train the Prophet Model. However we seperate the observations of the year 2018 and the first half of 2019 into a test set. After training the Prophet Model we predict 1,5 years of the Index Price and compare it to the realised observations in the test set. The prediction results will be shown as a solid blue line with an 80% confidence intervall(light blue area). In orange color we represent the true Index prices realised during the year 2018 and the first half of 2019.\n",
    "\n",
    "One of the biggest upsides of the Prophet Model is that it does not only predict a possible realisation of a price at a certain time in the future, but also predicts the prices with a 80% confidence intervall. In the context of Stock or Index price realisations, confidence intervalls are an essential feature of forcasting models, since future price realisations are highly uncertain no matter how good the model is. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>trend</th>\n",
       "      <th>yhat_lower</th>\n",
       "      <th>yhat_upper</th>\n",
       "      <th>trend_lower</th>\n",
       "      <th>trend_upper</th>\n",
       "      <th>additive_terms</th>\n",
       "      <th>additive_terms_lower</th>\n",
       "      <th>additive_terms_upper</th>\n",
       "      <th>self_define_cycle</th>\n",
       "      <th>self_define_cycle_lower</th>\n",
       "      <th>self_define_cycle_upper</th>\n",
       "      <th>multiplicative_terms</th>\n",
       "      <th>multiplicative_terms_lower</th>\n",
       "      <th>multiplicative_terms_upper</th>\n",
       "      <th>yhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>2019-05-24</td>\n",
       "      <td>2866.524379</td>\n",
       "      <td>2455.703798</td>\n",
       "      <td>3285.374903</td>\n",
       "      <td>2463.381531</td>\n",
       "      <td>3282.977951</td>\n",
       "      <td>-1.943778</td>\n",
       "      <td>-1.943778</td>\n",
       "      <td>-1.943778</td>\n",
       "      <td>-1.943778</td>\n",
       "      <td>-1.943778</td>\n",
       "      <td>-1.943778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2864.580601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>2867.520741</td>\n",
       "      <td>2453.174844</td>\n",
       "      <td>3281.925824</td>\n",
       "      <td>2463.437083</td>\n",
       "      <td>3285.816413</td>\n",
       "      <td>-2.194547</td>\n",
       "      <td>-2.194547</td>\n",
       "      <td>-2.194547</td>\n",
       "      <td>-2.194547</td>\n",
       "      <td>-2.194547</td>\n",
       "      <td>-2.194547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2865.326194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>2019-05-26</td>\n",
       "      <td>2868.517103</td>\n",
       "      <td>2446.117007</td>\n",
       "      <td>3290.219546</td>\n",
       "      <td>2463.534902</td>\n",
       "      <td>3288.654875</td>\n",
       "      <td>-3.054872</td>\n",
       "      <td>-3.054872</td>\n",
       "      <td>-3.054872</td>\n",
       "      <td>-3.054872</td>\n",
       "      <td>-3.054872</td>\n",
       "      <td>-3.054872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2865.462231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>2869.513465</td>\n",
       "      <td>2465.454991</td>\n",
       "      <td>3302.846664</td>\n",
       "      <td>2463.508045</td>\n",
       "      <td>3291.213592</td>\n",
       "      <td>-3.828502</td>\n",
       "      <td>-3.828502</td>\n",
       "      <td>-3.828502</td>\n",
       "      <td>-3.828502</td>\n",
       "      <td>-3.828502</td>\n",
       "      <td>-3.828502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2865.684963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>2019-05-28</td>\n",
       "      <td>2870.509827</td>\n",
       "      <td>2446.775219</td>\n",
       "      <td>3310.775138</td>\n",
       "      <td>2462.523341</td>\n",
       "      <td>3293.531656</td>\n",
       "      <td>-1.224318</td>\n",
       "      <td>-1.224318</td>\n",
       "      <td>-1.224318</td>\n",
       "      <td>-1.224318</td>\n",
       "      <td>-1.224318</td>\n",
       "      <td>-1.224318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2869.285509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds        trend   yhat_lower   yhat_upper  trend_lower  \\\n",
       "3297 2019-05-24  2866.524379  2455.703798  3285.374903  2463.381531   \n",
       "3298 2019-05-25  2867.520741  2453.174844  3281.925824  2463.437083   \n",
       "3299 2019-05-26  2868.517103  2446.117007  3290.219546  2463.534902   \n",
       "3300 2019-05-27  2869.513465  2465.454991  3302.846664  2463.508045   \n",
       "3301 2019-05-28  2870.509827  2446.775219  3310.775138  2462.523341   \n",
       "\n",
       "      trend_upper  additive_terms  additive_terms_lower  additive_terms_upper  \\\n",
       "3297  3282.977951       -1.943778             -1.943778             -1.943778   \n",
       "3298  3285.816413       -2.194547             -2.194547             -2.194547   \n",
       "3299  3288.654875       -3.054872             -3.054872             -3.054872   \n",
       "3300  3291.213592       -3.828502             -3.828502             -3.828502   \n",
       "3301  3293.531656       -1.224318             -1.224318             -1.224318   \n",
       "\n",
       "      self_define_cycle  self_define_cycle_lower  self_define_cycle_upper  \\\n",
       "3297          -1.943778                -1.943778                -1.943778   \n",
       "3298          -2.194547                -2.194547                -2.194547   \n",
       "3299          -3.054872                -3.054872                -3.054872   \n",
       "3300          -3.828502                -3.828502                -3.828502   \n",
       "3301          -1.224318                -1.224318                -1.224318   \n",
       "\n",
       "      multiplicative_terms  multiplicative_terms_lower  \\\n",
       "3297                   0.0                         0.0   \n",
       "3298                   0.0                         0.0   \n",
       "3299                   0.0                         0.0   \n",
       "3300                   0.0                         0.0   \n",
       "3301                   0.0                         0.0   \n",
       "\n",
       "      multiplicative_terms_upper         yhat  \n",
       "3297                         0.0  2864.580601  \n",
       "3298                         0.0  2865.326194  \n",
       "3299                         0.0  2865.462231  \n",
       "3300                         0.0  2865.684963  \n",
       "3301                         0.0  2869.285509  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  !! fitting the model can take significant amount of time and computation power !!\n",
    "\n",
    "# setting up the model specifications\n",
    "def cycle_analysis(data,split_date,cycle,mode='additive',forecast_plot = False,print_ind=False):\n",
    "    \n",
    "    # splitting the data in training and test set\n",
    "    training = data[:split_date].iloc[:-1,]\n",
    "    testing = data[split_date:]\n",
    "    \n",
    "    # defining the period to predict\n",
    "    predict_period = len(pd.date_range(split_date,max(data.index)))\n",
    "    df = training.reset_index()\n",
    "    df.columns = ['ds','y']\n",
    "    \n",
    "    # exchanging default seasionality with self defined cycle\n",
    "    model = Prophet(weekly_seasonality=False,yearly_seasonality=False,daily_seasonality=False)\n",
    "    model.add_seasonality('self_define_cycle',period=cycle,fourier_order=8,mode=mode)\n",
    "    \n",
    "    # fitting the model\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=predict_period)\n",
    "    forecast = model.predict(future)\n",
    "   \n",
    "    # visualising the results\n",
    "    if forecast_plot:\n",
    "        model.plot(forecast)\n",
    "        plt.plot(testing.index,testing.values,color='orangered',alpha=0.8)\n",
    "        plt.title(\"TecDAX Index - Prophet prediction\",fontweight='bold',fontsize=22)\n",
    "        plt.xlabel('Date',fontsize=18,color='black')\n",
    "        plt.ylabel('Price',fontsize=18,color='black')\n",
    "        plt.show()\n",
    "    ret = max(forecast.self_define_cycle)-min(forecast.self_define_cycle)\n",
    "    model_tb = forecast['yhat']\n",
    "    model_tb.index = forecast['ds'].map(lambda x:x.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    #show model results (last 5 predictions)\n",
    "    return forecast.tail()\n",
    "      \n",
    "# deploying the model to the TecDAX Index data\n",
    "cycle_analysis(TecDAX_close,'2018-01-02',30,forecast_plot=True,print_ind=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Recurrent Neural Network Forecasting\n",
    "---\n",
    "Long Short-Term Memory Models (LSTM Models) are recurrent neural networks that are specifically designed to address the vanishing gradient problem. Neural networks weights change according to the partial derivation of the error function with resepect to the current weight in each iteration of training. Traditionally activation functions have gradients in the range of 0 to 1. With backpropagation computing gradients become too small when multiple layers are used. This can prevent training from working effectively.\n",
    "\n",
    "We train the LSTM Model with the TecDAX Index price data from 01.01.2007 till 30.04.2019. We seperate the price data of May, 2019 in a test set to evaluate the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observations:  3120\n",
      "Testing observations:   23\n"
     ]
    }
   ],
   "source": [
    "# reshaping\n",
    "TecDAX_close = TecDAX_close.values.reshape(len(TecDAX),1)\n",
    "\n",
    "# spliting the data into training and test set\n",
    "train_size = int(len(TecDAX_close) * 0.993)\n",
    "test_size  = len(TecDAX_close) - train_size\n",
    "TecDAX_train, TecDAX_test = TecDAX_close[0:train_size,:], TecDAX_close[train_size:len(TecDAX),:]\n",
    "\n",
    "print(\"Training observations: \", len(TecDAX_train))\n",
    "print(\"Testing observations:  \", len(TecDAX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# normalizing the data\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "TecDAX_train.shape\n",
    "training_set_scaled = sc.fit_transform(TecDAX_train)\n",
    "test_set_scaled     = sc.fit_transform(TecDAX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 0.0037\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 6.5989e-04\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 4.8365e-04\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 4.3539e-04\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 4.1333e-04\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 3.6955e-04\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 3.6678e-04\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 3.2436e-04\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 3.3803e-04\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 3.1953e-04\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 3.1050e-04\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 2.9889e-04\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.7843e-04\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.8584e-04\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 3.1225e-04\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.3824e-04\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.7051e-04\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.6598e-04\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.5112e-04\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.4149e-04\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.3131e-04\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.2890e-04\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 2.0827e-04\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 2.0414e-04\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 2.1043e-04\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 2.1169e-04\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 2.1736e-04\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.9524e-04\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 1.8652e-04\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 1.8253e-04\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 1.7609e-04\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.6957e-04\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.7329e-04\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 1.7361e-04\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 1.6734e-04\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 1.5192e-04\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 1.4567e-04\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 13s 10ms/step - loss: 1.7394e-04\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.5935e-04\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.4380e-04\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.4280e-04\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.6692e-04\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 1.3838e-04\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.4184e-04\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 1.4569e-04\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.3713e-04\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 1.2748e-04\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.4136e-04\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 1.3912e-04\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 1.1956e-04\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.1741e-04\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 1.1534e-04\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 1.1705e-04\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.2839e-04\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 1.1646e-04\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 1.1316e-04\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.1621e-04\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 14s 11ms/step - loss: 1.0349e-04\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 1.1298e-04\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 1.1105e-04\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.2102e-04\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 14s 12ms/step - loss: 1.0365e-04\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 1.0874e-04\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 1.0509e-04\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 9.5782e-05\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 1.0301e-04\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 1.0124e-04\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 8.9225e-05\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 9.3434e-05\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 15s 12ms/step - loss: 9.3915e-05\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 8.8506e-05\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 8.8050e-05\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 9.5418e-05\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 8.2329e-05\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 13s 10ms/step - loss: 8.4781e-05\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 9.4933e-05\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 15s 12ms/step - loss: 8.8546e-05\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 15s 12ms/step - loss: 8.9516e-05\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 8.7750e-05\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 9.1312e-05\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 8.5658e-05\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 1.0228e-04\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 11s 10ms/step - loss: 8.7474e-05\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 7.6196e-05\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 16s 13ms/step - loss: 7.9464e-05\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 8.4086e-05\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 8.3968e-05\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 8.4444e-05\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 12s 10ms/step - loss: 7.8660e-05\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 9.1597e-05\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 13s 11ms/step - loss: 7.8956e-05\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 7.2711e-05\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 14s 12ms/step - loss: 8.3796e-05\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 6.8514e-05\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 14s 12ms/step - loss: 7.5327e-05\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 15s 12ms/step - loss: 7.6247e-05\n",
      "Epoch 97/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 8.0481e-05\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 15s 13ms/step - loss: 9.7568e-05\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 7.9409e-05\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 14s 11ms/step - loss: 7.7981e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3a6e27b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !! fitting the model can take significant amount of time and computation power !!\n",
    "\n",
    "# initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# getting the predicted stock price\n",
    "dataset_total = pd.concat((pd.Series(TecDAX_train[:,0]),\n",
    "                           pd.Series(TecDAX_test[:,0])), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(TecDAX_test) - 60:]\n",
    "inputs = inputs.values.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.plot(TecDAX_test[0:20,:], color = 'royalblue', label = 'TecDAX')\n",
    "plt.plot(predicted_stock_price, color = 'orangered', label = 'Prediction')\n",
    "plt.title('TecDAX - LSTM prediction', fontweight='bold',fontsize=22)\n",
    "plt.xlabel('Predicted days',fontsize=18,color='black')\n",
    "plt.ylabel('Price',fontsize=18,color='black')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Report**. All datasets and code can be found in our GitHub repository:\n",
    "https://github.com/innovationsteam/Index_Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
