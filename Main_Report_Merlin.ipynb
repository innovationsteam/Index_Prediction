{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Finance and Financial Management\n",
    "\n",
    "\n",
    "## Time-series analysis, replication and forecasting of the German TecDAX stock index\n",
    "\n",
    "By Merlin Bartel and Max Veltwisch\n",
    "\n",
    "Trying to predict the stock market is an enticing prospect to us, not so much as a desire for material gain, but for the challenge. Already in 1965 the Economist and Nobel Price winner Paul Samuelson postulated in his influencial paper: \"Properly Anticipated Prices Fluctuate Randomly\" that returns of the past have no prediction power for future returns. He concluded that market prices could be considered a Martingale. This implies that the analysis and knownledge of historical price development is of no use for future price predictions. Nevertheless by looking at the daily up and downs of the market, we imagine there must be patterns we can model to beat all odds. Our analysis follows three main sections:\n",
    "\n",
    "1. Exploratory analysis on the TecDAX Index price data\n",
    "2. Applied Deep-Learning method to predict stock weights of the TecDAX index.\n",
    "3. Forecasting the TecDAX price development using the Facebook Prophet model.\n",
    "4. Forecasting the TecDAX price development using a LSTM model approach.\n",
    "\n",
    "The first section serves the goal to better understand the general structure and charateristics of the historical TecDAX Index price development. Section two differs from the following sections by trying to estimate the weights of the stocks contained in the TecDAX Index in a way that replication becomes as precise as possible. The following sections make use of different model approaches with the goal of recognising patterns in the historical price movements of the TecDAX Index and using them to forecast future Index Prices.\n",
    "\n",
    "\n",
    "Before we start with our research we load the required python packeges into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading required packages\n",
    "import math\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import optimizers\n",
    "\n",
    "import statistics\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from scipy import special, optimize\n",
    "\n",
    "# from fbprophet import Prophet\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploratory analysis on the TecDAX index price data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first want to analyse patterns and characteristics in the historically realized price development of the TecDAX Index. We are looking at the adjusted close prices of the TecDax Index from 01.01.2007 till 01.06.2019. The source of the TecDAX dataset and all following datasets is: finance.yahoo.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing TecDAX Index price data\n",
    "# TecDAX = pd.read_csv(\"TECDAX.csv\", index_col=0, parse_dates=True, squeeze=True)\n",
    "# # extracting adjusted close prices (variable of interest)\n",
    "# TecDAX = TecDAX[\"Adj Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # showing general statisitcal values\n",
    "# TecDAX.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # plotting historical price movement\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# TecDAX.plot(figsize=(16,8), color='royalblue')\n",
    "# plt.xlabel(\"Date\",fontsize=18,color='black')\n",
    "# plt.ylabel('Price',fontsize=18,color='black')\n",
    "# plt.title(\"TecDAX Index\",fontweight='bold',fontsize=22)\n",
    "# plt.xticks(rotation=0)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # calculating the daily returns\n",
    "# daily_returns = TecDAX.shift(1) / TecDAX - 1\n",
    "\n",
    "# # visualising the daily returns\n",
    "# daily_returns.plot(figsize=(16,8), color='royalblue')\n",
    "# plt.xlabel(\"Date\",fontsize=18,color='black')\n",
    "# plt.ylabel(\"Returns\",fontsize=18,color='black')\n",
    "# plt.title(\"TecDAX Index daily returns\", fontweight='bold',fontsize=22)\n",
    "# plt.xticks(rotation=0)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # computing the first-difference\n",
    "# training_diff = TecDAX.diff()\n",
    "\n",
    "# # visualising first-difference\n",
    "# training_diff.plot(figsize=(16,8), color='royalblue')\n",
    "# plt.xlabel(\"Date\",fontsize=18,color='black')\n",
    "# plt.ylabel(\"Difference\",fontsize=18,color='black')\n",
    "# plt.title(\"TecDAX Index first difference\", fontweight='bold',fontsize=22)\n",
    "# plt.xticks(rotation=0)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep-Learning method to predict stock weights of the TecDAX index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and indexing data\n",
    "data = pd.read_csv(\"Alltogether.csv\", encoding='latin-1')  #Adj. Closing prices of Yahoo, instead any other dataset can be used \n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "data.set_index(\"Date\", inplace=True)  \n",
    "\n",
    "# Split TecDAX-Data from Stock-Data\n",
    "y = data[\"TecDAX\"]\n",
    "y = y.values.reshape(len(y),1)\n",
    "x = data.iloc [0:,1:]\n",
    "x = x.values.reshape(len(y),41)\n",
    "\n",
    "# Defining prediction horizon\n",
    "test_size = 27\n",
    "train_size  = len(y) - test_size\n",
    "\n",
    "# Split training and test\n",
    "y_train, y_test = y[0:train_size], y[train_size:len(y)]\n",
    "x_train, x_test = x[0:train_size,:], x[train_size:len(y),:]\n",
    "\n",
    "\n",
    "# Split Validation Set from training Set\n",
    "\n",
    "# Define size\n",
    "train_size2  = int(train_size * 0.7)\n",
    "\n",
    "# Create Buffer-Variable to combine TecDAX with underlying stocks\n",
    "buff = np.append(y_train, x_train, axis =1)\n",
    "\n",
    "# Randomize Sample and Split TecDAX from Stocks again\n",
    "np.random.shuffle(buff)\n",
    "y_buff = buff[:,0]\n",
    "x_buff = buff[:,1:]\n",
    "np.reshape(x_buff, (len(x_buff),41))\n",
    "np.reshape(y_buff, (len(x_buff),1))\n",
    "\n",
    "# Rearrange original Data\n",
    "y_train, y_valid = y_buff[0:train_size2], y_buff[train_size2:train_size]\n",
    "x_train, x_valid = x_buff[0:train_size2,:], x_buff[train_size2:train_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "def index_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    model.add(Dense(1,activation='linear',input_dim=x_train.shape[1])) #the activation function can be linear, since the network consists of only 1 neuron\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1)) # Scalar Regression -> No Actication Function\n",
    "    \n",
    "    # Compile the model, define optimizer and loss function\n",
    "    model.compile(optimizer='Adam',loss='mean_absolute_error') #We implemented the best choices of both in this code\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 685 samples, validate on 294 samples\n",
      "Epoch 1/20000\n",
      "685/685 [==============================] - 0s 525us/step - loss: 2186.0843 - val_loss: 2149.7347\n",
      "Epoch 2/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 2151.7430 - val_loss: 2114.4192\n",
      "Epoch 3/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 2114.7923 - val_loss: 2076.6960\n",
      "Epoch 4/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 2075.5199 - val_loss: 2036.1158\n",
      "Epoch 5/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 2033.2949 - val_loss: 1992.8669\n",
      "Epoch 6/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1988.4107 - val_loss: 1946.7510\n",
      "Epoch 7/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1940.4194 - val_loss: 1898.0397\n",
      "Epoch 8/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 1889.9074 - val_loss: 1846.0871\n",
      "Epoch 9/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1836.2227 - val_loss: 1791.4416\n",
      "Epoch 10/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1779.6886 - val_loss: 1733.5721\n",
      "Epoch 11/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1719.8634 - val_loss: 1672.8735\n",
      "Epoch 12/20000\n",
      "685/685 [==============================] - 0s 137us/step - loss: 1657.0494 - val_loss: 1609.2599\n",
      "Epoch 13/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 1591.3856 - val_loss: 1542.7150\n",
      "Epoch 14/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1522.9408 - val_loss: 1473.2529\n",
      "Epoch 15/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1451.3682 - val_loss: 1400.7662\n",
      "Epoch 16/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1376.8016 - val_loss: 1325.0869\n",
      "Epoch 17/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1298.9938 - val_loss: 1246.9076\n",
      "Epoch 18/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1218.5461 - val_loss: 1165.1567\n",
      "Epoch 19/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1134.8027 - val_loss: 1080.7276\n",
      "Epoch 20/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 1047.9234 - val_loss: 993.6721\n",
      "Epoch 21/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 958.7290 - val_loss: 902.6938\n",
      "Epoch 22/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 865.9210 - val_loss: 809.5845\n",
      "Epoch 23/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 770.4944 - val_loss: 713.6205\n",
      "Epoch 24/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 672.0434 - val_loss: 614.6157\n",
      "Epoch 25/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 570.6042 - val_loss: 512.3250\n",
      "Epoch 26/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 466.1577 - val_loss: 407.3869\n",
      "Epoch 27/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 358.5953 - val_loss: 299.1308\n",
      "Epoch 28/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 248.3142 - val_loss: 188.0408\n",
      "Epoch 29/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 135.1791 - val_loss: 78.3811\n",
      "Epoch 30/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 53.6327 - val_loss: 43.6290\n",
      "Epoch 31/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 41.6479 - val_loss: 40.8890\n",
      "Epoch 32/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 37.9119 - val_loss: 38.0353\n",
      "Epoch 33/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 34.6772 - val_loss: 35.6411\n",
      "Epoch 34/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 32.1153 - val_loss: 33.7946\n",
      "Epoch 35/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 30.2663 - val_loss: 32.3298\n",
      "Epoch 36/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 28.9600 - val_loss: 31.2734\n",
      "Epoch 37/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 27.9005 - val_loss: 30.5151\n",
      "Epoch 38/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 27.1592 - val_loss: 30.0338\n",
      "Epoch 39/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 26.7183 - val_loss: 29.6299\n",
      "Epoch 40/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 26.2246 - val_loss: 29.2736\n",
      "Epoch 41/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 25.9293 - val_loss: 29.0167\n",
      "Epoch 42/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 25.6387 - val_loss: 28.7012\n",
      "Epoch 43/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 25.3801 - val_loss: 28.4703\n",
      "Epoch 44/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 25.1870 - val_loss: 28.3806\n",
      "Epoch 45/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 24.9523 - val_loss: 28.1330\n",
      "Epoch 46/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 24.7589 - val_loss: 27.9722\n",
      "Epoch 47/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 24.5887 - val_loss: 27.8460\n",
      "Epoch 48/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 24.6436 - val_loss: 27.7287\n",
      "Epoch 49/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 24.4585 - val_loss: 27.6919\n",
      "Epoch 50/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 24.3014 - val_loss: 27.4964\n",
      "Epoch 51/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 24.1682 - val_loss: 27.3521\n",
      "Epoch 52/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 24.0938 - val_loss: 27.2515\n",
      "Epoch 53/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 23.9182 - val_loss: 27.1271\n",
      "Epoch 54/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 23.8368 - val_loss: 27.0957\n",
      "Epoch 55/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 23.7412 - val_loss: 26.9094\n",
      "Epoch 56/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 23.6340 - val_loss: 26.8257\n",
      "Epoch 57/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 23.5133 - val_loss: 26.7489\n",
      "Epoch 58/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 23.4495 - val_loss: 26.6586\n",
      "Epoch 59/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 23.3195 - val_loss: 26.5324\n",
      "Epoch 60/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 23.2721 - val_loss: 26.4606\n",
      "Epoch 61/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 23.1600 - val_loss: 26.2991\n",
      "Epoch 62/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 23.0438 - val_loss: 26.2190\n",
      "Epoch 63/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 23.0083 - val_loss: 26.1573\n",
      "Epoch 64/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 22.9150 - val_loss: 26.0196\n",
      "Epoch 65/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 22.9564 - val_loss: 25.9276\n",
      "Epoch 66/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 22.6782 - val_loss: 25.8038\n",
      "Epoch 67/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 22.6460 - val_loss: 25.7058\n",
      "Epoch 68/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 22.4905 - val_loss: 25.6636\n",
      "Epoch 69/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 22.3969 - val_loss: 25.4935\n",
      "Epoch 70/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 22.3658 - val_loss: 25.4833\n",
      "Epoch 71/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 22.2514 - val_loss: 25.2980\n",
      "Epoch 72/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 22.1455 - val_loss: 25.2035\n",
      "Epoch 73/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 22.0837 - val_loss: 25.1578\n",
      "Epoch 74/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 21.9996 - val_loss: 24.9927\n",
      "Epoch 75/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 21.9385 - val_loss: 24.8962\n",
      "Epoch 76/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 21.8066 - val_loss: 24.9257\n",
      "Epoch 77/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 21.7222 - val_loss: 24.6987\n",
      "Epoch 78/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 21.6701 - val_loss: 24.7890\n",
      "Epoch 79/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 21.6533 - val_loss: 24.6059\n",
      "Epoch 80/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 21.4477 - val_loss: 24.6362\n",
      "Epoch 81/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 21.5463 - val_loss: 24.4081\n",
      "Epoch 82/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 21.2121 - val_loss: 24.1900\n",
      "Epoch 83/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 21.1561 - val_loss: 24.1796\n",
      "Epoch 84/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 21.1389 - val_loss: 23.9917\n",
      "Epoch 85/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 21.0551 - val_loss: 23.9237\n",
      "Epoch 86/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 21.0731 - val_loss: 23.8763\n",
      "Epoch 87/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.7947 - val_loss: 23.7270\n",
      "Epoch 88/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.7485 - val_loss: 23.7621\n",
      "Epoch 89/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 20.9369 - val_loss: 23.7044\n",
      "Epoch 90/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.5662 - val_loss: 23.4346\n",
      "Epoch 91/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.4851 - val_loss: 23.3929\n",
      "Epoch 92/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.3552 - val_loss: 23.4117\n",
      "Epoch 93/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.4064 - val_loss: 23.1744\n",
      "Epoch 94/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 20.1760 - val_loss: 22.9931\n",
      "Epoch 95/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 20.1675 - val_loss: 22.8765\n",
      "Epoch 96/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.1000 - val_loss: 22.9093\n",
      "Epoch 97/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 20.0278 - val_loss: 22.7434\n",
      "Epoch 98/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 19.7906 - val_loss: 22.6798\n",
      "Epoch 99/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.8746 - val_loss: 22.5715\n",
      "Epoch 100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.7796 - val_loss: 22.4561\n",
      "Epoch 101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.6354 - val_loss: 22.4301\n",
      "Epoch 102/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 19.6018 - val_loss: 22.2561\n",
      "Epoch 103/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.4532 - val_loss: 22.1093\n",
      "Epoch 104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.5900 - val_loss: 22.4047\n",
      "Epoch 105/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 19.3546 - val_loss: 21.9543\n",
      "Epoch 106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.2443 - val_loss: 21.9739\n",
      "Epoch 107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.2548 - val_loss: 21.7545\n",
      "Epoch 108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.0882 - val_loss: 21.7731\n",
      "Epoch 109/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 19.0232 - val_loss: 21.6231\n",
      "Epoch 110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.0077 - val_loss: 21.5816\n",
      "Epoch 111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.1034 - val_loss: 21.3743\n",
      "Epoch 112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 19.0100 - val_loss: 21.2550\n",
      "Epoch 113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.7589 - val_loss: 21.2475\n",
      "Epoch 114/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.7058 - val_loss: 21.2855\n",
      "Epoch 115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.8865 - val_loss: 21.0332\n",
      "Epoch 116/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 18.6233 - val_loss: 20.9438\n",
      "Epoch 117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.5143 - val_loss: 21.0143\n",
      "Epoch 118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.4135 - val_loss: 20.8308\n",
      "Epoch 119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.3337 - val_loss: 20.7294\n",
      "Epoch 120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.3248 - val_loss: 20.8284\n",
      "Epoch 121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.2528 - val_loss: 20.7215\n",
      "Epoch 122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.3303 - val_loss: 20.6111\n",
      "Epoch 123/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 18.1558 - val_loss: 20.4844\n",
      "Epoch 124/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 18.0595 - val_loss: 20.5353\n",
      "Epoch 125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 18.0294 - val_loss: 21.3604\n",
      "Epoch 126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 18.1272 - val_loss: 20.2339\n",
      "Epoch 127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.9448 - val_loss: 20.1431\n",
      "Epoch 128/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 17.8768 - val_loss: 20.0659\n",
      "Epoch 129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.7063 - val_loss: 20.3556\n",
      "Epoch 130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.8690 - val_loss: 19.9595\n",
      "Epoch 131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.8475 - val_loss: 19.8859\n",
      "Epoch 132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.7236 - val_loss: 19.9878\n",
      "Epoch 133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.5594 - val_loss: 19.9034\n",
      "Epoch 134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 17.5327 - val_loss: 19.7154\n",
      "Epoch 135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.4212 - val_loss: 19.6494\n",
      "Epoch 136/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 17.6240 - val_loss: 19.6294\n",
      "Epoch 137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.5507 - val_loss: 19.5267\n",
      "Epoch 138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.3290 - val_loss: 19.6356\n",
      "Epoch 139/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 17.4635 - val_loss: 19.4939\n",
      "Epoch 140/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 17.1930 - val_loss: 19.4311\n",
      "Epoch 141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.1585 - val_loss: 19.3171\n",
      "Epoch 142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 17.2979 - val_loss: 19.2874\n",
      "Epoch 143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.0632 - val_loss: 20.0353\n",
      "Epoch 144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.1253 - val_loss: 19.1902\n",
      "Epoch 145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.9989 - val_loss: 19.1207\n",
      "Epoch 146/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 17.1599 - val_loss: 19.2005\n",
      "Epoch 147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 17.2154 - val_loss: 19.4080\n",
      "Epoch 148/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.9269 - val_loss: 19.0374\n",
      "Epoch 149/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.9258 - val_loss: 19.0212\n",
      "Epoch 150/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.8310 - val_loss: 18.9744\n",
      "Epoch 151/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.7992 - val_loss: 18.9234\n",
      "Epoch 152/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 16.8686 - val_loss: 18.9444\n",
      "Epoch 153/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.7867 - val_loss: 19.3543\n",
      "Epoch 154/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.6702 - val_loss: 19.0401\n",
      "Epoch 155/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.7822 - val_loss: 18.6808\n",
      "Epoch 156/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.5419 - val_loss: 18.6339\n",
      "Epoch 157/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.6899 - val_loss: 18.6068\n",
      "Epoch 158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 16.5002 - val_loss: 18.5359\n",
      "Epoch 159/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.5284 - val_loss: 18.5483\n",
      "Epoch 160/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.3921 - val_loss: 18.6515\n",
      "Epoch 161/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.4265 - val_loss: 18.8675\n",
      "Epoch 162/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.5824 - val_loss: 18.7784\n",
      "Epoch 163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 16.2817 - val_loss: 18.3762\n",
      "Epoch 164/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.3029 - val_loss: 18.3586\n",
      "Epoch 165/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.3790 - val_loss: 18.2839\n",
      "Epoch 166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 16.4883 - val_loss: 18.1981\n",
      "Epoch 167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 16.9082 - val_loss: 18.5264\n",
      "Epoch 168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 16.3146 - val_loss: 19.2153\n",
      "Epoch 169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 16.4681 - val_loss: 18.0673\n",
      "Epoch 170/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.1437 - val_loss: 18.2737\n",
      "Epoch 171/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.0601 - val_loss: 18.0155\n",
      "Epoch 172/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.9713 - val_loss: 18.0785\n",
      "Epoch 173/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.0316 - val_loss: 17.9512\n",
      "Epoch 174/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.9517 - val_loss: 17.9195\n",
      "Epoch 175/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 16.0248 - val_loss: 18.2555\n",
      "Epoch 176/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 16.0395 - val_loss: 17.8192\n",
      "Epoch 177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.7773 - val_loss: 17.8500\n",
      "Epoch 178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.7427 - val_loss: 17.8506\n",
      "Epoch 179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.8726 - val_loss: 17.6574\n",
      "Epoch 180/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.8069 - val_loss: 18.1101\n",
      "Epoch 181/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.8491 - val_loss: 18.4091\n",
      "Epoch 182/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.8298 - val_loss: 17.6016\n",
      "Epoch 183/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.7284 - val_loss: 17.5212\n",
      "Epoch 184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.6504 - val_loss: 17.4821\n",
      "Epoch 185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.5321 - val_loss: 17.4675\n",
      "Epoch 186/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.5625 - val_loss: 17.4310\n",
      "Epoch 187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.6342 - val_loss: 17.4165\n",
      "Epoch 188/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.8003 - val_loss: 17.3401\n",
      "Epoch 189/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.6995 - val_loss: 17.6401\n",
      "Epoch 190/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.6526 - val_loss: 17.6807\n",
      "Epoch 191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.3380 - val_loss: 17.8602\n",
      "Epoch 192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.5177 - val_loss: 17.1676\n",
      "Epoch 193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.3598 - val_loss: 17.1399\n",
      "Epoch 194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.3762 - val_loss: 18.2854\n",
      "Epoch 195/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.5898 - val_loss: 17.1609\n",
      "Epoch 196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.4196 - val_loss: 17.3637\n",
      "Epoch 197/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.4740 - val_loss: 17.1011\n",
      "Epoch 198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.2083 - val_loss: 17.2793\n",
      "Epoch 199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.1776 - val_loss: 17.4841\n",
      "Epoch 200/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 15.1873 - val_loss: 16.8674\n",
      "Epoch 201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.0730 - val_loss: 16.8438\n",
      "Epoch 202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.0691 - val_loss: 16.9917\n",
      "Epoch 203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.9636 - val_loss: 16.9885\n",
      "Epoch 204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 15.0508 - val_loss: 17.0531\n",
      "Epoch 205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.9103 - val_loss: 16.8052\n",
      "Epoch 206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.8569 - val_loss: 16.6724\n",
      "Epoch 207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.8618 - val_loss: 16.6329\n",
      "Epoch 208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.8772 - val_loss: 16.7824\n",
      "Epoch 209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.9013 - val_loss: 16.6870\n",
      "Epoch 210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.7894 - val_loss: 16.8842\n",
      "Epoch 211/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.8625 - val_loss: 16.7783\n",
      "Epoch 212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.7841 - val_loss: 17.4627\n",
      "Epoch 213/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.9362 - val_loss: 16.9523\n",
      "Epoch 214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.5562 - val_loss: 16.7629\n",
      "Epoch 215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.8348 - val_loss: 16.6347\n",
      "Epoch 216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.6493 - val_loss: 16.7152\n",
      "Epoch 217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.6440 - val_loss: 17.1582\n",
      "Epoch 218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.8295 - val_loss: 16.7619\n",
      "Epoch 219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.6804 - val_loss: 16.6903\n",
      "Epoch 220/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.5746 - val_loss: 16.2271\n",
      "Epoch 221/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.5874 - val_loss: 16.3837\n",
      "Epoch 222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.7242 - val_loss: 16.4155\n",
      "Epoch 223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.4597 - val_loss: 16.1250\n",
      "Epoch 224/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.4127 - val_loss: 16.4810\n",
      "Epoch 225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.5673 - val_loss: 17.0394\n",
      "Epoch 226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.5742 - val_loss: 16.1375\n",
      "Epoch 227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.3546 - val_loss: 16.9059\n",
      "Epoch 228/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 91us/step - loss: 14.9600 - val_loss: 16.9024\n",
      "Epoch 229/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.6091 - val_loss: 16.0732\n",
      "Epoch 230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.2714 - val_loss: 16.8118\n",
      "Epoch 231/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 14.6306 - val_loss: 16.2074\n",
      "Epoch 232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.4805 - val_loss: 15.8999\n",
      "Epoch 233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.2476 - val_loss: 15.7983\n",
      "Epoch 234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.3424 - val_loss: 15.7759\n",
      "Epoch 235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.1733 - val_loss: 15.9068\n",
      "Epoch 236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.1690 - val_loss: 15.7267\n",
      "Epoch 237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.1605 - val_loss: 15.6841\n",
      "Epoch 238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.0936 - val_loss: 15.6995\n",
      "Epoch 239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.6263 - val_loss: 16.4130\n",
      "Epoch 240/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.3664 - val_loss: 15.6103\n",
      "Epoch 241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.0643 - val_loss: 15.6985\n",
      "Epoch 242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.0608 - val_loss: 15.7985\n",
      "Epoch 243/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 14.0261 - val_loss: 15.8285\n",
      "Epoch 244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.9525 - val_loss: 15.6551\n",
      "Epoch 245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.9180 - val_loss: 16.0612\n",
      "Epoch 246/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.9221 - val_loss: 15.4458\n",
      "Epoch 247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.8848 - val_loss: 15.4487\n",
      "Epoch 248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.8269 - val_loss: 15.3946\n",
      "Epoch 249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.7433 - val_loss: 16.0259\n",
      "Epoch 250/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.8108 - val_loss: 15.3599\n",
      "Epoch 251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.0708 - val_loss: 15.3386\n",
      "Epoch 252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.7126 - val_loss: 15.5514\n",
      "Epoch 253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.7594 - val_loss: 15.5003\n",
      "Epoch 254/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.6900 - val_loss: 15.2611\n",
      "Epoch 255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.6131 - val_loss: 15.2798\n",
      "Epoch 256/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.8349 - val_loss: 15.4640\n",
      "Epoch 257/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.9301 - val_loss: 15.2242\n",
      "Epoch 258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.7309 - val_loss: 15.1626\n",
      "Epoch 259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5215 - val_loss: 15.1133\n",
      "Epoch 260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5336 - val_loss: 15.2315\n",
      "Epoch 261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5979 - val_loss: 15.3268\n",
      "Epoch 262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5918 - val_loss: 15.2750\n",
      "Epoch 263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5057 - val_loss: 15.0699\n",
      "Epoch 264/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.6149 - val_loss: 16.1797\n",
      "Epoch 265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.6862 - val_loss: 15.2609\n",
      "Epoch 266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.6522 - val_loss: 15.6839\n",
      "Epoch 267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5781 - val_loss: 15.3104\n",
      "Epoch 268/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.6008 - val_loss: 15.0106\n",
      "Epoch 269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.4032 - val_loss: 14.9321\n",
      "Epoch 270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.3642 - val_loss: 15.2541\n",
      "Epoch 271/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.2802 - val_loss: 15.4046\n",
      "Epoch 272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5046 - val_loss: 15.5143\n",
      "Epoch 273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.4661 - val_loss: 16.0149\n",
      "Epoch 274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5168 - val_loss: 14.9732\n",
      "Epoch 275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.2341 - val_loss: 14.8212\n",
      "Epoch 276/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.3202 - val_loss: 14.8146\n",
      "Epoch 277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.1575 - val_loss: 15.2993\n",
      "Epoch 278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 14.2999 - val_loss: 17.0144\n",
      "Epoch 279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.7821 - val_loss: 15.2812\n",
      "Epoch 280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.3131 - val_loss: 14.7434\n",
      "Epoch 281/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.3237 - val_loss: 15.4571\n",
      "Epoch 282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.3377 - val_loss: 14.6174\n",
      "Epoch 283/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.0993 - val_loss: 14.8856\n",
      "Epoch 284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.2319 - val_loss: 14.5515\n",
      "Epoch 285/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 13.1662 - val_loss: 14.6080\n",
      "Epoch 286/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.0479 - val_loss: 14.6194\n",
      "Epoch 287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.0209 - val_loss: 14.5079\n",
      "Epoch 288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.9814 - val_loss: 14.4842\n",
      "Epoch 289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.9727 - val_loss: 14.5085\n",
      "Epoch 290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.5087 - val_loss: 14.4594\n",
      "Epoch 291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.9517 - val_loss: 14.4086\n",
      "Epoch 292/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.8801 - val_loss: 14.3998\n",
      "Epoch 293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.9386 - val_loss: 14.4065\n",
      "Epoch 294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.8769 - val_loss: 14.3639\n",
      "Epoch 295/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.9903 - val_loss: 14.3174\n",
      "Epoch 296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.8432 - val_loss: 14.8991\n",
      "Epoch 297/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.0142 - val_loss: 14.5378\n",
      "Epoch 298/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.8076 - val_loss: 14.4208\n",
      "Epoch 299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.8455 - val_loss: 14.4154\n",
      "Epoch 300/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 12.9523 - val_loss: 14.2342\n",
      "Epoch 301/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 13.0267 - val_loss: 14.2493\n",
      "Epoch 302/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.7687 - val_loss: 14.1913\n",
      "Epoch 303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.8560 - val_loss: 14.1653\n",
      "Epoch 304/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 12.8636 - val_loss: 14.8392\n",
      "Epoch 305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 13.2454 - val_loss: 14.1311\n",
      "Epoch 306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.6580 - val_loss: 14.1106\n",
      "Epoch 307/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.7901 - val_loss: 14.0637\n",
      "Epoch 308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.6126 - val_loss: 14.0609\n",
      "Epoch 309/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.5889 - val_loss: 14.0824\n",
      "Epoch 310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.6256 - val_loss: 14.0885\n",
      "Epoch 311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.5561 - val_loss: 13.9964\n",
      "Epoch 312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.5956 - val_loss: 14.2106\n",
      "Epoch 313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.7296 - val_loss: 13.9955\n",
      "Epoch 314/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.5104 - val_loss: 14.0333\n",
      "Epoch 315/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.6557 - val_loss: 13.9491\n",
      "Epoch 316/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.6021 - val_loss: 13.9091\n",
      "Epoch 317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.8190 - val_loss: 14.1686\n",
      "Epoch 318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.5864 - val_loss: 14.2608\n",
      "Epoch 319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.5311 - val_loss: 13.9124\n",
      "Epoch 320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.8536 - val_loss: 13.8532\n",
      "Epoch 321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.6946 - val_loss: 15.0926\n",
      "Epoch 322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.6025 - val_loss: 14.0240\n",
      "Epoch 323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.4095 - val_loss: 13.8023\n",
      "Epoch 324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.4121 - val_loss: 13.7885\n",
      "Epoch 325/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.4193 - val_loss: 13.8877\n",
      "Epoch 326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.3522 - val_loss: 13.7595\n",
      "Epoch 327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.3212 - val_loss: 13.7539\n",
      "Epoch 328/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 12.4008 - val_loss: 14.0577\n",
      "Epoch 329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.3327 - val_loss: 14.1823\n",
      "Epoch 330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.5521 - val_loss: 14.9180\n",
      "Epoch 331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.3535 - val_loss: 13.6445\n",
      "Epoch 332/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.4692 - val_loss: 13.6083\n",
      "Epoch 333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.4086 - val_loss: 15.7506\n",
      "Epoch 334/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.6996 - val_loss: 13.6323\n",
      "Epoch 335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.2757 - val_loss: 13.5652\n",
      "Epoch 336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.2768 - val_loss: 13.5971\n",
      "Epoch 337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.2248 - val_loss: 13.5408\n",
      "Epoch 338/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.2684 - val_loss: 13.7291\n",
      "Epoch 339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.2286 - val_loss: 13.7185\n",
      "Epoch 340/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.1517 - val_loss: 13.4882\n",
      "Epoch 341/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.1888 - val_loss: 14.0209\n",
      "Epoch 342/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 12.2017 - val_loss: 13.7738\n",
      "Epoch 343/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.1947 - val_loss: 14.3270\n",
      "Epoch 344/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.2606 - val_loss: 13.5063\n",
      "Epoch 345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.0907 - val_loss: 13.7163\n",
      "Epoch 346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.0482 - val_loss: 13.4900\n",
      "Epoch 347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.3085 - val_loss: 13.4965\n",
      "Epoch 348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.1050 - val_loss: 13.3169\n",
      "Epoch 349/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.2082 - val_loss: 13.4508\n",
      "Epoch 350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.0777 - val_loss: 13.3474\n",
      "Epoch 351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.0859 - val_loss: 13.3250\n",
      "Epoch 352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.9887 - val_loss: 13.2623\n",
      "Epoch 353/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.0339 - val_loss: 13.7763\n",
      "Epoch 354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.1740 - val_loss: 13.4798\n",
      "Epoch 355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.1366 - val_loss: 13.2705\n",
      "Epoch 356/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.2142 - val_loss: 13.1881\n",
      "Epoch 357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.9499 - val_loss: 13.2417\n",
      "Epoch 358/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.1795 - val_loss: 13.2588\n",
      "Epoch 359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 12.0468 - val_loss: 13.2518\n",
      "Epoch 360/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 12.0358 - val_loss: 13.7391\n",
      "Epoch 361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.8800 - val_loss: 13.1241\n",
      "Epoch 362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.7786 - val_loss: 13.3786\n",
      "Epoch 363/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.8871 - val_loss: 13.3489\n",
      "Epoch 364/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.8298 - val_loss: 13.3101\n",
      "Epoch 365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.8990 - val_loss: 13.1378\n",
      "Epoch 366/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.8034 - val_loss: 13.0259\n",
      "Epoch 367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.8239 - val_loss: 13.1349\n",
      "Epoch 368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.8345 - val_loss: 13.2566\n",
      "Epoch 369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.7239 - val_loss: 13.0007\n",
      "Epoch 370/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.6956 - val_loss: 12.9886\n",
      "Epoch 371/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 11.7896 - val_loss: 13.0710\n",
      "Epoch 372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.7260 - val_loss: 12.9598\n",
      "Epoch 373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.6587 - val_loss: 13.2720\n",
      "Epoch 374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.6844 - val_loss: 13.3475\n",
      "Epoch 375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.7058 - val_loss: 13.0556\n",
      "Epoch 376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.8044 - val_loss: 12.9017\n",
      "Epoch 377/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.8298 - val_loss: 12.8681\n",
      "Epoch 378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.5458 - val_loss: 12.8599\n",
      "Epoch 379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.5381 - val_loss: 12.8518\n",
      "Epoch 380/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 11.5601 - val_loss: 13.0904\n",
      "Epoch 381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.5527 - val_loss: 13.0806\n",
      "Epoch 382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.6079 - val_loss: 12.8043\n",
      "Epoch 383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.5419 - val_loss: 12.7925\n",
      "Epoch 384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.6490 - val_loss: 12.7806\n",
      "Epoch 385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.5244 - val_loss: 12.7806\n",
      "Epoch 386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.6645 - val_loss: 12.8472\n",
      "Epoch 387/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.8031 - val_loss: 13.7091\n",
      "Epoch 388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.8894 - val_loss: 13.4042\n",
      "Epoch 389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.5239 - val_loss: 12.9933\n",
      "Epoch 390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3548 - val_loss: 13.2120\n",
      "Epoch 391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.5040 - val_loss: 12.8982\n",
      "Epoch 392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.4326 - val_loss: 12.7232\n",
      "Epoch 393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.4463 - val_loss: 12.6585\n",
      "Epoch 394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.4011 - val_loss: 12.6127\n",
      "Epoch 395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3450 - val_loss: 12.7088\n",
      "Epoch 396/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.6922 - val_loss: 12.5909\n",
      "Epoch 397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.4926 - val_loss: 12.8464\n",
      "Epoch 398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3217 - val_loss: 12.6497\n",
      "Epoch 399/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 11.4306 - val_loss: 12.7394\n",
      "Epoch 400/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.3732 - val_loss: 12.5380\n",
      "Epoch 401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3132 - val_loss: 12.5142\n",
      "Epoch 402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.2903 - val_loss: 12.5899\n",
      "Epoch 403/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.2044 - val_loss: 12.7423\n",
      "Epoch 404/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3746 - val_loss: 13.2143\n",
      "Epoch 405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.2879 - val_loss: 12.8933\n",
      "Epoch 406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3566 - val_loss: 12.5106\n",
      "Epoch 407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.2257 - val_loss: 12.4107\n",
      "Epoch 408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3744 - val_loss: 12.9441\n",
      "Epoch 409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1919 - val_loss: 12.5696\n",
      "Epoch 410/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.1531 - val_loss: 12.4417\n",
      "Epoch 411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1769 - val_loss: 12.3757\n",
      "Epoch 412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1016 - val_loss: 12.5617\n",
      "Epoch 413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1431 - val_loss: 12.5409\n",
      "Epoch 414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1795 - val_loss: 12.3606\n",
      "Epoch 415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1367 - val_loss: 12.4597\n",
      "Epoch 416/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.6619 - val_loss: 12.4011\n",
      "Epoch 417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.3825 - val_loss: 12.2818\n",
      "Epoch 418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.2092 - val_loss: 12.2797\n",
      "Epoch 419/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.0705 - val_loss: 12.2335\n",
      "Epoch 420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.0374 - val_loss: 12.2503\n",
      "Epoch 421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1715 - val_loss: 12.2231\n",
      "Epoch 422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.0725 - val_loss: 12.2078\n",
      "Epoch 423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.2420 - val_loss: 12.2049\n",
      "Epoch 424/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.9548 - val_loss: 12.6745\n",
      "Epoch 425/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.2622 - val_loss: 12.2462\n",
      "Epoch 426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.8975 - val_loss: 12.3423\n",
      "Epoch 427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.0621 - val_loss: 12.3050\n",
      "Epoch 428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.9496 - val_loss: 12.1949\n",
      "Epoch 429/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.8782 - val_loss: 12.6532\n",
      "Epoch 430/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 11.1122 - val_loss: 12.3708\n",
      "Epoch 431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.9944 - val_loss: 12.1418\n",
      "Epoch 432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.8758 - val_loss: 12.2031\n",
      "Epoch 433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.8638 - val_loss: 12.0531\n",
      "Epoch 434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.9645 - val_loss: 12.5514\n",
      "Epoch 435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 11.0318 - val_loss: 12.0936\n",
      "Epoch 436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.7944 - val_loss: 12.3897\n",
      "Epoch 437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.8044 - val_loss: 12.0241\n",
      "Epoch 438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.8098 - val_loss: 12.0707\n",
      "Epoch 439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.8807 - val_loss: 12.1800\n",
      "Epoch 440/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.7878 - val_loss: 12.2198\n",
      "Epoch 441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.7931 - val_loss: 12.5528\n",
      "Epoch 442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.9322 - val_loss: 12.1553\n",
      "Epoch 443/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.9867 - val_loss: 11.9512\n",
      "Epoch 444/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.7613 - val_loss: 12.0505\n",
      "Epoch 445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.7197 - val_loss: 12.0375\n",
      "Epoch 446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.6558 - val_loss: 12.1692\n",
      "Epoch 447/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.8598 - val_loss: 12.8344\n",
      "Epoch 448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.9634 - val_loss: 12.0400\n",
      "Epoch 449/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.8728 - val_loss: 11.8739\n",
      "Epoch 450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.6550 - val_loss: 11.8571\n",
      "Epoch 451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.7541 - val_loss: 12.1806\n",
      "Epoch 452/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.6289 - val_loss: 11.8732\n",
      "Epoch 453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.7279 - val_loss: 11.8616\n",
      "Epoch 454/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.6167 - val_loss: 11.8539\n",
      "Epoch 455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.7360 - val_loss: 11.8458\n",
      "Epoch 456/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 10.5889 - val_loss: 11.8588\n",
      "Epoch 457/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.5863 - val_loss: 11.8092\n",
      "Epoch 458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.6082 - val_loss: 11.8694\n",
      "Epoch 459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.6102 - val_loss: 11.8720\n",
      "Epoch 460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.6344 - val_loss: 11.8049\n",
      "Epoch 461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.6510 - val_loss: 11.8401\n",
      "Epoch 462/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.5028 - val_loss: 12.7638\n",
      "Epoch 463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.8761 - val_loss: 11.7483\n",
      "Epoch 464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.5107 - val_loss: 11.7136\n",
      "Epoch 465/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.5080 - val_loss: 11.7053\n",
      "Epoch 466/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.4778 - val_loss: 11.7560\n",
      "Epoch 467/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.5094 - val_loss: 11.8103\n",
      "Epoch 468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.4014 - val_loss: 11.9118\n",
      "Epoch 469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.4610 - val_loss: 11.6387\n",
      "Epoch 470/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.4440 - val_loss: 11.6353\n",
      "Epoch 471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.4408 - val_loss: 11.6565\n",
      "Epoch 472/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.4725 - val_loss: 11.6441\n",
      "Epoch 473/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.5210 - val_loss: 11.7746\n",
      "Epoch 474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.5451 - val_loss: 11.5886\n",
      "Epoch 475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3622 - val_loss: 11.8158\n",
      "Epoch 476/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.5003 - val_loss: 11.6624\n",
      "Epoch 477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.5942 - val_loss: 11.5356\n",
      "Epoch 478/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.5595 - val_loss: 11.7106\n",
      "Epoch 479/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.4407 - val_loss: 11.5284\n",
      "Epoch 480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3416 - val_loss: 11.4971\n",
      "Epoch 481/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.3548 - val_loss: 11.4891\n",
      "Epoch 482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3973 - val_loss: 11.4840\n",
      "Epoch 483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3996 - val_loss: 12.2334\n",
      "Epoch 484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.4257 - val_loss: 11.4461\n",
      "Epoch 485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.3290 - val_loss: 11.5142\n",
      "Epoch 486/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.2579 - val_loss: 11.4461\n",
      "Epoch 487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2802 - val_loss: 11.5319\n",
      "Epoch 488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.4709 - val_loss: 11.4046\n",
      "Epoch 489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2972 - val_loss: 11.4122\n",
      "Epoch 490/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3527 - val_loss: 11.3812\n",
      "Epoch 491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.4967 - val_loss: 11.5242\n",
      "Epoch 492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2030 - val_loss: 11.3647\n",
      "Epoch 493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3504 - val_loss: 11.7459\n",
      "Epoch 494/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2209 - val_loss: 11.4087\n",
      "Epoch 495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2336 - val_loss: 11.3372\n",
      "Epoch 496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2935 - val_loss: 11.3073\n",
      "Epoch 497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2658 - val_loss: 11.2879\n",
      "Epoch 498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.1957 - val_loss: 11.3141\n",
      "Epoch 499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.1910 - val_loss: 11.2823\n",
      "Epoch 500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2388 - val_loss: 11.2602\n",
      "Epoch 501/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.1514 - val_loss: 11.2953\n",
      "Epoch 502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.1615 - val_loss: 11.3288\n",
      "Epoch 503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3455 - val_loss: 11.6582\n",
      "Epoch 504/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.2490 - val_loss: 11.4848\n",
      "Epoch 505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3629 - val_loss: 11.2439\n",
      "Epoch 506/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.1276 - val_loss: 11.8236\n",
      "Epoch 507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.1869 - val_loss: 11.5929\n",
      "Epoch 508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3253 - val_loss: 11.3806\n",
      "Epoch 509/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.3320 - val_loss: 11.2097\n",
      "Epoch 510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.0916 - val_loss: 11.6872\n",
      "Epoch 511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3259 - val_loss: 11.7779\n",
      "Epoch 512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.1685 - val_loss: 11.1638\n",
      "Epoch 513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2041 - val_loss: 11.4880\n",
      "Epoch 514/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.7895 - val_loss: 11.6625\n",
      "Epoch 515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.3020 - val_loss: 11.1505\n",
      "Epoch 516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.0635 - val_loss: 11.2005\n",
      "Epoch 517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2425 - val_loss: 11.7280\n",
      "Epoch 518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.0315 - val_loss: 11.0934\n",
      "Epoch 519/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.9713 - val_loss: 11.0733\n",
      "Epoch 520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.9509 - val_loss: 11.0431\n",
      "Epoch 521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.9360 - val_loss: 11.3171\n",
      "Epoch 522/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2350 - val_loss: 11.5663\n",
      "Epoch 523/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.9474 - val_loss: 11.0127\n",
      "Epoch 524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.0148 - val_loss: 11.0540\n",
      "Epoch 525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.0180 - val_loss: 11.3288\n",
      "Epoch 526/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 11.95 - 0s 46us/step - loss: 10.0243 - val_loss: 11.0162\n",
      "Epoch 527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.9222 - val_loss: 11.0335\n",
      "Epoch 528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2025 - val_loss: 11.3346\n",
      "Epoch 529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.9534 - val_loss: 11.0518\n",
      "Epoch 530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.1131 - val_loss: 11.0128\n",
      "Epoch 531/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.8713 - val_loss: 10.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8965 - val_loss: 10.9196\n",
      "Epoch 533/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 9.503 - 0s 46us/step - loss: 10.0502 - val_loss: 11.6534\n",
      "Epoch 534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.0325 - val_loss: 10.9211\n",
      "Epoch 535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8240 - val_loss: 10.8815\n",
      "Epoch 536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.8067 - val_loss: 11.1056\n",
      "Epoch 537/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.9199 - val_loss: 11.1879\n",
      "Epoch 538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.9857 - val_loss: 10.9597\n",
      "Epoch 539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8922 - val_loss: 10.8586\n",
      "Epoch 540/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8617 - val_loss: 10.8501\n",
      "Epoch 541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.1230 - val_loss: 10.8613\n",
      "Epoch 542/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 10.0154 - val_loss: 11.2732\n",
      "Epoch 543/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.9173 - val_loss: 10.8115\n",
      "Epoch 544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8239 - val_loss: 10.8401\n",
      "Epoch 545/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7954 - val_loss: 10.8337\n",
      "Epoch 546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8282 - val_loss: 10.7843\n",
      "Epoch 547/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 9.7420 - val_loss: 10.7666\n",
      "Epoch 548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7374 - val_loss: 10.7814\n",
      "Epoch 549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8210 - val_loss: 10.8637\n",
      "Epoch 550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8655 - val_loss: 11.1843\n",
      "Epoch 551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7627 - val_loss: 10.7539\n",
      "Epoch 552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7786 - val_loss: 10.7202\n",
      "Epoch 553/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7888 - val_loss: 10.9061\n",
      "Epoch 554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.9073 - val_loss: 10.7095\n",
      "Epoch 555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7512 - val_loss: 10.6829\n",
      "Epoch 556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.6459 - val_loss: 10.7775\n",
      "Epoch 557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.6636 - val_loss: 10.6652\n",
      "Epoch 558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.8833 - val_loss: 10.6757\n",
      "Epoch 559/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6431 - val_loss: 10.6576\n",
      "Epoch 560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7046 - val_loss: 10.7910\n",
      "Epoch 561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6590 - val_loss: 10.6403\n",
      "Epoch 562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.6457 - val_loss: 10.7018\n",
      "Epoch 563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.6439 - val_loss: 10.6430\n",
      "Epoch 564/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 9.7628 - val_loss: 10.6901\n",
      "Epoch 565/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6005 - val_loss: 10.6201\n",
      "Epoch 566/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6447 - val_loss: 10.8233\n",
      "Epoch 567/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.5819 - val_loss: 10.5987\n",
      "Epoch 568/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6789 - val_loss: 10.5783\n",
      "Epoch 569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6416 - val_loss: 10.5668\n",
      "Epoch 570/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.5882 - val_loss: 10.5778\n",
      "Epoch 571/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6510 - val_loss: 10.9607\n",
      "Epoch 572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.6141 - val_loss: 10.6668\n",
      "Epoch 573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.5536 - val_loss: 10.7615\n",
      "Epoch 574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.6943 - val_loss: 10.6616\n",
      "Epoch 575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 10.2281 - val_loss: 10.5448\n",
      "Epoch 576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.5540 - val_loss: 10.8894\n",
      "Epoch 577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7630 - val_loss: 10.5643\n",
      "Epoch 578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.6558 - val_loss: 10.4753\n",
      "Epoch 579/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.4336 - val_loss: 10.7241\n",
      "Epoch 580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.5667 - val_loss: 10.4566\n",
      "Epoch 581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.5500 - val_loss: 10.4485\n",
      "Epoch 582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4952 - val_loss: 10.4433\n",
      "Epoch 583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.4991 - val_loss: 10.4382\n",
      "Epoch 584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.5424 - val_loss: 10.4680\n",
      "Epoch 585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4532 - val_loss: 10.8689\n",
      "Epoch 586/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.4788 - val_loss: 10.5245\n",
      "Epoch 587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4494 - val_loss: 10.9063\n",
      "Epoch 588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.7394 - val_loss: 10.4087\n",
      "Epoch 589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.5773 - val_loss: 10.3916\n",
      "Epoch 590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4972 - val_loss: 10.5161\n",
      "Epoch 591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4512 - val_loss: 10.3662\n",
      "Epoch 592/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6196 - val_loss: 10.5584\n",
      "Epoch 593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4515 - val_loss: 11.2115\n",
      "Epoch 594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4512 - val_loss: 10.3774\n",
      "Epoch 595/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3957 - val_loss: 10.5988\n",
      "Epoch 596/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.4132 - val_loss: 10.5930\n",
      "Epoch 597/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.6271 - val_loss: 10.3173\n",
      "Epoch 598/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3306 - val_loss: 10.3759\n",
      "Epoch 599/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3881 - val_loss: 10.3180\n",
      "Epoch 600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3508 - val_loss: 10.3119\n",
      "Epoch 601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4042 - val_loss: 10.3132\n",
      "Epoch 602/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3567 - val_loss: 10.2708\n",
      "Epoch 603/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3087 - val_loss: 10.2619\n",
      "Epoch 604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4628 - val_loss: 10.3476\n",
      "Epoch 605/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3772 - val_loss: 10.2669\n",
      "Epoch 606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3441 - val_loss: 10.3562\n",
      "Epoch 607/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.4565 - val_loss: 10.3774\n",
      "Epoch 608/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 9.4876 - val_loss: 10.5822\n",
      "Epoch 609/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.4234 - val_loss: 10.2945\n",
      "Epoch 610/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3833 - val_loss: 10.2672\n",
      "Epoch 611/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2739 - val_loss: 10.2087\n",
      "Epoch 612/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.2833 - val_loss: 10.2008\n",
      "Epoch 613/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3757 - val_loss: 10.1886\n",
      "Epoch 614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.5660 - val_loss: 10.4075\n",
      "Epoch 615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3100 - val_loss: 10.4824\n",
      "Epoch 616/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3808 - val_loss: 10.1656\n",
      "Epoch 617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2173 - val_loss: 10.1499\n",
      "Epoch 618/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.2556 - val_loss: 10.2139\n",
      "Epoch 619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1890 - val_loss: 10.1463\n",
      "Epoch 620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2156 - val_loss: 10.1944\n",
      "Epoch 621/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.1905 - val_loss: 10.1092\n",
      "Epoch 622/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.1673 - val_loss: 10.2276\n",
      "Epoch 623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2100 - val_loss: 10.0908\n",
      "Epoch 624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2384 - val_loss: 10.1068\n",
      "Epoch 625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1341 - val_loss: 10.0750\n",
      "Epoch 626/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.2243 - val_loss: 10.0670\n",
      "Epoch 627/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1687 - val_loss: 10.5335\n",
      "Epoch 628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3619 - val_loss: 10.1810\n",
      "Epoch 629/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2231 - val_loss: 10.0822\n",
      "Epoch 630/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3070 - val_loss: 10.1157\n",
      "Epoch 631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.3196 - val_loss: 10.0684\n",
      "Epoch 632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1355 - val_loss: 10.0126\n",
      "Epoch 633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0476 - val_loss: 10.1496\n",
      "Epoch 634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1060 - val_loss: 10.2243\n",
      "Epoch 635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.1083 - val_loss: 9.9922\n",
      "Epoch 636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0929 - val_loss: 10.0520\n",
      "Epoch 637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.3091 - val_loss: 9.9714\n",
      "Epoch 638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1476 - val_loss: 9.9619\n",
      "Epoch 639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1605 - val_loss: 9.9629\n",
      "Epoch 640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1866 - val_loss: 10.5170\n",
      "Epoch 641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.2632 - val_loss: 9.9554\n",
      "Epoch 642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.0546 - val_loss: 10.1599\n",
      "Epoch 643/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.1031 - val_loss: 9.9207\n",
      "Epoch 644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2457 - val_loss: 9.9399\n",
      "Epoch 645/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 9.2327 - val_loss: 9.9151\n",
      "Epoch 646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0460 - val_loss: 9.8964\n",
      "Epoch 647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0749 - val_loss: 10.1946\n",
      "Epoch 648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0004 - val_loss: 10.0542\n",
      "Epoch 649/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.0617 - val_loss: 9.9255\n",
      "Epoch 650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0076 - val_loss: 9.8661\n",
      "Epoch 651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9719 - val_loss: 9.9135\n",
      "Epoch 652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0017 - val_loss: 9.8804\n",
      "Epoch 653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.2270 - val_loss: 10.0583\n",
      "Epoch 654/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.1786 - val_loss: 9.9854\n",
      "Epoch 655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1854 - val_loss: 9.8444\n",
      "Epoch 656/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9429 - val_loss: 9.8129\n",
      "Epoch 657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0231 - val_loss: 9.8364\n",
      "Epoch 658/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 8.9535 - val_loss: 9.8218\n",
      "Epoch 659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9254 - val_loss: 9.8412\n",
      "Epoch 660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.8925 - val_loss: 9.8654\n",
      "Epoch 661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9416 - val_loss: 9.7890\n",
      "Epoch 662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9580 - val_loss: 9.7725\n",
      "Epoch 663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.9342 - val_loss: 9.8900\n",
      "Epoch 664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.8966 - val_loss: 9.7675\n",
      "Epoch 665/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.8828 - val_loss: 10.2069\n",
      "Epoch 666/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.2724 - val_loss: 10.2452\n",
      "Epoch 667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.4828 - val_loss: 9.8935\n",
      "Epoch 668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9499 - val_loss: 9.7256\n",
      "Epoch 669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9088 - val_loss: 9.7148\n",
      "Epoch 670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.8423 - val_loss: 10.3520\n",
      "Epoch 671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9579 - val_loss: 9.7239\n",
      "Epoch 672/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.0441 - val_loss: 9.8261\n",
      "Epoch 673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1562 - val_loss: 9.7520\n",
      "Epoch 674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.8656 - val_loss: 9.6739\n",
      "Epoch 675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9166 - val_loss: 9.6705\n",
      "Epoch 676/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 9.0385 - val_loss: 9.8403\n",
      "Epoch 677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.9220 - val_loss: 9.7706\n",
      "Epoch 678/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0053 - val_loss: 9.6845\n",
      "Epoch 679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0970 - val_loss: 9.6462\n",
      "Epoch 680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9278 - val_loss: 9.6537\n",
      "Epoch 681/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.9050 - val_loss: 9.6575\n",
      "Epoch 682/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 8.8301 - val_loss: 9.8856\n",
      "Epoch 683/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9393 - val_loss: 10.0381\n",
      "Epoch 684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.8582 - val_loss: 9.6470\n",
      "Epoch 685/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 8.9185 - val_loss: 9.7328\n",
      "Epoch 686/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 8.8995 - val_loss: 9.5984\n",
      "Epoch 687/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.8081 - val_loss: 9.5902\n",
      "Epoch 688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6991 - val_loss: 9.7792\n",
      "Epoch 689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.0087 - val_loss: 9.8012\n",
      "Epoch 690/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.8688 - val_loss: 9.6020\n",
      "Epoch 691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7841 - val_loss: 9.5805\n",
      "Epoch 692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.9556 - val_loss: 9.6092\n",
      "Epoch 693/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 8.7796 - val_loss: 9.5695\n",
      "Epoch 694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7178 - val_loss: 9.5462\n",
      "Epoch 695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7960 - val_loss: 9.6235\n",
      "Epoch 696/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.9138 - val_loss: 9.6960\n",
      "Epoch 697/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.8556 - val_loss: 10.9545\n",
      "Epoch 698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 9.1062 - val_loss: 9.6884\n",
      "Epoch 699/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.8064 - val_loss: 9.5331\n",
      "Epoch 700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7350 - val_loss: 9.6243\n",
      "Epoch 701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6865 - val_loss: 9.5738\n",
      "Epoch 702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7489 - val_loss: 9.7658\n",
      "Epoch 703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7684 - val_loss: 9.5285\n",
      "Epoch 704/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.6982 - val_loss: 9.4710\n",
      "Epoch 705/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 8.6836 - val_loss: 9.5453\n",
      "Epoch 706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7815 - val_loss: 9.7086\n",
      "Epoch 707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7438 - val_loss: 9.4762\n",
      "Epoch 708/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6588 - val_loss: 9.5039\n",
      "Epoch 709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6511 - val_loss: 9.4883\n",
      "Epoch 710/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.6686 - val_loss: 9.4482\n",
      "Epoch 711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6628 - val_loss: 9.4464\n",
      "Epoch 712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6463 - val_loss: 9.7541\n",
      "Epoch 713/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7576 - val_loss: 9.4584\n",
      "Epoch 714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.6514 - val_loss: 9.6743\n",
      "Epoch 715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7102 - val_loss: 9.4697\n",
      "Epoch 716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5947 - val_loss: 9.5243\n",
      "Epoch 717/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.8944 - val_loss: 9.8508\n",
      "Epoch 718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6565 - val_loss: 9.3880\n",
      "Epoch 719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.8217 - val_loss: 9.3802\n",
      "Epoch 720/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7003 - val_loss: 9.3592\n",
      "Epoch 721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6381 - val_loss: 9.4096\n",
      "Epoch 722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5987 - val_loss: 9.3377\n",
      "Epoch 723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5592 - val_loss: 9.3791\n",
      "Epoch 724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6654 - val_loss: 9.3202\n",
      "Epoch 725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6114 - val_loss: 9.3684\n",
      "Epoch 726/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6164 - val_loss: 9.3451\n",
      "Epoch 727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5357 - val_loss: 9.3110\n",
      "Epoch 728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6057 - val_loss: 9.3425\n",
      "Epoch 729/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5423 - val_loss: 9.3218\n",
      "Epoch 730/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.7743 - val_loss: 10.0269\n",
      "Epoch 731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.8057 - val_loss: 9.3658\n",
      "Epoch 732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6039 - val_loss: 9.5766\n",
      "Epoch 733/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 8.5784 - val_loss: 9.2913\n",
      "Epoch 734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5376 - val_loss: 9.3519\n",
      "Epoch 735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6616 - val_loss: 9.2729\n",
      "Epoch 736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4755 - val_loss: 9.2606\n",
      "Epoch 737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5481 - val_loss: 9.3049\n",
      "Epoch 738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4933 - val_loss: 9.3606\n",
      "Epoch 739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6015 - val_loss: 9.2555\n",
      "Epoch 740/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 9.497 - 0s 46us/step - loss: 8.6315 - val_loss: 9.3629\n",
      "Epoch 741/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7964 - val_loss: 9.2492\n",
      "Epoch 742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7864 - val_loss: 9.6252\n",
      "Epoch 743/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.7933 - val_loss: 9.4641\n",
      "Epoch 744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6485 - val_loss: 9.1967\n",
      "Epoch 745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5704 - val_loss: 9.2549\n",
      "Epoch 746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5511 - val_loss: 9.3505\n",
      "Epoch 747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4506 - val_loss: 9.6425\n",
      "Epoch 748/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7215 - val_loss: 9.4698\n",
      "Epoch 749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4172 - val_loss: 9.2311\n",
      "Epoch 750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5800 - val_loss: 9.1986\n",
      "Epoch 751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5668 - val_loss: 9.2861\n",
      "Epoch 752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4377 - val_loss: 9.1553\n",
      "Epoch 753/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.5770 - val_loss: 9.3162\n",
      "Epoch 754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4963 - val_loss: 9.1743\n",
      "Epoch 755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4842 - val_loss: 9.6899\n",
      "Epoch 756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6630 - val_loss: 10.1540\n",
      "Epoch 757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7238 - val_loss: 9.3754\n",
      "Epoch 758/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6075 - val_loss: 9.1761\n",
      "Epoch 759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4866 - val_loss: 9.1705\n",
      "Epoch 760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4665 - val_loss: 9.1535\n",
      "Epoch 761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.4188 - val_loss: 9.3523\n",
      "Epoch 762/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 8.3901 - val_loss: 9.2606\n",
      "Epoch 763/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.2951 - val_loss: 9.1520\n",
      "Epoch 764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6294 - val_loss: 9.2628\n",
      "Epoch 765/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4473 - val_loss: 9.0814\n",
      "Epoch 766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3377 - val_loss: 9.1395\n",
      "Epoch 767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5693 - val_loss: 9.1610\n",
      "Epoch 768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.4275 - val_loss: 9.4524\n",
      "Epoch 769/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3441 - val_loss: 9.1873\n",
      "Epoch 770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4389 - val_loss: 9.4751\n",
      "Epoch 771/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.6425 - val_loss: 9.1131\n",
      "Epoch 772/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4196 - val_loss: 9.0650\n",
      "Epoch 773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2963 - val_loss: 9.1101\n",
      "Epoch 774/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3055 - val_loss: 9.1130\n",
      "Epoch 775/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4432 - val_loss: 9.8783\n",
      "Epoch 776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5988 - val_loss: 9.0516\n",
      "Epoch 777/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.5310 - val_loss: 9.0430\n",
      "Epoch 778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4753 - val_loss: 9.1717\n",
      "Epoch 779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3463 - val_loss: 9.2118\n",
      "Epoch 780/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.4050 - val_loss: 9.1767\n",
      "Epoch 781/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3371 - val_loss: 8.9914\n",
      "Epoch 782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2770 - val_loss: 9.0333\n",
      "Epoch 783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3706 - val_loss: 9.0628\n",
      "Epoch 784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3376 - val_loss: 8.9881\n",
      "Epoch 785/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3556 - val_loss: 8.9854\n",
      "Epoch 786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5373 - val_loss: 9.3314\n",
      "Epoch 787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2970 - val_loss: 8.9492\n",
      "Epoch 788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2391 - val_loss: 8.9798\n",
      "Epoch 789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3168 - val_loss: 9.0376\n",
      "Epoch 790/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3881 - val_loss: 8.9426\n",
      "Epoch 791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3735 - val_loss: 9.0187\n",
      "Epoch 792/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3160 - val_loss: 8.9641\n",
      "Epoch 793/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2041 - val_loss: 9.0168\n",
      "Epoch 794/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.2758 - val_loss: 8.9728\n",
      "Epoch 795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2567 - val_loss: 8.9992\n",
      "Epoch 796/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4200 - val_loss: 9.1272\n",
      "Epoch 797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1832 - val_loss: 9.0039\n",
      "Epoch 798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3046 - val_loss: 9.0549\n",
      "Epoch 799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.4518 - val_loss: 9.0082\n",
      "Epoch 800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2459 - val_loss: 8.9750\n",
      "Epoch 801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2324 - val_loss: 9.0179\n",
      "Epoch 802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3351 - val_loss: 8.8964\n",
      "Epoch 803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3384 - val_loss: 9.0627\n",
      "Epoch 804/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3612 - val_loss: 9.0371\n",
      "Epoch 805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5308 - val_loss: 8.9790\n",
      "Epoch 806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.5724 - val_loss: 10.1001\n",
      "Epoch 807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.7666 - val_loss: 8.9574\n",
      "Epoch 808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1691 - val_loss: 8.9486\n",
      "Epoch 809/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.3499 - val_loss: 8.8967\n",
      "Epoch 810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3134 - val_loss: 8.9822\n",
      "Epoch 811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2229 - val_loss: 8.8519\n",
      "Epoch 812/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1731 - val_loss: 9.0096\n",
      "Epoch 813/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2206 - val_loss: 9.0324\n",
      "Epoch 814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2422 - val_loss: 8.8774\n",
      "Epoch 815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1667 - val_loss: 8.9375\n",
      "Epoch 816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.2700 - val_loss: 8.8872\n",
      "Epoch 817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1564 - val_loss: 8.8210\n",
      "Epoch 818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3099 - val_loss: 8.9134\n",
      "Epoch 819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0919 - val_loss: 9.1565\n",
      "Epoch 820/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.2317 - val_loss: 8.9332\n",
      "Epoch 821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.3097 - val_loss: 8.7950\n",
      "Epoch 822/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1235 - val_loss: 8.8467\n",
      "Epoch 823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1719 - val_loss: 8.7848\n",
      "Epoch 824/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1101 - val_loss: 9.1031\n",
      "Epoch 825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1970 - val_loss: 8.7759\n",
      "Epoch 826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1167 - val_loss: 9.0897\n",
      "Epoch 827/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.2550 - val_loss: 8.8689\n",
      "Epoch 828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1667 - val_loss: 8.7831\n",
      "Epoch 829/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1102 - val_loss: 8.8075\n",
      "Epoch 830/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.0784 - val_loss: 8.9181\n",
      "Epoch 831/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1913 - val_loss: 8.9980\n",
      "Epoch 832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1514 - val_loss: 8.7567\n",
      "Epoch 833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0436 - val_loss: 8.7254\n",
      "Epoch 834/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 8.0552 - val_loss: 8.7757\n",
      "Epoch 835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0816 - val_loss: 9.1245\n",
      "Epoch 836/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 8.0961 - val_loss: 8.7729\n",
      "Epoch 837/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1962 - val_loss: 8.7538\n",
      "Epoch 838/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1137 - val_loss: 8.7547\n",
      "Epoch 839/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 8.3689 - val_loss: 8.8391\n",
      "Epoch 840/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1239 - val_loss: 9.2262\n",
      "Epoch 841/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 7.784 - 0s 46us/step - loss: 8.2406 - val_loss: 8.6983\n",
      "Epoch 842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1540 - val_loss: 8.7140\n",
      "Epoch 843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1058 - val_loss: 8.7153\n",
      "Epoch 844/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1338 - val_loss: 8.7438\n",
      "Epoch 845/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 9.255 - 0s 46us/step - loss: 8.1551 - val_loss: 8.8208\n",
      "Epoch 846/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 8.0876 - val_loss: 8.6832\n",
      "Epoch 847/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.0195 - val_loss: 9.0063\n",
      "Epoch 848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0184 - val_loss: 8.7103\n",
      "Epoch 849/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.0641 - val_loss: 8.8616\n",
      "Epoch 850/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.0168 - val_loss: 8.6674\n",
      "Epoch 851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9985 - val_loss: 8.7773\n",
      "Epoch 852/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0377 - val_loss: 8.7131\n",
      "Epoch 853/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0828 - val_loss: 8.6545\n",
      "Epoch 854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0165 - val_loss: 8.7411\n",
      "Epoch 855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0042 - val_loss: 8.8337\n",
      "Epoch 856/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.2900 - val_loss: 8.6897\n",
      "Epoch 857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0249 - val_loss: 8.7109\n",
      "Epoch 858/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9981 - val_loss: 8.5988\n",
      "Epoch 859/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 7.9669 - val_loss: 9.0624\n",
      "Epoch 860/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.0741 - val_loss: 8.7918\n",
      "Epoch 861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1135 - val_loss: 8.7883\n",
      "Epoch 862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9735 - val_loss: 8.5930\n",
      "Epoch 863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9313 - val_loss: 8.6894\n",
      "Epoch 864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9487 - val_loss: 9.0470\n",
      "Epoch 865/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1563 - val_loss: 8.6076\n",
      "Epoch 866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9710 - val_loss: 8.7726\n",
      "Epoch 867/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 8.0922 - val_loss: 8.8689\n",
      "Epoch 868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0943 - val_loss: 8.7577\n",
      "Epoch 869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0226 - val_loss: 8.6424\n",
      "Epoch 870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.9489 - val_loss: 8.5839\n",
      "Epoch 871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9528 - val_loss: 8.8267\n",
      "Epoch 872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0824 - val_loss: 8.6936\n",
      "Epoch 873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0988 - val_loss: 8.6519\n",
      "Epoch 874/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1149 - val_loss: 8.5574\n",
      "Epoch 875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0078 - val_loss: 8.6989\n",
      "Epoch 876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0095 - val_loss: 8.6166\n",
      "Epoch 877/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.9514 - val_loss: 8.5378\n",
      "Epoch 878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0195 - val_loss: 8.9797\n",
      "Epoch 879/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.9331 - val_loss: 8.5545\n",
      "Epoch 880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9197 - val_loss: 8.6433\n",
      "Epoch 881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9410 - val_loss: 8.5102\n",
      "Epoch 882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9828 - val_loss: 8.5390\n",
      "Epoch 883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8929 - val_loss: 8.5606\n",
      "Epoch 884/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8743 - val_loss: 8.5689\n",
      "Epoch 885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0463 - val_loss: 8.6315\n",
      "Epoch 886/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9856 - val_loss: 8.5179\n",
      "Epoch 887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1650 - val_loss: 8.5657\n",
      "Epoch 888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8723 - val_loss: 8.9724\n",
      "Epoch 889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0069 - val_loss: 8.6430\n",
      "Epoch 890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1068 - val_loss: 8.4917\n",
      "Epoch 891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9249 - val_loss: 8.5716\n",
      "Epoch 892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8366 - val_loss: 8.5742\n",
      "Epoch 893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8452 - val_loss: 8.5510\n",
      "Epoch 894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8362 - val_loss: 8.8495\n",
      "Epoch 895/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.9195 - val_loss: 8.5600\n",
      "Epoch 896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9327 - val_loss: 8.4963\n",
      "Epoch 897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9210 - val_loss: 8.5883\n",
      "Epoch 898/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 7.9001 - val_loss: 8.5797\n",
      "Epoch 899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9411 - val_loss: 8.7518\n",
      "Epoch 900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9293 - val_loss: 8.5215\n",
      "Epoch 901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9152 - val_loss: 8.6490\n",
      "Epoch 902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9202 - val_loss: 8.6355\n",
      "Epoch 903/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.1181 - val_loss: 8.4433\n",
      "Epoch 904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9172 - val_loss: 8.4012\n",
      "Epoch 905/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 7.8139 - val_loss: 9.0194\n",
      "Epoch 906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8831 - val_loss: 8.4518\n",
      "Epoch 907/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 7.7942 - val_loss: 8.4562\n",
      "Epoch 908/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7913 - val_loss: 8.4028\n",
      "Epoch 909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8434 - val_loss: 8.4859\n",
      "Epoch 910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7953 - val_loss: 8.6501\n",
      "Epoch 911/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7323 - val_loss: 8.5579\n",
      "Epoch 912/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8152 - val_loss: 8.4411\n",
      "Epoch 913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8880 - val_loss: 8.4416\n",
      "Epoch 914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8016 - val_loss: 8.4097\n",
      "Epoch 915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8100 - val_loss: 8.4771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 916/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7543 - val_loss: 8.3865\n",
      "Epoch 917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8945 - val_loss: 8.7336\n",
      "Epoch 918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7486 - val_loss: 8.4239\n",
      "Epoch 919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7946 - val_loss: 8.5181\n",
      "Epoch 920/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8169 - val_loss: 8.5484\n",
      "Epoch 921/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9568 - val_loss: 8.3872\n",
      "Epoch 922/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0009 - val_loss: 8.4737\n",
      "Epoch 923/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7915 - val_loss: 8.5920\n",
      "Epoch 924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.0156 - val_loss: 8.6020\n",
      "Epoch 925/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8732 - val_loss: 8.5251\n",
      "Epoch 926/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8844 - val_loss: 8.6314\n",
      "Epoch 927/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7530 - val_loss: 8.3720\n",
      "Epoch 928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7567 - val_loss: 8.4615\n",
      "Epoch 929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8983 - val_loss: 8.8674\n",
      "Epoch 930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9800 - val_loss: 8.3719\n",
      "Epoch 931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 8.1060 - val_loss: 8.4177\n",
      "Epoch 932/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8011 - val_loss: 8.5765\n",
      "Epoch 933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6988 - val_loss: 8.3742\n",
      "Epoch 934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8543 - val_loss: 8.4341\n",
      "Epoch 935/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7675 - val_loss: 8.3373\n",
      "Epoch 936/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8063 - val_loss: 9.0112\n",
      "Epoch 937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8516 - val_loss: 8.3176\n",
      "Epoch 938/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7257 - val_loss: 8.2963\n",
      "Epoch 939/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8163 - val_loss: 9.0453\n",
      "Epoch 940/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7528 - val_loss: 8.3429\n",
      "Epoch 941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7607 - val_loss: 8.3588\n",
      "Epoch 942/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 7.7136 - val_loss: 8.2735\n",
      "Epoch 943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7827 - val_loss: 8.3466\n",
      "Epoch 944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7176 - val_loss: 8.6294\n",
      "Epoch 945/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8790 - val_loss: 8.2829\n",
      "Epoch 946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7983 - val_loss: 8.3600\n",
      "Epoch 947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8377 - val_loss: 8.9690\n",
      "Epoch 948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7449 - val_loss: 8.7300\n",
      "Epoch 949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6888 - val_loss: 8.6044\n",
      "Epoch 950/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6999 - val_loss: 8.4798\n",
      "Epoch 951/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8471 - val_loss: 8.5103\n",
      "Epoch 952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7328 - val_loss: 8.5944\n",
      "Epoch 953/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7778 - val_loss: 8.6329\n",
      "Epoch 954/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7432 - val_loss: 8.4489\n",
      "Epoch 955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7718 - val_loss: 8.2404\n",
      "Epoch 956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6743 - val_loss: 8.3405\n",
      "Epoch 957/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6954 - val_loss: 8.2708\n",
      "Epoch 958/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7952 - val_loss: 8.5903\n",
      "Epoch 959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7954 - val_loss: 8.3582\n",
      "Epoch 960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6399 - val_loss: 8.5972\n",
      "Epoch 961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7679 - val_loss: 8.4670\n",
      "Epoch 962/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.6099 - val_loss: 8.8252\n",
      "Epoch 963/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.8197 - val_loss: 8.9119\n",
      "Epoch 964/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 8.0818 - val_loss: 8.4488\n",
      "Epoch 965/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.6521 - val_loss: 8.2299\n",
      "Epoch 966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6251 - val_loss: 8.7045\n",
      "Epoch 967/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7433 - val_loss: 8.4390\n",
      "Epoch 968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8203 - val_loss: 8.1977\n",
      "Epoch 969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6367 - val_loss: 8.3782\n",
      "Epoch 970/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7532 - val_loss: 8.5795\n",
      "Epoch 971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8640 - val_loss: 8.9540\n",
      "Epoch 972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7799 - val_loss: 8.1934\n",
      "Epoch 973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6172 - val_loss: 8.4943\n",
      "Epoch 974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6699 - val_loss: 8.4025\n",
      "Epoch 975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6070 - val_loss: 8.1947\n",
      "Epoch 976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6328 - val_loss: 8.1502\n",
      "Epoch 977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5914 - val_loss: 8.1833\n",
      "Epoch 978/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6765 - val_loss: 8.3777\n",
      "Epoch 979/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.6556 - val_loss: 8.2131\n",
      "Epoch 980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6329 - val_loss: 8.7645\n",
      "Epoch 981/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 7.7644 - val_loss: 8.3253\n",
      "Epoch 982/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.6445 - val_loss: 8.1333\n",
      "Epoch 983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6142 - val_loss: 8.2597\n",
      "Epoch 984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6723 - val_loss: 8.3238\n",
      "Epoch 985/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.6580 - val_loss: 8.2687\n",
      "Epoch 986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5420 - val_loss: 8.2726\n",
      "Epoch 987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5741 - val_loss: 8.3868\n",
      "Epoch 988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5876 - val_loss: 8.2232\n",
      "Epoch 989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6701 - val_loss: 8.3489\n",
      "Epoch 990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7024 - val_loss: 8.0934\n",
      "Epoch 991/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5865 - val_loss: 8.5546\n",
      "Epoch 992/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 7.7342 - val_loss: 8.1744\n",
      "Epoch 993/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 23us/step - loss: 7.6254 - val_loss: 8.2329\n",
      "Epoch 994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5461 - val_loss: 8.2972\n",
      "Epoch 995/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 7.8037 - val_loss: 8.1414\n",
      "Epoch 996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5484 - val_loss: 8.2094\n",
      "Epoch 997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5134 - val_loss: 8.0834\n",
      "Epoch 998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6366 - val_loss: 8.2266\n",
      "Epoch 999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5520 - val_loss: 8.1981\n",
      "Epoch 1000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6104 - val_loss: 8.5260\n",
      "Epoch 1001/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.6935 - val_loss: 8.1195\n",
      "Epoch 1002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5684 - val_loss: 8.1834\n",
      "Epoch 1003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4846 - val_loss: 8.2615\n",
      "Epoch 1004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7344 - val_loss: 8.7002\n",
      "Epoch 1005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6457 - val_loss: 8.1219\n",
      "Epoch 1006/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5054 - val_loss: 8.0904\n",
      "Epoch 1007/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5074 - val_loss: 8.1657\n",
      "Epoch 1008/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5892 - val_loss: 8.6458\n",
      "Epoch 1009/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6997 - val_loss: 8.0915\n",
      "Epoch 1010/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5677 - val_loss: 8.1994\n",
      "Epoch 1011/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4859 - val_loss: 8.0569\n",
      "Epoch 1012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4655 - val_loss: 8.4981\n",
      "Epoch 1013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.9444 - val_loss: 8.1743\n",
      "Epoch 1014/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4973 - val_loss: 8.0592\n",
      "Epoch 1015/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5061 - val_loss: 8.0956\n",
      "Epoch 1016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6195 - val_loss: 8.1354\n",
      "Epoch 1017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7377 - val_loss: 8.5166\n",
      "Epoch 1018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7054 - val_loss: 8.2116\n",
      "Epoch 1019/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.6796 - val_loss: 8.0579\n",
      "Epoch 1020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5805 - val_loss: 8.1611\n",
      "Epoch 1021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7139 - val_loss: 8.0289\n",
      "Epoch 1022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4974 - val_loss: 8.0944\n",
      "Epoch 1023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4669 - val_loss: 8.1437\n",
      "Epoch 1024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4862 - val_loss: 8.0598\n",
      "Epoch 1025/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4556 - val_loss: 8.0513\n",
      "Epoch 1026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5425 - val_loss: 8.2203\n",
      "Epoch 1027/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5542 - val_loss: 8.2533\n",
      "Epoch 1028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6879 - val_loss: 8.1639\n",
      "Epoch 1029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5334 - val_loss: 8.1795\n",
      "Epoch 1030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5437 - val_loss: 8.3590\n",
      "Epoch 1031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6601 - val_loss: 7.9754\n",
      "Epoch 1032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4395 - val_loss: 8.0995\n",
      "Epoch 1033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4900 - val_loss: 8.2580\n",
      "Epoch 1034/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4200 - val_loss: 8.4153\n",
      "Epoch 1035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6025 - val_loss: 8.3966\n",
      "Epoch 1036/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4792 - val_loss: 8.1115\n",
      "Epoch 1037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6560 - val_loss: 7.9701\n",
      "Epoch 1038/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5504 - val_loss: 8.0896\n",
      "Epoch 1039/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4604 - val_loss: 7.9521\n",
      "Epoch 1040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4896 - val_loss: 8.1512\n",
      "Epoch 1041/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 7.5055 - val_loss: 7.9802\n",
      "Epoch 1042/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5179 - val_loss: 7.9944\n",
      "Epoch 1043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4284 - val_loss: 7.9402\n",
      "Epoch 1044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4972 - val_loss: 8.0094\n",
      "Epoch 1045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4476 - val_loss: 7.9545\n",
      "Epoch 1046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4750 - val_loss: 8.2571\n",
      "Epoch 1047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4956 - val_loss: 8.3323\n",
      "Epoch 1048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6068 - val_loss: 7.9701\n",
      "Epoch 1049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4585 - val_loss: 8.1224\n",
      "Epoch 1050/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4382 - val_loss: 8.0845\n",
      "Epoch 1051/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5544 - val_loss: 8.0063\n",
      "Epoch 1052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4818 - val_loss: 7.9529\n",
      "Epoch 1053/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4554 - val_loss: 7.9308\n",
      "Epoch 1054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3927 - val_loss: 7.9309\n",
      "Epoch 1055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3755 - val_loss: 8.1057\n",
      "Epoch 1056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5662 - val_loss: 8.5765\n",
      "Epoch 1057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8575 - val_loss: 8.2817\n",
      "Epoch 1058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4269 - val_loss: 7.9238\n",
      "Epoch 1059/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5059 - val_loss: 7.9719\n",
      "Epoch 1060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4405 - val_loss: 7.9535\n",
      "Epoch 1061/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3257 - val_loss: 7.9419\n",
      "Epoch 1062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4065 - val_loss: 8.2010\n",
      "Epoch 1063/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3473 - val_loss: 7.8896\n",
      "Epoch 1064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4271 - val_loss: 7.9065\n",
      "Epoch 1065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3504 - val_loss: 7.9220\n",
      "Epoch 1066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4065 - val_loss: 7.8917\n",
      "Epoch 1067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4382 - val_loss: 8.1880\n",
      "Epoch 1068/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7466 - val_loss: 8.1117\n",
      "Epoch 1069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3701 - val_loss: 7.8987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3473 - val_loss: 7.8441\n",
      "Epoch 1071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3659 - val_loss: 7.9089\n",
      "Epoch 1072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4002 - val_loss: 7.8900\n",
      "Epoch 1073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5173 - val_loss: 7.9520\n",
      "Epoch 1074/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3559 - val_loss: 7.8882\n",
      "Epoch 1075/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3993 - val_loss: 7.8820\n",
      "Epoch 1076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3845 - val_loss: 7.9308\n",
      "Epoch 1077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3876 - val_loss: 7.9339\n",
      "Epoch 1078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6396 - val_loss: 7.9536\n",
      "Epoch 1079/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3540 - val_loss: 7.9583\n",
      "Epoch 1080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3529 - val_loss: 7.8593\n",
      "Epoch 1081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3418 - val_loss: 7.8734\n",
      "Epoch 1082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4270 - val_loss: 8.4573\n",
      "Epoch 1083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7413 - val_loss: 8.8986\n",
      "Epoch 1084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6114 - val_loss: 8.4441\n",
      "Epoch 1085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3380 - val_loss: 7.9845\n",
      "Epoch 1086/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3133 - val_loss: 7.9621\n",
      "Epoch 1087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5924 - val_loss: 8.3882\n",
      "Epoch 1088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3481 - val_loss: 8.1632\n",
      "Epoch 1089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8313 - val_loss: 8.6303\n",
      "Epoch 1090/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4153 - val_loss: 7.8398\n",
      "Epoch 1091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3944 - val_loss: 7.9284\n",
      "Epoch 1092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5094 - val_loss: 7.9164\n",
      "Epoch 1093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3689 - val_loss: 7.8410\n",
      "Epoch 1094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3461 - val_loss: 7.8671\n",
      "Epoch 1095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5173 - val_loss: 7.8915\n",
      "Epoch 1096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4805 - val_loss: 7.8789\n",
      "Epoch 1097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4146 - val_loss: 7.8057\n",
      "Epoch 1098/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3635 - val_loss: 8.0337\n",
      "Epoch 1099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4076 - val_loss: 7.9099\n",
      "Epoch 1100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5545 - val_loss: 8.0248\n",
      "Epoch 1101/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2704 - val_loss: 7.8417\n",
      "Epoch 1102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3002 - val_loss: 7.8423\n",
      "Epoch 1103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2292 - val_loss: 8.1574\n",
      "Epoch 1104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3676 - val_loss: 7.8284\n",
      "Epoch 1105/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2722 - val_loss: 7.8231\n",
      "Epoch 1106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3030 - val_loss: 7.8523\n",
      "Epoch 1107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5122 - val_loss: 9.3928\n",
      "Epoch 1108/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5975 - val_loss: 7.8345\n",
      "Epoch 1109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5231 - val_loss: 8.2626\n",
      "Epoch 1110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7983 - val_loss: 9.2530\n",
      "Epoch 1111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.7637 - val_loss: 8.7043\n",
      "Epoch 1112/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.7262 - val_loss: 7.8074\n",
      "Epoch 1113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4152 - val_loss: 8.5565\n",
      "Epoch 1114/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3917 - val_loss: 7.7477\n",
      "Epoch 1115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3225 - val_loss: 7.7482\n",
      "Epoch 1116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5480 - val_loss: 8.0387\n",
      "Epoch 1117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2833 - val_loss: 7.9748\n",
      "Epoch 1118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5628 - val_loss: 8.1081\n",
      "Epoch 1119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.5482 - val_loss: 8.1446\n",
      "Epoch 1120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2915 - val_loss: 7.7727\n",
      "Epoch 1121/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4164 - val_loss: 7.7803\n",
      "Epoch 1122/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 7.2402 - val_loss: 7.7924\n",
      "Epoch 1123/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2608 - val_loss: 7.8874\n",
      "Epoch 1124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2428 - val_loss: 8.2072\n",
      "Epoch 1125/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4658 - val_loss: 7.7428\n",
      "Epoch 1126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2313 - val_loss: 7.7266\n",
      "Epoch 1127/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3023 - val_loss: 7.9885\n",
      "Epoch 1128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3842 - val_loss: 8.3268\n",
      "Epoch 1129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6325 - val_loss: 7.8325\n",
      "Epoch 1130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3305 - val_loss: 7.7745\n",
      "Epoch 1131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2204 - val_loss: 7.9629\n",
      "Epoch 1132/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2740 - val_loss: 7.8625\n",
      "Epoch 1133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2393 - val_loss: 7.8454\n",
      "Epoch 1134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3371 - val_loss: 8.0918\n",
      "Epoch 1135/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2398 - val_loss: 7.7527\n",
      "Epoch 1136/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3040 - val_loss: 7.7911\n",
      "Epoch 1137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2597 - val_loss: 7.8064\n",
      "Epoch 1138/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4614 - val_loss: 7.8278\n",
      "Epoch 1139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6101 - val_loss: 7.7703\n",
      "Epoch 1140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2440 - val_loss: 7.8450\n",
      "Epoch 1141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2204 - val_loss: 7.7298\n",
      "Epoch 1142/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2022 - val_loss: 7.7168\n",
      "Epoch 1143/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1498 - val_loss: 7.7557\n",
      "Epoch 1144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2196 - val_loss: 7.7529\n",
      "Epoch 1145/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2308 - val_loss: 7.9349\n",
      "Epoch 1146/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 7.2882 - val_loss: 7.7071\n",
      "Epoch 1147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4065 - val_loss: 7.8268\n",
      "Epoch 1148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2432 - val_loss: 7.7413\n",
      "Epoch 1149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1746 - val_loss: 7.7381\n",
      "Epoch 1150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2728 - val_loss: 7.7701\n",
      "Epoch 1151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2119 - val_loss: 7.8009\n",
      "Epoch 1152/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1812 - val_loss: 7.8550\n",
      "Epoch 1153/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2539 - val_loss: 8.3197\n",
      "Epoch 1154/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5808 - val_loss: 7.7339\n",
      "Epoch 1155/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3592 - val_loss: 7.7617\n",
      "Epoch 1156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4777 - val_loss: 7.7039\n",
      "Epoch 1157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2050 - val_loss: 7.8086\n",
      "Epoch 1158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1825 - val_loss: 7.7253\n",
      "Epoch 1159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1628 - val_loss: 7.7358\n",
      "Epoch 1160/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2230 - val_loss: 7.7023\n",
      "Epoch 1161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4036 - val_loss: 7.6710\n",
      "Epoch 1162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1914 - val_loss: 7.6887\n",
      "Epoch 1163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2725 - val_loss: 8.1093\n",
      "Epoch 1164/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2886 - val_loss: 7.6939\n",
      "Epoch 1165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2369 - val_loss: 7.7394\n",
      "Epoch 1166/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1999 - val_loss: 7.8248\n",
      "Epoch 1167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.6901 - val_loss: 7.6948\n",
      "Epoch 1168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4239 - val_loss: 8.1418\n",
      "Epoch 1169/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5804 - val_loss: 8.0235\n",
      "Epoch 1170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2470 - val_loss: 7.7624\n",
      "Epoch 1171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2241 - val_loss: 7.8141\n",
      "Epoch 1172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3342 - val_loss: 7.7092\n",
      "Epoch 1173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1905 - val_loss: 7.7833\n",
      "Epoch 1174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1573 - val_loss: 7.7640\n",
      "Epoch 1175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1303 - val_loss: 7.7260\n",
      "Epoch 1176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1524 - val_loss: 7.6752\n",
      "Epoch 1177/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2049 - val_loss: 7.9037\n",
      "Epoch 1178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1762 - val_loss: 7.6964\n",
      "Epoch 1179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1866 - val_loss: 7.7829\n",
      "Epoch 1180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1468 - val_loss: 7.8999\n",
      "Epoch 1181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2583 - val_loss: 7.7628\n",
      "Epoch 1182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2471 - val_loss: 7.6913\n",
      "Epoch 1183/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1595 - val_loss: 7.6500\n",
      "Epoch 1184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1709 - val_loss: 7.7240\n",
      "Epoch 1185/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.4096 - val_loss: 7.6977\n",
      "Epoch 1186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3032 - val_loss: 7.6935\n",
      "Epoch 1187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1524 - val_loss: 7.6525\n",
      "Epoch 1188/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2114 - val_loss: 7.9903\n",
      "Epoch 1189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1277 - val_loss: 7.6769\n",
      "Epoch 1190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2150 - val_loss: 7.7025\n",
      "Epoch 1191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1016 - val_loss: 7.6431\n",
      "Epoch 1192/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1491 - val_loss: 7.7097\n",
      "Epoch 1193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1055 - val_loss: 7.6551\n",
      "Epoch 1194/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0759 - val_loss: 7.6867\n",
      "Epoch 1195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1538 - val_loss: 7.7895\n",
      "Epoch 1196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1166 - val_loss: 7.6308\n",
      "Epoch 1197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1482 - val_loss: 7.6502\n",
      "Epoch 1198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1074 - val_loss: 7.6282\n",
      "Epoch 1199/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.3026 - val_loss: 8.0143\n",
      "Epoch 1200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4106 - val_loss: 7.6271\n",
      "Epoch 1201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1422 - val_loss: 7.7545\n",
      "Epoch 1202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0372 - val_loss: 7.6737\n",
      "Epoch 1203/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0826 - val_loss: 7.7277\n",
      "Epoch 1204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3110 - val_loss: 7.7766\n",
      "Epoch 1205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1931 - val_loss: 7.7121\n",
      "Epoch 1206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0741 - val_loss: 7.7094\n",
      "Epoch 1207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0843 - val_loss: 7.6327\n",
      "Epoch 1208/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0818 - val_loss: 7.6967\n",
      "Epoch 1209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0497 - val_loss: 7.8255\n",
      "Epoch 1210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1556 - val_loss: 7.6642\n",
      "Epoch 1211/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2078 - val_loss: 7.5966\n",
      "Epoch 1212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1251 - val_loss: 7.6761\n",
      "Epoch 1213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1662 - val_loss: 7.7322\n",
      "Epoch 1214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2020 - val_loss: 7.8253\n",
      "Epoch 1215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1825 - val_loss: 7.5616\n",
      "Epoch 1216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0414 - val_loss: 7.7625\n",
      "Epoch 1217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1514 - val_loss: 7.5605\n",
      "Epoch 1218/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0433 - val_loss: 7.5808\n",
      "Epoch 1219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1037 - val_loss: 7.6354\n",
      "Epoch 1220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0646 - val_loss: 7.5745\n",
      "Epoch 1221/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1826 - val_loss: 7.7551\n",
      "Epoch 1222/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 7.0563 - val_loss: 7.6492\n",
      "Epoch 1223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1035 - val_loss: 7.7001\n",
      "Epoch 1224/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1355 - val_loss: 7.9987\n",
      "Epoch 1225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1501 - val_loss: 7.5944\n",
      "Epoch 1226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1262 - val_loss: 7.5925\n",
      "Epoch 1227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1041 - val_loss: 7.5730\n",
      "Epoch 1228/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1797 - val_loss: 7.6125\n",
      "Epoch 1229/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0818 - val_loss: 7.5691\n",
      "Epoch 1230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0383 - val_loss: 7.5920\n",
      "Epoch 1231/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0519 - val_loss: 7.5891\n",
      "Epoch 1232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2268 - val_loss: 8.1018\n",
      "Epoch 1233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1554 - val_loss: 7.5946\n",
      "Epoch 1234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0419 - val_loss: 7.6230\n",
      "Epoch 1235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1450 - val_loss: 8.1125\n",
      "Epoch 1236/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2654 - val_loss: 7.6177\n",
      "Epoch 1237/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1565 - val_loss: 7.7048\n",
      "Epoch 1238/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 7.0448 - val_loss: 7.6456\n",
      "Epoch 1239/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0139 - val_loss: 7.6826\n",
      "Epoch 1240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0391 - val_loss: 7.5610\n",
      "Epoch 1241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0223 - val_loss: 7.5816\n",
      "Epoch 1242/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0759 - val_loss: 7.6768\n",
      "Epoch 1243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0173 - val_loss: 7.6402\n",
      "Epoch 1244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0394 - val_loss: 7.5428\n",
      "Epoch 1245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0010 - val_loss: 8.1369\n",
      "Epoch 1246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2605 - val_loss: 7.6012\n",
      "Epoch 1247/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1243 - val_loss: 7.6587\n",
      "Epoch 1248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0320 - val_loss: 7.5283\n",
      "Epoch 1249/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9723 - val_loss: 7.5162\n",
      "Epoch 1250/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0138 - val_loss: 7.5185\n",
      "Epoch 1251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0881 - val_loss: 7.5028\n",
      "Epoch 1252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0497 - val_loss: 7.5724\n",
      "Epoch 1253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0039 - val_loss: 7.5595\n",
      "Epoch 1254/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1016 - val_loss: 7.5534\n",
      "Epoch 1255/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0565 - val_loss: 7.5328\n",
      "Epoch 1256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0594 - val_loss: 7.5208\n",
      "Epoch 1257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0900 - val_loss: 7.5076\n",
      "Epoch 1258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1011 - val_loss: 7.4883\n",
      "Epoch 1259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9923 - val_loss: 7.6967\n",
      "Epoch 1260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0044 - val_loss: 7.4865\n",
      "Epoch 1261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1522 - val_loss: 7.5517\n",
      "Epoch 1262/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2462 - val_loss: 7.4801\n",
      "Epoch 1263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3337 - val_loss: 7.6405\n",
      "Epoch 1264/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1424 - val_loss: 7.7443\n",
      "Epoch 1265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1154 - val_loss: 7.5134\n",
      "Epoch 1266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0746 - val_loss: 7.5932\n",
      "Epoch 1267/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0463 - val_loss: 7.4581\n",
      "Epoch 1268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9173 - val_loss: 7.7035\n",
      "Epoch 1269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1144 - val_loss: 7.5338\n",
      "Epoch 1270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9857 - val_loss: 7.5190\n",
      "Epoch 1271/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9875 - val_loss: 7.5292\n",
      "Epoch 1272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0482 - val_loss: 7.7129\n",
      "Epoch 1273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0113 - val_loss: 7.5365\n",
      "Epoch 1274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9730 - val_loss: 7.6908\n",
      "Epoch 1275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0559 - val_loss: 7.5723\n",
      "Epoch 1276/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3630 - val_loss: 7.5208\n",
      "Epoch 1277/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0862 - val_loss: 7.5486\n",
      "Epoch 1278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0456 - val_loss: 7.5260\n",
      "Epoch 1279/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.2225 - val_loss: 7.6867\n",
      "Epoch 1280/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0404 - val_loss: 8.4511\n",
      "Epoch 1281/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1920 - val_loss: 7.6080\n",
      "Epoch 1282/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.9561 - val_loss: 7.4323\n",
      "Epoch 1283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9972 - val_loss: 7.4638\n",
      "Epoch 1284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9984 - val_loss: 7.7655\n",
      "Epoch 1285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9819 - val_loss: 7.5090\n",
      "Epoch 1286/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.5328 - val_loss: 7.6339\n",
      "Epoch 1287/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0046 - val_loss: 7.7049\n",
      "Epoch 1288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0241 - val_loss: 7.4411\n",
      "Epoch 1289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9518 - val_loss: 7.4991\n",
      "Epoch 1290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9966 - val_loss: 7.4425\n",
      "Epoch 1291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9673 - val_loss: 7.4328\n",
      "Epoch 1292/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9625 - val_loss: 7.4576\n",
      "Epoch 1293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9327 - val_loss: 7.4527\n",
      "Epoch 1294/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0324 - val_loss: 7.4313\n",
      "Epoch 1295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0805 - val_loss: 7.4925\n",
      "Epoch 1296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0977 - val_loss: 7.4185\n",
      "Epoch 1297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1145 - val_loss: 7.4323\n",
      "Epoch 1298/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 7.0175 - val_loss: 7.3833\n",
      "Epoch 1299/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8694 - val_loss: 7.4649\n",
      "Epoch 1300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0112 - val_loss: 7.4359\n",
      "Epoch 1301/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9410 - val_loss: 7.3973\n",
      "Epoch 1302/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0406 - val_loss: 7.4521\n",
      "Epoch 1303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9547 - val_loss: 7.5549\n",
      "Epoch 1304/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0524 - val_loss: 7.6640\n",
      "Epoch 1305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1398 - val_loss: 7.7202\n",
      "Epoch 1306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1277 - val_loss: 7.7435\n",
      "Epoch 1307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0018 - val_loss: 7.5615\n",
      "Epoch 1308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0023 - val_loss: 7.4053\n",
      "Epoch 1309/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9321 - val_loss: 7.4226\n",
      "Epoch 1310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9487 - val_loss: 7.4429\n",
      "Epoch 1311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8762 - val_loss: 7.9105\n",
      "Epoch 1312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9717 - val_loss: 7.8398\n",
      "Epoch 1313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2382 - val_loss: 7.6619\n",
      "Epoch 1314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8866 - val_loss: 8.0691\n",
      "Epoch 1315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.4113 - val_loss: 7.4128\n",
      "Epoch 1316/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0774 - val_loss: 7.4080\n",
      "Epoch 1317/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0273 - val_loss: 7.6500\n",
      "Epoch 1318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8548 - val_loss: 7.5849\n",
      "Epoch 1319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0417 - val_loss: 7.4129\n",
      "Epoch 1320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8986 - val_loss: 7.4093\n",
      "Epoch 1321/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8536 - val_loss: 7.4535\n",
      "Epoch 1322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8820 - val_loss: 7.3902\n",
      "Epoch 1323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9331 - val_loss: 7.4832\n",
      "Epoch 1324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9216 - val_loss: 7.6697\n",
      "Epoch 1325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0961 - val_loss: 7.7374\n",
      "Epoch 1326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2787 - val_loss: 7.4095\n",
      "Epoch 1327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.8102 - val_loss: 8.3678\n",
      "Epoch 1328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0357 - val_loss: 7.6986\n",
      "Epoch 1329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0479 - val_loss: 7.3618\n",
      "Epoch 1330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9083 - val_loss: 7.4058\n",
      "Epoch 1331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8771 - val_loss: 7.3760\n",
      "Epoch 1332/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9308 - val_loss: 7.3889\n",
      "Epoch 1333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8689 - val_loss: 7.3860\n",
      "Epoch 1334/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8739 - val_loss: 7.7607\n",
      "Epoch 1335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8857 - val_loss: 7.3701\n",
      "Epoch 1336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8066 - val_loss: 7.3844\n",
      "Epoch 1337/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8973 - val_loss: 7.5533\n",
      "Epoch 1338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9554 - val_loss: 8.3070\n",
      "Epoch 1339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2085 - val_loss: 7.3882\n",
      "Epoch 1340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8995 - val_loss: 7.7229\n",
      "Epoch 1341/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9148 - val_loss: 7.3924\n",
      "Epoch 1342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9124 - val_loss: 7.3972\n",
      "Epoch 1343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8987 - val_loss: 7.7706\n",
      "Epoch 1344/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8706 - val_loss: 7.5786\n",
      "Epoch 1345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8185 - val_loss: 7.3652\n",
      "Epoch 1346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8553 - val_loss: 7.5682\n",
      "Epoch 1347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9767 - val_loss: 7.4072\n",
      "Epoch 1348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9083 - val_loss: 7.4668\n",
      "Epoch 1349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8710 - val_loss: 7.4068\n",
      "Epoch 1350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9962 - val_loss: 7.4050\n",
      "Epoch 1351/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8501 - val_loss: 7.6995\n",
      "Epoch 1352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.2265 - val_loss: 7.3934\n",
      "Epoch 1353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9020 - val_loss: 7.3395\n",
      "Epoch 1354/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8960 - val_loss: 7.3272\n",
      "Epoch 1355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0819 - val_loss: 8.3589\n",
      "Epoch 1356/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1756 - val_loss: 7.3314\n",
      "Epoch 1357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7923 - val_loss: 7.3197\n",
      "Epoch 1358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8205 - val_loss: 7.3821\n",
      "Epoch 1359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8380 - val_loss: 7.4147\n",
      "Epoch 1360/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8273 - val_loss: 7.3700\n",
      "Epoch 1361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8210 - val_loss: 7.3536\n",
      "Epoch 1362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7991 - val_loss: 7.4131\n",
      "Epoch 1363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.1381 - val_loss: 8.0667\n",
      "Epoch 1364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0497 - val_loss: 7.3035\n",
      "Epoch 1365/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.369 - 0s 46us/step - loss: 6.9380 - val_loss: 7.4580\n",
      "Epoch 1366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8121 - val_loss: 7.5003\n",
      "Epoch 1367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8280 - val_loss: 7.3665\n",
      "Epoch 1368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9100 - val_loss: 7.2954\n",
      "Epoch 1369/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8339 - val_loss: 7.6157\n",
      "Epoch 1370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8553 - val_loss: 7.3205\n",
      "Epoch 1371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8161 - val_loss: 7.2913\n",
      "Epoch 1372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8524 - val_loss: 7.5896\n",
      "Epoch 1373/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8458 - val_loss: 7.3407\n",
      "Epoch 1374/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.8373 - val_loss: 7.3297\n",
      "Epoch 1375/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7778 - val_loss: 7.5118\n",
      "Epoch 1376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8856 - val_loss: 7.3217\n",
      "Epoch 1377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8504 - val_loss: 7.2909\n",
      "Epoch 1378/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7752 - val_loss: 7.4612\n",
      "Epoch 1379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8060 - val_loss: 7.3424\n",
      "Epoch 1380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8057 - val_loss: 7.4299\n",
      "Epoch 1381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8461 - val_loss: 7.3254\n",
      "Epoch 1382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7625 - val_loss: 7.2956\n",
      "Epoch 1383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8333 - val_loss: 7.2841\n",
      "Epoch 1384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8633 - val_loss: 7.3030\n",
      "Epoch 1385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8011 - val_loss: 7.2994\n",
      "Epoch 1386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8263 - val_loss: 7.3072\n",
      "Epoch 1387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9160 - val_loss: 7.2800\n",
      "Epoch 1388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9720 - val_loss: 7.7835\n",
      "Epoch 1389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1473 - val_loss: 7.6555\n",
      "Epoch 1390/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8939 - val_loss: 7.3071\n",
      "Epoch 1391/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7432 - val_loss: 8.0018\n",
      "Epoch 1392/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7556 - val_loss: 7.3134\n",
      "Epoch 1393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8194 - val_loss: 7.8761\n",
      "Epoch 1394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1847 - val_loss: 7.3045\n",
      "Epoch 1395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9498 - val_loss: 7.6144\n",
      "Epoch 1396/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.317 - 0s 46us/step - loss: 6.9992 - val_loss: 7.7694\n",
      "Epoch 1397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9222 - val_loss: 7.3205\n",
      "Epoch 1398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7766 - val_loss: 7.2676\n",
      "Epoch 1399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7602 - val_loss: 7.2757\n",
      "Epoch 1400/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7628 - val_loss: 7.2520\n",
      "Epoch 1401/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.7450 - val_loss: 7.2200\n",
      "Epoch 1402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7556 - val_loss: 7.4694\n",
      "Epoch 1403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7657 - val_loss: 7.2636\n",
      "Epoch 1404/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8430 - val_loss: 7.2429\n",
      "Epoch 1405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8305 - val_loss: 7.2290\n",
      "Epoch 1406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7279 - val_loss: 7.2570\n",
      "Epoch 1407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8474 - val_loss: 7.3075\n",
      "Epoch 1408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7837 - val_loss: 7.2526\n",
      "Epoch 1409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7315 - val_loss: 7.4136\n",
      "Epoch 1410/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9351 - val_loss: 7.2649\n",
      "Epoch 1411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8247 - val_loss: 7.3236\n",
      "Epoch 1412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9186 - val_loss: 7.2388\n",
      "Epoch 1413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1268 - val_loss: 7.9054\n",
      "Epoch 1414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9891 - val_loss: 7.2673\n",
      "Epoch 1415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7626 - val_loss: 7.2636\n",
      "Epoch 1416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7144 - val_loss: 7.3988\n",
      "Epoch 1417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8409 - val_loss: 7.5110\n",
      "Epoch 1418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0459 - val_loss: 7.3046\n",
      "Epoch 1419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7154 - val_loss: 7.2483\n",
      "Epoch 1420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7139 - val_loss: 7.2862\n",
      "Epoch 1421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9028 - val_loss: 7.3129\n",
      "Epoch 1422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8738 - val_loss: 7.2818\n",
      "Epoch 1423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7243 - val_loss: 7.2765\n",
      "Epoch 1424/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8315 - val_loss: 7.3424\n",
      "Epoch 1425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7734 - val_loss: 7.3245\n",
      "Epoch 1426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7257 - val_loss: 7.2663\n",
      "Epoch 1427/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8036 - val_loss: 7.2438\n",
      "Epoch 1428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7213 - val_loss: 7.2395\n",
      "Epoch 1429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7157 - val_loss: 7.3769\n",
      "Epoch 1430/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7394 - val_loss: 7.2312\n",
      "Epoch 1431/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6752 - val_loss: 7.2181\n",
      "Epoch 1432/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6919 - val_loss: 7.2423\n",
      "Epoch 1433/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7308 - val_loss: 7.2601\n",
      "Epoch 1434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7012 - val_loss: 7.2552\n",
      "Epoch 1435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6811 - val_loss: 7.4080\n",
      "Epoch 1436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8289 - val_loss: 7.5335\n",
      "Epoch 1437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8448 - val_loss: 7.3109\n",
      "Epoch 1438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7900 - val_loss: 7.2372\n",
      "Epoch 1439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7338 - val_loss: 7.2564\n",
      "Epoch 1440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8648 - val_loss: 7.2715\n",
      "Epoch 1441/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9689 - val_loss: 7.6325\n",
      "Epoch 1442/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9012 - val_loss: 7.1663\n",
      "Epoch 1443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7334 - val_loss: 7.2391\n",
      "Epoch 1444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7395 - val_loss: 7.2340\n",
      "Epoch 1445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7149 - val_loss: 7.1955\n",
      "Epoch 1446/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6839 - val_loss: 7.1942\n",
      "Epoch 1447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7123 - val_loss: 7.5497\n",
      "Epoch 1448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7525 - val_loss: 7.2133\n",
      "Epoch 1449/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6985 - val_loss: 7.5291\n",
      "Epoch 1450/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.7704 - val_loss: 7.1599\n",
      "Epoch 1451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8644 - val_loss: 7.2806\n",
      "Epoch 1452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7025 - val_loss: 7.2510\n",
      "Epoch 1453/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0758 - val_loss: 8.2715\n",
      "Epoch 1454/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.1207 - val_loss: 7.2998\n",
      "Epoch 1455/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8307 - val_loss: 7.2821\n",
      "Epoch 1456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7059 - val_loss: 7.8108\n",
      "Epoch 1457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0204 - val_loss: 7.1672\n",
      "Epoch 1458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7096 - val_loss: 7.4282\n",
      "Epoch 1459/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6479 - val_loss: 7.1514\n",
      "Epoch 1460/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7268 - val_loss: 7.2461\n",
      "Epoch 1461/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7577 - val_loss: 7.4117\n",
      "Epoch 1462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8100 - val_loss: 7.5491\n",
      "Epoch 1463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7273 - val_loss: 7.2128\n",
      "Epoch 1464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6798 - val_loss: 7.3058\n",
      "Epoch 1465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0216 - val_loss: 7.4827\n",
      "Epoch 1466/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9962 - val_loss: 7.4968\n",
      "Epoch 1467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0152 - val_loss: 7.6341\n",
      "Epoch 1468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8039 - val_loss: 7.3115\n",
      "Epoch 1469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7831 - val_loss: 7.1586\n",
      "Epoch 1470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6366 - val_loss: 7.1515\n",
      "Epoch 1471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7068 - val_loss: 7.1397\n",
      "Epoch 1472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6257 - val_loss: 7.2772\n",
      "Epoch 1473/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6602 - val_loss: 7.4840\n",
      "Epoch 1474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7176 - val_loss: 7.1439\n",
      "Epoch 1475/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6278 - val_loss: 7.3630\n",
      "Epoch 1476/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7068 - val_loss: 7.2544\n",
      "Epoch 1477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6125 - val_loss: 7.1091\n",
      "Epoch 1478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7441 - val_loss: 7.4672\n",
      "Epoch 1479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7056 - val_loss: 7.2172\n",
      "Epoch 1480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8646 - val_loss: 7.1106\n",
      "Epoch 1481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6909 - val_loss: 7.1679\n",
      "Epoch 1482/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7620 - val_loss: 7.1623\n",
      "Epoch 1483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7201 - val_loss: 7.0679\n",
      "Epoch 1484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7149 - val_loss: 7.2289\n",
      "Epoch 1485/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9813 - val_loss: 7.2839\n",
      "Epoch 1486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6798 - val_loss: 7.1491\n",
      "Epoch 1487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7506 - val_loss: 7.1557\n",
      "Epoch 1488/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6287 - val_loss: 7.2411\n",
      "Epoch 1489/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7415 - val_loss: 7.2774\n",
      "Epoch 1490/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7177 - val_loss: 7.5626\n",
      "Epoch 1491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8080 - val_loss: 7.2451\n",
      "Epoch 1492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6494 - val_loss: 7.1122\n",
      "Epoch 1493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8016 - val_loss: 7.2694\n",
      "Epoch 1494/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9370 - val_loss: 7.2934\n",
      "Epoch 1495/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 7.0466 - val_loss: 7.1162\n",
      "Epoch 1496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8441 - val_loss: 7.0846\n",
      "Epoch 1497/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8423 - val_loss: 7.0557\n",
      "Epoch 1498/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7149 - val_loss: 7.1093\n",
      "Epoch 1499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5897 - val_loss: 7.0610\n",
      "Epoch 1500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6075 - val_loss: 7.1273\n",
      "Epoch 1501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6295 - val_loss: 7.0903\n",
      "Epoch 1502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6551 - val_loss: 7.0962\n",
      "Epoch 1503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7172 - val_loss: 7.2554\n",
      "Epoch 1504/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6282 - val_loss: 7.0574\n",
      "Epoch 1505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6803 - val_loss: 7.1520\n",
      "Epoch 1506/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7395 - val_loss: 7.1190\n",
      "Epoch 1507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7274 - val_loss: 7.0910\n",
      "Epoch 1508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5909 - val_loss: 7.1263\n",
      "Epoch 1509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5729 - val_loss: 7.2441\n",
      "Epoch 1510/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.9119 - val_loss: 7.0922\n",
      "Epoch 1511/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7073 - val_loss: 7.1453\n",
      "Epoch 1512/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7154 - val_loss: 7.0866\n",
      "Epoch 1513/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6559 - val_loss: 7.3226\n",
      "Epoch 1514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6088 - val_loss: 7.1087\n",
      "Epoch 1515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6152 - val_loss: 7.0652\n",
      "Epoch 1516/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6037 - val_loss: 7.1060\n",
      "Epoch 1517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7237 - val_loss: 8.4913\n",
      "Epoch 1518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9004 - val_loss: 7.1853\n",
      "Epoch 1519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6624 - val_loss: 7.1085\n",
      "Epoch 1520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8135 - val_loss: 7.1129\n",
      "Epoch 1521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7411 - val_loss: 7.1192\n",
      "Epoch 1522/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7170 - val_loss: 7.4833\n",
      "Epoch 1523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6672 - val_loss: 7.0781\n",
      "Epoch 1524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7321 - val_loss: 7.0600\n",
      "Epoch 1525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5697 - val_loss: 7.0468\n",
      "Epoch 1526/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.8459 - val_loss: 7.1221\n",
      "Epoch 1527/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6105 - val_loss: 7.1224\n",
      "Epoch 1528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6348 - val_loss: 7.1763\n",
      "Epoch 1529/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5642 - val_loss: 7.0571\n",
      "Epoch 1530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7087 - val_loss: 7.3731\n",
      "Epoch 1531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5879 - val_loss: 7.6088\n",
      "Epoch 1532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7805 - val_loss: 7.9773\n",
      "Epoch 1533/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7070 - val_loss: 7.0157\n",
      "Epoch 1534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6373 - val_loss: 7.0339\n",
      "Epoch 1535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5467 - val_loss: 7.0753\n",
      "Epoch 1536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.8439 - val_loss: 7.2708\n",
      "Epoch 1537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.3423 - val_loss: 6.9932\n",
      "Epoch 1538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7944 - val_loss: 7.3741\n",
      "Epoch 1539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6379 - val_loss: 7.0876\n",
      "Epoch 1540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5431 - val_loss: 7.0595\n",
      "Epoch 1541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6203 - val_loss: 7.1366\n",
      "Epoch 1542/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0551 - val_loss: 7.1147\n",
      "Epoch 1543/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7384 - val_loss: 7.0513\n",
      "Epoch 1544/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5542 - val_loss: 7.1719\n",
      "Epoch 1545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6658 - val_loss: 7.0178\n",
      "Epoch 1546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6869 - val_loss: 7.0577\n",
      "Epoch 1547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7556 - val_loss: 7.0123\n",
      "Epoch 1548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6463 - val_loss: 7.3310\n",
      "Epoch 1549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6099 - val_loss: 7.1184\n",
      "Epoch 1550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6110 - val_loss: 7.0663\n",
      "Epoch 1551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6120 - val_loss: 7.0035\n",
      "Epoch 1552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9489 - val_loss: 7.1524\n",
      "Epoch 1553/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5825 - val_loss: 7.0405\n",
      "Epoch 1554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6170 - val_loss: 7.0582\n",
      "Epoch 1555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7021 - val_loss: 7.2184\n",
      "Epoch 1556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5812 - val_loss: 6.9977\n",
      "Epoch 1557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6091 - val_loss: 7.2291\n",
      "Epoch 1558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6177 - val_loss: 7.0393\n",
      "Epoch 1559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5604 - val_loss: 7.0968\n",
      "Epoch 1560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5718 - val_loss: 7.4220\n",
      "Epoch 1561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7078 - val_loss: 7.1563\n",
      "Epoch 1562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6390 - val_loss: 7.0556\n",
      "Epoch 1563/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6143 - val_loss: 7.0536\n",
      "Epoch 1564/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5826 - val_loss: 7.1013\n",
      "Epoch 1565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5628 - val_loss: 6.9815\n",
      "Epoch 1566/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5936 - val_loss: 7.0624\n",
      "Epoch 1567/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5217 - val_loss: 7.0399\n",
      "Epoch 1568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5253 - val_loss: 7.0299\n",
      "Epoch 1569/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6674 - val_loss: 7.3484\n",
      "Epoch 1570/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6692 - val_loss: 7.2641\n",
      "Epoch 1571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5889 - val_loss: 7.0019\n",
      "Epoch 1572/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5361 - val_loss: 7.1406\n",
      "Epoch 1573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6518 - val_loss: 7.0562\n",
      "Epoch 1574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5732 - val_loss: 7.5459\n",
      "Epoch 1575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5336 - val_loss: 7.0133\n",
      "Epoch 1576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5316 - val_loss: 6.9817\n",
      "Epoch 1577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5116 - val_loss: 7.0679\n",
      "Epoch 1578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5920 - val_loss: 7.1247\n",
      "Epoch 1579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5192 - val_loss: 7.2723\n",
      "Epoch 1580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6973 - val_loss: 6.9320\n",
      "Epoch 1581/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6102 - val_loss: 6.9737\n",
      "Epoch 1582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6192 - val_loss: 7.0353\n",
      "Epoch 1583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6329 - val_loss: 6.9396\n",
      "Epoch 1584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5655 - val_loss: 6.9570\n",
      "Epoch 1585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5757 - val_loss: 6.9745\n",
      "Epoch 1586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8947 - val_loss: 6.9969\n",
      "Epoch 1587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7013 - val_loss: 7.2069\n",
      "Epoch 1588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7289 - val_loss: 6.9201\n",
      "Epoch 1589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5272 - val_loss: 6.9955\n",
      "Epoch 1590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5903 - val_loss: 6.9756\n",
      "Epoch 1591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5670 - val_loss: 6.9696\n",
      "Epoch 1592/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4442 - val_loss: 7.1308\n",
      "Epoch 1593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5917 - val_loss: 7.0539\n",
      "Epoch 1594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5148 - val_loss: 6.9647\n",
      "Epoch 1595/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5564 - val_loss: 6.9500\n",
      "Epoch 1596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6262 - val_loss: 6.9661\n",
      "Epoch 1597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5743 - val_loss: 7.4429\n",
      "Epoch 1598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5540 - val_loss: 7.0315\n",
      "Epoch 1599/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.9489 - val_loss: 6.9794\n",
      "Epoch 1600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8088 - val_loss: 6.9687\n",
      "Epoch 1601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6356 - val_loss: 6.9519\n",
      "Epoch 1602/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.4776 - val_loss: 7.9483\n",
      "Epoch 1603/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7755 - val_loss: 7.1025\n",
      "Epoch 1604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5398 - val_loss: 7.1677\n",
      "Epoch 1605/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5074 - val_loss: 6.9485\n",
      "Epoch 1606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5093 - val_loss: 7.1990\n",
      "Epoch 1607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5543 - val_loss: 7.0934\n",
      "Epoch 1608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5516 - val_loss: 7.0415\n",
      "Epoch 1609/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4847 - val_loss: 6.9820\n",
      "Epoch 1610/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5503 - val_loss: 6.9311\n",
      "Epoch 1611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5728 - val_loss: 6.9866\n",
      "Epoch 1612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5482 - val_loss: 6.9288\n",
      "Epoch 1613/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4639 - val_loss: 7.1462\n",
      "Epoch 1614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4885 - val_loss: 6.9647\n",
      "Epoch 1615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5031 - val_loss: 6.9905\n",
      "Epoch 1616/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4445 - val_loss: 6.9156\n",
      "Epoch 1617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4277 - val_loss: 7.0912\n",
      "Epoch 1618/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5193 - val_loss: 6.9531\n",
      "Epoch 1619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5192 - val_loss: 6.9002\n",
      "Epoch 1620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6565 - val_loss: 7.1221\n",
      "Epoch 1621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7552 - val_loss: 6.9322\n",
      "Epoch 1622/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6393 - val_loss: 6.9231\n",
      "Epoch 1623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4898 - val_loss: 6.9016\n",
      "Epoch 1624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4364 - val_loss: 7.0268\n",
      "Epoch 1625/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4233 - val_loss: 6.8967\n",
      "Epoch 1626/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4345 - val_loss: 6.9332\n",
      "Epoch 1627/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7818 - val_loss: 7.5325\n",
      "Epoch 1628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4695 - val_loss: 6.9784\n",
      "Epoch 1629/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6076 - val_loss: 7.1486\n",
      "Epoch 1630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 7.0135 - val_loss: 7.6934\n",
      "Epoch 1631/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5673 - val_loss: 7.3933\n",
      "Epoch 1632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4446 - val_loss: 6.8901\n",
      "Epoch 1633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4484 - val_loss: 7.0650\n",
      "Epoch 1634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7175 - val_loss: 6.9995\n",
      "Epoch 1635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5790 - val_loss: 6.8759\n",
      "Epoch 1636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6424 - val_loss: 6.8583\n",
      "Epoch 1637/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6116 - val_loss: 7.0342\n",
      "Epoch 1638/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5056 - val_loss: 6.8766\n",
      "Epoch 1639/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3962 - val_loss: 7.0606\n",
      "Epoch 1640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4602 - val_loss: 6.8862\n",
      "Epoch 1641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4107 - val_loss: 6.8997\n",
      "Epoch 1642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4968 - val_loss: 6.9258\n",
      "Epoch 1643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5309 - val_loss: 6.9268\n",
      "Epoch 1644/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4673 - val_loss: 6.9217\n",
      "Epoch 1645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4655 - val_loss: 6.8694\n",
      "Epoch 1646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5092 - val_loss: 6.9129\n",
      "Epoch 1647/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5809 - val_loss: 6.8792\n",
      "Epoch 1648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6070 - val_loss: 6.9489\n",
      "Epoch 1649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6622 - val_loss: 6.9732\n",
      "Epoch 1650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5151 - val_loss: 6.9448\n",
      "Epoch 1651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4844 - val_loss: 6.9121\n",
      "Epoch 1652/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4230 - val_loss: 7.0590\n",
      "Epoch 1653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5026 - val_loss: 6.9696\n",
      "Epoch 1654/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5608 - val_loss: 6.8739\n",
      "Epoch 1655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4219 - val_loss: 6.8349\n",
      "Epoch 1656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5058 - val_loss: 6.8536\n",
      "Epoch 1657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5336 - val_loss: 6.9079\n",
      "Epoch 1658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6002 - val_loss: 6.9619\n",
      "Epoch 1659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4261 - val_loss: 6.8869\n",
      "Epoch 1660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4156 - val_loss: 7.0547\n",
      "Epoch 1661/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6171 - val_loss: 6.8841\n",
      "Epoch 1662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4337 - val_loss: 6.9103\n",
      "Epoch 1663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5357 - val_loss: 7.0283\n",
      "Epoch 1664/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5001 - val_loss: 6.8824\n",
      "Epoch 1665/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4379 - val_loss: 7.6310\n",
      "Epoch 1666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6848 - val_loss: 6.9542\n",
      "Epoch 1667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5154 - val_loss: 6.8537\n",
      "Epoch 1668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4973 - val_loss: 6.8522\n",
      "Epoch 1669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5295 - val_loss: 7.0501\n",
      "Epoch 1670/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3897 - val_loss: 7.0369\n",
      "Epoch 1671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4942 - val_loss: 6.8399\n",
      "Epoch 1672/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4046 - val_loss: 6.8477\n",
      "Epoch 1673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4051 - val_loss: 6.8088\n",
      "Epoch 1674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4256 - val_loss: 6.8392\n",
      "Epoch 1675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6472 - val_loss: 6.8754\n",
      "Epoch 1676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4133 - val_loss: 6.8169\n",
      "Epoch 1677/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5063 - val_loss: 6.9257\n",
      "Epoch 1678/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 91us/step - loss: 6.3933 - val_loss: 6.9325\n",
      "Epoch 1679/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.3894 - val_loss: 6.8534\n",
      "Epoch 1680/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4109 - val_loss: 6.9000\n",
      "Epoch 1681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3700 - val_loss: 7.0348\n",
      "Epoch 1682/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5640 - val_loss: 7.5122\n",
      "Epoch 1683/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5172 - val_loss: 7.0273\n",
      "Epoch 1684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4755 - val_loss: 7.0136\n",
      "Epoch 1685/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4860 - val_loss: 6.8689\n",
      "Epoch 1686/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6709 - val_loss: 6.9584\n",
      "Epoch 1687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5197 - val_loss: 6.8267\n",
      "Epoch 1688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3790 - val_loss: 6.8627\n",
      "Epoch 1689/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4715 - val_loss: 6.8040\n",
      "Epoch 1690/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4059 - val_loss: 6.7734\n",
      "Epoch 1691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3975 - val_loss: 7.4075\n",
      "Epoch 1692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6945 - val_loss: 6.7685\n",
      "Epoch 1693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4081 - val_loss: 6.8556\n",
      "Epoch 1694/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3988 - val_loss: 6.8081\n",
      "Epoch 1695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4922 - val_loss: 6.8327\n",
      "Epoch 1696/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5281 - val_loss: 6.8452\n",
      "Epoch 1697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4330 - val_loss: 7.1561\n",
      "Epoch 1698/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.6155 - val_loss: 7.2054\n",
      "Epoch 1699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5415 - val_loss: 6.8667\n",
      "Epoch 1700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3402 - val_loss: 6.7649\n",
      "Epoch 1701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3803 - val_loss: 7.0235\n",
      "Epoch 1702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3948 - val_loss: 6.8615\n",
      "Epoch 1703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3488 - val_loss: 6.8646\n",
      "Epoch 1704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3893 - val_loss: 6.7534\n",
      "Epoch 1705/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4489 - val_loss: 6.7706\n",
      "Epoch 1706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4437 - val_loss: 6.7733\n",
      "Epoch 1707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3268 - val_loss: 6.9401\n",
      "Epoch 1708/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 6.4795 - val_loss: 6.8180\n",
      "Epoch 1709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3806 - val_loss: 6.7897\n",
      "Epoch 1710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3880 - val_loss: 6.7776\n",
      "Epoch 1711/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3196 - val_loss: 7.0883\n",
      "Epoch 1712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5159 - val_loss: 7.0953\n",
      "Epoch 1713/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5060 - val_loss: 6.8762\n",
      "Epoch 1714/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4069 - val_loss: 7.8629\n",
      "Epoch 1715/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4157 - val_loss: 6.7966\n",
      "Epoch 1716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3510 - val_loss: 6.7526\n",
      "Epoch 1717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3396 - val_loss: 6.7558\n",
      "Epoch 1718/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3482 - val_loss: 6.7508\n",
      "Epoch 1719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4409 - val_loss: 6.7707\n",
      "Epoch 1720/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4774 - val_loss: 7.3861\n",
      "Epoch 1721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5206 - val_loss: 6.7284\n",
      "Epoch 1722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4228 - val_loss: 6.8061\n",
      "Epoch 1723/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4247 - val_loss: 6.7858\n",
      "Epoch 1724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4106 - val_loss: 7.0533\n",
      "Epoch 1725/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4318 - val_loss: 6.7449\n",
      "Epoch 1726/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3214 - val_loss: 6.7582\n",
      "Epoch 1727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3116 - val_loss: 6.8026\n",
      "Epoch 1728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5507 - val_loss: 7.1300\n",
      "Epoch 1729/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6040 - val_loss: 6.7682\n",
      "Epoch 1730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4030 - val_loss: 6.9804\n",
      "Epoch 1731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5133 - val_loss: 6.7794\n",
      "Epoch 1732/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3285 - val_loss: 6.7330\n",
      "Epoch 1733/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3049 - val_loss: 7.5003\n",
      "Epoch 1734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4051 - val_loss: 7.3032\n",
      "Epoch 1735/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4483 - val_loss: 6.7521\n",
      "Epoch 1736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2929 - val_loss: 6.8027\n",
      "Epoch 1737/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.6211 - val_loss: 6.7289\n",
      "Epoch 1738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8572 - val_loss: 6.7732\n",
      "Epoch 1739/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5191 - val_loss: 6.6935\n",
      "Epoch 1740/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4222 - val_loss: 6.7192\n",
      "Epoch 1741/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4604 - val_loss: 6.7631\n",
      "Epoch 1742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3674 - val_loss: 6.7438\n",
      "Epoch 1743/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3148 - val_loss: 6.7318\n",
      "Epoch 1744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2994 - val_loss: 6.7224\n",
      "Epoch 1745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3610 - val_loss: 7.1993\n",
      "Epoch 1746/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4744 - val_loss: 6.8635\n",
      "Epoch 1747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5088 - val_loss: 7.0268\n",
      "Epoch 1748/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3775 - val_loss: 6.7046\n",
      "Epoch 1749/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3097 - val_loss: 7.1359\n",
      "Epoch 1750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4672 - val_loss: 6.8414\n",
      "Epoch 1751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3303 - val_loss: 6.9356\n",
      "Epoch 1752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4423 - val_loss: 6.7914\n",
      "Epoch 1753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3119 - val_loss: 6.6963\n",
      "Epoch 1754/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.4275 - val_loss: 6.7238\n",
      "Epoch 1755/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4227 - val_loss: 6.7392\n",
      "Epoch 1756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3026 - val_loss: 6.7494\n",
      "Epoch 1757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3476 - val_loss: 6.9253\n",
      "Epoch 1758/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.7824 - val_loss: 6.7181\n",
      "Epoch 1759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3418 - val_loss: 6.6823\n",
      "Epoch 1760/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2973 - val_loss: 6.7136\n",
      "Epoch 1761/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3631 - val_loss: 6.7513\n",
      "Epoch 1762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3761 - val_loss: 6.6982\n",
      "Epoch 1763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2864 - val_loss: 6.7459\n",
      "Epoch 1764/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3248 - val_loss: 6.7735\n",
      "Epoch 1765/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5088 - val_loss: 6.8346\n",
      "Epoch 1766/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3647 - val_loss: 6.7824\n",
      "Epoch 1767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3740 - val_loss: 6.8543\n",
      "Epoch 1768/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3652 - val_loss: 6.7729\n",
      "Epoch 1769/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3512 - val_loss: 6.7613\n",
      "Epoch 1770/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2682 - val_loss: 6.7682\n",
      "Epoch 1771/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3203 - val_loss: 6.8185\n",
      "Epoch 1772/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3824 - val_loss: 6.8963\n",
      "Epoch 1773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4032 - val_loss: 6.6943\n",
      "Epoch 1774/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2979 - val_loss: 6.6929\n",
      "Epoch 1775/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2622 - val_loss: 6.7029\n",
      "Epoch 1776/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2789 - val_loss: 6.9282\n",
      "Epoch 1777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4364 - val_loss: 6.7309\n",
      "Epoch 1778/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3308 - val_loss: 6.6346\n",
      "Epoch 1779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3150 - val_loss: 6.7296\n",
      "Epoch 1780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2542 - val_loss: 6.7061\n",
      "Epoch 1781/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5160 - val_loss: 6.6454\n",
      "Epoch 1782/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.6487 - val_loss: 6.7221\n",
      "Epoch 1783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2327 - val_loss: 6.6707\n",
      "Epoch 1784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3460 - val_loss: 6.7053\n",
      "Epoch 1785/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.7563 - val_loss: 6.9546\n",
      "Epoch 1786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3675 - val_loss: 6.6915\n",
      "Epoch 1787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2145 - val_loss: 6.7471\n",
      "Epoch 1788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2558 - val_loss: 6.8733\n",
      "Epoch 1789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3078 - val_loss: 6.8378\n",
      "Epoch 1790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3506 - val_loss: 6.6393\n",
      "Epoch 1791/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3310 - val_loss: 6.6830\n",
      "Epoch 1792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3278 - val_loss: 6.7705\n",
      "Epoch 1793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3890 - val_loss: 8.0459\n",
      "Epoch 1794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6202 - val_loss: 6.7449\n",
      "Epoch 1795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3282 - val_loss: 6.9652\n",
      "Epoch 1796/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3034 - val_loss: 6.6560\n",
      "Epoch 1797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3884 - val_loss: 6.7055\n",
      "Epoch 1798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3201 - val_loss: 6.6818\n",
      "Epoch 1799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3417 - val_loss: 6.6670\n",
      "Epoch 1800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3037 - val_loss: 6.6157\n",
      "Epoch 1801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2229 - val_loss: 6.7165\n",
      "Epoch 1802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4119 - val_loss: 6.6589\n",
      "Epoch 1803/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2472 - val_loss: 6.6756\n",
      "Epoch 1804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4482 - val_loss: 7.0350\n",
      "Epoch 1805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3818 - val_loss: 6.7582\n",
      "Epoch 1806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6284 - val_loss: 6.6496\n",
      "Epoch 1807/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5514 - val_loss: 6.6935\n",
      "Epoch 1808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2949 - val_loss: 6.6155\n",
      "Epoch 1809/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.2208 - val_loss: 6.6546\n",
      "Epoch 1810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2947 - val_loss: 6.6378\n",
      "Epoch 1811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2476 - val_loss: 6.6729\n",
      "Epoch 1812/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3046 - val_loss: 7.0526\n",
      "Epoch 1813/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4048 - val_loss: 6.6493\n",
      "Epoch 1814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2798 - val_loss: 7.1911\n",
      "Epoch 1815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4279 - val_loss: 6.8566\n",
      "Epoch 1816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2785 - val_loss: 6.7016\n",
      "Epoch 1817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2653 - val_loss: 6.6009\n",
      "Epoch 1818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2875 - val_loss: 6.6127\n",
      "Epoch 1819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2327 - val_loss: 6.7786\n",
      "Epoch 1820/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2687 - val_loss: 6.6837\n",
      "Epoch 1821/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2070 - val_loss: 6.6145\n",
      "Epoch 1822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2904 - val_loss: 6.6014\n",
      "Epoch 1823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2340 - val_loss: 6.6557\n",
      "Epoch 1824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2216 - val_loss: 6.7588\n",
      "Epoch 1825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4144 - val_loss: 6.6113\n",
      "Epoch 1826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2325 - val_loss: 6.8917\n",
      "Epoch 1827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2438 - val_loss: 6.8215\n",
      "Epoch 1828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3692 - val_loss: 6.8196\n",
      "Epoch 1829/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2586 - val_loss: 6.6415\n",
      "Epoch 1830/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 6.2775 - val_loss: 6.6123\n",
      "Epoch 1831/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3352 - val_loss: 6.5985\n",
      "Epoch 1832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2533 - val_loss: 6.5938\n",
      "Epoch 1833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4316 - val_loss: 6.5924\n",
      "Epoch 1834/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1892 - val_loss: 6.5957\n",
      "Epoch 1835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1985 - val_loss: 6.6844\n",
      "Epoch 1836/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2722 - val_loss: 6.6248\n",
      "Epoch 1837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3102 - val_loss: 6.6083\n",
      "Epoch 1838/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2229 - val_loss: 7.1853\n",
      "Epoch 1839/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2962 - val_loss: 6.8570\n",
      "Epoch 1840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2420 - val_loss: 6.5930\n",
      "Epoch 1841/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1953 - val_loss: 6.7114\n",
      "Epoch 1842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2948 - val_loss: 6.7881\n",
      "Epoch 1843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3061 - val_loss: 6.6981\n",
      "Epoch 1844/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3755 - val_loss: 6.7964\n",
      "Epoch 1845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2863 - val_loss: 6.9946\n",
      "Epoch 1846/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3453 - val_loss: 6.7874\n",
      "Epoch 1847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2099 - val_loss: 6.6694\n",
      "Epoch 1848/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3341 - val_loss: 6.6760\n",
      "Epoch 1849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1700 - val_loss: 6.8557\n",
      "Epoch 1850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1957 - val_loss: 6.6452\n",
      "Epoch 1851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2030 - val_loss: 6.9396\n",
      "Epoch 1852/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5158 - val_loss: 6.6996\n",
      "Epoch 1853/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2738 - val_loss: 6.5555\n",
      "Epoch 1854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1699 - val_loss: 6.6585\n",
      "Epoch 1855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1502 - val_loss: 6.6723\n",
      "Epoch 1856/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2007 - val_loss: 6.5764\n",
      "Epoch 1857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2106 - val_loss: 6.6661\n",
      "Epoch 1858/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.4346 - val_loss: 6.6161\n",
      "Epoch 1859/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2528 - val_loss: 6.8918\n",
      "Epoch 1860/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5482 - val_loss: 7.7085\n",
      "Epoch 1861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3525 - val_loss: 6.5428\n",
      "Epoch 1862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2857 - val_loss: 6.9342\n",
      "Epoch 1863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3381 - val_loss: 6.6934\n",
      "Epoch 1864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2350 - val_loss: 6.6224\n",
      "Epoch 1865/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3112 - val_loss: 6.9599\n",
      "Epoch 1866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3665 - val_loss: 6.5832\n",
      "Epoch 1867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3018 - val_loss: 6.5511\n",
      "Epoch 1868/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3396 - val_loss: 6.8552\n",
      "Epoch 1869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2880 - val_loss: 6.5372\n",
      "Epoch 1870/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2686 - val_loss: 6.5777\n",
      "Epoch 1871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2635 - val_loss: 6.8304\n",
      "Epoch 1872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2799 - val_loss: 6.5427\n",
      "Epoch 1873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2563 - val_loss: 6.5296\n",
      "Epoch 1874/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1378 - val_loss: 6.6384\n",
      "Epoch 1875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2000 - val_loss: 6.6000\n",
      "Epoch 1876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1766 - val_loss: 6.6969\n",
      "Epoch 1877/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2584 - val_loss: 6.6075\n",
      "Epoch 1878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1676 - val_loss: 6.5464\n",
      "Epoch 1879/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2205 - val_loss: 6.5256\n",
      "Epoch 1880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1348 - val_loss: 6.5939\n",
      "Epoch 1881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2220 - val_loss: 6.5121\n",
      "Epoch 1882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1791 - val_loss: 6.7692\n",
      "Epoch 1883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3007 - val_loss: 6.5859\n",
      "Epoch 1884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1936 - val_loss: 6.5719\n",
      "Epoch 1885/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2901 - val_loss: 6.5130\n",
      "Epoch 1886/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3390 - val_loss: 6.8514\n",
      "Epoch 1887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.8633 - val_loss: 6.5567\n",
      "Epoch 1888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.6335 - val_loss: 6.5034\n",
      "Epoch 1889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1434 - val_loss: 6.5589\n",
      "Epoch 1890/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1676 - val_loss: 6.6926\n",
      "Epoch 1891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1935 - val_loss: 6.5628\n",
      "Epoch 1892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2016 - val_loss: 6.5448\n",
      "Epoch 1893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3383 - val_loss: 6.5531\n",
      "Epoch 1894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.9887 - val_loss: 6.5106\n",
      "Epoch 1895/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 6.2539 - val_loss: 6.6460\n",
      "Epoch 1896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2195 - val_loss: 6.9959\n",
      "Epoch 1897/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.5329 - val_loss: 6.8956\n",
      "Epoch 1898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2765 - val_loss: 6.6092\n",
      "Epoch 1899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1839 - val_loss: 6.5456\n",
      "Epoch 1900/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1799 - val_loss: 6.6273\n",
      "Epoch 1901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2215 - val_loss: 6.5752\n",
      "Epoch 1902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1631 - val_loss: 6.5412\n",
      "Epoch 1903/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1192 - val_loss: 6.5733\n",
      "Epoch 1904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2119 - val_loss: 6.5628\n",
      "Epoch 1905/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1532 - val_loss: 6.9222\n",
      "Epoch 1906/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.2006 - val_loss: 6.5625\n",
      "Epoch 1907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1415 - val_loss: 6.6883\n",
      "Epoch 1908/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3382 - val_loss: 6.5354\n",
      "Epoch 1909/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2792 - val_loss: 6.5396\n",
      "Epoch 1910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1713 - val_loss: 7.0402\n",
      "Epoch 1911/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 6.1992 - val_loss: 6.6756\n",
      "Epoch 1912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2539 - val_loss: 6.6826\n",
      "Epoch 1913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3209 - val_loss: 6.6163\n",
      "Epoch 1914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2138 - val_loss: 6.5338\n",
      "Epoch 1915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3274 - val_loss: 6.5604\n",
      "Epoch 1916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1495 - val_loss: 6.5334\n",
      "Epoch 1917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4934 - val_loss: 7.0179\n",
      "Epoch 1918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5533 - val_loss: 6.6135\n",
      "Epoch 1919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1237 - val_loss: 7.0403\n",
      "Epoch 1920/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1982 - val_loss: 6.5085\n",
      "Epoch 1921/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3171 - val_loss: 6.6865\n",
      "Epoch 1922/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.626 - 0s 46us/step - loss: 6.1930 - val_loss: 6.4929\n",
      "Epoch 1923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1584 - val_loss: 6.9177\n",
      "Epoch 1924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4251 - val_loss: 6.6009\n",
      "Epoch 1925/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1807 - val_loss: 6.5354\n",
      "Epoch 1926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2201 - val_loss: 6.4852\n",
      "Epoch 1927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2795 - val_loss: 7.0854\n",
      "Epoch 1928/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2428 - val_loss: 6.4864\n",
      "Epoch 1929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4138 - val_loss: 6.4732\n",
      "Epoch 1930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1716 - val_loss: 6.6853\n",
      "Epoch 1931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1898 - val_loss: 6.5024\n",
      "Epoch 1932/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1036 - val_loss: 6.5197\n",
      "Epoch 1933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2358 - val_loss: 6.5815\n",
      "Epoch 1934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0945 - val_loss: 6.5621\n",
      "Epoch 1935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2456 - val_loss: 6.4909\n",
      "Epoch 1936/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3576 - val_loss: 6.6334\n",
      "Epoch 1937/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2195 - val_loss: 6.5021\n",
      "Epoch 1938/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1890 - val_loss: 6.5009\n",
      "Epoch 1939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3932 - val_loss: 6.6138\n",
      "Epoch 1940/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5662 - val_loss: 6.9967\n",
      "Epoch 1941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3757 - val_loss: 6.6250\n",
      "Epoch 1942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1703 - val_loss: 6.9138\n",
      "Epoch 1943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2192 - val_loss: 6.5953\n",
      "Epoch 1944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1724 - val_loss: 6.4683\n",
      "Epoch 1945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0815 - val_loss: 6.5277\n",
      "Epoch 1946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0892 - val_loss: 6.6328\n",
      "Epoch 1947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1481 - val_loss: 6.4643\n",
      "Epoch 1948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3345 - val_loss: 6.7438\n",
      "Epoch 1949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3300 - val_loss: 6.6282\n",
      "Epoch 1950/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1286 - val_loss: 6.5488\n",
      "Epoch 1951/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2415 - val_loss: 6.5838\n",
      "Epoch 1952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2125 - val_loss: 6.8838\n",
      "Epoch 1953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2931 - val_loss: 6.4850\n",
      "Epoch 1954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0890 - val_loss: 6.4688\n",
      "Epoch 1955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1138 - val_loss: 6.6081\n",
      "Epoch 1956/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.1554 - val_loss: 6.5255\n",
      "Epoch 1957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1646 - val_loss: 6.4403\n",
      "Epoch 1958/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1934 - val_loss: 7.0555\n",
      "Epoch 1959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4538 - val_loss: 6.9185\n",
      "Epoch 1960/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2549 - val_loss: 6.5533\n",
      "Epoch 1961/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.1217 - val_loss: 6.5396\n",
      "Epoch 1962/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0911 - val_loss: 6.5524\n",
      "Epoch 1963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1046 - val_loss: 6.6216\n",
      "Epoch 1964/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2830 - val_loss: 6.5558\n",
      "Epoch 1965/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 6.0896 - val_loss: 6.5841\n",
      "Epoch 1966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1919 - val_loss: 6.4658\n",
      "Epoch 1967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1277 - val_loss: 6.4447\n",
      "Epoch 1968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1709 - val_loss: 6.4716\n",
      "Epoch 1969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1789 - val_loss: 6.7743\n",
      "Epoch 1970/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 6.1043 - val_loss: 6.6639\n",
      "Epoch 1971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1928 - val_loss: 6.4724\n",
      "Epoch 1972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2128 - val_loss: 6.7465\n",
      "Epoch 1973/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.3460 - val_loss: 6.5073\n",
      "Epoch 1974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0942 - val_loss: 6.5146\n",
      "Epoch 1975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1265 - val_loss: 6.6221\n",
      "Epoch 1976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1966 - val_loss: 6.5328\n",
      "Epoch 1977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0532 - val_loss: 6.4400\n",
      "Epoch 1978/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0301 - val_loss: 6.4954\n",
      "Epoch 1979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0507 - val_loss: 6.6103\n",
      "Epoch 1980/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1374 - val_loss: 6.7038\n",
      "Epoch 1981/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.1053 - val_loss: 6.6019\n",
      "Epoch 1982/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.0512 - val_loss: 6.4765\n",
      "Epoch 1983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1372 - val_loss: 6.4590\n",
      "Epoch 1984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1830 - val_loss: 6.4019\n",
      "Epoch 1985/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 6.1411 - val_loss: 6.7451\n",
      "Epoch 1986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1337 - val_loss: 6.4481\n",
      "Epoch 1987/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1622 - val_loss: 6.4661\n",
      "Epoch 1988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0578 - val_loss: 6.4386\n",
      "Epoch 1989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1517 - val_loss: 6.7337\n",
      "Epoch 1990/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0978 - val_loss: 6.4569\n",
      "Epoch 1991/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0354 - val_loss: 6.4207\n",
      "Epoch 1992/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0963 - val_loss: 6.4629\n",
      "Epoch 1993/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.1577 - val_loss: 6.7063\n",
      "Epoch 1994/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2605 - val_loss: 7.2920\n",
      "Epoch 1995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2169 - val_loss: 6.5325\n",
      "Epoch 1996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1753 - val_loss: 6.7563\n",
      "Epoch 1997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1358 - val_loss: 6.4302\n",
      "Epoch 1998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0871 - val_loss: 6.5524\n",
      "Epoch 1999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3450 - val_loss: 7.1934\n",
      "Epoch 2000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2557 - val_loss: 6.3958\n",
      "Epoch 2001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2229 - val_loss: 6.3963\n",
      "Epoch 2002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0738 - val_loss: 6.4113\n",
      "Epoch 2003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1637 - val_loss: 6.5242\n",
      "Epoch 2004/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 6.0881 - val_loss: 6.4491\n",
      "Epoch 2005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1358 - val_loss: 6.4329\n",
      "Epoch 2006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0228 - val_loss: 6.4857\n",
      "Epoch 2007/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0805 - val_loss: 6.4249\n",
      "Epoch 2008/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0414 - val_loss: 6.3791\n",
      "Epoch 2009/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0220 - val_loss: 6.4826\n",
      "Epoch 2010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0512 - val_loss: 6.3885\n",
      "Epoch 2011/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0811 - val_loss: 6.4330\n",
      "Epoch 2012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0143 - val_loss: 6.4399\n",
      "Epoch 2013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0663 - val_loss: 6.4456\n",
      "Epoch 2014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1509 - val_loss: 6.9972\n",
      "Epoch 2015/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0826 - val_loss: 6.4410\n",
      "Epoch 2016/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0749 - val_loss: 6.5856\n",
      "Epoch 2017/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0728 - val_loss: 6.8664\n",
      "Epoch 2018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2186 - val_loss: 6.4424\n",
      "Epoch 2019/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0251 - val_loss: 6.3478\n",
      "Epoch 2020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0333 - val_loss: 6.4473\n",
      "Epoch 2021/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2047 - val_loss: 6.4082\n",
      "Epoch 2022/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1694 - val_loss: 6.3515\n",
      "Epoch 2023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0737 - val_loss: 6.3663\n",
      "Epoch 2024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0720 - val_loss: 6.5249\n",
      "Epoch 2025/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2768 - val_loss: 6.3835\n",
      "Epoch 2026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0551 - val_loss: 6.4865\n",
      "Epoch 2027/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2361 - val_loss: 6.6323\n",
      "Epoch 2028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0186 - val_loss: 6.3584\n",
      "Epoch 2029/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.2126 - val_loss: 6.5183\n",
      "Epoch 2030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2221 - val_loss: 6.4122\n",
      "Epoch 2031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1572 - val_loss: 6.5030\n",
      "Epoch 2032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0634 - val_loss: 6.3689\n",
      "Epoch 2033/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0336 - val_loss: 6.3676\n",
      "Epoch 2034/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9763 - val_loss: 6.9403\n",
      "Epoch 2035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9860 - val_loss: 6.6587\n",
      "Epoch 2036/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9940 - val_loss: 6.7346\n",
      "Epoch 2037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1103 - val_loss: 6.4401\n",
      "Epoch 2038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0111 - val_loss: 6.3915\n",
      "Epoch 2039/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 7.762 - 0s 46us/step - loss: 6.0736 - val_loss: 6.8033\n",
      "Epoch 2040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0978 - val_loss: 7.0531\n",
      "Epoch 2041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1762 - val_loss: 6.3741\n",
      "Epoch 2042/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1132 - val_loss: 6.4542\n",
      "Epoch 2043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9822 - val_loss: 6.4840\n",
      "Epoch 2044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1011 - val_loss: 6.3193\n",
      "Epoch 2045/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1085 - val_loss: 6.3780\n",
      "Epoch 2046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9851 - val_loss: 6.7219\n",
      "Epoch 2047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1666 - val_loss: 6.8347\n",
      "Epoch 2048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1301 - val_loss: 6.4482\n",
      "Epoch 2049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1535 - val_loss: 6.6064\n",
      "Epoch 2050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0753 - val_loss: 6.3606\n",
      "Epoch 2051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9521 - val_loss: 6.4823\n",
      "Epoch 2052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9779 - val_loss: 6.4949\n",
      "Epoch 2053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0821 - val_loss: 6.5444\n",
      "Epoch 2054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0082 - val_loss: 6.3447\n",
      "Epoch 2055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9803 - val_loss: 6.3731\n",
      "Epoch 2056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0612 - val_loss: 6.4008\n",
      "Epoch 2057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1477 - val_loss: 6.7162\n",
      "Epoch 2058/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.3344 - val_loss: 6.3467\n",
      "Epoch 2059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9885 - val_loss: 6.3653\n",
      "Epoch 2060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9854 - val_loss: 6.6035\n",
      "Epoch 2061/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1829 - val_loss: 6.4735\n",
      "Epoch 2062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9998 - val_loss: 6.3566\n",
      "Epoch 2063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.4653 - val_loss: 6.5596\n",
      "Epoch 2064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0854 - val_loss: 6.3114\n",
      "Epoch 2065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2098 - val_loss: 6.4076\n",
      "Epoch 2066/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0466 - val_loss: 6.3729\n",
      "Epoch 2067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0142 - val_loss: 6.3099\n",
      "Epoch 2068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9724 - val_loss: 6.3926\n",
      "Epoch 2069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0098 - val_loss: 6.4937\n",
      "Epoch 2070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0012 - val_loss: 6.3654\n",
      "Epoch 2071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0258 - val_loss: 6.8838\n",
      "Epoch 2072/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1144 - val_loss: 6.3815\n",
      "Epoch 2073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0949 - val_loss: 7.1270\n",
      "Epoch 2074/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3181 - val_loss: 6.8315\n",
      "Epoch 2075/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2248 - val_loss: 6.6970\n",
      "Epoch 2076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1017 - val_loss: 6.3428\n",
      "Epoch 2077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9659 - val_loss: 6.3464\n",
      "Epoch 2078/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.9599 - val_loss: 6.3560\n",
      "Epoch 2079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1566 - val_loss: 6.4768\n",
      "Epoch 2080/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1359 - val_loss: 6.3314\n",
      "Epoch 2081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0242 - val_loss: 6.3771\n",
      "Epoch 2082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9245 - val_loss: 6.3611\n",
      "Epoch 2083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0027 - val_loss: 6.3311\n",
      "Epoch 2084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9514 - val_loss: 6.2878\n",
      "Epoch 2085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9469 - val_loss: 6.3268\n",
      "Epoch 2086/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9371 - val_loss: 6.3561\n",
      "Epoch 2087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9757 - val_loss: 6.3040\n",
      "Epoch 2088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9136 - val_loss: 6.3111\n",
      "Epoch 2089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9280 - val_loss: 6.3100\n",
      "Epoch 2090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1711 - val_loss: 6.2623\n",
      "Epoch 2091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0231 - val_loss: 6.3145\n",
      "Epoch 2092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1023 - val_loss: 6.5071\n",
      "Epoch 2093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0017 - val_loss: 6.3040\n",
      "Epoch 2094/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1336 - val_loss: 6.8282\n",
      "Epoch 2095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0508 - val_loss: 6.3107\n",
      "Epoch 2096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9305 - val_loss: 6.2877\n",
      "Epoch 2097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9276 - val_loss: 6.3032\n",
      "Epoch 2098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0220 - val_loss: 6.3906\n",
      "Epoch 2099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9679 - val_loss: 6.6421\n",
      "Epoch 2100/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 6.1301 - val_loss: 6.6023\n",
      "Epoch 2101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0926 - val_loss: 6.4882\n",
      "Epoch 2102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1041 - val_loss: 6.5691\n",
      "Epoch 2103/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9974 - val_loss: 6.3557\n",
      "Epoch 2104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9540 - val_loss: 6.3083\n",
      "Epoch 2105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0340 - val_loss: 6.2973\n",
      "Epoch 2106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9402 - val_loss: 6.6931\n",
      "Epoch 2107/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1079 - val_loss: 6.4822\n",
      "Epoch 2108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0616 - val_loss: 6.2841\n",
      "Epoch 2109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0884 - val_loss: 6.5231\n",
      "Epoch 2110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2866 - val_loss: 6.5614\n",
      "Epoch 2111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1593 - val_loss: 7.9479\n",
      "Epoch 2112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2542 - val_loss: 6.4362\n",
      "Epoch 2113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1635 - val_loss: 6.2872\n",
      "Epoch 2114/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0040 - val_loss: 6.4466\n",
      "Epoch 2115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0763 - val_loss: 6.2738\n",
      "Epoch 2116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9929 - val_loss: 6.3424\n",
      "Epoch 2117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9174 - val_loss: 6.5132\n",
      "Epoch 2118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0723 - val_loss: 6.8524\n",
      "Epoch 2119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0305 - val_loss: 6.3556\n",
      "Epoch 2120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9114 - val_loss: 6.3026\n",
      "Epoch 2121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9104 - val_loss: 6.4487\n",
      "Epoch 2122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9882 - val_loss: 6.3408\n",
      "Epoch 2123/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9944 - val_loss: 6.3473\n",
      "Epoch 2124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9710 - val_loss: 6.3013\n",
      "Epoch 2125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0254 - val_loss: 6.6968\n",
      "Epoch 2126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9253 - val_loss: 6.2889\n",
      "Epoch 2127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1618 - val_loss: 6.4136\n",
      "Epoch 2128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1886 - val_loss: 6.3231\n",
      "Epoch 2129/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9370 - val_loss: 6.5577\n",
      "Epoch 2130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0531 - val_loss: 6.2849\n",
      "Epoch 2131/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.9940 - val_loss: 6.2711\n",
      "Epoch 2132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9315 - val_loss: 6.2483\n",
      "Epoch 2133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9246 - val_loss: 6.2840\n",
      "Epoch 2134/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 6.0510 - val_loss: 7.1871\n",
      "Epoch 2135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.5919 - val_loss: 6.5905\n",
      "Epoch 2136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1185 - val_loss: 6.2928\n",
      "Epoch 2137/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 6.0582 - val_loss: 6.5741\n",
      "Epoch 2138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1241 - val_loss: 6.2589\n",
      "Epoch 2139/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9463 - val_loss: 6.2457\n",
      "Epoch 2140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9103 - val_loss: 6.2934\n",
      "Epoch 2141/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9067 - val_loss: 6.2728\n",
      "Epoch 2142/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0047 - val_loss: 6.5076\n",
      "Epoch 2143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9561 - val_loss: 6.2486\n",
      "Epoch 2144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9287 - val_loss: 6.2547\n",
      "Epoch 2145/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9627 - val_loss: 6.2401\n",
      "Epoch 2146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9135 - val_loss: 6.6881\n",
      "Epoch 2147/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.1527 - val_loss: 6.2344\n",
      "Epoch 2148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8837 - val_loss: 6.3055\n",
      "Epoch 2149/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9212 - val_loss: 6.2052\n",
      "Epoch 2150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9101 - val_loss: 6.3347\n",
      "Epoch 2151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0043 - val_loss: 6.2898\n",
      "Epoch 2152/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 6.2232 - val_loss: 6.3173\n",
      "Epoch 2153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9444 - val_loss: 6.2038\n",
      "Epoch 2154/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8831 - val_loss: 6.2962\n",
      "Epoch 2155/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1280 - val_loss: 6.2762\n",
      "Epoch 2156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1130 - val_loss: 6.2939\n",
      "Epoch 2157/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.9514 - val_loss: 6.2679\n",
      "Epoch 2158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8725 - val_loss: 6.2853\n",
      "Epoch 2159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0153 - val_loss: 6.2877\n",
      "Epoch 2160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9679 - val_loss: 6.2194\n",
      "Epoch 2161/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9434 - val_loss: 6.2810\n",
      "Epoch 2162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8404 - val_loss: 6.3826\n",
      "Epoch 2163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9327 - val_loss: 6.3096\n",
      "Epoch 2164/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8750 - val_loss: 6.4130\n",
      "Epoch 2165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9401 - val_loss: 6.3406\n",
      "Epoch 2166/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8811 - val_loss: 6.2983\n",
      "Epoch 2167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9195 - val_loss: 6.3055\n",
      "Epoch 2168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8981 - val_loss: 6.3662\n",
      "Epoch 2169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8879 - val_loss: 6.2543\n",
      "Epoch 2170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0932 - val_loss: 6.1832\n",
      "Epoch 2171/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9937 - val_loss: 6.1986\n",
      "Epoch 2172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1330 - val_loss: 6.7421\n",
      "Epoch 2173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0869 - val_loss: 6.2712\n",
      "Epoch 2174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9004 - val_loss: 6.2630\n",
      "Epoch 2175/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9103 - val_loss: 6.2874\n",
      "Epoch 2176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1607 - val_loss: 6.2301\n",
      "Epoch 2177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2059 - val_loss: 6.3595\n",
      "Epoch 2178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8365 - val_loss: 6.2638\n",
      "Epoch 2179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0122 - val_loss: 6.4303\n",
      "Epoch 2180/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9635 - val_loss: 6.1827\n",
      "Epoch 2181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8645 - val_loss: 6.2141\n",
      "Epoch 2182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9498 - val_loss: 6.2353\n",
      "Epoch 2183/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.9508 - val_loss: 6.5173\n",
      "Epoch 2184/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9237 - val_loss: 6.2856\n",
      "Epoch 2185/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9376 - val_loss: 6.4894\n",
      "Epoch 2186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9383 - val_loss: 6.6189\n",
      "Epoch 2187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9533 - val_loss: 6.4079\n",
      "Epoch 2188/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9885 - val_loss: 6.5934\n",
      "Epoch 2189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9464 - val_loss: 6.3157\n",
      "Epoch 2190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8277 - val_loss: 6.3761\n",
      "Epoch 2191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9405 - val_loss: 6.5140\n",
      "Epoch 2192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0011 - val_loss: 6.9937\n",
      "Epoch 2193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0380 - val_loss: 6.4994\n",
      "Epoch 2194/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8587 - val_loss: 6.1676\n",
      "Epoch 2195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8658 - val_loss: 6.2346\n",
      "Epoch 2196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8607 - val_loss: 6.5389\n",
      "Epoch 2197/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8642 - val_loss: 6.2945\n",
      "Epoch 2198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8317 - val_loss: 6.4236\n",
      "Epoch 2199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9115 - val_loss: 6.2275\n",
      "Epoch 2200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8621 - val_loss: 6.2252\n",
      "Epoch 2201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8621 - val_loss: 6.2804\n",
      "Epoch 2202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8684 - val_loss: 6.2735\n",
      "Epoch 2203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8984 - val_loss: 6.2241\n",
      "Epoch 2204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1803 - val_loss: 6.2006\n",
      "Epoch 2205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.3183 - val_loss: 6.2037\n",
      "Epoch 2206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0112 - val_loss: 6.3520\n",
      "Epoch 2207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9181 - val_loss: 6.2471\n",
      "Epoch 2208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8404 - val_loss: 6.2907\n",
      "Epoch 2209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9989 - val_loss: 6.1815\n",
      "Epoch 2210/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.8578 - val_loss: 6.1746\n",
      "Epoch 2211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1115 - val_loss: 6.3384\n",
      "Epoch 2212/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8606 - val_loss: 6.3018\n",
      "Epoch 2213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8525 - val_loss: 6.6178\n",
      "Epoch 2214/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 6.2580 - val_loss: 6.3068\n",
      "Epoch 2215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0229 - val_loss: 7.1622\n",
      "Epoch 2216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9236 - val_loss: 6.1872\n",
      "Epoch 2217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8818 - val_loss: 6.2741\n",
      "Epoch 2218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8666 - val_loss: 6.2376\n",
      "Epoch 2219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8354 - val_loss: 6.2719\n",
      "Epoch 2220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8825 - val_loss: 6.2626\n",
      "Epoch 2221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8126 - val_loss: 6.3627\n",
      "Epoch 2222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9244 - val_loss: 6.3756\n",
      "Epoch 2223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1130 - val_loss: 6.5040\n",
      "Epoch 2224/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8727 - val_loss: 6.2541\n",
      "Epoch 2225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8185 - val_loss: 6.1953\n",
      "Epoch 2226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9282 - val_loss: 6.2755\n",
      "Epoch 2227/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9773 - val_loss: 6.2574\n",
      "Epoch 2228/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2160 - val_loss: 6.5116\n",
      "Epoch 2229/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.443 - 0s 46us/step - loss: 6.0274 - val_loss: 6.6712\n",
      "Epoch 2230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1239 - val_loss: 6.2049\n",
      "Epoch 2231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9722 - val_loss: 6.4249\n",
      "Epoch 2232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1675 - val_loss: 6.4805\n",
      "Epoch 2233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9800 - val_loss: 6.3466\n",
      "Epoch 2234/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8951 - val_loss: 6.2565\n",
      "Epoch 2235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9449 - val_loss: 6.1957\n",
      "Epoch 2236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9542 - val_loss: 6.1092\n",
      "Epoch 2237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9049 - val_loss: 6.2686\n",
      "Epoch 2238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8426 - val_loss: 6.1511\n",
      "Epoch 2239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8244 - val_loss: 6.2037\n",
      "Epoch 2240/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0128 - val_loss: 6.2986\n",
      "Epoch 2241/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.8462 - val_loss: 6.1716\n",
      "Epoch 2242/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8366 - val_loss: 6.1374\n",
      "Epoch 2243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8324 - val_loss: 6.3445\n",
      "Epoch 2244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8713 - val_loss: 6.2569\n",
      "Epoch 2245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8479 - val_loss: 6.1287\n",
      "Epoch 2246/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9894 - val_loss: 6.1287\n",
      "Epoch 2247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8913 - val_loss: 6.2437\n",
      "Epoch 2248/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9875 - val_loss: 6.5778\n",
      "Epoch 2249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8536 - val_loss: 6.1292\n",
      "Epoch 2250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9360 - val_loss: 6.1714\n",
      "Epoch 2251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0084 - val_loss: 6.4647\n",
      "Epoch 2252/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8960 - val_loss: 6.1513\n",
      "Epoch 2253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7979 - val_loss: 6.1591\n",
      "Epoch 2254/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8369 - val_loss: 6.2201\n",
      "Epoch 2255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0097 - val_loss: 6.7433\n",
      "Epoch 2256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8777 - val_loss: 6.7069\n",
      "Epoch 2257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0857 - val_loss: 6.2876\n",
      "Epoch 2258/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8693 - val_loss: 6.7533\n",
      "Epoch 2259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9336 - val_loss: 6.4057\n",
      "Epoch 2260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9834 - val_loss: 7.0920\n",
      "Epoch 2261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9746 - val_loss: 6.1098\n",
      "Epoch 2262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8626 - val_loss: 6.4903\n",
      "Epoch 2263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0607 - val_loss: 6.1374\n",
      "Epoch 2264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8444 - val_loss: 6.3323\n",
      "Epoch 2265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8641 - val_loss: 6.1155\n",
      "Epoch 2266/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8683 - val_loss: 6.2619\n",
      "Epoch 2267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8796 - val_loss: 6.2179\n",
      "Epoch 2268/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7914 - val_loss: 6.2577\n",
      "Epoch 2269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9101 - val_loss: 6.6354\n",
      "Epoch 2270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9084 - val_loss: 6.1897\n",
      "Epoch 2271/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 5.8180 - val_loss: 6.1441\n",
      "Epoch 2272/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 5.8857 - val_loss: 6.1295\n",
      "Epoch 2273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8858 - val_loss: 6.6639\n",
      "Epoch 2274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8180 - val_loss: 6.2392\n",
      "Epoch 2275/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0819 - val_loss: 6.8149\n",
      "Epoch 2276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9049 - val_loss: 6.0858\n",
      "Epoch 2277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8321 - val_loss: 6.1659\n",
      "Epoch 2278/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8945 - val_loss: 6.4675\n",
      "Epoch 2279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9937 - val_loss: 6.2131\n",
      "Epoch 2280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2738 - val_loss: 6.3626\n",
      "Epoch 2281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8581 - val_loss: 6.1331\n",
      "Epoch 2282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8049 - val_loss: 6.2162\n",
      "Epoch 2283/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7657 - val_loss: 6.0877\n",
      "Epoch 2284/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9116 - val_loss: 6.0948\n",
      "Epoch 2285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8909 - val_loss: 6.2710\n",
      "Epoch 2286/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.9183 - val_loss: 6.5693\n",
      "Epoch 2287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7994 - val_loss: 6.1344\n",
      "Epoch 2288/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7773 - val_loss: 6.4350\n",
      "Epoch 2289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8309 - val_loss: 6.1430\n",
      "Epoch 2290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7758 - val_loss: 6.1281\n",
      "Epoch 2291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1060 - val_loss: 7.2298\n",
      "Epoch 2292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9729 - val_loss: 6.1601\n",
      "Epoch 2293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0255 - val_loss: 6.3287\n",
      "Epoch 2294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7984 - val_loss: 6.1506\n",
      "Epoch 2295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8387 - val_loss: 6.0912\n",
      "Epoch 2296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7756 - val_loss: 6.1177\n",
      "Epoch 2297/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8365 - val_loss: 6.2622\n",
      "Epoch 2298/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9405 - val_loss: 6.2546\n",
      "Epoch 2299/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8939 - val_loss: 6.2216\n",
      "Epoch 2300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8960 - val_loss: 6.1496\n",
      "Epoch 2301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9197 - val_loss: 6.4311\n",
      "Epoch 2302/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8758 - val_loss: 6.1558\n",
      "Epoch 2303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9010 - val_loss: 6.1402\n",
      "Epoch 2304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9247 - val_loss: 6.0887\n",
      "Epoch 2305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7662 - val_loss: 6.4661\n",
      "Epoch 2306/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 5.9548 - val_loss: 6.1033\n",
      "Epoch 2307/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0790 - val_loss: 6.0808\n",
      "Epoch 2308/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9089 - val_loss: 6.0917\n",
      "Epoch 2309/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8571 - val_loss: 6.2821\n",
      "Epoch 2310/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8002 - val_loss: 6.1045\n",
      "Epoch 2311/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8135 - val_loss: 6.4285\n",
      "Epoch 2312/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9266 - val_loss: 6.2841\n",
      "Epoch 2313/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8176 - val_loss: 6.0812\n",
      "Epoch 2314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7468 - val_loss: 6.2898\n",
      "Epoch 2315/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9419 - val_loss: 6.4893\n",
      "Epoch 2316/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7468 - val_loss: 6.3289\n",
      "Epoch 2317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8512 - val_loss: 6.1745\n",
      "Epoch 2318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7890 - val_loss: 6.1232\n",
      "Epoch 2319/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0079 - val_loss: 6.6506\n",
      "Epoch 2320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8831 - val_loss: 6.1285\n",
      "Epoch 2321/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7477 - val_loss: 6.1597\n",
      "Epoch 2322/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8013 - val_loss: 6.1630\n",
      "Epoch 2323/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8409 - val_loss: 6.1162\n",
      "Epoch 2324/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8139 - val_loss: 6.1724\n",
      "Epoch 2325/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7408 - val_loss: 6.1066\n",
      "Epoch 2326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9561 - val_loss: 6.1322\n",
      "Epoch 2327/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8823 - val_loss: 6.1068\n",
      "Epoch 2328/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8527 - val_loss: 6.1121\n",
      "Epoch 2329/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7413 - val_loss: 6.0933\n",
      "Epoch 2330/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.7320 - val_loss: 6.0449\n",
      "Epoch 2331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7129 - val_loss: 6.0519\n",
      "Epoch 2332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8000 - val_loss: 6.1204\n",
      "Epoch 2333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9115 - val_loss: 6.0608\n",
      "Epoch 2334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9670 - val_loss: 6.0945\n",
      "Epoch 2335/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7499 - val_loss: 6.2321\n",
      "Epoch 2336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7667 - val_loss: 6.0566\n",
      "Epoch 2337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8483 - val_loss: 6.1193\n",
      "Epoch 2338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9183 - val_loss: 6.1101\n",
      "Epoch 2339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7382 - val_loss: 6.0879\n",
      "Epoch 2340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7878 - val_loss: 6.0556\n",
      "Epoch 2341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7747 - val_loss: 6.2391\n",
      "Epoch 2342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7780 - val_loss: 6.2388\n",
      "Epoch 2343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8379 - val_loss: 6.0707\n",
      "Epoch 2344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9139 - val_loss: 6.7813\n",
      "Epoch 2345/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8667 - val_loss: 6.0506\n",
      "Epoch 2346/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6828 - val_loss: 6.5820\n",
      "Epoch 2347/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.834 - 0s 46us/step - loss: 5.8263 - val_loss: 6.3164\n",
      "Epoch 2348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8022 - val_loss: 6.0705\n",
      "Epoch 2349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7209 - val_loss: 6.1363\n",
      "Epoch 2350/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.9241 - val_loss: 6.0867\n",
      "Epoch 2351/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8626 - val_loss: 6.1522\n",
      "Epoch 2352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9593 - val_loss: 6.0884\n",
      "Epoch 2353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0280 - val_loss: 6.3372\n",
      "Epoch 2354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8677 - val_loss: 6.0849\n",
      "Epoch 2355/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9120 - val_loss: 6.1106\n",
      "Epoch 2356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7186 - val_loss: 6.0786\n",
      "Epoch 2357/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7693 - val_loss: 6.3316\n",
      "Epoch 2358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8289 - val_loss: 6.0481\n",
      "Epoch 2359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6908 - val_loss: 6.2101\n",
      "Epoch 2360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8197 - val_loss: 6.2886\n",
      "Epoch 2361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8235 - val_loss: 6.4789\n",
      "Epoch 2362/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.9508 - val_loss: 6.3531\n",
      "Epoch 2363/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8587 - val_loss: 6.0387\n",
      "Epoch 2364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7419 - val_loss: 6.1513\n",
      "Epoch 2365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8024 - val_loss: 6.1813\n",
      "Epoch 2366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7449 - val_loss: 6.2144\n",
      "Epoch 2367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8706 - val_loss: 6.1937\n",
      "Epoch 2368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7611 - val_loss: 6.0152\n",
      "Epoch 2369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6989 - val_loss: 6.0340\n",
      "Epoch 2370/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7737 - val_loss: 6.0506\n",
      "Epoch 2371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8414 - val_loss: 6.2569\n",
      "Epoch 2372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8839 - val_loss: 6.2500\n",
      "Epoch 2373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7043 - val_loss: 6.4628\n",
      "Epoch 2374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7936 - val_loss: 6.1642\n",
      "Epoch 2375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7806 - val_loss: 6.0461\n",
      "Epoch 2376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8201 - val_loss: 6.0742\n",
      "Epoch 2377/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7704 - val_loss: 6.0483\n",
      "Epoch 2378/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7359 - val_loss: 6.0188\n",
      "Epoch 2379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8658 - val_loss: 6.1390\n",
      "Epoch 2380/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7484 - val_loss: 6.0230\n",
      "Epoch 2381/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7322 - val_loss: 6.3388\n",
      "Epoch 2382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7101 - val_loss: 6.2434\n",
      "Epoch 2383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8392 - val_loss: 6.1693\n",
      "Epoch 2384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6921 - val_loss: 6.0466\n",
      "Epoch 2385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7112 - val_loss: 6.1497\n",
      "Epoch 2386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7748 - val_loss: 6.1823\n",
      "Epoch 2387/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7749 - val_loss: 6.3096\n",
      "Epoch 2388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0133 - val_loss: 6.0572\n",
      "Epoch 2389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7740 - val_loss: 6.1133\n",
      "Epoch 2390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7473 - val_loss: 6.0679\n",
      "Epoch 2391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7812 - val_loss: 6.3709\n",
      "Epoch 2392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8793 - val_loss: 6.1334\n",
      "Epoch 2393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7194 - val_loss: 6.0203\n",
      "Epoch 2394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6862 - val_loss: 6.0565\n",
      "Epoch 2395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7314 - val_loss: 6.0751\n",
      "Epoch 2396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6870 - val_loss: 6.0682\n",
      "Epoch 2397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7355 - val_loss: 5.9972\n",
      "Epoch 2398/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.7134 - val_loss: 6.2280\n",
      "Epoch 2399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8848 - val_loss: 6.4504\n",
      "Epoch 2400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9436 - val_loss: 6.1829\n",
      "Epoch 2401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8903 - val_loss: 6.0514\n",
      "Epoch 2402/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7789 - val_loss: 6.0204\n",
      "Epoch 2403/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6356 - val_loss: 6.2899\n",
      "Epoch 2404/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9720 - val_loss: 6.0815\n",
      "Epoch 2405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6719 - val_loss: 6.0401\n",
      "Epoch 2406/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6802 - val_loss: 6.0929\n",
      "Epoch 2407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7441 - val_loss: 6.0071\n",
      "Epoch 2408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9668 - val_loss: 5.9748\n",
      "Epoch 2409/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9081 - val_loss: 6.0092\n",
      "Epoch 2410/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9249 - val_loss: 6.2886\n",
      "Epoch 2411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9881 - val_loss: 6.0138\n",
      "Epoch 2412/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7299 - val_loss: 6.0971\n",
      "Epoch 2413/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7387 - val_loss: 6.0578\n",
      "Epoch 2414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6690 - val_loss: 6.0592\n",
      "Epoch 2415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8157 - val_loss: 6.0671\n",
      "Epoch 2416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8175 - val_loss: 6.0010\n",
      "Epoch 2417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9228 - val_loss: 6.3852\n",
      "Epoch 2418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1388 - val_loss: 6.5700\n",
      "Epoch 2419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7889 - val_loss: 6.1674\n",
      "Epoch 2420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7216 - val_loss: 6.1331\n",
      "Epoch 2421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7807 - val_loss: 6.0889\n",
      "Epoch 2422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8096 - val_loss: 6.1684\n",
      "Epoch 2423/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9094 - val_loss: 6.2435\n",
      "Epoch 2424/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8170 - val_loss: 6.0334\n",
      "Epoch 2425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6438 - val_loss: 6.2345\n",
      "Epoch 2426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7073 - val_loss: 6.0176\n",
      "Epoch 2427/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6676 - val_loss: 6.0397\n",
      "Epoch 2428/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7605 - val_loss: 6.2533\n",
      "Epoch 2429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7494 - val_loss: 6.0340\n",
      "Epoch 2430/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6601 - val_loss: 6.0388\n",
      "Epoch 2431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8365 - val_loss: 6.0380\n",
      "Epoch 2432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7955 - val_loss: 6.1016\n",
      "Epoch 2433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8880 - val_loss: 5.9914\n",
      "Epoch 2434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8366 - val_loss: 6.1129\n",
      "Epoch 2435/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7230 - val_loss: 6.0154\n",
      "Epoch 2436/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8213 - val_loss: 6.0247\n",
      "Epoch 2437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7571 - val_loss: 5.9518\n",
      "Epoch 2438/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.8353 - val_loss: 6.3814\n",
      "Epoch 2439/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9471 - val_loss: 6.0650\n",
      "Epoch 2440/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.7390 - val_loss: 6.5171\n",
      "Epoch 2441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1392 - val_loss: 6.0152\n",
      "Epoch 2442/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7756 - val_loss: 6.0379\n",
      "Epoch 2443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8747 - val_loss: 5.9817\n",
      "Epoch 2444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7038 - val_loss: 6.0192\n",
      "Epoch 2445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8210 - val_loss: 6.0639\n",
      "Epoch 2446/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7374 - val_loss: 6.0290\n",
      "Epoch 2447/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7906 - val_loss: 6.2833\n",
      "Epoch 2448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6966 - val_loss: 6.1091\n",
      "Epoch 2449/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6916 - val_loss: 6.2059\n",
      "Epoch 2450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7611 - val_loss: 6.4720\n",
      "Epoch 2451/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8274 - val_loss: 6.0638\n",
      "Epoch 2452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7644 - val_loss: 6.1622\n",
      "Epoch 2453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9530 - val_loss: 6.8531\n",
      "Epoch 2454/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8910 - val_loss: 6.2438\n",
      "Epoch 2455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6715 - val_loss: 5.9528\n",
      "Epoch 2456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6983 - val_loss: 5.9667\n",
      "Epoch 2457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6689 - val_loss: 5.9589\n",
      "Epoch 2458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6273 - val_loss: 5.9844\n",
      "Epoch 2459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6941 - val_loss: 6.0778\n",
      "Epoch 2460/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9434 - val_loss: 6.0137\n",
      "Epoch 2461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8496 - val_loss: 5.9860\n",
      "Epoch 2462/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7640 - val_loss: 6.0117\n",
      "Epoch 2463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6737 - val_loss: 6.5457\n",
      "Epoch 2464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.0602 - val_loss: 5.9656\n",
      "Epoch 2465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6512 - val_loss: 5.9508\n",
      "Epoch 2466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7005 - val_loss: 6.0838\n",
      "Epoch 2467/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6781 - val_loss: 5.9831\n",
      "Epoch 2468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7660 - val_loss: 5.9380\n",
      "Epoch 2469/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7193 - val_loss: 6.3958\n",
      "Epoch 2470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7106 - val_loss: 5.9844\n",
      "Epoch 2471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6262 - val_loss: 5.9421\n",
      "Epoch 2472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7243 - val_loss: 6.1063\n",
      "Epoch 2473/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6899 - val_loss: 6.3066\n",
      "Epoch 2474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8838 - val_loss: 6.0167\n",
      "Epoch 2475/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6770 - val_loss: 5.9777\n",
      "Epoch 2476/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6132 - val_loss: 5.9909\n",
      "Epoch 2477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8633 - val_loss: 6.4342\n",
      "Epoch 2478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8464 - val_loss: 5.9918\n",
      "Epoch 2479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8431 - val_loss: 6.0708\n",
      "Epoch 2480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7718 - val_loss: 6.0510\n",
      "Epoch 2481/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7126 - val_loss: 5.9350\n",
      "Epoch 2482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6136 - val_loss: 5.9507\n",
      "Epoch 2483/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6883 - val_loss: 6.1123\n",
      "Epoch 2484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7950 - val_loss: 6.0676\n",
      "Epoch 2485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7023 - val_loss: 5.9645\n",
      "Epoch 2486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7021 - val_loss: 5.9319\n",
      "Epoch 2487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7747 - val_loss: 5.9793\n",
      "Epoch 2488/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7744 - val_loss: 6.0098\n",
      "Epoch 2489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6993 - val_loss: 5.9613\n",
      "Epoch 2490/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6751 - val_loss: 6.1370\n",
      "Epoch 2491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9109 - val_loss: 6.6983\n",
      "Epoch 2492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7938 - val_loss: 5.9473\n",
      "Epoch 2493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6593 - val_loss: 5.9617\n",
      "Epoch 2494/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.6474 - val_loss: 5.9231\n",
      "Epoch 2495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8569 - val_loss: 6.1524\n",
      "Epoch 2496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.2530 - val_loss: 6.4514\n",
      "Epoch 2497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7825 - val_loss: 5.9414\n",
      "Epoch 2498/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.6257 - val_loss: 6.0628\n",
      "Epoch 2499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7105 - val_loss: 5.9498\n",
      "Epoch 2500/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5956 - val_loss: 5.9292\n",
      "Epoch 2501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5988 - val_loss: 6.0015\n",
      "Epoch 2502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6269 - val_loss: 5.9273\n",
      "Epoch 2503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6627 - val_loss: 5.9303\n",
      "Epoch 2504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6037 - val_loss: 6.1388\n",
      "Epoch 2505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7230 - val_loss: 6.1183\n",
      "Epoch 2506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7559 - val_loss: 5.9663\n",
      "Epoch 2507/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5974 - val_loss: 5.9142\n",
      "Epoch 2508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6350 - val_loss: 6.0517\n",
      "Epoch 2509/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5944 - val_loss: 6.0910\n",
      "Epoch 2510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6356 - val_loss: 5.9804\n",
      "Epoch 2511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5842 - val_loss: 5.9747\n",
      "Epoch 2512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5844 - val_loss: 5.9152\n",
      "Epoch 2513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7533 - val_loss: 5.9485\n",
      "Epoch 2514/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.6371 - val_loss: 6.3690\n",
      "Epoch 2515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6541 - val_loss: 5.9991\n",
      "Epoch 2516/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5555 - val_loss: 5.9538\n",
      "Epoch 2517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7149 - val_loss: 5.9668\n",
      "Epoch 2518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7683 - val_loss: 6.0821\n",
      "Epoch 2519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6293 - val_loss: 6.3444\n",
      "Epoch 2520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8434 - val_loss: 6.3507\n",
      "Epoch 2521/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 5.7690 - val_loss: 5.9247\n",
      "Epoch 2522/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.6803 - val_loss: 5.9031\n",
      "Epoch 2523/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5912 - val_loss: 5.9267\n",
      "Epoch 2524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7691 - val_loss: 5.9510\n",
      "Epoch 2525/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7125 - val_loss: 5.9208\n",
      "Epoch 2526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8451 - val_loss: 6.5420\n",
      "Epoch 2527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8798 - val_loss: 5.9351\n",
      "Epoch 2528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6597 - val_loss: 5.8941\n",
      "Epoch 2529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6443 - val_loss: 6.0663\n",
      "Epoch 2530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8061 - val_loss: 5.9090\n",
      "Epoch 2531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7668 - val_loss: 5.9851\n",
      "Epoch 2532/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6140 - val_loss: 5.9571\n",
      "Epoch 2533/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.6139 - val_loss: 6.0282\n",
      "Epoch 2534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8147 - val_loss: 5.9749\n",
      "Epoch 2535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6339 - val_loss: 5.9639\n",
      "Epoch 2536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6356 - val_loss: 5.9088\n",
      "Epoch 2537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5794 - val_loss: 5.9437\n",
      "Epoch 2538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6568 - val_loss: 6.1814\n",
      "Epoch 2539/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6110 - val_loss: 5.8746\n",
      "Epoch 2540/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7105 - val_loss: 5.8788\n",
      "Epoch 2541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6147 - val_loss: 6.0741\n",
      "Epoch 2542/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6298 - val_loss: 6.1886\n",
      "Epoch 2543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7660 - val_loss: 5.9432\n",
      "Epoch 2544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7119 - val_loss: 5.9666\n",
      "Epoch 2545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6350 - val_loss: 6.2433\n",
      "Epoch 2546/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6691 - val_loss: 6.1459\n",
      "Epoch 2547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6663 - val_loss: 6.1051\n",
      "Epoch 2548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7744 - val_loss: 5.9189\n",
      "Epoch 2549/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6379 - val_loss: 6.0091\n",
      "Epoch 2550/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6555 - val_loss: 5.9600\n",
      "Epoch 2551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6215 - val_loss: 5.9652\n",
      "Epoch 2552/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6240 - val_loss: 5.9719\n",
      "Epoch 2553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8552 - val_loss: 5.9636\n",
      "Epoch 2554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8809 - val_loss: 5.9644\n",
      "Epoch 2555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7745 - val_loss: 5.9233\n",
      "Epoch 2556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5707 - val_loss: 6.0778\n",
      "Epoch 2557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7683 - val_loss: 5.9317\n",
      "Epoch 2558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7260 - val_loss: 5.9168\n",
      "Epoch 2559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8409 - val_loss: 5.9299\n",
      "Epoch 2560/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6226 - val_loss: 5.9365\n",
      "Epoch 2561/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.6290 - val_loss: 6.0180\n",
      "Epoch 2562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8553 - val_loss: 5.8559\n",
      "Epoch 2563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.9742 - val_loss: 6.5170\n",
      "Epoch 2564/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6167 - val_loss: 5.9068\n",
      "Epoch 2565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5795 - val_loss: 5.9585\n",
      "Epoch 2566/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6209 - val_loss: 5.9772\n",
      "Epoch 2567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6627 - val_loss: 6.0617\n",
      "Epoch 2568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7900 - val_loss: 5.9342\n",
      "Epoch 2569/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5946 - val_loss: 6.1944\n",
      "Epoch 2570/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7130 - val_loss: 5.9080\n",
      "Epoch 2571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5950 - val_loss: 5.9484\n",
      "Epoch 2572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6191 - val_loss: 5.8750\n",
      "Epoch 2573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6297 - val_loss: 5.8869\n",
      "Epoch 2574/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6546 - val_loss: 5.8723\n",
      "Epoch 2575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7354 - val_loss: 5.9885\n",
      "Epoch 2576/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6223 - val_loss: 6.2037\n",
      "Epoch 2577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6696 - val_loss: 5.8462\n",
      "Epoch 2578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5802 - val_loss: 5.8792\n",
      "Epoch 2579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5296 - val_loss: 5.9916\n",
      "Epoch 2580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5218 - val_loss: 6.1499\n",
      "Epoch 2581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6546 - val_loss: 6.3700\n",
      "Epoch 2582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7341 - val_loss: 5.9270\n",
      "Epoch 2583/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5951 - val_loss: 6.5484\n",
      "Epoch 2584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8345 - val_loss: 6.3367\n",
      "Epoch 2585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7076 - val_loss: 6.0889\n",
      "Epoch 2586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6495 - val_loss: 5.8494\n",
      "Epoch 2587/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5275 - val_loss: 6.2583\n",
      "Epoch 2588/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7332 - val_loss: 6.2676\n",
      "Epoch 2589/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7792 - val_loss: 5.8811\n",
      "Epoch 2590/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 91us/step - loss: 5.5320 - val_loss: 5.9077\n",
      "Epoch 2591/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5980 - val_loss: 5.9384\n",
      "Epoch 2592/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9006 - val_loss: 6.2280\n",
      "Epoch 2593/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7365 - val_loss: 5.9003\n",
      "Epoch 2594/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.6390 - val_loss: 6.6968\n",
      "Epoch 2595/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7121 - val_loss: 6.0075\n",
      "Epoch 2596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6007 - val_loss: 6.0268\n",
      "Epoch 2597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5939 - val_loss: 5.8490\n",
      "Epoch 2598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5504 - val_loss: 5.9609\n",
      "Epoch 2599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6699 - val_loss: 6.2857\n",
      "Epoch 2600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6812 - val_loss: 5.8815\n",
      "Epoch 2601/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.5279 - val_loss: 5.9304\n",
      "Epoch 2602/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5875 - val_loss: 5.9391\n",
      "Epoch 2603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5745 - val_loss: 5.8735\n",
      "Epoch 2604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6805 - val_loss: 5.8462\n",
      "Epoch 2605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7364 - val_loss: 5.8646\n",
      "Epoch 2606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6359 - val_loss: 5.9023\n",
      "Epoch 2607/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5953 - val_loss: 5.8705\n",
      "Epoch 2608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5426 - val_loss: 6.1951\n",
      "Epoch 2609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6018 - val_loss: 5.9774\n",
      "Epoch 2610/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6672 - val_loss: 5.8820\n",
      "Epoch 2611/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5705 - val_loss: 5.8642\n",
      "Epoch 2612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5524 - val_loss: 5.8341\n",
      "Epoch 2613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6305 - val_loss: 5.9287\n",
      "Epoch 2614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6543 - val_loss: 5.8827\n",
      "Epoch 2615/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6840 - val_loss: 5.8525\n",
      "Epoch 2616/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5163 - val_loss: 5.9024\n",
      "Epoch 2617/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7832 - val_loss: 5.9167\n",
      "Epoch 2618/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 6.1211 - val_loss: 5.8440\n",
      "Epoch 2619/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5282 - val_loss: 5.8720\n",
      "Epoch 2620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7092 - val_loss: 5.8990\n",
      "Epoch 2621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5883 - val_loss: 5.8471\n",
      "Epoch 2622/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5420 - val_loss: 5.8239\n",
      "Epoch 2623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5279 - val_loss: 6.1860\n",
      "Epoch 2624/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7069 - val_loss: 5.9159\n",
      "Epoch 2625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8431 - val_loss: 6.2509\n",
      "Epoch 2626/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7788 - val_loss: 5.9093\n",
      "Epoch 2627/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6476 - val_loss: 5.8732\n",
      "Epoch 2628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5184 - val_loss: 5.8743\n",
      "Epoch 2629/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6016 - val_loss: 5.8352\n",
      "Epoch 2630/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.4885 - val_loss: 5.8475\n",
      "Epoch 2631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5926 - val_loss: 5.9278\n",
      "Epoch 2632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6932 - val_loss: 5.9143\n",
      "Epoch 2633/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5834 - val_loss: 5.9106\n",
      "Epoch 2634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7093 - val_loss: 6.3214\n",
      "Epoch 2635/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8762 - val_loss: 5.8451\n",
      "Epoch 2636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5370 - val_loss: 5.9749\n",
      "Epoch 2637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6957 - val_loss: 5.8417\n",
      "Epoch 2638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7400 - val_loss: 5.8847\n",
      "Epoch 2639/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6512 - val_loss: 5.8388\n",
      "Epoch 2640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5323 - val_loss: 5.9227\n",
      "Epoch 2641/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7804 - val_loss: 6.0543\n",
      "Epoch 2642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5711 - val_loss: 5.8888\n",
      "Epoch 2643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4916 - val_loss: 6.2094\n",
      "Epoch 2644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5911 - val_loss: 5.8724\n",
      "Epoch 2645/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5041 - val_loss: 5.8455\n",
      "Epoch 2646/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6064 - val_loss: 5.8743\n",
      "Epoch 2647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5045 - val_loss: 5.8492\n",
      "Epoch 2648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5318 - val_loss: 5.8456\n",
      "Epoch 2649/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5815 - val_loss: 5.8495\n",
      "Epoch 2650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4959 - val_loss: 5.8388\n",
      "Epoch 2651/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5192 - val_loss: 5.8648\n",
      "Epoch 2652/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6305 - val_loss: 6.4879\n",
      "Epoch 2653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6300 - val_loss: 5.8815\n",
      "Epoch 2654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6888 - val_loss: 5.9745\n",
      "Epoch 2655/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.5870 - val_loss: 5.9009\n",
      "Epoch 2656/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7130 - val_loss: 6.0094\n",
      "Epoch 2657/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5122 - val_loss: 5.8296\n",
      "Epoch 2658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6151 - val_loss: 6.0595\n",
      "Epoch 2659/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5415 - val_loss: 6.1697\n",
      "Epoch 2660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4937 - val_loss: 5.8226\n",
      "Epoch 2661/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5494 - val_loss: 5.8989\n",
      "Epoch 2662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7282 - val_loss: 6.0624\n",
      "Epoch 2663/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6517 - val_loss: 5.9389\n",
      "Epoch 2664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5066 - val_loss: 5.9235\n",
      "Epoch 2665/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5170 - val_loss: 5.8774\n",
      "Epoch 2666/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.5239 - val_loss: 5.9069\n",
      "Epoch 2667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6170 - val_loss: 5.8627\n",
      "Epoch 2668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5482 - val_loss: 5.9532\n",
      "Epoch 2669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5914 - val_loss: 5.7987\n",
      "Epoch 2670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5249 - val_loss: 5.8620\n",
      "Epoch 2671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4741 - val_loss: 5.8263\n",
      "Epoch 2672/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5647 - val_loss: 5.8711\n",
      "Epoch 2673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7172 - val_loss: 6.3392\n",
      "Epoch 2674/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4990 - val_loss: 5.8685\n",
      "Epoch 2675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5715 - val_loss: 5.8538\n",
      "Epoch 2676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4633 - val_loss: 6.6511\n",
      "Epoch 2677/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5984 - val_loss: 5.8095\n",
      "Epoch 2678/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5155 - val_loss: 5.8386\n",
      "Epoch 2679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5290 - val_loss: 5.8075\n",
      "Epoch 2680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6077 - val_loss: 5.8484\n",
      "Epoch 2681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6224 - val_loss: 5.8580\n",
      "Epoch 2682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5045 - val_loss: 5.8184\n",
      "Epoch 2683/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5817 - val_loss: 5.8702\n",
      "Epoch 2684/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8263 - val_loss: 5.9631\n",
      "Epoch 2685/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6619 - val_loss: 5.9967\n",
      "Epoch 2686/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5128 - val_loss: 5.8706\n",
      "Epoch 2687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4749 - val_loss: 5.8404\n",
      "Epoch 2688/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5200 - val_loss: 6.1667\n",
      "Epoch 2689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5282 - val_loss: 5.9011\n",
      "Epoch 2690/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5307 - val_loss: 5.9980\n",
      "Epoch 2691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5935 - val_loss: 5.8864\n",
      "Epoch 2692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6915 - val_loss: 6.0048\n",
      "Epoch 2693/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6665 - val_loss: 5.8078\n",
      "Epoch 2694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6151 - val_loss: 6.9237\n",
      "Epoch 2695/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.7502 - val_loss: 5.8802\n",
      "Epoch 2696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4414 - val_loss: 5.8405\n",
      "Epoch 2697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4731 - val_loss: 5.8447\n",
      "Epoch 2698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5676 - val_loss: 6.3369\n",
      "Epoch 2699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5596 - val_loss: 5.9651\n",
      "Epoch 2700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6617 - val_loss: 6.3385\n",
      "Epoch 2701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7696 - val_loss: 5.8854\n",
      "Epoch 2702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5376 - val_loss: 6.1054\n",
      "Epoch 2703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5940 - val_loss: 6.0175\n",
      "Epoch 2704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5635 - val_loss: 6.2382\n",
      "Epoch 2705/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4823 - val_loss: 5.8038\n",
      "Epoch 2706/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5801 - val_loss: 5.8848\n",
      "Epoch 2707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5552 - val_loss: 5.9609\n",
      "Epoch 2708/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5652 - val_loss: 5.9953\n",
      "Epoch 2709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5430 - val_loss: 5.9156\n",
      "Epoch 2710/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5770 - val_loss: 5.7668\n",
      "Epoch 2711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5171 - val_loss: 5.7983\n",
      "Epoch 2712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6076 - val_loss: 5.8452\n",
      "Epoch 2713/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6154 - val_loss: 5.8005\n",
      "Epoch 2714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5387 - val_loss: 6.0088\n",
      "Epoch 2715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6550 - val_loss: 5.8406\n",
      "Epoch 2716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6314 - val_loss: 6.0017\n",
      "Epoch 2717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5172 - val_loss: 5.7699\n",
      "Epoch 2718/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4768 - val_loss: 5.7775\n",
      "Epoch 2719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6219 - val_loss: 6.3208\n",
      "Epoch 2720/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.9535 - val_loss: 5.9031\n",
      "Epoch 2721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5577 - val_loss: 5.8002\n",
      "Epoch 2722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4774 - val_loss: 5.8180\n",
      "Epoch 2723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5546 - val_loss: 5.8655\n",
      "Epoch 2724/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5432 - val_loss: 6.3346\n",
      "Epoch 2725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7383 - val_loss: 5.8047\n",
      "Epoch 2726/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4692 - val_loss: 6.1798\n",
      "Epoch 2727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6050 - val_loss: 5.8104\n",
      "Epoch 2728/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6436 - val_loss: 5.7748\n",
      "Epoch 2729/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4716 - val_loss: 5.8902\n",
      "Epoch 2730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5241 - val_loss: 5.8876\n",
      "Epoch 2731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6277 - val_loss: 5.8116\n",
      "Epoch 2732/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6880 - val_loss: 5.7800\n",
      "Epoch 2733/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4699 - val_loss: 5.9762\n",
      "Epoch 2734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7553 - val_loss: 5.8074\n",
      "Epoch 2735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5606 - val_loss: 6.0653\n",
      "Epoch 2736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4882 - val_loss: 5.9756\n",
      "Epoch 2737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5982 - val_loss: 5.7739\n",
      "Epoch 2738/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6142 - val_loss: 5.8308\n",
      "Epoch 2739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8107 - val_loss: 6.6281\n",
      "Epoch 2740/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6010 - val_loss: 5.8989\n",
      "Epoch 2741/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5716 - val_loss: 5.7904\n",
      "Epoch 2742/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.5178 - val_loss: 5.8115\n",
      "Epoch 2743/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6573 - val_loss: 6.0795\n",
      "Epoch 2744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4936 - val_loss: 5.7844\n",
      "Epoch 2745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4935 - val_loss: 5.8200\n",
      "Epoch 2746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4700 - val_loss: 5.7857\n",
      "Epoch 2747/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4646 - val_loss: 5.8107\n",
      "Epoch 2748/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5392 - val_loss: 5.8121\n",
      "Epoch 2749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4425 - val_loss: 5.7402\n",
      "Epoch 2750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4864 - val_loss: 5.7912\n",
      "Epoch 2751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4554 - val_loss: 5.7779\n",
      "Epoch 2752/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4478 - val_loss: 5.7844\n",
      "Epoch 2753/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5230 - val_loss: 6.0446\n",
      "Epoch 2754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5330 - val_loss: 5.8078\n",
      "Epoch 2755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6825 - val_loss: 5.8129\n",
      "Epoch 2756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4605 - val_loss: 5.9065\n",
      "Epoch 2757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5041 - val_loss: 5.9107\n",
      "Epoch 2758/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7308 - val_loss: 5.8007\n",
      "Epoch 2759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5869 - val_loss: 6.1505\n",
      "Epoch 2760/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6875 - val_loss: 6.1901\n",
      "Epoch 2761/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8803 - val_loss: 5.8804\n",
      "Epoch 2762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4305 - val_loss: 5.7312\n",
      "Epoch 2763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4322 - val_loss: 5.8231\n",
      "Epoch 2764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6061 - val_loss: 5.8092\n",
      "Epoch 2765/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4989 - val_loss: 5.7703\n",
      "Epoch 2766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5082 - val_loss: 5.8349\n",
      "Epoch 2767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4693 - val_loss: 5.8995\n",
      "Epoch 2768/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4341 - val_loss: 5.7765\n",
      "Epoch 2769/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5162 - val_loss: 5.7603\n",
      "Epoch 2770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5385 - val_loss: 5.7869\n",
      "Epoch 2771/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7260 - val_loss: 5.9068\n",
      "Epoch 2772/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5068 - val_loss: 5.8331\n",
      "Epoch 2773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4722 - val_loss: 5.9395\n",
      "Epoch 2774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4447 - val_loss: 5.9460\n",
      "Epoch 2775/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4450 - val_loss: 6.0880\n",
      "Epoch 2776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8740 - val_loss: 6.0942\n",
      "Epoch 2777/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 6.0223 - val_loss: 5.8322\n",
      "Epoch 2778/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5098 - val_loss: 6.0710\n",
      "Epoch 2779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6244 - val_loss: 6.3629\n",
      "Epoch 2780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6569 - val_loss: 5.8831\n",
      "Epoch 2781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5714 - val_loss: 5.9169\n",
      "Epoch 2782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5911 - val_loss: 5.8344\n",
      "Epoch 2783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4500 - val_loss: 5.9015\n",
      "Epoch 2784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7334 - val_loss: 5.7269\n",
      "Epoch 2785/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6573 - val_loss: 5.9200\n",
      "Epoch 2786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5097 - val_loss: 5.8846\n",
      "Epoch 2787/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4648 - val_loss: 5.9567\n",
      "Epoch 2788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5725 - val_loss: 5.8143\n",
      "Epoch 2789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4463 - val_loss: 5.9313\n",
      "Epoch 2790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5098 - val_loss: 5.7876\n",
      "Epoch 2791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5166 - val_loss: 5.8733\n",
      "Epoch 2792/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.5117 - val_loss: 6.0070\n",
      "Epoch 2793/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.550 - 0s 68us/step - loss: 5.6211 - val_loss: 5.7286\n",
      "Epoch 2794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4504 - val_loss: 5.7465\n",
      "Epoch 2795/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4315 - val_loss: 5.8048\n",
      "Epoch 2796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6217 - val_loss: 5.9122\n",
      "Epoch 2797/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.3852 - val_loss: 5.8602\n",
      "Epoch 2798/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4878 - val_loss: 5.9814\n",
      "Epoch 2799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4974 - val_loss: 5.9051\n",
      "Epoch 2800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4753 - val_loss: 5.7604\n",
      "Epoch 2801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4588 - val_loss: 5.7867\n",
      "Epoch 2802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4632 - val_loss: 5.7791\n",
      "Epoch 2803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4628 - val_loss: 5.8549\n",
      "Epoch 2804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5772 - val_loss: 5.7799\n",
      "Epoch 2805/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4242 - val_loss: 5.7756\n",
      "Epoch 2806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4232 - val_loss: 5.7521\n",
      "Epoch 2807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4577 - val_loss: 6.2269\n",
      "Epoch 2808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4764 - val_loss: 6.1814\n",
      "Epoch 2809/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6214 - val_loss: 5.8889\n",
      "Epoch 2810/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5960 - val_loss: 5.7836\n",
      "Epoch 2811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4764 - val_loss: 5.9802\n",
      "Epoch 2812/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6312 - val_loss: 5.7835\n",
      "Epoch 2813/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6265 - val_loss: 5.7247\n",
      "Epoch 2814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4487 - val_loss: 5.8464\n",
      "Epoch 2815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4376 - val_loss: 5.8908\n",
      "Epoch 2816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4320 - val_loss: 5.9261\n",
      "Epoch 2817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5413 - val_loss: 5.7453\n",
      "Epoch 2818/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.6029 - val_loss: 5.7638\n",
      "Epoch 2819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5576 - val_loss: 5.7446\n",
      "Epoch 2820/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7206 - val_loss: 5.8029\n",
      "Epoch 2821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4792 - val_loss: 6.6860\n",
      "Epoch 2822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7878 - val_loss: 5.8007\n",
      "Epoch 2823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5583 - val_loss: 5.7113\n",
      "Epoch 2824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4357 - val_loss: 6.2692\n",
      "Epoch 2825/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4548 - val_loss: 5.7093\n",
      "Epoch 2826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4153 - val_loss: 5.7413\n",
      "Epoch 2827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5273 - val_loss: 5.9499\n",
      "Epoch 2828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5337 - val_loss: 5.7913\n",
      "Epoch 2829/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4603 - val_loss: 5.9636\n",
      "Epoch 2830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5478 - val_loss: 5.7795\n",
      "Epoch 2831/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6440 - val_loss: 5.7064\n",
      "Epoch 2832/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4249 - val_loss: 5.7691\n",
      "Epoch 2833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3885 - val_loss: 5.8040\n",
      "Epoch 2834/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5317 - val_loss: 5.7325\n",
      "Epoch 2835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4868 - val_loss: 5.8791\n",
      "Epoch 2836/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6101 - val_loss: 5.7644\n",
      "Epoch 2837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4754 - val_loss: 6.0347\n",
      "Epoch 2838/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5523 - val_loss: 5.7251\n",
      "Epoch 2839/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4830 - val_loss: 5.7275\n",
      "Epoch 2840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4561 - val_loss: 5.7720\n",
      "Epoch 2841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4443 - val_loss: 5.7973\n",
      "Epoch 2842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4672 - val_loss: 6.0811\n",
      "Epoch 2843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4411 - val_loss: 5.8782\n",
      "Epoch 2844/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5075 - val_loss: 5.7469\n",
      "Epoch 2845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4380 - val_loss: 6.4713\n",
      "Epoch 2846/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7160 - val_loss: 5.9581\n",
      "Epoch 2847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5978 - val_loss: 5.8372\n",
      "Epoch 2848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4789 - val_loss: 5.8055\n",
      "Epoch 2849/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8589 - val_loss: 6.3978\n",
      "Epoch 2850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6908 - val_loss: 5.7991\n",
      "Epoch 2851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6327 - val_loss: 5.7622\n",
      "Epoch 2852/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5165 - val_loss: 5.7448\n",
      "Epoch 2853/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5612 - val_loss: 5.8313\n",
      "Epoch 2854/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4689 - val_loss: 5.7998\n",
      "Epoch 2855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4294 - val_loss: 5.7332\n",
      "Epoch 2856/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4165 - val_loss: 5.7223\n",
      "Epoch 2857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3921 - val_loss: 5.7519\n",
      "Epoch 2858/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4003 - val_loss: 5.8428\n",
      "Epoch 2859/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3894 - val_loss: 5.8866\n",
      "Epoch 2860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4232 - val_loss: 5.7830\n",
      "Epoch 2861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4532 - val_loss: 5.7194\n",
      "Epoch 2862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4432 - val_loss: 6.4145\n",
      "Epoch 2863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4176 - val_loss: 5.7247\n",
      "Epoch 2864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4609 - val_loss: 5.8195\n",
      "Epoch 2865/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4489 - val_loss: 5.8088\n",
      "Epoch 2866/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4883 - val_loss: 5.7175\n",
      "Epoch 2867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4022 - val_loss: 5.6879\n",
      "Epoch 2868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4528 - val_loss: 5.8769\n",
      "Epoch 2869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5444 - val_loss: 5.7714\n",
      "Epoch 2870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3744 - val_loss: 5.7240\n",
      "Epoch 2871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4834 - val_loss: 6.0267\n",
      "Epoch 2872/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6309 - val_loss: 5.7251\n",
      "Epoch 2873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4005 - val_loss: 5.8483\n",
      "Epoch 2874/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4449 - val_loss: 5.7363\n",
      "Epoch 2875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4694 - val_loss: 5.7982\n",
      "Epoch 2876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4796 - val_loss: 5.8886\n",
      "Epoch 2877/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8012 - val_loss: 5.9050\n",
      "Epoch 2878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4015 - val_loss: 5.7173\n",
      "Epoch 2879/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3733 - val_loss: 5.7579\n",
      "Epoch 2880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4407 - val_loss: 5.7519\n",
      "Epoch 2881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3735 - val_loss: 5.7176\n",
      "Epoch 2882/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4552 - val_loss: 5.8537\n",
      "Epoch 2883/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5693 - val_loss: 6.0256\n",
      "Epoch 2884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5265 - val_loss: 5.7112\n",
      "Epoch 2885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4072 - val_loss: 5.7255\n",
      "Epoch 2886/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3966 - val_loss: 5.7107\n",
      "Epoch 2887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4947 - val_loss: 6.0329\n",
      "Epoch 2888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6407 - val_loss: 5.9945\n",
      "Epoch 2889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4078 - val_loss: 5.7452\n",
      "Epoch 2890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4731 - val_loss: 5.9831\n",
      "Epoch 2891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4673 - val_loss: 5.9785\n",
      "Epoch 2892/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7288 - val_loss: 5.7079\n",
      "Epoch 2893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5196 - val_loss: 5.6706\n",
      "Epoch 2894/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.7145 - val_loss: 6.6081\n",
      "Epoch 2895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5961 - val_loss: 5.9679\n",
      "Epoch 2896/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5899 - val_loss: 5.7380\n",
      "Epoch 2897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3620 - val_loss: 5.6998\n",
      "Epoch 2898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4932 - val_loss: 6.1144\n",
      "Epoch 2899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5650 - val_loss: 5.7603\n",
      "Epoch 2900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5361 - val_loss: 5.7172\n",
      "Epoch 2901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4025 - val_loss: 5.8230\n",
      "Epoch 2902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3948 - val_loss: 5.8143\n",
      "Epoch 2903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4163 - val_loss: 5.7213\n",
      "Epoch 2904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4808 - val_loss: 5.9820\n",
      "Epoch 2905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5031 - val_loss: 5.8542\n",
      "Epoch 2906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3782 - val_loss: 6.0775\n",
      "Epoch 2907/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8059 - val_loss: 5.6814\n",
      "Epoch 2908/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3831 - val_loss: 5.9093\n",
      "Epoch 2909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4985 - val_loss: 5.8960\n",
      "Epoch 2910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5685 - val_loss: 5.7706\n",
      "Epoch 2911/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.3902 - val_loss: 5.7306\n",
      "Epoch 2912/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4955 - val_loss: 5.8077\n",
      "Epoch 2913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4202 - val_loss: 5.7235\n",
      "Epoch 2914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6368 - val_loss: 5.9725\n",
      "Epoch 2915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4167 - val_loss: 5.6930\n",
      "Epoch 2916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4407 - val_loss: 5.8522\n",
      "Epoch 2917/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3982 - val_loss: 5.7857\n",
      "Epoch 2918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3742 - val_loss: 5.6762\n",
      "Epoch 2919/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3717 - val_loss: 5.9756\n",
      "Epoch 2920/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4039 - val_loss: 5.8931\n",
      "Epoch 2921/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4920 - val_loss: 5.7218\n",
      "Epoch 2922/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4345 - val_loss: 5.6723\n",
      "Epoch 2923/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4304 - val_loss: 5.8957\n",
      "Epoch 2924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4492 - val_loss: 5.9987\n",
      "Epoch 2925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8246 - val_loss: 5.8991\n",
      "Epoch 2926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3245 - val_loss: 5.6876\n",
      "Epoch 2927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3932 - val_loss: 5.6817\n",
      "Epoch 2928/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5716 - val_loss: 5.8990\n",
      "Epoch 2929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4069 - val_loss: 5.6460\n",
      "Epoch 2930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4339 - val_loss: 5.8231\n",
      "Epoch 2931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3747 - val_loss: 5.7402\n",
      "Epoch 2932/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4347 - val_loss: 5.6874\n",
      "Epoch 2933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4076 - val_loss: 5.6890\n",
      "Epoch 2934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4614 - val_loss: 5.6840\n",
      "Epoch 2935/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4865 - val_loss: 5.6643\n",
      "Epoch 2936/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3629 - val_loss: 5.6599\n",
      "Epoch 2937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4888 - val_loss: 5.6844\n",
      "Epoch 2938/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3559 - val_loss: 5.7520\n",
      "Epoch 2939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5454 - val_loss: 5.6868\n",
      "Epoch 2940/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4906 - val_loss: 5.6472\n",
      "Epoch 2941/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3853 - val_loss: 5.7379\n",
      "Epoch 2942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4161 - val_loss: 5.7031\n",
      "Epoch 2943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6932 - val_loss: 6.1174\n",
      "Epoch 2944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6170 - val_loss: 5.6802\n",
      "Epoch 2945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4382 - val_loss: 5.6902\n",
      "Epoch 2946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6322 - val_loss: 6.1166\n",
      "Epoch 2947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4956 - val_loss: 5.8569\n",
      "Epoch 2948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5592 - val_loss: 5.7954\n",
      "Epoch 2949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3950 - val_loss: 5.6731\n",
      "Epoch 2950/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4212 - val_loss: 5.9518\n",
      "Epoch 2951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3791 - val_loss: 5.6884\n",
      "Epoch 2952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4487 - val_loss: 6.2376\n",
      "Epoch 2953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4642 - val_loss: 5.9071\n",
      "Epoch 2954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4076 - val_loss: 5.7675\n",
      "Epoch 2955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3829 - val_loss: 5.7881\n",
      "Epoch 2956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4011 - val_loss: 5.7063\n",
      "Epoch 2957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5230 - val_loss: 5.7302\n",
      "Epoch 2958/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4526 - val_loss: 6.0084\n",
      "Epoch 2959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4281 - val_loss: 5.6651\n",
      "Epoch 2960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3286 - val_loss: 5.6696\n",
      "Epoch 2961/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3886 - val_loss: 5.6903\n",
      "Epoch 2962/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7061 - val_loss: 5.7738\n",
      "Epoch 2963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3877 - val_loss: 5.7448\n",
      "Epoch 2964/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4397 - val_loss: 5.8239\n",
      "Epoch 2965/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4052 - val_loss: 5.6826\n",
      "Epoch 2966/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4087 - val_loss: 5.8181\n",
      "Epoch 2967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4272 - val_loss: 5.7087\n",
      "Epoch 2968/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.3933 - val_loss: 5.7166\n",
      "Epoch 2969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4392 - val_loss: 5.6712\n",
      "Epoch 2970/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.3367 - val_loss: 5.6957\n",
      "Epoch 2971/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3440 - val_loss: 5.6801\n",
      "Epoch 2972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3908 - val_loss: 5.8248\n",
      "Epoch 2973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3202 - val_loss: 5.7586\n",
      "Epoch 2974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3607 - val_loss: 5.7000\n",
      "Epoch 2975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4080 - val_loss: 5.6640\n",
      "Epoch 2976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4180 - val_loss: 5.6577\n",
      "Epoch 2977/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6447 - val_loss: 6.1068\n",
      "Epoch 2978/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4241 - val_loss: 5.7726\n",
      "Epoch 2979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4460 - val_loss: 5.6370\n",
      "Epoch 2980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3846 - val_loss: 5.6965\n",
      "Epoch 2981/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3366 - val_loss: 5.8932\n",
      "Epoch 2982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3732 - val_loss: 5.7022\n",
      "Epoch 2983/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4185 - val_loss: 5.6408\n",
      "Epoch 2984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4503 - val_loss: 6.4124\n",
      "Epoch 2985/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5684 - val_loss: 5.6521\n",
      "Epoch 2986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5257 - val_loss: 5.6782\n",
      "Epoch 2987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6393 - val_loss: 5.7448\n",
      "Epoch 2988/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4458 - val_loss: 6.0807\n",
      "Epoch 2989/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5197 - val_loss: 5.8267\n",
      "Epoch 2990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4719 - val_loss: 5.6850\n",
      "Epoch 2991/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4403 - val_loss: 5.6331\n",
      "Epoch 2992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3401 - val_loss: 5.6906\n",
      "Epoch 2993/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3840 - val_loss: 5.7276\n",
      "Epoch 2994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4760 - val_loss: 5.7335\n",
      "Epoch 2995/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4381 - val_loss: 5.9592\n",
      "Epoch 2996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4513 - val_loss: 5.6785\n",
      "Epoch 2997/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.805 - 0s 46us/step - loss: 5.3448 - val_loss: 5.7271\n",
      "Epoch 2998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3265 - val_loss: 5.9233\n",
      "Epoch 2999/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.3995 - val_loss: 5.6770\n",
      "Epoch 3000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3406 - val_loss: 5.9750\n",
      "Epoch 3001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6215 - val_loss: 5.6205\n",
      "Epoch 3002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5231 - val_loss: 5.9168\n",
      "Epoch 3003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4426 - val_loss: 6.0252\n",
      "Epoch 3004/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5466 - val_loss: 5.9374\n",
      "Epoch 3005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3862 - val_loss: 5.6868\n",
      "Epoch 3006/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.3178 - val_loss: 5.7196\n",
      "Epoch 3007/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4152 - val_loss: 5.8839\n",
      "Epoch 3008/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4245 - val_loss: 5.6597\n",
      "Epoch 3009/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.112 - 0s 46us/step - loss: 5.3140 - val_loss: 6.1812\n",
      "Epoch 3010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5106 - val_loss: 5.6136\n",
      "Epoch 3011/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3180 - val_loss: 5.6702\n",
      "Epoch 3012/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3527 - val_loss: 5.6351\n",
      "Epoch 3013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4990 - val_loss: 5.6720\n",
      "Epoch 3014/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4829 - val_loss: 6.6263\n",
      "Epoch 3015/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.8027 - val_loss: 5.6968\n",
      "Epoch 3016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4458 - val_loss: 5.7321\n",
      "Epoch 3017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4204 - val_loss: 5.6887\n",
      "Epoch 3018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4028 - val_loss: 5.9907\n",
      "Epoch 3019/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5610 - val_loss: 5.7667\n",
      "Epoch 3020/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5652 - val_loss: 5.7851\n",
      "Epoch 3021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3649 - val_loss: 6.1653\n",
      "Epoch 3022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6830 - val_loss: 5.6100\n",
      "Epoch 3023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3480 - val_loss: 5.6736\n",
      "Epoch 3024/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3021 - val_loss: 5.7268\n",
      "Epoch 3025/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2921 - val_loss: 5.6376\n",
      "Epoch 3026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3755 - val_loss: 5.9065\n",
      "Epoch 3027/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3127 - val_loss: 5.6383\n",
      "Epoch 3028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3154 - val_loss: 5.6828\n",
      "Epoch 3029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3560 - val_loss: 6.2053\n",
      "Epoch 3030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4048 - val_loss: 5.6488\n",
      "Epoch 3031/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3617 - val_loss: 5.6297\n",
      "Epoch 3032/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3389 - val_loss: 5.6556\n",
      "Epoch 3033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5210 - val_loss: 6.0479\n",
      "Epoch 3034/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5304 - val_loss: 6.0596\n",
      "Epoch 3035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3854 - val_loss: 5.6281\n",
      "Epoch 3036/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3025 - val_loss: 5.8176\n",
      "Epoch 3037/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4321 - val_loss: 5.6427\n",
      "Epoch 3038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3563 - val_loss: 5.6198\n",
      "Epoch 3039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3529 - val_loss: 5.7282\n",
      "Epoch 3040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4068 - val_loss: 5.8872\n",
      "Epoch 3041/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3782 - val_loss: 5.6108\n",
      "Epoch 3042/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3773 - val_loss: 5.6165\n",
      "Epoch 3043/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5121 - val_loss: 5.6484\n",
      "Epoch 3044/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3001 - val_loss: 5.6335\n",
      "Epoch 3045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3401 - val_loss: 6.1407\n",
      "Epoch 3046/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.4579 - val_loss: 5.6997\n",
      "Epoch 3047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3272 - val_loss: 5.6512\n",
      "Epoch 3048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3254 - val_loss: 5.6406\n",
      "Epoch 3049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3525 - val_loss: 5.8496\n",
      "Epoch 3050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3145 - val_loss: 5.8440\n",
      "Epoch 3051/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3910 - val_loss: 5.8037\n",
      "Epoch 3052/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3269 - val_loss: 5.6202\n",
      "Epoch 3053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3202 - val_loss: 5.6306\n",
      "Epoch 3054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3137 - val_loss: 5.6981\n",
      "Epoch 3055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4295 - val_loss: 5.7317\n",
      "Epoch 3056/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4567 - val_loss: 6.1920\n",
      "Epoch 3057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5140 - val_loss: 6.0156\n",
      "Epoch 3058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3160 - val_loss: 5.7433\n",
      "Epoch 3059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2987 - val_loss: 5.7750\n",
      "Epoch 3060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4589 - val_loss: 5.6589\n",
      "Epoch 3061/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5286 - val_loss: 5.6408\n",
      "Epoch 3062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3650 - val_loss: 5.7039\n",
      "Epoch 3063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4503 - val_loss: 5.8988\n",
      "Epoch 3064/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3236 - val_loss: 5.7565\n",
      "Epoch 3065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3898 - val_loss: 6.0103\n",
      "Epoch 3066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3883 - val_loss: 5.7583\n",
      "Epoch 3067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5009 - val_loss: 5.7664\n",
      "Epoch 3068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3533 - val_loss: 6.0046\n",
      "Epoch 3069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4216 - val_loss: 5.6468\n",
      "Epoch 3070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5030 - val_loss: 5.6339\n",
      "Epoch 3071/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.3734 - val_loss: 6.1631\n",
      "Epoch 3072/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.276 - 0s 68us/step - loss: 5.6382 - val_loss: 6.0784\n",
      "Epoch 3073/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3173 - val_loss: 5.6615\n",
      "Epoch 3074/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3060 - val_loss: 5.6978\n",
      "Epoch 3075/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3778 - val_loss: 5.5933\n",
      "Epoch 3076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3094 - val_loss: 5.6295\n",
      "Epoch 3077/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3132 - val_loss: 5.7915\n",
      "Epoch 3078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3750 - val_loss: 5.7602\n",
      "Epoch 3079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3543 - val_loss: 5.5847\n",
      "Epoch 3080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2892 - val_loss: 5.6206\n",
      "Epoch 3081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3171 - val_loss: 5.7092\n",
      "Epoch 3082/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5261 - val_loss: 6.3193\n",
      "Epoch 3083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5895 - val_loss: 5.6255\n",
      "Epoch 3084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3455 - val_loss: 5.6104\n",
      "Epoch 3085/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3831 - val_loss: 5.6472\n",
      "Epoch 3086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3835 - val_loss: 5.5953\n",
      "Epoch 3087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6556 - val_loss: 6.6403\n",
      "Epoch 3088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5027 - val_loss: 5.7124\n",
      "Epoch 3089/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3244 - val_loss: 5.5994\n",
      "Epoch 3090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2673 - val_loss: 5.6410\n",
      "Epoch 3091/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.3112 - val_loss: 5.6390\n",
      "Epoch 3092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3336 - val_loss: 5.7235\n",
      "Epoch 3093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3343 - val_loss: 5.7696\n",
      "Epoch 3094/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.7340 - val_loss: 5.7925\n",
      "Epoch 3095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3991 - val_loss: 5.6245\n",
      "Epoch 3096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3325 - val_loss: 5.6207\n",
      "Epoch 3097/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3523 - val_loss: 5.6189\n",
      "Epoch 3098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2469 - val_loss: 5.5704\n",
      "Epoch 3099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2710 - val_loss: 5.6033\n",
      "Epoch 3100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2681 - val_loss: 5.6569\n",
      "Epoch 3101/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3625 - val_loss: 5.5720\n",
      "Epoch 3102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2764 - val_loss: 5.7336\n",
      "Epoch 3103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4898 - val_loss: 5.6841\n",
      "Epoch 3104/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2747 - val_loss: 5.6209\n",
      "Epoch 3105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3873 - val_loss: 5.7418\n",
      "Epoch 3106/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4320 - val_loss: 6.2411\n",
      "Epoch 3107/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5447 - val_loss: 5.5471\n",
      "Epoch 3108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3520 - val_loss: 5.5693\n",
      "Epoch 3109/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3779 - val_loss: 6.0681\n",
      "Epoch 3110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5620 - val_loss: 5.8545\n",
      "Epoch 3111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3866 - val_loss: 5.6038\n",
      "Epoch 3112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4042 - val_loss: 5.6937\n",
      "Epoch 3113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3736 - val_loss: 5.7203\n",
      "Epoch 3114/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2709 - val_loss: 5.6225\n",
      "Epoch 3115/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2878 - val_loss: 5.7224\n",
      "Epoch 3116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3442 - val_loss: 5.7908\n",
      "Epoch 3117/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5772 - val_loss: 5.9788\n",
      "Epoch 3118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6853 - val_loss: 5.6348\n",
      "Epoch 3119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2820 - val_loss: 5.6920\n",
      "Epoch 3120/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3804 - val_loss: 5.6096\n",
      "Epoch 3121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6572 - val_loss: 5.5983\n",
      "Epoch 3122/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.2984 - val_loss: 5.6048\n",
      "Epoch 3123/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2871 - val_loss: 5.5650\n",
      "Epoch 3124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2883 - val_loss: 5.6295\n",
      "Epoch 3125/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3167 - val_loss: 5.8301\n",
      "Epoch 3126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8233 - val_loss: 5.7576\n",
      "Epoch 3127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3159 - val_loss: 5.6131\n",
      "Epoch 3128/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3078 - val_loss: 5.6034\n",
      "Epoch 3129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3073 - val_loss: 5.7821\n",
      "Epoch 3130/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4618 - val_loss: 5.6660\n",
      "Epoch 3131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4996 - val_loss: 5.5574\n",
      "Epoch 3132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3762 - val_loss: 5.6390\n",
      "Epoch 3133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3743 - val_loss: 5.6575\n",
      "Epoch 3134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3727 - val_loss: 5.7415\n",
      "Epoch 3135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4196 - val_loss: 6.1894\n",
      "Epoch 3136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4162 - val_loss: 5.6063\n",
      "Epoch 3137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3296 - val_loss: 5.6656\n",
      "Epoch 3138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3100 - val_loss: 5.6264\n",
      "Epoch 3139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4459 - val_loss: 5.9963\n",
      "Epoch 3140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5597 - val_loss: 6.6699\n",
      "Epoch 3141/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6868 - val_loss: 5.5659\n",
      "Epoch 3142/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3122 - val_loss: 6.0181\n",
      "Epoch 3143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3048 - val_loss: 5.6118\n",
      "Epoch 3144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4102 - val_loss: 5.6296\n",
      "Epoch 3145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3081 - val_loss: 5.6781\n",
      "Epoch 3146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3321 - val_loss: 5.6079\n",
      "Epoch 3147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4459 - val_loss: 5.6100\n",
      "Epoch 3148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2569 - val_loss: 6.0143\n",
      "Epoch 3149/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6023 - val_loss: 5.6193\n",
      "Epoch 3150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2829 - val_loss: 5.6189\n",
      "Epoch 3151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3254 - val_loss: 5.5882\n",
      "Epoch 3152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2878 - val_loss: 5.5830\n",
      "Epoch 3153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3351 - val_loss: 5.6122\n",
      "Epoch 3154/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5463 - val_loss: 5.5549\n",
      "Epoch 3155/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7886 - val_loss: 5.6029\n",
      "Epoch 3156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3693 - val_loss: 5.9765\n",
      "Epoch 3157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4040 - val_loss: 5.7358\n",
      "Epoch 3158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3466 - val_loss: 5.7072\n",
      "Epoch 3159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3723 - val_loss: 5.6832\n",
      "Epoch 3160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2577 - val_loss: 5.6267\n",
      "Epoch 3161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3030 - val_loss: 5.9174\n",
      "Epoch 3162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5057 - val_loss: 5.6637\n",
      "Epoch 3163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2747 - val_loss: 5.6318\n",
      "Epoch 3164/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 5.3104 - val_loss: 5.5914\n",
      "Epoch 3165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3271 - val_loss: 5.7195\n",
      "Epoch 3166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2748 - val_loss: 5.5774\n",
      "Epoch 3167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2427 - val_loss: 6.1875\n",
      "Epoch 3168/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4703 - val_loss: 6.4874\n",
      "Epoch 3169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5250 - val_loss: 5.6047\n",
      "Epoch 3170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2587 - val_loss: 5.5909\n",
      "Epoch 3171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2548 - val_loss: 5.7934\n",
      "Epoch 3172/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.056 - 0s 46us/step - loss: 5.6216 - val_loss: 5.5723\n",
      "Epoch 3173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.8064 - val_loss: 5.9250\n",
      "Epoch 3174/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5576 - val_loss: 5.5852\n",
      "Epoch 3175/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4625 - val_loss: 6.0575\n",
      "Epoch 3176/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5711 - val_loss: 6.0297\n",
      "Epoch 3177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2804 - val_loss: 5.7615\n",
      "Epoch 3178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3509 - val_loss: 5.6501\n",
      "Epoch 3179/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2792 - val_loss: 5.5807\n",
      "Epoch 3180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2764 - val_loss: 5.6000\n",
      "Epoch 3181/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4177 - val_loss: 5.8858\n",
      "Epoch 3182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2764 - val_loss: 5.5625\n",
      "Epoch 3183/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2353 - val_loss: 5.7381\n",
      "Epoch 3184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3397 - val_loss: 5.9511\n",
      "Epoch 3185/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3323 - val_loss: 5.8653\n",
      "Epoch 3186/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4323 - val_loss: 5.6427\n",
      "Epoch 3187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3211 - val_loss: 5.5656\n",
      "Epoch 3188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5257 - val_loss: 5.5617\n",
      "Epoch 3189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3569 - val_loss: 5.6485\n",
      "Epoch 3190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2703 - val_loss: 5.9037\n",
      "Epoch 3191/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2936 - val_loss: 5.5830\n",
      "Epoch 3192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3099 - val_loss: 5.8064\n",
      "Epoch 3193/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4386 - val_loss: 5.5517\n",
      "Epoch 3194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2990 - val_loss: 5.6609\n",
      "Epoch 3195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2979 - val_loss: 5.6167\n",
      "Epoch 3196/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2393 - val_loss: 5.6036\n",
      "Epoch 3197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3281 - val_loss: 5.6842\n",
      "Epoch 3198/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.3103 - val_loss: 5.5803\n",
      "Epoch 3199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2390 - val_loss: 5.6008\n",
      "Epoch 3200/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3210 - val_loss: 5.7532\n",
      "Epoch 3201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2824 - val_loss: 5.6234\n",
      "Epoch 3202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2860 - val_loss: 5.6957\n",
      "Epoch 3203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3830 - val_loss: 5.7020\n",
      "Epoch 3204/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2562 - val_loss: 5.5483\n",
      "Epoch 3205/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2234 - val_loss: 5.5692\n",
      "Epoch 3206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3629 - val_loss: 5.6016\n",
      "Epoch 3207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3370 - val_loss: 5.5493\n",
      "Epoch 3208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2541 - val_loss: 5.5629\n",
      "Epoch 3209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7586 - val_loss: 5.8953\n",
      "Epoch 3210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2743 - val_loss: 5.6338\n",
      "Epoch 3211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6116 - val_loss: 6.0600\n",
      "Epoch 3212/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3837 - val_loss: 5.5442\n",
      "Epoch 3213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2428 - val_loss: 5.5426\n",
      "Epoch 3214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3336 - val_loss: 5.8614\n",
      "Epoch 3215/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4877 - val_loss: 5.6798\n",
      "Epoch 3216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4973 - val_loss: 5.7641\n",
      "Epoch 3217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.7062 - val_loss: 5.5849\n",
      "Epoch 3218/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5968 - val_loss: 5.7015\n",
      "Epoch 3219/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2503 - val_loss: 5.6680\n",
      "Epoch 3220/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4621 - val_loss: 5.7752\n",
      "Epoch 3221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4774 - val_loss: 5.5635\n",
      "Epoch 3222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3754 - val_loss: 5.9744\n",
      "Epoch 3223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4067 - val_loss: 5.6059\n",
      "Epoch 3224/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4034 - val_loss: 5.5278\n",
      "Epoch 3225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3990 - val_loss: 5.8041\n",
      "Epoch 3226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2639 - val_loss: 5.7655\n",
      "Epoch 3227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4467 - val_loss: 5.6015\n",
      "Epoch 3228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2564 - val_loss: 5.5830\n",
      "Epoch 3229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2339 - val_loss: 5.6034\n",
      "Epoch 3230/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2122 - val_loss: 5.5491\n",
      "Epoch 3231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2455 - val_loss: 5.5990\n",
      "Epoch 3232/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2503 - val_loss: 5.5705\n",
      "Epoch 3233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3385 - val_loss: 5.8594\n",
      "Epoch 3234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2771 - val_loss: 5.7951\n",
      "Epoch 3235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5186 - val_loss: 5.7012\n",
      "Epoch 3236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2354 - val_loss: 5.5904\n",
      "Epoch 3237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3361 - val_loss: 5.9544\n",
      "Epoch 3238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5608 - val_loss: 5.5485\n",
      "Epoch 3239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3910 - val_loss: 5.5845\n",
      "Epoch 3240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3248 - val_loss: 5.7854\n",
      "Epoch 3241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2577 - val_loss: 5.5250\n",
      "Epoch 3242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2447 - val_loss: 5.9070\n",
      "Epoch 3243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2696 - val_loss: 5.8301\n",
      "Epoch 3244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3578 - val_loss: 5.6011\n",
      "Epoch 3245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2253 - val_loss: 5.6147\n",
      "Epoch 3246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3136 - val_loss: 5.5550\n",
      "Epoch 3247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5943 - val_loss: 6.2543\n",
      "Epoch 3248/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3577 - val_loss: 5.7354\n",
      "Epoch 3249/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4352 - val_loss: 5.7808\n",
      "Epoch 3250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3836 - val_loss: 5.8798\n",
      "Epoch 3251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5077 - val_loss: 5.7603\n",
      "Epoch 3252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2960 - val_loss: 5.5654\n",
      "Epoch 3253/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2316 - val_loss: 5.5185\n",
      "Epoch 3254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3588 - val_loss: 5.6136\n",
      "Epoch 3255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3775 - val_loss: 5.5526\n",
      "Epoch 3256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2994 - val_loss: 5.5717\n",
      "Epoch 3257/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2622 - val_loss: 5.6838\n",
      "Epoch 3258/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2405 - val_loss: 5.5348\n",
      "Epoch 3259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3970 - val_loss: 5.7896\n",
      "Epoch 3260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2728 - val_loss: 5.6592\n",
      "Epoch 3261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2099 - val_loss: 5.5399\n",
      "Epoch 3262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2279 - val_loss: 6.1366\n",
      "Epoch 3263/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2621 - val_loss: 5.5440\n",
      "Epoch 3264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2325 - val_loss: 5.7089\n",
      "Epoch 3265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2154 - val_loss: 5.5977\n",
      "Epoch 3266/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3867 - val_loss: 5.5502\n",
      "Epoch 3267/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1933 - val_loss: 5.6327\n",
      "Epoch 3268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4746 - val_loss: 5.7822\n",
      "Epoch 3269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3711 - val_loss: 5.5664\n",
      "Epoch 3270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5111 - val_loss: 5.6505\n",
      "Epoch 3271/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3762 - val_loss: 5.6708\n",
      "Epoch 3272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3963 - val_loss: 5.6353\n",
      "Epoch 3273/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3042 - val_loss: 5.5389\n",
      "Epoch 3274/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.3432 - val_loss: 5.5430\n",
      "Epoch 3275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3085 - val_loss: 5.6799\n",
      "Epoch 3276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2996 - val_loss: 5.6294\n",
      "Epoch 3277/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2154 - val_loss: 5.7261\n",
      "Epoch 3278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3034 - val_loss: 5.5775\n",
      "Epoch 3279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2961 - val_loss: 5.6984\n",
      "Epoch 3280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4140 - val_loss: 5.5373\n",
      "Epoch 3281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2754 - val_loss: 5.5722\n",
      "Epoch 3282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5830 - val_loss: 5.6096\n",
      "Epoch 3283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2994 - val_loss: 5.7471\n",
      "Epoch 3284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3187 - val_loss: 5.5769\n",
      "Epoch 3285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2963 - val_loss: 5.5556\n",
      "Epoch 3286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4160 - val_loss: 5.9665\n",
      "Epoch 3287/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3196 - val_loss: 5.5961\n",
      "Epoch 3288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3332 - val_loss: 5.5725\n",
      "Epoch 3289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4474 - val_loss: 5.5265\n",
      "Epoch 3290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2324 - val_loss: 5.5428\n",
      "Epoch 3291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2156 - val_loss: 5.7462\n",
      "Epoch 3292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2104 - val_loss: 5.5106\n",
      "Epoch 3293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4906 - val_loss: 5.7853\n",
      "Epoch 3294/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.6075 - val_loss: 5.8273\n",
      "Epoch 3295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4031 - val_loss: 5.6326\n",
      "Epoch 3296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2709 - val_loss: 5.5566\n",
      "Epoch 3297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2925 - val_loss: 5.8290\n",
      "Epoch 3298/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2996 - val_loss: 5.5409\n",
      "Epoch 3299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4707 - val_loss: 5.8233\n",
      "Epoch 3300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2969 - val_loss: 5.6123\n",
      "Epoch 3301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6586 - val_loss: 5.6952\n",
      "Epoch 3302/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5703 - val_loss: 6.0129\n",
      "Epoch 3303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2777 - val_loss: 5.5256\n",
      "Epoch 3304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4614 - val_loss: 5.8548\n",
      "Epoch 3305/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2615 - val_loss: 5.5231\n",
      "Epoch 3306/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2860 - val_loss: 5.5198\n",
      "Epoch 3307/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2983 - val_loss: 5.6603\n",
      "Epoch 3308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5343 - val_loss: 5.8568\n",
      "Epoch 3309/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2464 - val_loss: 5.7151\n",
      "Epoch 3310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3831 - val_loss: 5.8901\n",
      "Epoch 3311/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2944 - val_loss: 5.5541\n",
      "Epoch 3312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3078 - val_loss: 5.5167\n",
      "Epoch 3313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5568 - val_loss: 5.5974\n",
      "Epoch 3314/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1789 - val_loss: 5.8042\n",
      "Epoch 3315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3658 - val_loss: 5.6768\n",
      "Epoch 3316/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 7.270 - 0s 46us/step - loss: 5.4281 - val_loss: 5.5232\n",
      "Epoch 3317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2776 - val_loss: 5.5264\n",
      "Epoch 3318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2224 - val_loss: 5.6612\n",
      "Epoch 3319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1717 - val_loss: 5.8347\n",
      "Epoch 3320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3031 - val_loss: 5.5813\n",
      "Epoch 3321/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2099 - val_loss: 5.5334\n",
      "Epoch 3322/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2082 - val_loss: 5.5840\n",
      "Epoch 3323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2952 - val_loss: 6.0735\n",
      "Epoch 3324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3904 - val_loss: 6.2033\n",
      "Epoch 3325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3661 - val_loss: 5.5100\n",
      "Epoch 3326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2440 - val_loss: 5.6172\n",
      "Epoch 3327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3545 - val_loss: 5.5183\n",
      "Epoch 3328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2700 - val_loss: 5.5523\n",
      "Epoch 3329/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4321 - val_loss: 5.8751\n",
      "Epoch 3330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6080 - val_loss: 5.8529\n",
      "Epoch 3331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2300 - val_loss: 5.5505\n",
      "Epoch 3332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1570 - val_loss: 5.5307\n",
      "Epoch 3333/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2142 - val_loss: 5.5301\n",
      "Epoch 3334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2941 - val_loss: 5.5578\n",
      "Epoch 3335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2659 - val_loss: 5.5208\n",
      "Epoch 3336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2622 - val_loss: 5.5620\n",
      "Epoch 3337/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2283 - val_loss: 5.5955\n",
      "Epoch 3338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2736 - val_loss: 5.5386\n",
      "Epoch 3339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2158 - val_loss: 5.5280\n",
      "Epoch 3340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2723 - val_loss: 5.5047\n",
      "Epoch 3341/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3504 - val_loss: 5.5379\n",
      "Epoch 3342/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1818 - val_loss: 5.5879\n",
      "Epoch 3343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2834 - val_loss: 5.7680\n",
      "Epoch 3344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3979 - val_loss: 5.5010\n",
      "Epoch 3345/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2824 - val_loss: 6.2313\n",
      "Epoch 3346/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2388 - val_loss: 5.4965\n",
      "Epoch 3347/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.392 - 0s 46us/step - loss: 5.2772 - val_loss: 6.0286\n",
      "Epoch 3348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5228 - val_loss: 5.6627\n",
      "Epoch 3349/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.4608 - val_loss: 5.6403\n",
      "Epoch 3350/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.4144 - val_loss: 5.6045\n",
      "Epoch 3351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2551 - val_loss: 5.5266\n",
      "Epoch 3352/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3208 - val_loss: 5.5069\n",
      "Epoch 3353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1933 - val_loss: 5.5367\n",
      "Epoch 3354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3531 - val_loss: 5.5831\n",
      "Epoch 3355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2089 - val_loss: 5.5996\n",
      "Epoch 3356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3765 - val_loss: 5.7515\n",
      "Epoch 3357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2589 - val_loss: 5.5069\n",
      "Epoch 3358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2180 - val_loss: 5.5279\n",
      "Epoch 3359/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.2822 - val_loss: 5.8166\n",
      "Epoch 3360/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3973 - val_loss: 5.5299\n",
      "Epoch 3361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2742 - val_loss: 5.5176\n",
      "Epoch 3362/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2214 - val_loss: 6.1057\n",
      "Epoch 3363/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3989 - val_loss: 5.4835\n",
      "Epoch 3364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3924 - val_loss: 5.9800\n",
      "Epoch 3365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2582 - val_loss: 5.5482\n",
      "Epoch 3366/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2311 - val_loss: 5.5065\n",
      "Epoch 3367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1740 - val_loss: 5.7891\n",
      "Epoch 3368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4700 - val_loss: 5.5165\n",
      "Epoch 3369/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3078 - val_loss: 5.7776\n",
      "Epoch 3370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3412 - val_loss: 5.5559\n",
      "Epoch 3371/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1855 - val_loss: 5.6874\n",
      "Epoch 3372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3753 - val_loss: 5.6732\n",
      "Epoch 3373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3428 - val_loss: 5.7078\n",
      "Epoch 3374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4898 - val_loss: 5.5680\n",
      "Epoch 3375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2148 - val_loss: 5.4772\n",
      "Epoch 3376/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2737 - val_loss: 5.6243\n",
      "Epoch 3377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2738 - val_loss: 5.5179\n",
      "Epoch 3378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1852 - val_loss: 5.5830\n",
      "Epoch 3379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2672 - val_loss: 5.6576\n",
      "Epoch 3380/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3414 - val_loss: 6.0839\n",
      "Epoch 3381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3020 - val_loss: 5.5524\n",
      "Epoch 3382/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3622 - val_loss: 5.8437\n",
      "Epoch 3383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6039 - val_loss: 6.0611\n",
      "Epoch 3384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4215 - val_loss: 5.5246\n",
      "Epoch 3385/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1747 - val_loss: 5.4884\n",
      "Epoch 3386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1866 - val_loss: 5.4956\n",
      "Epoch 3387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2008 - val_loss: 5.5204\n",
      "Epoch 3388/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1805 - val_loss: 5.8290\n",
      "Epoch 3389/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2517 - val_loss: 5.5097\n",
      "Epoch 3390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2712 - val_loss: 5.5138\n",
      "Epoch 3391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1867 - val_loss: 5.8826\n",
      "Epoch 3392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2570 - val_loss: 5.4669\n",
      "Epoch 3393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1972 - val_loss: 5.5405\n",
      "Epoch 3394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1931 - val_loss: 5.5062\n",
      "Epoch 3395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2135 - val_loss: 5.8440\n",
      "Epoch 3396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4235 - val_loss: 5.4583\n",
      "Epoch 3397/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2297 - val_loss: 5.5599\n",
      "Epoch 3398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3017 - val_loss: 5.6596\n",
      "Epoch 3399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2847 - val_loss: 5.5329\n",
      "Epoch 3400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2185 - val_loss: 5.7509\n",
      "Epoch 3401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2952 - val_loss: 5.5043\n",
      "Epoch 3402/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2601 - val_loss: 5.4923\n",
      "Epoch 3403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3679 - val_loss: 5.9916\n",
      "Epoch 3404/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1840 - val_loss: 5.8193\n",
      "Epoch 3405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2482 - val_loss: 5.5451\n",
      "Epoch 3406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1958 - val_loss: 5.8120\n",
      "Epoch 3407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2218 - val_loss: 5.5554\n",
      "Epoch 3408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1579 - val_loss: 5.5020\n",
      "Epoch 3409/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3270 - val_loss: 6.5370\n",
      "Epoch 3410/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6455 - val_loss: 5.5288\n",
      "Epoch 3411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2501 - val_loss: 5.5930\n",
      "Epoch 3412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3248 - val_loss: 5.5902\n",
      "Epoch 3413/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2992 - val_loss: 5.5296\n",
      "Epoch 3414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2275 - val_loss: 5.4723\n",
      "Epoch 3415/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.2231 - val_loss: 5.4943\n",
      "Epoch 3416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3062 - val_loss: 5.7074\n",
      "Epoch 3417/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2209 - val_loss: 5.6236\n",
      "Epoch 3418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3430 - val_loss: 5.6536\n",
      "Epoch 3419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2153 - val_loss: 5.4690\n",
      "Epoch 3420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1842 - val_loss: 5.6013\n",
      "Epoch 3421/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1514 - val_loss: 5.5253\n",
      "Epoch 3422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1837 - val_loss: 5.6118\n",
      "Epoch 3423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1921 - val_loss: 5.9478\n",
      "Epoch 3424/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 7.661 - 0s 46us/step - loss: 5.2012 - val_loss: 5.6593\n",
      "Epoch 3425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2292 - val_loss: 5.5154\n",
      "Epoch 3426/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.2431 - val_loss: 5.9716\n",
      "Epoch 3427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4910 - val_loss: 5.4927\n",
      "Epoch 3428/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1688 - val_loss: 5.5023\n",
      "Epoch 3429/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2144 - val_loss: 5.6830\n",
      "Epoch 3430/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3941 - val_loss: 5.6334\n",
      "Epoch 3431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3388 - val_loss: 5.4607\n",
      "Epoch 3432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2028 - val_loss: 5.4703\n",
      "Epoch 3433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1849 - val_loss: 5.6213\n",
      "Epoch 3434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1413 - val_loss: 5.4661\n",
      "Epoch 3435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2168 - val_loss: 5.7560\n",
      "Epoch 3436/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2270 - val_loss: 5.5044\n",
      "Epoch 3437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1629 - val_loss: 5.5003\n",
      "Epoch 3438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2498 - val_loss: 5.7441\n",
      "Epoch 3439/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1725 - val_loss: 6.0391\n",
      "Epoch 3440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3783 - val_loss: 5.4828\n",
      "Epoch 3441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2445 - val_loss: 5.4827\n",
      "Epoch 3442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2542 - val_loss: 5.6193\n",
      "Epoch 3443/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.805 - 0s 46us/step - loss: 5.4397 - val_loss: 6.0312\n",
      "Epoch 3444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3338 - val_loss: 5.8690\n",
      "Epoch 3445/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3002 - val_loss: 5.5256\n",
      "Epoch 3446/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2090 - val_loss: 5.4573\n",
      "Epoch 3447/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1743 - val_loss: 5.4777\n",
      "Epoch 3448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1773 - val_loss: 5.5534\n",
      "Epoch 3449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2204 - val_loss: 5.4709\n",
      "Epoch 3450/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1518 - val_loss: 5.5134\n",
      "Epoch 3451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1854 - val_loss: 5.5656\n",
      "Epoch 3452/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1670 - val_loss: 5.4598\n",
      "Epoch 3453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1731 - val_loss: 5.7146\n",
      "Epoch 3454/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1816 - val_loss: 5.5106\n",
      "Epoch 3455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2650 - val_loss: 5.6636\n",
      "Epoch 3456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3472 - val_loss: 5.5246\n",
      "Epoch 3457/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3904 - val_loss: 5.7181\n",
      "Epoch 3458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2484 - val_loss: 5.5301\n",
      "Epoch 3459/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2031 - val_loss: 5.4980\n",
      "Epoch 3460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1766 - val_loss: 5.6423\n",
      "Epoch 3461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1515 - val_loss: 5.5260\n",
      "Epoch 3462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2956 - val_loss: 5.4515\n",
      "Epoch 3463/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.4080 - val_loss: 5.4830\n",
      "Epoch 3464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1971 - val_loss: 5.7077\n",
      "Epoch 3465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2099 - val_loss: 5.4869\n",
      "Epoch 3466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1941 - val_loss: 5.6942\n",
      "Epoch 3467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1995 - val_loss: 5.6340\n",
      "Epoch 3468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2467 - val_loss: 5.4619\n",
      "Epoch 3469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2152 - val_loss: 5.5299\n",
      "Epoch 3470/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1501 - val_loss: 5.5877\n",
      "Epoch 3471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3054 - val_loss: 5.8344\n",
      "Epoch 3472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5422 - val_loss: 6.0652\n",
      "Epoch 3473/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6834 - val_loss: 5.6031\n",
      "Epoch 3474/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1584 - val_loss: 5.5410\n",
      "Epoch 3475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1763 - val_loss: 5.7165\n",
      "Epoch 3476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2613 - val_loss: 5.4703\n",
      "Epoch 3477/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1691 - val_loss: 5.4841\n",
      "Epoch 3478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2034 - val_loss: 5.4868\n",
      "Epoch 3479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1857 - val_loss: 5.5124\n",
      "Epoch 3480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1999 - val_loss: 5.4819\n",
      "Epoch 3481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1510 - val_loss: 5.4586\n",
      "Epoch 3482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3267 - val_loss: 5.5316\n",
      "Epoch 3483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1477 - val_loss: 5.4943\n",
      "Epoch 3484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1695 - val_loss: 5.6361\n",
      "Epoch 3485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2862 - val_loss: 5.5286\n",
      "Epoch 3486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1717 - val_loss: 5.4577\n",
      "Epoch 3487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1314 - val_loss: 5.6158\n",
      "Epoch 3488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1594 - val_loss: 5.5054\n",
      "Epoch 3489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1709 - val_loss: 5.4736\n",
      "Epoch 3490/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1719 - val_loss: 5.6865\n",
      "Epoch 3491/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1739 - val_loss: 5.4458\n",
      "Epoch 3492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2287 - val_loss: 5.4521\n",
      "Epoch 3493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2391 - val_loss: 5.6224\n",
      "Epoch 3494/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2033 - val_loss: 5.4329\n",
      "Epoch 3495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1973 - val_loss: 5.4414\n",
      "Epoch 3496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2304 - val_loss: 5.5541\n",
      "Epoch 3497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2840 - val_loss: 5.5107\n",
      "Epoch 3498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1390 - val_loss: 5.4573\n",
      "Epoch 3499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1829 - val_loss: 5.6182\n",
      "Epoch 3500/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1915 - val_loss: 5.6673\n",
      "Epoch 3501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1840 - val_loss: 5.5416\n",
      "Epoch 3502/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.1758 - val_loss: 5.5010\n",
      "Epoch 3503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1661 - val_loss: 5.4497\n",
      "Epoch 3504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2991 - val_loss: 5.7573\n",
      "Epoch 3505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1516 - val_loss: 5.4235\n",
      "Epoch 3506/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1571 - val_loss: 5.4418\n",
      "Epoch 3507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1262 - val_loss: 5.8860\n",
      "Epoch 3508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4155 - val_loss: 5.4666\n",
      "Epoch 3509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4048 - val_loss: 5.5143\n",
      "Epoch 3510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2555 - val_loss: 5.8728\n",
      "Epoch 3511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2647 - val_loss: 5.4273\n",
      "Epoch 3512/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2472 - val_loss: 5.7708\n",
      "Epoch 3513/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2857 - val_loss: 5.4338\n",
      "Epoch 3514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1971 - val_loss: 5.5331\n",
      "Epoch 3515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3783 - val_loss: 5.7229\n",
      "Epoch 3516/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5099 - val_loss: 5.5651\n",
      "Epoch 3517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3949 - val_loss: 5.7870\n",
      "Epoch 3518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2912 - val_loss: 5.5170\n",
      "Epoch 3519/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2623 - val_loss: 5.5938\n",
      "Epoch 3520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2151 - val_loss: 5.5270\n",
      "Epoch 3521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2699 - val_loss: 5.4442\n",
      "Epoch 3522/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2790 - val_loss: 5.4381\n",
      "Epoch 3523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1444 - val_loss: 5.6966\n",
      "Epoch 3524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3266 - val_loss: 5.9570\n",
      "Epoch 3525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4300 - val_loss: 5.4251\n",
      "Epoch 3526/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1743 - val_loss: 5.4645\n",
      "Epoch 3527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4152 - val_loss: 5.6830\n",
      "Epoch 3528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1408 - val_loss: 5.5307\n",
      "Epoch 3529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2471 - val_loss: 5.4852\n",
      "Epoch 3530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1791 - val_loss: 5.4400\n",
      "Epoch 3531/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1601 - val_loss: 5.6241\n",
      "Epoch 3532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1711 - val_loss: 5.5165\n",
      "Epoch 3533/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1261 - val_loss: 5.6297\n",
      "Epoch 3534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2188 - val_loss: 5.6903\n",
      "Epoch 3535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4538 - val_loss: 5.4388\n",
      "Epoch 3536/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1824 - val_loss: 5.4470\n",
      "Epoch 3537/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2305 - val_loss: 5.4212\n",
      "Epoch 3538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2609 - val_loss: 5.5912\n",
      "Epoch 3539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1890 - val_loss: 5.6334\n",
      "Epoch 3540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1742 - val_loss: 5.4588\n",
      "Epoch 3541/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1455 - val_loss: 5.6496\n",
      "Epoch 3542/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1356 - val_loss: 5.4568\n",
      "Epoch 3543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2150 - val_loss: 5.5930\n",
      "Epoch 3544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3346 - val_loss: 5.5216\n",
      "Epoch 3545/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1208 - val_loss: 5.6715\n",
      "Epoch 3546/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2774 - val_loss: 5.4874\n",
      "Epoch 3547/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4994 - val_loss: 5.6480\n",
      "Epoch 3548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1498 - val_loss: 5.4281\n",
      "Epoch 3549/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2099 - val_loss: 5.4379\n",
      "Epoch 3550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1101 - val_loss: 5.4747\n",
      "Epoch 3551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1538 - val_loss: 5.4602\n",
      "Epoch 3552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1955 - val_loss: 5.5054\n",
      "Epoch 3553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1975 - val_loss: 6.2846\n",
      "Epoch 3554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4419 - val_loss: 5.6127\n",
      "Epoch 3555/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3312 - val_loss: 5.6030\n",
      "Epoch 3556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1728 - val_loss: 5.4417\n",
      "Epoch 3557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2192 - val_loss: 5.4729\n",
      "Epoch 3558/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1656 - val_loss: 5.4842\n",
      "Epoch 3559/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1813 - val_loss: 5.4482\n",
      "Epoch 3560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1410 - val_loss: 5.4454\n",
      "Epoch 3561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2013 - val_loss: 5.4228\n",
      "Epoch 3562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1977 - val_loss: 5.4417\n",
      "Epoch 3563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1927 - val_loss: 5.6402\n",
      "Epoch 3564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1286 - val_loss: 5.4369\n",
      "Epoch 3565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3031 - val_loss: 5.6529\n",
      "Epoch 3566/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4018 - val_loss: 5.4377\n",
      "Epoch 3567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1877 - val_loss: 5.5712\n",
      "Epoch 3568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1037 - val_loss: 5.6367\n",
      "Epoch 3569/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1968 - val_loss: 5.5105\n",
      "Epoch 3570/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2488 - val_loss: 5.8335\n",
      "Epoch 3571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2781 - val_loss: 5.4269\n",
      "Epoch 3572/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.1325 - val_loss: 5.4127\n",
      "Epoch 3573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2021 - val_loss: 5.4248\n",
      "Epoch 3574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1775 - val_loss: 5.4048\n",
      "Epoch 3575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1152 - val_loss: 5.4948\n",
      "Epoch 3576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1429 - val_loss: 5.4721\n",
      "Epoch 3577/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2870 - val_loss: 5.4315\n",
      "Epoch 3578/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.3987 - val_loss: 5.6906\n",
      "Epoch 3579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4664 - val_loss: 5.4921\n",
      "Epoch 3580/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2398 - val_loss: 5.5144\n",
      "Epoch 3581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1954 - val_loss: 5.6739\n",
      "Epoch 3582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2388 - val_loss: 5.4214\n",
      "Epoch 3583/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1614 - val_loss: 5.5071\n",
      "Epoch 3584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1157 - val_loss: 5.4712\n",
      "Epoch 3585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3602 - val_loss: 5.4139\n",
      "Epoch 3586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1742 - val_loss: 5.5572\n",
      "Epoch 3587/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1199 - val_loss: 5.5204\n",
      "Epoch 3588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1449 - val_loss: 5.5042\n",
      "Epoch 3589/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1951 - val_loss: 5.4717\n",
      "Epoch 3590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6701 - val_loss: 6.4397\n",
      "Epoch 3591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3940 - val_loss: 5.6886\n",
      "Epoch 3592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2575 - val_loss: 5.6626\n",
      "Epoch 3593/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1619 - val_loss: 5.5657\n",
      "Epoch 3594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2126 - val_loss: 5.6720\n",
      "Epoch 3595/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5050 - val_loss: 5.6764\n",
      "Epoch 3596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2872 - val_loss: 5.6462\n",
      "Epoch 3597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1338 - val_loss: 5.4194\n",
      "Epoch 3598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1922 - val_loss: 5.4509\n",
      "Epoch 3599/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1549 - val_loss: 5.5529\n",
      "Epoch 3600/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2159 - val_loss: 5.5662\n",
      "Epoch 3601/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0860 - val_loss: 5.4769\n",
      "Epoch 3602/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2187 - val_loss: 5.3911\n",
      "Epoch 3603/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1724 - val_loss: 5.6459\n",
      "Epoch 3604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3789 - val_loss: 5.7316\n",
      "Epoch 3605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3404 - val_loss: 5.9454\n",
      "Epoch 3606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1766 - val_loss: 5.5047\n",
      "Epoch 3607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1225 - val_loss: 5.4409\n",
      "Epoch 3608/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.2116 - val_loss: 6.4365\n",
      "Epoch 3609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4883 - val_loss: 5.6668\n",
      "Epoch 3610/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1549 - val_loss: 5.4451\n",
      "Epoch 3611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0884 - val_loss: 5.4223\n",
      "Epoch 3612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1065 - val_loss: 5.4254\n",
      "Epoch 3613/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1592 - val_loss: 5.4064\n",
      "Epoch 3614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1107 - val_loss: 5.4716\n",
      "Epoch 3615/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1159 - val_loss: 5.6049\n",
      "Epoch 3616/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2173 - val_loss: 5.7478\n",
      "Epoch 3617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3196 - val_loss: 5.4048\n",
      "Epoch 3618/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1443 - val_loss: 5.4284\n",
      "Epoch 3619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1971 - val_loss: 5.4258\n",
      "Epoch 3620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1289 - val_loss: 5.4518\n",
      "Epoch 3621/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1456 - val_loss: 5.4127\n",
      "Epoch 3622/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1446 - val_loss: 5.6402\n",
      "Epoch 3623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1757 - val_loss: 5.6143\n",
      "Epoch 3624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3396 - val_loss: 5.5480\n",
      "Epoch 3625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1864 - val_loss: 5.5847\n",
      "Epoch 3626/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2460 - val_loss: 5.4477\n",
      "Epoch 3627/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.1523 - val_loss: 5.4610\n",
      "Epoch 3628/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1094 - val_loss: 5.4266\n",
      "Epoch 3629/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1140 - val_loss: 5.6023\n",
      "Epoch 3630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0893 - val_loss: 5.5234\n",
      "Epoch 3631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1426 - val_loss: 5.4580\n",
      "Epoch 3632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2486 - val_loss: 5.6287\n",
      "Epoch 3633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1589 - val_loss: 5.4066\n",
      "Epoch 3634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1766 - val_loss: 5.4310\n",
      "Epoch 3635/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2846 - val_loss: 5.3887\n",
      "Epoch 3636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2775 - val_loss: 5.6048\n",
      "Epoch 3637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3953 - val_loss: 5.9214\n",
      "Epoch 3638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3163 - val_loss: 5.8709\n",
      "Epoch 3639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2467 - val_loss: 5.4002\n",
      "Epoch 3640/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1309 - val_loss: 5.4740\n",
      "Epoch 3641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3445 - val_loss: 5.4333\n",
      "Epoch 3642/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1056 - val_loss: 5.4729\n",
      "Epoch 3643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0947 - val_loss: 5.5357\n",
      "Epoch 3644/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2303 - val_loss: 5.6589\n",
      "Epoch 3645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1129 - val_loss: 5.4445\n",
      "Epoch 3646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1299 - val_loss: 5.4144\n",
      "Epoch 3647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2034 - val_loss: 5.4041\n",
      "Epoch 3648/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1692 - val_loss: 5.4227\n",
      "Epoch 3649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3062 - val_loss: 5.4196\n",
      "Epoch 3650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1630 - val_loss: 5.4410\n",
      "Epoch 3651/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1407 - val_loss: 5.4606\n",
      "Epoch 3652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2424 - val_loss: 5.8258\n",
      "Epoch 3653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3842 - val_loss: 5.7635\n",
      "Epoch 3654/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.2548 - val_loss: 5.4044\n",
      "Epoch 3655/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2048 - val_loss: 5.4933\n",
      "Epoch 3656/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1546 - val_loss: 5.4252\n",
      "Epoch 3657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1164 - val_loss: 5.6625\n",
      "Epoch 3658/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2823 - val_loss: 5.6130\n",
      "Epoch 3659/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4699 - val_loss: 5.6652\n",
      "Epoch 3660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1491 - val_loss: 5.6223\n",
      "Epoch 3661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2151 - val_loss: 5.9378\n",
      "Epoch 3662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2038 - val_loss: 5.9279\n",
      "Epoch 3663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4974 - val_loss: 5.4937\n",
      "Epoch 3664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1005 - val_loss: 5.3817\n",
      "Epoch 3665/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1421 - val_loss: 5.3915\n",
      "Epoch 3666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3816 - val_loss: 5.8435\n",
      "Epoch 3667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2359 - val_loss: 5.3928\n",
      "Epoch 3668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1114 - val_loss: 5.6043\n",
      "Epoch 3669/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2525 - val_loss: 5.3928\n",
      "Epoch 3670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3599 - val_loss: 5.4201\n",
      "Epoch 3671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2612 - val_loss: 5.4450\n",
      "Epoch 3672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1245 - val_loss: 5.3996\n",
      "Epoch 3673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2624 - val_loss: 5.8729\n",
      "Epoch 3674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2321 - val_loss: 5.3971\n",
      "Epoch 3675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0867 - val_loss: 5.4114\n",
      "Epoch 3676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1084 - val_loss: 5.4840\n",
      "Epoch 3677/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1464 - val_loss: 5.7294\n",
      "Epoch 3678/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0960 - val_loss: 5.4917\n",
      "Epoch 3679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0925 - val_loss: 5.4158\n",
      "Epoch 3680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1883 - val_loss: 5.4814\n",
      "Epoch 3681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0989 - val_loss: 5.6205\n",
      "Epoch 3682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2550 - val_loss: 5.4374\n",
      "Epoch 3683/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2499 - val_loss: 5.4001\n",
      "Epoch 3684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1248 - val_loss: 5.3987\n",
      "Epoch 3685/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1798 - val_loss: 5.4815\n",
      "Epoch 3686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1519 - val_loss: 5.5894\n",
      "Epoch 3687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1896 - val_loss: 5.4608\n",
      "Epoch 3688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1712 - val_loss: 5.3813\n",
      "Epoch 3689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1414 - val_loss: 5.8982\n",
      "Epoch 3690/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3403 - val_loss: 5.4516\n",
      "Epoch 3691/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1415 - val_loss: 5.9436\n",
      "Epoch 3692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3773 - val_loss: 6.3173\n",
      "Epoch 3693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1917 - val_loss: 5.5484\n",
      "Epoch 3694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2079 - val_loss: 5.4338\n",
      "Epoch 3695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2226 - val_loss: 5.4285\n",
      "Epoch 3696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1257 - val_loss: 5.6548\n",
      "Epoch 3697/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1789 - val_loss: 6.2724\n",
      "Epoch 3698/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3355 - val_loss: 5.8401\n",
      "Epoch 3699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2389 - val_loss: 5.3895\n",
      "Epoch 3700/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1622 - val_loss: 5.6715\n",
      "Epoch 3701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1126 - val_loss: 5.4450\n",
      "Epoch 3702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0499 - val_loss: 5.4147\n",
      "Epoch 3703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2448 - val_loss: 5.4096\n",
      "Epoch 3704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0714 - val_loss: 5.4000\n",
      "Epoch 3705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1525 - val_loss: 5.8753\n",
      "Epoch 3706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2551 - val_loss: 5.4845\n",
      "Epoch 3707/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1006 - val_loss: 5.3672\n",
      "Epoch 3708/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0850 - val_loss: 5.4117\n",
      "Epoch 3709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0543 - val_loss: 5.6483\n",
      "Epoch 3710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1310 - val_loss: 5.4176\n",
      "Epoch 3711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1085 - val_loss: 5.4704\n",
      "Epoch 3712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4566 - val_loss: 6.2034\n",
      "Epoch 3713/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1927 - val_loss: 5.6123\n",
      "Epoch 3714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0837 - val_loss: 5.4574\n",
      "Epoch 3715/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1754 - val_loss: 5.6314\n",
      "Epoch 3716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2037 - val_loss: 5.5901\n",
      "Epoch 3717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3727 - val_loss: 5.5136\n",
      "Epoch 3718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2611 - val_loss: 5.6473\n",
      "Epoch 3719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2258 - val_loss: 5.3796\n",
      "Epoch 3720/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.1483 - val_loss: 5.3944\n",
      "Epoch 3721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0777 - val_loss: 5.4865\n",
      "Epoch 3722/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0922 - val_loss: 5.3763\n",
      "Epoch 3723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1787 - val_loss: 5.4999\n",
      "Epoch 3724/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2368 - val_loss: 5.4067\n",
      "Epoch 3725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2369 - val_loss: 5.5022\n",
      "Epoch 3726/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1057 - val_loss: 5.4624\n",
      "Epoch 3727/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1537 - val_loss: 5.5067\n",
      "Epoch 3728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0408 - val_loss: 5.3682\n",
      "Epoch 3729/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1134 - val_loss: 5.5142\n",
      "Epoch 3730/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.1734 - val_loss: 5.6270\n",
      "Epoch 3731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1100 - val_loss: 5.3775\n",
      "Epoch 3732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0674 - val_loss: 5.5252\n",
      "Epoch 3733/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1784 - val_loss: 5.6197\n",
      "Epoch 3734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1282 - val_loss: 5.3938\n",
      "Epoch 3735/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2188 - val_loss: 5.3742\n",
      "Epoch 3736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2257 - val_loss: 5.3702\n",
      "Epoch 3737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1712 - val_loss: 5.8921\n",
      "Epoch 3738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1536 - val_loss: 5.4077\n",
      "Epoch 3739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1696 - val_loss: 5.3764\n",
      "Epoch 3740/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0714 - val_loss: 5.4580\n",
      "Epoch 3741/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1408 - val_loss: 5.4084\n",
      "Epoch 3742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0970 - val_loss: 5.4135\n",
      "Epoch 3743/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1266 - val_loss: 5.4101\n",
      "Epoch 3744/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0524 - val_loss: 5.4744\n",
      "Epoch 3745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0967 - val_loss: 5.3816\n",
      "Epoch 3746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1172 - val_loss: 5.6697\n",
      "Epoch 3747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3058 - val_loss: 5.3853\n",
      "Epoch 3748/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1856 - val_loss: 5.3918\n",
      "Epoch 3749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1146 - val_loss: 5.5130\n",
      "Epoch 3750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0555 - val_loss: 5.4106\n",
      "Epoch 3751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0946 - val_loss: 5.5711\n",
      "Epoch 3752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1576 - val_loss: 5.3936\n",
      "Epoch 3753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0751 - val_loss: 5.9942\n",
      "Epoch 3754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4829 - val_loss: 5.4516\n",
      "Epoch 3755/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0799 - val_loss: 5.4727\n",
      "Epoch 3756/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3678 - val_loss: 5.3679\n",
      "Epoch 3757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2866 - val_loss: 5.4179\n",
      "Epoch 3758/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1092 - val_loss: 5.4096\n",
      "Epoch 3759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1105 - val_loss: 5.5515\n",
      "Epoch 3760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1305 - val_loss: 5.6191\n",
      "Epoch 3761/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1703 - val_loss: 5.4325\n",
      "Epoch 3762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3123 - val_loss: 5.5921\n",
      "Epoch 3763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2484 - val_loss: 5.3936\n",
      "Epoch 3764/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0707 - val_loss: 5.6659\n",
      "Epoch 3765/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2430 - val_loss: 5.3592\n",
      "Epoch 3766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2324 - val_loss: 5.3802\n",
      "Epoch 3767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2482 - val_loss: 5.3820\n",
      "Epoch 3768/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1367 - val_loss: 5.3647\n",
      "Epoch 3769/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1338 - val_loss: 5.3582\n",
      "Epoch 3770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1126 - val_loss: 5.7303\n",
      "Epoch 3771/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3882 - val_loss: 5.3644\n",
      "Epoch 3772/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0819 - val_loss: 5.6298\n",
      "Epoch 3773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2160 - val_loss: 5.4185\n",
      "Epoch 3774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1162 - val_loss: 5.4212\n",
      "Epoch 3775/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0440 - val_loss: 5.3912\n",
      "Epoch 3776/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3993 - val_loss: 5.3951\n",
      "Epoch 3777/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2341 - val_loss: 5.4917\n",
      "Epoch 3778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1002 - val_loss: 5.5601\n",
      "Epoch 3779/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1819 - val_loss: 5.3629\n",
      "Epoch 3780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1195 - val_loss: 5.8298\n",
      "Epoch 3781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3130 - val_loss: 5.3468\n",
      "Epoch 3782/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2027 - val_loss: 5.5949\n",
      "Epoch 3783/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1862 - val_loss: 5.4989\n",
      "Epoch 3784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2765 - val_loss: 5.3562\n",
      "Epoch 3785/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0647 - val_loss: 5.4081\n",
      "Epoch 3786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0763 - val_loss: 5.4217\n",
      "Epoch 3787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2033 - val_loss: 5.3734\n",
      "Epoch 3788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1860 - val_loss: 5.5095\n",
      "Epoch 3789/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2243 - val_loss: 5.3654\n",
      "Epoch 3790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3117 - val_loss: 5.4141\n",
      "Epoch 3791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2725 - val_loss: 5.5256\n",
      "Epoch 3792/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1273 - val_loss: 5.4456\n",
      "Epoch 3793/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1033 - val_loss: 5.4580\n",
      "Epoch 3794/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3263 - val_loss: 5.3569\n",
      "Epoch 3795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1959 - val_loss: 5.3830\n",
      "Epoch 3796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0893 - val_loss: 5.4737\n",
      "Epoch 3797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2229 - val_loss: 5.3567\n",
      "Epoch 3798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1538 - val_loss: 5.3887\n",
      "Epoch 3799/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1507 - val_loss: 5.3777\n",
      "Epoch 3800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0843 - val_loss: 5.4895\n",
      "Epoch 3801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1040 - val_loss: 5.4860\n",
      "Epoch 3802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1489 - val_loss: 5.8633\n",
      "Epoch 3803/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2247 - val_loss: 5.3934\n",
      "Epoch 3804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5513 - val_loss: 5.3864\n",
      "Epoch 3805/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5732 - val_loss: 5.3547\n",
      "Epoch 3806/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.1290 - val_loss: 5.4989\n",
      "Epoch 3807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1760 - val_loss: 5.3658\n",
      "Epoch 3808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1333 - val_loss: 5.3825\n",
      "Epoch 3809/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0475 - val_loss: 5.3799\n",
      "Epoch 3810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0464 - val_loss: 5.5354\n",
      "Epoch 3811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1405 - val_loss: 5.6699\n",
      "Epoch 3812/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3254 - val_loss: 5.8992\n",
      "Epoch 3813/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3983 - val_loss: 5.3513\n",
      "Epoch 3814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3073 - val_loss: 5.4072\n",
      "Epoch 3815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0691 - val_loss: 5.3663\n",
      "Epoch 3816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0423 - val_loss: 5.4399\n",
      "Epoch 3817/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1334 - val_loss: 5.4466\n",
      "Epoch 3818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0772 - val_loss: 5.3869\n",
      "Epoch 3819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0995 - val_loss: 5.4569\n",
      "Epoch 3820/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0718 - val_loss: 5.3984\n",
      "Epoch 3821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1371 - val_loss: 5.5425\n",
      "Epoch 3822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1335 - val_loss: 5.7090\n",
      "Epoch 3823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2012 - val_loss: 5.4019\n",
      "Epoch 3824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0399 - val_loss: 5.3652\n",
      "Epoch 3825/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0633 - val_loss: 5.4358\n",
      "Epoch 3826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1551 - val_loss: 5.4486\n",
      "Epoch 3827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1417 - val_loss: 5.4597\n",
      "Epoch 3828/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2767 - val_loss: 5.8288\n",
      "Epoch 3829/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3485 - val_loss: 5.8918\n",
      "Epoch 3830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3716 - val_loss: 5.6781\n",
      "Epoch 3831/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1004 - val_loss: 5.4065\n",
      "Epoch 3832/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1013 - val_loss: 5.6776\n",
      "Epoch 3833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1083 - val_loss: 5.3640\n",
      "Epoch 3834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0913 - val_loss: 5.5227\n",
      "Epoch 3835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0479 - val_loss: 5.5236\n",
      "Epoch 3836/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1751 - val_loss: 5.7234\n",
      "Epoch 3837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2147 - val_loss: 5.5097\n",
      "Epoch 3838/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1063 - val_loss: 5.4247\n",
      "Epoch 3839/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1193 - val_loss: 5.4877\n",
      "Epoch 3840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1552 - val_loss: 5.3461\n",
      "Epoch 3841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1199 - val_loss: 5.6960\n",
      "Epoch 3842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3247 - val_loss: 5.3863\n",
      "Epoch 3843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2420 - val_loss: 5.9458\n",
      "Epoch 3844/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3817 - val_loss: 5.3192\n",
      "Epoch 3845/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2280 - val_loss: 6.2983\n",
      "Epoch 3846/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2992 - val_loss: 5.3854\n",
      "Epoch 3847/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0995 - val_loss: 5.3912\n",
      "Epoch 3848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0325 - val_loss: 5.4163\n",
      "Epoch 3849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0886 - val_loss: 5.5360\n",
      "Epoch 3850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1530 - val_loss: 5.5972\n",
      "Epoch 3851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0838 - val_loss: 5.6548\n",
      "Epoch 3852/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2999 - val_loss: 5.3901\n",
      "Epoch 3853/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0818 - val_loss: 5.4100\n",
      "Epoch 3854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1930 - val_loss: 5.5781\n",
      "Epoch 3855/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 5.5092 - val_loss: 5.4665\n",
      "Epoch 3856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1095 - val_loss: 5.4173\n",
      "Epoch 3857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0229 - val_loss: 5.7788\n",
      "Epoch 3858/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1915 - val_loss: 5.3643\n",
      "Epoch 3859/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1273 - val_loss: 5.6545\n",
      "Epoch 3860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2374 - val_loss: 5.3421\n",
      "Epoch 3861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1857 - val_loss: 5.4002\n",
      "Epoch 3862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3143 - val_loss: 5.4370\n",
      "Epoch 3863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1959 - val_loss: 5.5807\n",
      "Epoch 3864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2422 - val_loss: 5.8327\n",
      "Epoch 3865/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1448 - val_loss: 5.7573\n",
      "Epoch 3866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2153 - val_loss: 5.4819\n",
      "Epoch 3867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0660 - val_loss: 5.3657\n",
      "Epoch 3868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1686 - val_loss: 5.5625\n",
      "Epoch 3869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2127 - val_loss: 5.3268\n",
      "Epoch 3870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2561 - val_loss: 5.8885\n",
      "Epoch 3871/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1494 - val_loss: 5.3278\n",
      "Epoch 3872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1470 - val_loss: 5.3707\n",
      "Epoch 3873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1556 - val_loss: 5.8127\n",
      "Epoch 3874/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1915 - val_loss: 5.7926\n",
      "Epoch 3875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4391 - val_loss: 5.9308\n",
      "Epoch 3876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2314 - val_loss: 6.0484\n",
      "Epoch 3877/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3294 - val_loss: 5.3539\n",
      "Epoch 3878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2325 - val_loss: 5.3507\n",
      "Epoch 3879/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1637 - val_loss: 5.4744\n",
      "Epoch 3880/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1626 - val_loss: 5.5044\n",
      "Epoch 3881/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1571 - val_loss: 5.7443\n",
      "Epoch 3882/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.3164 - val_loss: 5.3868\n",
      "Epoch 3883/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1466 - val_loss: 5.5711\n",
      "Epoch 3884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2042 - val_loss: 5.3579\n",
      "Epoch 3885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1079 - val_loss: 5.6756\n",
      "Epoch 3886/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0217 - val_loss: 5.4313\n",
      "Epoch 3887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0519 - val_loss: 5.4121\n",
      "Epoch 3888/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0893 - val_loss: 5.4276\n",
      "Epoch 3889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0942 - val_loss: 5.4301\n",
      "Epoch 3890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1699 - val_loss: 5.5283\n",
      "Epoch 3891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1385 - val_loss: 5.3318\n",
      "Epoch 3892/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0600 - val_loss: 5.3561\n",
      "Epoch 3893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1770 - val_loss: 5.6611\n",
      "Epoch 3894/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1202 - val_loss: 5.3892\n",
      "Epoch 3895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0575 - val_loss: 5.4109\n",
      "Epoch 3896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1683 - val_loss: 5.5569\n",
      "Epoch 3897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1464 - val_loss: 5.3437\n",
      "Epoch 3898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1165 - val_loss: 5.7242\n",
      "Epoch 3899/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1717 - val_loss: 5.4275\n",
      "Epoch 3900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0550 - val_loss: 5.6434\n",
      "Epoch 3901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3514 - val_loss: 5.3452\n",
      "Epoch 3902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0778 - val_loss: 5.5488\n",
      "Epoch 3903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0454 - val_loss: 5.3903\n",
      "Epoch 3904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1230 - val_loss: 5.3610\n",
      "Epoch 3905/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0534 - val_loss: 5.4585\n",
      "Epoch 3906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2531 - val_loss: 5.3511\n",
      "Epoch 3907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1805 - val_loss: 5.5936\n",
      "Epoch 3908/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1171 - val_loss: 5.5159\n",
      "Epoch 3909/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0764 - val_loss: 5.4470\n",
      "Epoch 3910/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3172 - val_loss: 5.3150\n",
      "Epoch 3911/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.494 - 0s 46us/step - loss: 5.1219 - val_loss: 5.3753\n",
      "Epoch 3912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0286 - val_loss: 5.3731\n",
      "Epoch 3913/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0613 - val_loss: 5.3244\n",
      "Epoch 3914/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0750 - val_loss: 5.3646\n",
      "Epoch 3915/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0504 - val_loss: 5.9923\n",
      "Epoch 3916/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2761 - val_loss: 5.4359\n",
      "Epoch 3917/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1643 - val_loss: 5.9150\n",
      "Epoch 3918/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.5390 - val_loss: 5.3617\n",
      "Epoch 3919/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2241 - val_loss: 5.5560\n",
      "Epoch 3920/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2404 - val_loss: 5.3823\n",
      "Epoch 3921/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1694 - val_loss: 5.4891\n",
      "Epoch 3922/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0726 - val_loss: 5.3290\n",
      "Epoch 3923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0333 - val_loss: 5.4144\n",
      "Epoch 3924/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1348 - val_loss: 5.5172\n",
      "Epoch 3925/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1294 - val_loss: 5.8553\n",
      "Epoch 3926/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0942 - val_loss: 5.3751\n",
      "Epoch 3927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0089 - val_loss: 5.3832\n",
      "Epoch 3928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9847 - val_loss: 5.5757\n",
      "Epoch 3929/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0884 - val_loss: 5.3751\n",
      "Epoch 3930/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1207 - val_loss: 5.4974\n",
      "Epoch 3931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1418 - val_loss: 5.3464\n",
      "Epoch 3932/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0921 - val_loss: 5.4523\n",
      "Epoch 3933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0461 - val_loss: 5.4894\n",
      "Epoch 3934/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0524 - val_loss: 5.3877\n",
      "Epoch 3935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0032 - val_loss: 5.3595\n",
      "Epoch 3936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1284 - val_loss: 5.5146\n",
      "Epoch 3937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1038 - val_loss: 5.4507\n",
      "Epoch 3938/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1909 - val_loss: 5.8054\n",
      "Epoch 3939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1937 - val_loss: 5.3313\n",
      "Epoch 3940/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1073 - val_loss: 5.5459\n",
      "Epoch 3941/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0384 - val_loss: 5.3594\n",
      "Epoch 3942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0899 - val_loss: 5.4135\n",
      "Epoch 3943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0317 - val_loss: 5.4363\n",
      "Epoch 3944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0710 - val_loss: 5.4895\n",
      "Epoch 3945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1281 - val_loss: 5.3384\n",
      "Epoch 3946/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2634 - val_loss: 5.5217\n",
      "Epoch 3947/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1687 - val_loss: 5.3034\n",
      "Epoch 3948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0125 - val_loss: 5.5913\n",
      "Epoch 3949/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1104 - val_loss: 5.5159\n",
      "Epoch 3950/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 5.1762 - val_loss: 5.4454\n",
      "Epoch 3951/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.0547 - val_loss: 5.3309\n",
      "Epoch 3952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0493 - val_loss: 5.3483\n",
      "Epoch 3953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0367 - val_loss: 5.3219\n",
      "Epoch 3954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1245 - val_loss: 5.9583\n",
      "Epoch 3955/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2037 - val_loss: 5.6781\n",
      "Epoch 3956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1651 - val_loss: 5.3901\n",
      "Epoch 3957/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0744 - val_loss: 5.8461\n",
      "Epoch 3958/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.1171 - val_loss: 5.5301\n",
      "Epoch 3959/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0363 - val_loss: 5.3439\n",
      "Epoch 3960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9782 - val_loss: 5.3285\n",
      "Epoch 3961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0670 - val_loss: 5.6349\n",
      "Epoch 3962/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1435 - val_loss: 5.3576\n",
      "Epoch 3963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2265 - val_loss: 5.4180\n",
      "Epoch 3964/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9985 - val_loss: 5.3505\n",
      "Epoch 3965/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0230 - val_loss: 5.3349\n",
      "Epoch 3966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9888 - val_loss: 5.3355\n",
      "Epoch 3967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0497 - val_loss: 5.4445\n",
      "Epoch 3968/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0852 - val_loss: 5.4712\n",
      "Epoch 3969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0325 - val_loss: 5.7060\n",
      "Epoch 3970/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1605 - val_loss: 5.3722\n",
      "Epoch 3971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2792 - val_loss: 5.4138\n",
      "Epoch 3972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0534 - val_loss: 5.3567\n",
      "Epoch 3973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0369 - val_loss: 5.5212\n",
      "Epoch 3974/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1210 - val_loss: 5.3800\n",
      "Epoch 3975/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0156 - val_loss: 5.3382\n",
      "Epoch 3976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2430 - val_loss: 5.5562\n",
      "Epoch 3977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2914 - val_loss: 5.7566\n",
      "Epoch 3978/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2123 - val_loss: 5.4039\n",
      "Epoch 3979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0345 - val_loss: 5.6356\n",
      "Epoch 3980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2529 - val_loss: 5.3780\n",
      "Epoch 3981/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0335 - val_loss: 5.4837\n",
      "Epoch 3982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0429 - val_loss: 5.3280\n",
      "Epoch 3983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0428 - val_loss: 5.6821\n",
      "Epoch 3984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2445 - val_loss: 5.3388\n",
      "Epoch 3985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1810 - val_loss: 6.2487\n",
      "Epoch 3986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5340 - val_loss: 5.4267\n",
      "Epoch 3987/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0158 - val_loss: 5.3228\n",
      "Epoch 3988/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0394 - val_loss: 5.4109\n",
      "Epoch 3989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1395 - val_loss: 5.3145\n",
      "Epoch 3990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0331 - val_loss: 5.3150\n",
      "Epoch 3991/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0131 - val_loss: 5.4496\n",
      "Epoch 3992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1216 - val_loss: 5.3791\n",
      "Epoch 3993/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0668 - val_loss: 5.4757\n",
      "Epoch 3994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1351 - val_loss: 5.3212\n",
      "Epoch 3995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1203 - val_loss: 5.3319\n",
      "Epoch 3996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2861 - val_loss: 5.4809\n",
      "Epoch 3997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3119 - val_loss: 5.5476\n",
      "Epoch 3998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0508 - val_loss: 5.3326\n",
      "Epoch 3999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1513 - val_loss: 5.4704\n",
      "Epoch 4000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1411 - val_loss: 5.5703\n",
      "Epoch 4001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1368 - val_loss: 5.3663\n",
      "Epoch 4002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0879 - val_loss: 5.4680\n",
      "Epoch 4003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2129 - val_loss: 5.4535\n",
      "Epoch 4004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1009 - val_loss: 5.3163\n",
      "Epoch 4005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0399 - val_loss: 5.3563\n",
      "Epoch 4006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9999 - val_loss: 5.3350\n",
      "Epoch 4007/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0299 - val_loss: 5.4604\n",
      "Epoch 4008/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0470 - val_loss: 5.5176\n",
      "Epoch 4009/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0212 - val_loss: 5.3458\n",
      "Epoch 4010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0078 - val_loss: 5.3609\n",
      "Epoch 4011/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0669 - val_loss: 5.5791\n",
      "Epoch 4012/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3558 - val_loss: 5.6414\n",
      "Epoch 4013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2135 - val_loss: 5.3807\n",
      "Epoch 4014/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0423 - val_loss: 5.8577\n",
      "Epoch 4015/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1830 - val_loss: 5.3627\n",
      "Epoch 4016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1591 - val_loss: 5.4209\n",
      "Epoch 4017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0381 - val_loss: 5.4269\n",
      "Epoch 4018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0783 - val_loss: 5.3283\n",
      "Epoch 4019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0165 - val_loss: 5.3189\n",
      "Epoch 4020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0567 - val_loss: 5.6383\n",
      "Epoch 4021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0693 - val_loss: 5.6206\n",
      "Epoch 4022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1925 - val_loss: 5.3870\n",
      "Epoch 4023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2532 - val_loss: 5.3223\n",
      "Epoch 4024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1943 - val_loss: 5.3862\n",
      "Epoch 4025/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1342 - val_loss: 5.4332\n",
      "Epoch 4026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1103 - val_loss: 5.3119\n",
      "Epoch 4027/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0741 - val_loss: 5.5931\n",
      "Epoch 4028/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0413 - val_loss: 5.3509\n",
      "Epoch 4029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0758 - val_loss: 5.3199\n",
      "Epoch 4030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2957 - val_loss: 5.4938\n",
      "Epoch 4031/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.0647 - val_loss: 5.3208\n",
      "Epoch 4032/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0490 - val_loss: 6.3837\n",
      "Epoch 4033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2709 - val_loss: 5.9104\n",
      "Epoch 4034/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.2099 - val_loss: 5.3090\n",
      "Epoch 4035/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2901 - val_loss: 5.9555\n",
      "Epoch 4036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2413 - val_loss: 5.3149\n",
      "Epoch 4037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1055 - val_loss: 5.3683\n",
      "Epoch 4038/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9832 - val_loss: 5.3338\n",
      "Epoch 4039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0983 - val_loss: 5.5240\n",
      "Epoch 4040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0619 - val_loss: 5.3490\n",
      "Epoch 4041/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1982 - val_loss: 5.3095\n",
      "Epoch 4042/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0504 - val_loss: 5.5043\n",
      "Epoch 4043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1695 - val_loss: 5.3229\n",
      "Epoch 4044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2340 - val_loss: 5.5780\n",
      "Epoch 4045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1234 - val_loss: 5.4035\n",
      "Epoch 4046/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0232 - val_loss: 5.3785\n",
      "Epoch 4047/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0273 - val_loss: 5.5478\n",
      "Epoch 4048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9799 - val_loss: 5.3148\n",
      "Epoch 4049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1515 - val_loss: 5.3567\n",
      "Epoch 4050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0323 - val_loss: 5.4200\n",
      "Epoch 4051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1409 - val_loss: 5.4325\n",
      "Epoch 4052/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0672 - val_loss: 5.4496\n",
      "Epoch 4053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0882 - val_loss: 5.3033\n",
      "Epoch 4054/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0224 - val_loss: 5.3110\n",
      "Epoch 4055/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9946 - val_loss: 5.3657\n",
      "Epoch 4056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1185 - val_loss: 5.3216\n",
      "Epoch 4057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0581 - val_loss: 5.6871\n",
      "Epoch 4058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1868 - val_loss: 5.5610\n",
      "Epoch 4059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1897 - val_loss: 5.3860\n",
      "Epoch 4060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0543 - val_loss: 5.3293\n",
      "Epoch 4061/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1199 - val_loss: 5.3123\n",
      "Epoch 4062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0661 - val_loss: 5.4991\n",
      "Epoch 4063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3652 - val_loss: 5.4661\n",
      "Epoch 4064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1310 - val_loss: 5.5929\n",
      "Epoch 4065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2232 - val_loss: 5.3506\n",
      "Epoch 4066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0409 - val_loss: 5.6004\n",
      "Epoch 4067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0403 - val_loss: 5.3339\n",
      "Epoch 4068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0355 - val_loss: 5.3088\n",
      "Epoch 4069/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0822 - val_loss: 5.3286\n",
      "Epoch 4070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0525 - val_loss: 5.3076\n",
      "Epoch 4071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0943 - val_loss: 5.7833\n",
      "Epoch 4072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4289 - val_loss: 5.3308\n",
      "Epoch 4073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9756 - val_loss: 5.6600\n",
      "Epoch 4074/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2372 - val_loss: 5.3032\n",
      "Epoch 4075/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0773 - val_loss: 5.4106\n",
      "Epoch 4076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0326 - val_loss: 5.3156\n",
      "Epoch 4077/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0159 - val_loss: 5.3671\n",
      "Epoch 4078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1337 - val_loss: 5.5706\n",
      "Epoch 4079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0177 - val_loss: 5.2840\n",
      "Epoch 4080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1407 - val_loss: 5.5566\n",
      "Epoch 4081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1318 - val_loss: 5.7524\n",
      "Epoch 4082/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2071 - val_loss: 5.3047\n",
      "Epoch 4083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0053 - val_loss: 5.3132\n",
      "Epoch 4084/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1412 - val_loss: 5.4477\n",
      "Epoch 4085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0412 - val_loss: 5.4183\n",
      "Epoch 4086/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1290 - val_loss: 5.3003\n",
      "Epoch 4087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2050 - val_loss: 5.3303\n",
      "Epoch 4088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5343 - val_loss: 5.3955\n",
      "Epoch 4089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1412 - val_loss: 5.3342\n",
      "Epoch 4090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0849 - val_loss: 5.3118\n",
      "Epoch 4091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0749 - val_loss: 5.5168\n",
      "Epoch 4092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0742 - val_loss: 5.9706\n",
      "Epoch 4093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1241 - val_loss: 5.3232\n",
      "Epoch 4094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2185 - val_loss: 5.6742\n",
      "Epoch 4095/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2286 - val_loss: 5.2874\n",
      "Epoch 4096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1134 - val_loss: 5.6504\n",
      "Epoch 4097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0410 - val_loss: 5.3365\n",
      "Epoch 4098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9922 - val_loss: 5.3116\n",
      "Epoch 4099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9737 - val_loss: 5.2929\n",
      "Epoch 4100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0174 - val_loss: 5.7265\n",
      "Epoch 4101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1152 - val_loss: 5.6562\n",
      "Epoch 4102/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2290 - val_loss: 5.6544\n",
      "Epoch 4103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1703 - val_loss: 5.3082\n",
      "Epoch 4104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0270 - val_loss: 5.4546\n",
      "Epoch 4105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0847 - val_loss: 5.4922\n",
      "Epoch 4106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0887 - val_loss: 5.3429\n",
      "Epoch 4107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1409 - val_loss: 5.3717\n",
      "Epoch 4108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9783 - val_loss: 5.3209\n",
      "Epoch 4109/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0628 - val_loss: 5.4132\n",
      "Epoch 4110/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.0089 - val_loss: 5.2980\n",
      "Epoch 4111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0442 - val_loss: 5.3076\n",
      "Epoch 4112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0710 - val_loss: 5.5423\n",
      "Epoch 4113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0908 - val_loss: 5.3031\n",
      "Epoch 4114/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0632 - val_loss: 5.2861\n",
      "Epoch 4115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9910 - val_loss: 5.7281\n",
      "Epoch 4116/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2344 - val_loss: 5.4927\n",
      "Epoch 4117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3674 - val_loss: 5.6708\n",
      "Epoch 4118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1611 - val_loss: 5.2694\n",
      "Epoch 4119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1027 - val_loss: 5.3647\n",
      "Epoch 4120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4437 - val_loss: 5.5228\n",
      "Epoch 4121/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0356 - val_loss: 5.3781\n",
      "Epoch 4122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9679 - val_loss: 5.5408\n",
      "Epoch 4123/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1573 - val_loss: 6.0133\n",
      "Epoch 4124/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1025 - val_loss: 5.3172\n",
      "Epoch 4125/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1580 - val_loss: 5.2779\n",
      "Epoch 4126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9949 - val_loss: 5.3154\n",
      "Epoch 4127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9796 - val_loss: 5.3307\n",
      "Epoch 4128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0064 - val_loss: 5.3542\n",
      "Epoch 4129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0200 - val_loss: 5.3915\n",
      "Epoch 4130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9604 - val_loss: 5.3158\n",
      "Epoch 4131/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9926 - val_loss: 5.3029\n",
      "Epoch 4132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0572 - val_loss: 5.3061\n",
      "Epoch 4133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3831 - val_loss: 5.3527\n",
      "Epoch 4134/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0071 - val_loss: 5.2814\n",
      "Epoch 4135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0154 - val_loss: 5.4398\n",
      "Epoch 4136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0313 - val_loss: 5.3127\n",
      "Epoch 4137/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0838 - val_loss: 5.5479\n",
      "Epoch 4138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9738 - val_loss: 5.2974\n",
      "Epoch 4139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0342 - val_loss: 5.5527\n",
      "Epoch 4140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0226 - val_loss: 5.3160\n",
      "Epoch 4141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9925 - val_loss: 5.4529\n",
      "Epoch 4142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0737 - val_loss: 5.3242\n",
      "Epoch 4143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0685 - val_loss: 5.2932\n",
      "Epoch 4144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0112 - val_loss: 5.4925\n",
      "Epoch 4145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9909 - val_loss: 5.5865\n",
      "Epoch 4146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0854 - val_loss: 5.4600\n",
      "Epoch 4147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0583 - val_loss: 5.3573\n",
      "Epoch 4148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9943 - val_loss: 5.3086\n",
      "Epoch 4149/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0574 - val_loss: 5.4871\n",
      "Epoch 4150/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9981 - val_loss: 5.3407\n",
      "Epoch 4151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9494 - val_loss: 5.3187\n",
      "Epoch 4152/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9732 - val_loss: 5.7955\n",
      "Epoch 4153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2297 - val_loss: 5.2826\n",
      "Epoch 4154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9700 - val_loss: 5.7827\n",
      "Epoch 4155/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2539 - val_loss: 5.2845\n",
      "Epoch 4156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5250 - val_loss: 6.6495\n",
      "Epoch 4157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2923 - val_loss: 5.3011\n",
      "Epoch 4158/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0911 - val_loss: 5.3017\n",
      "Epoch 4159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1159 - val_loss: 5.4269\n",
      "Epoch 4160/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1038 - val_loss: 5.3007\n",
      "Epoch 4161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0130 - val_loss: 5.3181\n",
      "Epoch 4162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3853 - val_loss: 5.7638\n",
      "Epoch 4163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3493 - val_loss: 5.2814\n",
      "Epoch 4164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5106 - val_loss: 5.4194\n",
      "Epoch 4165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0308 - val_loss: 5.4649\n",
      "Epoch 4166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1197 - val_loss: 5.4127\n",
      "Epoch 4167/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9692 - val_loss: 5.3449\n",
      "Epoch 4168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0933 - val_loss: 5.4013\n",
      "Epoch 4169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2871 - val_loss: 5.3844\n",
      "Epoch 4170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0346 - val_loss: 5.3052\n",
      "Epoch 4171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1292 - val_loss: 5.6666\n",
      "Epoch 4172/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1857 - val_loss: 5.4543\n",
      "Epoch 4173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9986 - val_loss: 5.5788\n",
      "Epoch 4174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0006 - val_loss: 5.2892\n",
      "Epoch 4175/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0046 - val_loss: 5.4301\n",
      "Epoch 4176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0827 - val_loss: 5.3374\n",
      "Epoch 4177/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0781 - val_loss: 5.2944\n",
      "Epoch 4178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1295 - val_loss: 5.7938\n",
      "Epoch 4179/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1299 - val_loss: 5.3498\n",
      "Epoch 4180/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0062 - val_loss: 5.3578\n",
      "Epoch 4181/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0996 - val_loss: 5.3075\n",
      "Epoch 4182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0828 - val_loss: 5.3273\n",
      "Epoch 4183/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9980 - val_loss: 5.3199\n",
      "Epoch 4184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1076 - val_loss: 5.4976\n",
      "Epoch 4185/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2491 - val_loss: 5.4031\n",
      "Epoch 4186/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.0293 - val_loss: 5.3632\n",
      "Epoch 4187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0957 - val_loss: 5.2973\n",
      "Epoch 4188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9761 - val_loss: 5.2759\n",
      "Epoch 4189/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0553 - val_loss: 5.3486\n",
      "Epoch 4190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0035 - val_loss: 5.3460\n",
      "Epoch 4191/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1101 - val_loss: 5.3312\n",
      "Epoch 4192/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0863 - val_loss: 5.3376\n",
      "Epoch 4193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0800 - val_loss: 5.2824\n",
      "Epoch 4194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9898 - val_loss: 5.3316\n",
      "Epoch 4195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1137 - val_loss: 5.3129\n",
      "Epoch 4196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0808 - val_loss: 5.3021\n",
      "Epoch 4197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0954 - val_loss: 5.2786\n",
      "Epoch 4198/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9932 - val_loss: 5.5663\n",
      "Epoch 4199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0489 - val_loss: 6.1003\n",
      "Epoch 4200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1931 - val_loss: 5.6527\n",
      "Epoch 4201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1455 - val_loss: 5.3101\n",
      "Epoch 4202/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9853 - val_loss: 5.2811\n",
      "Epoch 4203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0042 - val_loss: 5.2794\n",
      "Epoch 4204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0599 - val_loss: 5.5752\n",
      "Epoch 4205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1080 - val_loss: 5.3739\n",
      "Epoch 4206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0070 - val_loss: 5.2875\n",
      "Epoch 4207/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0233 - val_loss: 5.2941\n",
      "Epoch 4208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0552 - val_loss: 5.3754\n",
      "Epoch 4209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2702 - val_loss: 5.4068\n",
      "Epoch 4210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1158 - val_loss: 5.7266\n",
      "Epoch 4211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1950 - val_loss: 5.3918\n",
      "Epoch 4212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2256 - val_loss: 5.3171\n",
      "Epoch 4213/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1768 - val_loss: 5.2620\n",
      "Epoch 4214/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0867 - val_loss: 5.3158\n",
      "Epoch 4215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0654 - val_loss: 6.0203\n",
      "Epoch 4216/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2162 - val_loss: 5.2682\n",
      "Epoch 4217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0603 - val_loss: 5.4869\n",
      "Epoch 4218/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0059 - val_loss: 5.2831\n",
      "Epoch 4219/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0389 - val_loss: 5.4131\n",
      "Epoch 4220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1455 - val_loss: 5.7637\n",
      "Epoch 4221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5955 - val_loss: 5.7087\n",
      "Epoch 4222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2848 - val_loss: 5.4131\n",
      "Epoch 4223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0475 - val_loss: 5.3562\n",
      "Epoch 4224/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 5.0394 - val_loss: 5.5586\n",
      "Epoch 4225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0776 - val_loss: 5.3103\n",
      "Epoch 4226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0074 - val_loss: 5.2947\n",
      "Epoch 4227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0456 - val_loss: 5.4830\n",
      "Epoch 4228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0600 - val_loss: 5.3719\n",
      "Epoch 4229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0013 - val_loss: 5.3994\n",
      "Epoch 4230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0286 - val_loss: 5.2839\n",
      "Epoch 4231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9974 - val_loss: 5.3210\n",
      "Epoch 4232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0458 - val_loss: 5.5357\n",
      "Epoch 4233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0546 - val_loss: 5.4885\n",
      "Epoch 4234/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0199 - val_loss: 5.5018\n",
      "Epoch 4235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0523 - val_loss: 6.0297\n",
      "Epoch 4236/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.4939 - val_loss: 5.3689\n",
      "Epoch 4237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0911 - val_loss: 5.3990\n",
      "Epoch 4238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0752 - val_loss: 5.3062\n",
      "Epoch 4239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0173 - val_loss: 5.5686\n",
      "Epoch 4240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0750 - val_loss: 5.3014\n",
      "Epoch 4241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0575 - val_loss: 5.6024\n",
      "Epoch 4242/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.6423 - val_loss: 5.3660\n",
      "Epoch 4243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9754 - val_loss: 5.2733\n",
      "Epoch 4244/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9890 - val_loss: 5.2787\n",
      "Epoch 4245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9750 - val_loss: 5.2749\n",
      "Epoch 4246/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9808 - val_loss: 5.2981\n",
      "Epoch 4247/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0035 - val_loss: 5.2869\n",
      "Epoch 4248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9553 - val_loss: 5.3332\n",
      "Epoch 4249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9960 - val_loss: 5.3913\n",
      "Epoch 4250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0266 - val_loss: 5.3644\n",
      "Epoch 4251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0596 - val_loss: 5.3436\n",
      "Epoch 4252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9737 - val_loss: 5.2851\n",
      "Epoch 4253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9545 - val_loss: 5.3030\n",
      "Epoch 4254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0504 - val_loss: 5.2782\n",
      "Epoch 4255/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9537 - val_loss: 5.4397\n",
      "Epoch 4256/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0107 - val_loss: 5.6432\n",
      "Epoch 4257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1552 - val_loss: 5.3226\n",
      "Epoch 4258/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0491 - val_loss: 5.6004\n",
      "Epoch 4259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2334 - val_loss: 5.2849\n",
      "Epoch 4260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9686 - val_loss: 5.2706\n",
      "Epoch 4261/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0337 - val_loss: 5.3118\n",
      "Epoch 4262/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.0589 - val_loss: 5.2588\n",
      "Epoch 4263/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0011 - val_loss: 5.4585\n",
      "Epoch 4264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1382 - val_loss: 5.2620\n",
      "Epoch 4265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1805 - val_loss: 5.3703\n",
      "Epoch 4266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9831 - val_loss: 5.5152\n",
      "Epoch 4267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2223 - val_loss: 5.4946\n",
      "Epoch 4268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1036 - val_loss: 5.2712\n",
      "Epoch 4269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0359 - val_loss: 5.3122\n",
      "Epoch 4270/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9888 - val_loss: 5.2754\n",
      "Epoch 4271/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9452 - val_loss: 5.3096\n",
      "Epoch 4272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0527 - val_loss: 5.2945\n",
      "Epoch 4273/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1308 - val_loss: 5.3465\n",
      "Epoch 4274/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9406 - val_loss: 5.3032\n",
      "Epoch 4275/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9704 - val_loss: 5.2958\n",
      "Epoch 4276/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0243 - val_loss: 5.2916\n",
      "Epoch 4277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0926 - val_loss: 5.3416\n",
      "Epoch 4278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0063 - val_loss: 5.3595\n",
      "Epoch 4279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9788 - val_loss: 5.2942\n",
      "Epoch 4280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9909 - val_loss: 5.3042\n",
      "Epoch 4281/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0262 - val_loss: 5.5686\n",
      "Epoch 4282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0447 - val_loss: 5.4690\n",
      "Epoch 4283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0105 - val_loss: 5.7718\n",
      "Epoch 4284/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0891 - val_loss: 5.3648\n",
      "Epoch 4285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2567 - val_loss: 5.7296\n",
      "Epoch 4286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2134 - val_loss: 5.4114\n",
      "Epoch 4287/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.061 - 0s 46us/step - loss: 5.2338 - val_loss: 5.5675\n",
      "Epoch 4288/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2295 - val_loss: 5.3920\n",
      "Epoch 4289/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1055 - val_loss: 5.3321\n",
      "Epoch 4290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2270 - val_loss: 5.3885\n",
      "Epoch 4291/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9684 - val_loss: 5.3186\n",
      "Epoch 4292/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9689 - val_loss: 5.2670\n",
      "Epoch 4293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0756 - val_loss: 5.4086\n",
      "Epoch 4294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9703 - val_loss: 5.4849\n",
      "Epoch 4295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9845 - val_loss: 5.2899\n",
      "Epoch 4296/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9801 - val_loss: 5.3120\n",
      "Epoch 4297/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9627 - val_loss: 5.3178\n",
      "Epoch 4298/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0285 - val_loss: 5.4085\n",
      "Epoch 4299/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0861 - val_loss: 5.4488\n",
      "Epoch 4300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9785 - val_loss: 5.2972\n",
      "Epoch 4301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0771 - val_loss: 5.4761\n",
      "Epoch 4302/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0742 - val_loss: 5.2490\n",
      "Epoch 4303/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2320 - val_loss: 5.5799\n",
      "Epoch 4304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0656 - val_loss: 5.3156\n",
      "Epoch 4305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9703 - val_loss: 5.2940\n",
      "Epoch 4306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0168 - val_loss: 5.2940\n",
      "Epoch 4307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9477 - val_loss: 5.4635\n",
      "Epoch 4308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0075 - val_loss: 5.2676\n",
      "Epoch 4309/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9578 - val_loss: 5.3267\n",
      "Epoch 4310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0512 - val_loss: 5.4366\n",
      "Epoch 4311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0043 - val_loss: 5.2907\n",
      "Epoch 4312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9843 - val_loss: 5.7240\n",
      "Epoch 4313/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0957 - val_loss: 5.2553\n",
      "Epoch 4314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9579 - val_loss: 5.2590\n",
      "Epoch 4315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0186 - val_loss: 5.3756\n",
      "Epoch 4316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0765 - val_loss: 5.5411\n",
      "Epoch 4317/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2353 - val_loss: 5.6143\n",
      "Epoch 4318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0007 - val_loss: 5.3473\n",
      "Epoch 4319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9599 - val_loss: 5.2616\n",
      "Epoch 4320/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0106 - val_loss: 5.2526\n",
      "Epoch 4321/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0457 - val_loss: 5.2856\n",
      "Epoch 4322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9565 - val_loss: 5.3139\n",
      "Epoch 4323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0242 - val_loss: 5.2866\n",
      "Epoch 4324/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0252 - val_loss: 5.6314\n",
      "Epoch 4325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1462 - val_loss: 5.2397\n",
      "Epoch 4326/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1711 - val_loss: 5.5719\n",
      "Epoch 4327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0265 - val_loss: 5.2843\n",
      "Epoch 4328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9593 - val_loss: 5.7441\n",
      "Epoch 4329/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0296 - val_loss: 5.3150\n",
      "Epoch 4330/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9980 - val_loss: 5.5577\n",
      "Epoch 4331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1045 - val_loss: 5.3248\n",
      "Epoch 4332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2080 - val_loss: 5.2352\n",
      "Epoch 4333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9924 - val_loss: 5.5759\n",
      "Epoch 4334/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0960 - val_loss: 5.2865\n",
      "Epoch 4335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1699 - val_loss: 5.7580\n",
      "Epoch 4336/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1655 - val_loss: 5.4442\n",
      "Epoch 4337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0193 - val_loss: 5.3103\n",
      "Epoch 4338/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.0876 - val_loss: 5.6443\n",
      "Epoch 4339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1041 - val_loss: 5.2839\n",
      "Epoch 4340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9810 - val_loss: 5.2794\n",
      "Epoch 4341/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9883 - val_loss: 5.2617\n",
      "Epoch 4342/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9433 - val_loss: 5.3757\n",
      "Epoch 4343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9470 - val_loss: 5.2704\n",
      "Epoch 4344/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0110 - val_loss: 5.5213\n",
      "Epoch 4345/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9946 - val_loss: 5.4495\n",
      "Epoch 4346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0036 - val_loss: 5.3082\n",
      "Epoch 4347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9971 - val_loss: 5.2742\n",
      "Epoch 4348/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1141 - val_loss: 5.8577\n",
      "Epoch 4349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2787 - val_loss: 5.5540\n",
      "Epoch 4350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1810 - val_loss: 5.5222\n",
      "Epoch 4351/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1391 - val_loss: 5.2529\n",
      "Epoch 4352/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.9940 - val_loss: 5.3141\n",
      "Epoch 4353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0084 - val_loss: 5.4412\n",
      "Epoch 4354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0526 - val_loss: 5.2740\n",
      "Epoch 4355/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9297 - val_loss: 5.2690\n",
      "Epoch 4356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9557 - val_loss: 5.3411\n",
      "Epoch 4357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1537 - val_loss: 5.2726\n",
      "Epoch 4358/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2017 - val_loss: 5.2590\n",
      "Epoch 4359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9348 - val_loss: 5.3577\n",
      "Epoch 4360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9939 - val_loss: 5.2510\n",
      "Epoch 4361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9533 - val_loss: 5.2830\n",
      "Epoch 4362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9544 - val_loss: 5.3209\n",
      "Epoch 4363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9502 - val_loss: 5.4782\n",
      "Epoch 4364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0543 - val_loss: 5.2456\n",
      "Epoch 4365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9454 - val_loss: 5.4561\n",
      "Epoch 4366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9728 - val_loss: 5.4265\n",
      "Epoch 4367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1139 - val_loss: 5.3394\n",
      "Epoch 4368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9241 - val_loss: 5.3037\n",
      "Epoch 4369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0716 - val_loss: 5.3820\n",
      "Epoch 4370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0155 - val_loss: 5.2617\n",
      "Epoch 4371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9506 - val_loss: 5.2653\n",
      "Epoch 4372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9477 - val_loss: 5.3729\n",
      "Epoch 4373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9500 - val_loss: 5.3152\n",
      "Epoch 4374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0988 - val_loss: 5.2464\n",
      "Epoch 4375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1676 - val_loss: 5.3076\n",
      "Epoch 4376/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9754 - val_loss: 5.3496\n",
      "Epoch 4377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0120 - val_loss: 5.3068\n",
      "Epoch 4378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1064 - val_loss: 5.3473\n",
      "Epoch 4379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9627 - val_loss: 5.4174\n",
      "Epoch 4380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0387 - val_loss: 5.2775\n",
      "Epoch 4381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0134 - val_loss: 5.2757\n",
      "Epoch 4382/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1105 - val_loss: 5.4730\n",
      "Epoch 4383/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1939 - val_loss: 5.3875\n",
      "Epoch 4384/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1472 - val_loss: 5.4615\n",
      "Epoch 4385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1152 - val_loss: 5.3776\n",
      "Epoch 4386/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9483 - val_loss: 5.3490\n",
      "Epoch 4387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9338 - val_loss: 5.2631\n",
      "Epoch 4388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0120 - val_loss: 5.4259\n",
      "Epoch 4389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9587 - val_loss: 5.2440\n",
      "Epoch 4390/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9330 - val_loss: 5.2688\n",
      "Epoch 4391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0480 - val_loss: 5.3614\n",
      "Epoch 4392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0106 - val_loss: 5.2673\n",
      "Epoch 4393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9376 - val_loss: 5.2762\n",
      "Epoch 4394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0089 - val_loss: 5.5048\n",
      "Epoch 4395/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0737 - val_loss: 5.4442\n",
      "Epoch 4396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0447 - val_loss: 5.2584\n",
      "Epoch 4397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9657 - val_loss: 5.2576\n",
      "Epoch 4398/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9885 - val_loss: 5.6269\n",
      "Epoch 4399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1521 - val_loss: 5.2842\n",
      "Epoch 4400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0374 - val_loss: 5.2713\n",
      "Epoch 4401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9886 - val_loss: 5.3063\n",
      "Epoch 4402/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1175 - val_loss: 5.3491\n",
      "Epoch 4403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1964 - val_loss: 6.2951\n",
      "Epoch 4404/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3016 - val_loss: 5.3779\n",
      "Epoch 4405/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9896 - val_loss: 5.4138\n",
      "Epoch 4406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0421 - val_loss: 5.2397\n",
      "Epoch 4407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9824 - val_loss: 5.3858\n",
      "Epoch 4408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1228 - val_loss: 5.2978\n",
      "Epoch 4409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0096 - val_loss: 5.2386\n",
      "Epoch 4410/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0314 - val_loss: 5.2791\n",
      "Epoch 4411/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.087 - 0s 46us/step - loss: 5.0476 - val_loss: 5.3246\n",
      "Epoch 4412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9495 - val_loss: 5.3362\n",
      "Epoch 4413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9418 - val_loss: 5.2420\n",
      "Epoch 4414/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.0329 - val_loss: 5.3265\n",
      "Epoch 4415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9603 - val_loss: 5.2492\n",
      "Epoch 4416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9881 - val_loss: 5.3070\n",
      "Epoch 4417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9663 - val_loss: 5.2447\n",
      "Epoch 4418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0383 - val_loss: 5.7194\n",
      "Epoch 4419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0389 - val_loss: 5.3083\n",
      "Epoch 4420/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9470 - val_loss: 5.2599\n",
      "Epoch 4421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9505 - val_loss: 5.4232\n",
      "Epoch 4422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9438 - val_loss: 5.2392\n",
      "Epoch 4423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0158 - val_loss: 5.2580\n",
      "Epoch 4424/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0235 - val_loss: 5.5317\n",
      "Epoch 4425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9619 - val_loss: 5.2626\n",
      "Epoch 4426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0225 - val_loss: 5.4216\n",
      "Epoch 4427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9979 - val_loss: 5.7523\n",
      "Epoch 4428/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0610 - val_loss: 5.2557\n",
      "Epoch 4429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1586 - val_loss: 5.3056\n",
      "Epoch 4430/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0394 - val_loss: 5.3047\n",
      "Epoch 4431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0619 - val_loss: 5.3098\n",
      "Epoch 4432/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9315 - val_loss: 5.5382\n",
      "Epoch 4433/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1045 - val_loss: 5.6900\n",
      "Epoch 4434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1996 - val_loss: 5.2884\n",
      "Epoch 4435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1141 - val_loss: 5.2250\n",
      "Epoch 4436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9461 - val_loss: 5.2908\n",
      "Epoch 4437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9709 - val_loss: 5.3003\n",
      "Epoch 4438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9721 - val_loss: 5.2658\n",
      "Epoch 4439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9616 - val_loss: 5.2474\n",
      "Epoch 4440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0332 - val_loss: 5.3717\n",
      "Epoch 4441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9777 - val_loss: 5.2458\n",
      "Epoch 4442/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9592 - val_loss: 5.3172\n",
      "Epoch 4443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9834 - val_loss: 5.3173\n",
      "Epoch 4444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0588 - val_loss: 5.3662\n",
      "Epoch 4445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9607 - val_loss: 5.2431\n",
      "Epoch 4446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9644 - val_loss: 5.2514\n",
      "Epoch 4447/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9630 - val_loss: 5.2485\n",
      "Epoch 4448/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9964 - val_loss: 5.3237\n",
      "Epoch 4449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9594 - val_loss: 5.2464\n",
      "Epoch 4450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9372 - val_loss: 5.2305\n",
      "Epoch 4451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1343 - val_loss: 5.3299\n",
      "Epoch 4452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0684 - val_loss: 5.2641\n",
      "Epoch 4453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0009 - val_loss: 5.3450\n",
      "Epoch 4454/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0937 - val_loss: 5.3737\n",
      "Epoch 4455/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0838 - val_loss: 5.2092\n",
      "Epoch 4456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9539 - val_loss: 5.2497\n",
      "Epoch 4457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9326 - val_loss: 5.3002\n",
      "Epoch 4458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9522 - val_loss: 5.6032\n",
      "Epoch 4459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1941 - val_loss: 5.2529\n",
      "Epoch 4460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9930 - val_loss: 5.2550\n",
      "Epoch 4461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9813 - val_loss: 5.4363\n",
      "Epoch 4462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9563 - val_loss: 5.3092\n",
      "Epoch 4463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0693 - val_loss: 5.2875\n",
      "Epoch 4464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9578 - val_loss: 5.5024\n",
      "Epoch 4465/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0891 - val_loss: 5.2332\n",
      "Epoch 4466/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.1207 - val_loss: 5.3561\n",
      "Epoch 4467/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9376 - val_loss: 5.3650\n",
      "Epoch 4468/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.1836 - val_loss: 5.2724\n",
      "Epoch 4469/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9981 - val_loss: 5.3682\n",
      "Epoch 4470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9559 - val_loss: 5.4276\n",
      "Epoch 4471/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1640 - val_loss: 5.2452\n",
      "Epoch 4472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1550 - val_loss: 5.3478\n",
      "Epoch 4473/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1248 - val_loss: 5.2618\n",
      "Epoch 4474/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2116 - val_loss: 5.2573\n",
      "Epoch 4475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0101 - val_loss: 5.2694\n",
      "Epoch 4476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1338 - val_loss: 5.3449\n",
      "Epoch 4477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0332 - val_loss: 5.3668\n",
      "Epoch 4478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9648 - val_loss: 5.4050\n",
      "Epoch 4479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0641 - val_loss: 5.4940\n",
      "Epoch 4480/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0040 - val_loss: 5.2977\n",
      "Epoch 4481/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0769 - val_loss: 5.2713\n",
      "Epoch 4482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5394 - val_loss: 5.5814\n",
      "Epoch 4483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1317 - val_loss: 5.4131\n",
      "Epoch 4484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0780 - val_loss: 5.2579\n",
      "Epoch 4485/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9406 - val_loss: 5.4723\n",
      "Epoch 4486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0875 - val_loss: 6.0736\n",
      "Epoch 4487/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1777 - val_loss: 5.4998\n",
      "Epoch 4488/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2023 - val_loss: 5.4947\n",
      "Epoch 4489/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0419 - val_loss: 5.3523\n",
      "Epoch 4490/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.0132 - val_loss: 5.2702\n",
      "Epoch 4491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9797 - val_loss: 5.2600\n",
      "Epoch 4492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9579 - val_loss: 5.2606\n",
      "Epoch 4493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9578 - val_loss: 5.2414\n",
      "Epoch 4494/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9664 - val_loss: 5.2500\n",
      "Epoch 4495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9684 - val_loss: 5.2498\n",
      "Epoch 4496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9733 - val_loss: 5.3002\n",
      "Epoch 4497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1810 - val_loss: 5.5524\n",
      "Epoch 4498/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0220 - val_loss: 5.2645\n",
      "Epoch 4499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9530 - val_loss: 5.4333\n",
      "Epoch 4500/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0503 - val_loss: 5.2700\n",
      "Epoch 4501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0315 - val_loss: 5.2244\n",
      "Epoch 4502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0081 - val_loss: 5.3294\n",
      "Epoch 4503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9376 - val_loss: 5.2432\n",
      "Epoch 4504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9912 - val_loss: 5.2805\n",
      "Epoch 4505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9612 - val_loss: 5.2608\n",
      "Epoch 4506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0119 - val_loss: 5.3071\n",
      "Epoch 4507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3890 - val_loss: 5.9492\n",
      "Epoch 4508/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1267 - val_loss: 5.3155\n",
      "Epoch 4509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9083 - val_loss: 5.4368\n",
      "Epoch 4510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9647 - val_loss: 5.2404\n",
      "Epoch 4511/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0806 - val_loss: 5.2882\n",
      "Epoch 4512/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9625 - val_loss: 5.2229\n",
      "Epoch 4513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9302 - val_loss: 5.4619\n",
      "Epoch 4514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0175 - val_loss: 5.2266\n",
      "Epoch 4515/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9856 - val_loss: 5.3187\n",
      "Epoch 4516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0779 - val_loss: 5.3855\n",
      "Epoch 4517/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0275 - val_loss: 5.2476\n",
      "Epoch 4518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9269 - val_loss: 5.2756\n",
      "Epoch 4519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0655 - val_loss: 5.4467\n",
      "Epoch 4520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9923 - val_loss: 5.2758\n",
      "Epoch 4521/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1205 - val_loss: 5.4627\n",
      "Epoch 4522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9954 - val_loss: 5.2470\n",
      "Epoch 4523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9312 - val_loss: 5.3399\n",
      "Epoch 4524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9923 - val_loss: 5.5579\n",
      "Epoch 4525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0869 - val_loss: 5.6692\n",
      "Epoch 4526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9582 - val_loss: 5.2208\n",
      "Epoch 4527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0623 - val_loss: 5.2977\n",
      "Epoch 4528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0059 - val_loss: 5.6751\n",
      "Epoch 4529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0176 - val_loss: 5.4119\n",
      "Epoch 4530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9727 - val_loss: 5.2716\n",
      "Epoch 4531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0029 - val_loss: 5.3594\n",
      "Epoch 4532/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0752 - val_loss: 5.2387\n",
      "Epoch 4533/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9832 - val_loss: 5.2122\n",
      "Epoch 4534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9142 - val_loss: 5.3019\n",
      "Epoch 4535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0000 - val_loss: 5.5436\n",
      "Epoch 4536/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9595 - val_loss: 5.3506\n",
      "Epoch 4537/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9634 - val_loss: 5.2835\n",
      "Epoch 4538/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9167 - val_loss: 5.2611\n",
      "Epoch 4539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9471 - val_loss: 5.2768\n",
      "Epoch 4540/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0206 - val_loss: 5.3638\n",
      "Epoch 4541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9800 - val_loss: 5.3619\n",
      "Epoch 4542/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9285 - val_loss: 5.4680\n",
      "Epoch 4543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1083 - val_loss: 5.2437\n",
      "Epoch 4544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9293 - val_loss: 5.2744\n",
      "Epoch 4545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0739 - val_loss: 5.2723\n",
      "Epoch 4546/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0608 - val_loss: 5.2895\n",
      "Epoch 4547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9574 - val_loss: 5.3579\n",
      "Epoch 4548/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9463 - val_loss: 5.8061\n",
      "Epoch 4549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3887 - val_loss: 5.3278\n",
      "Epoch 4550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9301 - val_loss: 5.5980\n",
      "Epoch 4551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2012 - val_loss: 5.2547\n",
      "Epoch 4552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9509 - val_loss: 5.2394\n",
      "Epoch 4553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9711 - val_loss: 5.2837\n",
      "Epoch 4554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0500 - val_loss: 5.3250\n",
      "Epoch 4555/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0658 - val_loss: 5.3009\n",
      "Epoch 4556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9753 - val_loss: 5.2141\n",
      "Epoch 4557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0002 - val_loss: 5.3895\n",
      "Epoch 4558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0447 - val_loss: 5.2134\n",
      "Epoch 4559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9423 - val_loss: 5.2477\n",
      "Epoch 4560/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0373 - val_loss: 5.3209\n",
      "Epoch 4561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2142 - val_loss: 5.5028\n",
      "Epoch 4562/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0041 - val_loss: 5.5172\n",
      "Epoch 4563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0841 - val_loss: 5.3503\n",
      "Epoch 4564/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1031 - val_loss: 5.3560\n",
      "Epoch 4565/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3054 - val_loss: 5.4155\n",
      "Epoch 4566/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 114us/step - loss: 5.0744 - val_loss: 5.2773\n",
      "Epoch 4567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8813 - val_loss: 5.2977\n",
      "Epoch 4568/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9332 - val_loss: 5.2348\n",
      "Epoch 4569/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9159 - val_loss: 5.6920\n",
      "Epoch 4570/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0298 - val_loss: 5.2309\n",
      "Epoch 4571/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8787 - val_loss: 5.2268\n",
      "Epoch 4572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0407 - val_loss: 5.2449\n",
      "Epoch 4573/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9386 - val_loss: 5.3458\n",
      "Epoch 4574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9830 - val_loss: 5.2663\n",
      "Epoch 4575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9386 - val_loss: 5.3249\n",
      "Epoch 4576/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1507 - val_loss: 5.3548\n",
      "Epoch 4577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0317 - val_loss: 5.2149\n",
      "Epoch 4578/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9191 - val_loss: 5.2268\n",
      "Epoch 4579/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9082 - val_loss: 5.3013\n",
      "Epoch 4580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0730 - val_loss: 5.5321\n",
      "Epoch 4581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1270 - val_loss: 5.2296\n",
      "Epoch 4582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9604 - val_loss: 5.2491\n",
      "Epoch 4583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0229 - val_loss: 5.2469\n",
      "Epoch 4584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9038 - val_loss: 5.2369\n",
      "Epoch 4585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9211 - val_loss: 5.2408\n",
      "Epoch 4586/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8882 - val_loss: 5.3355\n",
      "Epoch 4587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9824 - val_loss: 5.3850\n",
      "Epoch 4588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1241 - val_loss: 5.3689\n",
      "Epoch 4589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9436 - val_loss: 5.5932\n",
      "Epoch 4590/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2117 - val_loss: 5.8330\n",
      "Epoch 4591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0983 - val_loss: 5.3800\n",
      "Epoch 4592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1034 - val_loss: 5.2510\n",
      "Epoch 4593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9482 - val_loss: 5.2481\n",
      "Epoch 4594/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9352 - val_loss: 5.3129\n",
      "Epoch 4595/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1131 - val_loss: 5.3479\n",
      "Epoch 4596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9689 - val_loss: 5.2373\n",
      "Epoch 4597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9100 - val_loss: 5.2313\n",
      "Epoch 4598/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8829 - val_loss: 5.2726\n",
      "Epoch 4599/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9253 - val_loss: 5.2485\n",
      "Epoch 4600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9450 - val_loss: 5.2235\n",
      "Epoch 4601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9447 - val_loss: 5.2457\n",
      "Epoch 4602/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9405 - val_loss: 5.3662\n",
      "Epoch 4603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0946 - val_loss: 5.2491\n",
      "Epoch 4604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9359 - val_loss: 5.2235\n",
      "Epoch 4605/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9364 - val_loss: 5.3495\n",
      "Epoch 4606/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0401 - val_loss: 5.2147\n",
      "Epoch 4607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9069 - val_loss: 5.4766\n",
      "Epoch 4608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0478 - val_loss: 5.2305\n",
      "Epoch 4609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1791 - val_loss: 5.2940\n",
      "Epoch 4610/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0791 - val_loss: 5.6414\n",
      "Epoch 4611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2203 - val_loss: 5.2804\n",
      "Epoch 4612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9246 - val_loss: 5.2299\n",
      "Epoch 4613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0010 - val_loss: 5.2271\n",
      "Epoch 4614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9962 - val_loss: 5.2566\n",
      "Epoch 4615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3152 - val_loss: 5.3474\n",
      "Epoch 4616/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8887 - val_loss: 5.3925\n",
      "Epoch 4617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1190 - val_loss: 5.9287\n",
      "Epoch 4618/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1278 - val_loss: 5.2470\n",
      "Epoch 4619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9338 - val_loss: 5.2575\n",
      "Epoch 4620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9708 - val_loss: 5.2213\n",
      "Epoch 4621/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0030 - val_loss: 5.2638\n",
      "Epoch 4622/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9602 - val_loss: 5.2508\n",
      "Epoch 4623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0176 - val_loss: 6.0078\n",
      "Epoch 4624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3043 - val_loss: 5.2291\n",
      "Epoch 4625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0266 - val_loss: 5.2614\n",
      "Epoch 4626/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9258 - val_loss: 5.2792\n",
      "Epoch 4627/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9380 - val_loss: 5.2011\n",
      "Epoch 4628/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9423 - val_loss: 5.2152\n",
      "Epoch 4629/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0253 - val_loss: 5.1967\n",
      "Epoch 4630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9394 - val_loss: 5.2696\n",
      "Epoch 4631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9843 - val_loss: 5.2708\n",
      "Epoch 4632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9640 - val_loss: 5.2466\n",
      "Epoch 4633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9420 - val_loss: 5.2777\n",
      "Epoch 4634/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0278 - val_loss: 5.5459\n",
      "Epoch 4635/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0735 - val_loss: 5.1888\n",
      "Epoch 4636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9815 - val_loss: 5.2134\n",
      "Epoch 4637/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9372 - val_loss: 5.3442\n",
      "Epoch 4638/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0195 - val_loss: 5.4622\n",
      "Epoch 4639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9773 - val_loss: 5.5920\n",
      "Epoch 4640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0566 - val_loss: 5.1998\n",
      "Epoch 4641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8795 - val_loss: 5.5746\n",
      "Epoch 4642/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.9325 - val_loss: 5.2383\n",
      "Epoch 4643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9679 - val_loss: 5.3413\n",
      "Epoch 4644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1650 - val_loss: 5.5696\n",
      "Epoch 4645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0862 - val_loss: 5.2034\n",
      "Epoch 4646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8594 - val_loss: 5.2713\n",
      "Epoch 4647/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9746 - val_loss: 5.3067\n",
      "Epoch 4648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8898 - val_loss: 5.2116\n",
      "Epoch 4649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0053 - val_loss: 5.5018\n",
      "Epoch 4650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9651 - val_loss: 5.2699\n",
      "Epoch 4651/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9515 - val_loss: 5.2421\n",
      "Epoch 4652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9064 - val_loss: 5.1976\n",
      "Epoch 4653/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9330 - val_loss: 5.2842\n",
      "Epoch 4654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9587 - val_loss: 5.4861\n",
      "Epoch 4655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9591 - val_loss: 5.3085\n",
      "Epoch 4656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9637 - val_loss: 5.4324\n",
      "Epoch 4657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9795 - val_loss: 5.2845\n",
      "Epoch 4658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9117 - val_loss: 5.4917\n",
      "Epoch 4659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9601 - val_loss: 5.2295\n",
      "Epoch 4660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9433 - val_loss: 5.2373\n",
      "Epoch 4661/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8665 - val_loss: 5.2038\n",
      "Epoch 4662/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0192 - val_loss: 5.2350\n",
      "Epoch 4663/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4289 - val_loss: 5.8690\n",
      "Epoch 4664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2300 - val_loss: 5.7548\n",
      "Epoch 4665/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0286 - val_loss: 5.3052\n",
      "Epoch 4666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9933 - val_loss: 5.2225\n",
      "Epoch 4667/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8815 - val_loss: 5.2331\n",
      "Epoch 4668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0669 - val_loss: 5.3457\n",
      "Epoch 4669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0726 - val_loss: 5.2369\n",
      "Epoch 4670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0230 - val_loss: 5.5383\n",
      "Epoch 4671/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0206 - val_loss: 5.2600\n",
      "Epoch 4672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0096 - val_loss: 5.2469\n",
      "Epoch 4673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9248 - val_loss: 5.3739\n",
      "Epoch 4674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0034 - val_loss: 5.2543\n",
      "Epoch 4675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0068 - val_loss: 5.2210\n",
      "Epoch 4676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9567 - val_loss: 5.2898\n",
      "Epoch 4677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8973 - val_loss: 5.3620\n",
      "Epoch 4678/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2677 - val_loss: 5.4651\n",
      "Epoch 4679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3443 - val_loss: 5.2465\n",
      "Epoch 4680/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0457 - val_loss: 5.2090\n",
      "Epoch 4681/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9646 - val_loss: 5.3359\n",
      "Epoch 4682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9432 - val_loss: 5.2321\n",
      "Epoch 4683/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1311 - val_loss: 5.9343\n",
      "Epoch 4684/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9531 - val_loss: 5.2145\n",
      "Epoch 4685/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8958 - val_loss: 5.2758\n",
      "Epoch 4686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0579 - val_loss: 5.6270\n",
      "Epoch 4687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1750 - val_loss: 5.3206\n",
      "Epoch 4688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9167 - val_loss: 5.2892\n",
      "Epoch 4689/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8844 - val_loss: 5.2158\n",
      "Epoch 4690/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9307 - val_loss: 5.3286\n",
      "Epoch 4691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1096 - val_loss: 5.2849\n",
      "Epoch 4692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9149 - val_loss: 5.5776\n",
      "Epoch 4693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0869 - val_loss: 5.4170\n",
      "Epoch 4694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0358 - val_loss: 5.1837\n",
      "Epoch 4695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8994 - val_loss: 5.3706\n",
      "Epoch 4696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0039 - val_loss: 5.2690\n",
      "Epoch 4697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9895 - val_loss: 5.3448\n",
      "Epoch 4698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2505 - val_loss: 5.3162\n",
      "Epoch 4699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1681 - val_loss: 5.4417\n",
      "Epoch 4700/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0502 - val_loss: 5.1717\n",
      "Epoch 4701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9411 - val_loss: 5.2727\n",
      "Epoch 4702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9948 - val_loss: 5.2476\n",
      "Epoch 4703/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9800 - val_loss: 5.3569\n",
      "Epoch 4704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9197 - val_loss: 5.2070\n",
      "Epoch 4705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8814 - val_loss: 5.3827\n",
      "Epoch 4706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8782 - val_loss: 5.3795\n",
      "Epoch 4707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9719 - val_loss: 5.4619\n",
      "Epoch 4708/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2741 - val_loss: 5.2567\n",
      "Epoch 4709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9327 - val_loss: 5.2688\n",
      "Epoch 4710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8819 - val_loss: 5.2339\n",
      "Epoch 4711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9665 - val_loss: 5.2009\n",
      "Epoch 4712/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8718 - val_loss: 5.2774\n",
      "Epoch 4713/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9124 - val_loss: 5.2292\n",
      "Epoch 4714/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9412 - val_loss: 5.2386\n",
      "Epoch 4715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0163 - val_loss: 5.4357\n",
      "Epoch 4716/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0980 - val_loss: 5.2570\n",
      "Epoch 4717/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9566 - val_loss: 5.5294\n",
      "Epoch 4718/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.9200 - val_loss: 5.4209\n",
      "Epoch 4719/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9145 - val_loss: 5.2405\n",
      "Epoch 4720/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9914 - val_loss: 5.2819\n",
      "Epoch 4721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9347 - val_loss: 5.2568\n",
      "Epoch 4722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9601 - val_loss: 5.2189\n",
      "Epoch 4723/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9003 - val_loss: 5.2333\n",
      "Epoch 4724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8942 - val_loss: 5.5472\n",
      "Epoch 4725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0300 - val_loss: 5.2945\n",
      "Epoch 4726/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9673 - val_loss: 5.4008\n",
      "Epoch 4727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0719 - val_loss: 5.3751\n",
      "Epoch 4728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9353 - val_loss: 5.2064\n",
      "Epoch 4729/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9619 - val_loss: 5.2066\n",
      "Epoch 4730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0803 - val_loss: 5.3021\n",
      "Epoch 4731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9424 - val_loss: 5.3003\n",
      "Epoch 4732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9041 - val_loss: 5.2304\n",
      "Epoch 4733/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9461 - val_loss: 5.2222\n",
      "Epoch 4734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9702 - val_loss: 5.2290\n",
      "Epoch 4735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1218 - val_loss: 5.2127\n",
      "Epoch 4736/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8907 - val_loss: 5.3109\n",
      "Epoch 4737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1471 - val_loss: 5.4849\n",
      "Epoch 4738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9673 - val_loss: 5.5344\n",
      "Epoch 4739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0688 - val_loss: 5.2577\n",
      "Epoch 4740/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8843 - val_loss: 5.2179\n",
      "Epoch 4741/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0362 - val_loss: 5.2094\n",
      "Epoch 4742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9714 - val_loss: 5.2589\n",
      "Epoch 4743/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.2484 - val_loss: 5.4891\n",
      "Epoch 4744/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9417 - val_loss: 5.1982\n",
      "Epoch 4745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0183 - val_loss: 5.1804\n",
      "Epoch 4746/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2671 - val_loss: 5.2454\n",
      "Epoch 4747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9445 - val_loss: 5.2264\n",
      "Epoch 4748/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8923 - val_loss: 5.2560\n",
      "Epoch 4749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9265 - val_loss: 5.2194\n",
      "Epoch 4750/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8959 - val_loss: 5.2905\n",
      "Epoch 4751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9412 - val_loss: 5.2458\n",
      "Epoch 4752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9391 - val_loss: 5.2136\n",
      "Epoch 4753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9970 - val_loss: 5.8902\n",
      "Epoch 4754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2681 - val_loss: 5.4788\n",
      "Epoch 4755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8894 - val_loss: 5.3213\n",
      "Epoch 4756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9528 - val_loss: 5.5326\n",
      "Epoch 4757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0015 - val_loss: 5.4052\n",
      "Epoch 4758/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8936 - val_loss: 5.7319\n",
      "Epoch 4759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1049 - val_loss: 5.3728\n",
      "Epoch 4760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0082 - val_loss: 5.6101\n",
      "Epoch 4761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1922 - val_loss: 5.2125\n",
      "Epoch 4762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0196 - val_loss: 5.3816\n",
      "Epoch 4763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9339 - val_loss: 5.2038\n",
      "Epoch 4764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8865 - val_loss: 5.3712\n",
      "Epoch 4765/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9814 - val_loss: 5.3267\n",
      "Epoch 4766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0190 - val_loss: 5.2292\n",
      "Epoch 4767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9168 - val_loss: 5.2142\n",
      "Epoch 4768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0510 - val_loss: 5.2947\n",
      "Epoch 4769/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8714 - val_loss: 5.5987\n",
      "Epoch 4770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0050 - val_loss: 5.3089\n",
      "Epoch 4771/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9049 - val_loss: 5.2327\n",
      "Epoch 4772/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0447 - val_loss: 5.3365\n",
      "Epoch 4773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0155 - val_loss: 5.5412\n",
      "Epoch 4774/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9142 - val_loss: 5.2608\n",
      "Epoch 4775/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9071 - val_loss: 5.2031\n",
      "Epoch 4776/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0162 - val_loss: 5.4201\n",
      "Epoch 4777/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0769 - val_loss: 5.2782\n",
      "Epoch 4778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9684 - val_loss: 5.3604\n",
      "Epoch 4779/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1070 - val_loss: 5.2580\n",
      "Epoch 4780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8955 - val_loss: 5.2263\n",
      "Epoch 4781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9385 - val_loss: 5.4305\n",
      "Epoch 4782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0402 - val_loss: 5.2945\n",
      "Epoch 4783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9174 - val_loss: 5.2055\n",
      "Epoch 4784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9095 - val_loss: 5.2034\n",
      "Epoch 4785/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9308 - val_loss: 5.2566\n",
      "Epoch 4786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8515 - val_loss: 5.3131\n",
      "Epoch 4787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2939 - val_loss: 5.2154\n",
      "Epoch 4788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1317 - val_loss: 5.3987\n",
      "Epoch 4789/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9281 - val_loss: 5.1924\n",
      "Epoch 4790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9010 - val_loss: 5.1905\n",
      "Epoch 4791/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8847 - val_loss: 5.3676\n",
      "Epoch 4792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9280 - val_loss: 5.3897\n",
      "Epoch 4793/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1187 - val_loss: 5.2947\n",
      "Epoch 4794/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 5.0492 - val_loss: 5.7663\n",
      "Epoch 4795/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9670 - val_loss: 5.2419\n",
      "Epoch 4796/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8638 - val_loss: 5.1949\n",
      "Epoch 4797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9846 - val_loss: 5.2618\n",
      "Epoch 4798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0008 - val_loss: 5.2752\n",
      "Epoch 4799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9144 - val_loss: 5.2165\n",
      "Epoch 4800/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.755 - 0s 46us/step - loss: 4.9907 - val_loss: 5.5778\n",
      "Epoch 4801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9623 - val_loss: 5.2585\n",
      "Epoch 4802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9040 - val_loss: 5.2308\n",
      "Epoch 4803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9361 - val_loss: 5.3489\n",
      "Epoch 4804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0629 - val_loss: 5.7563\n",
      "Epoch 4805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0634 - val_loss: 5.3428\n",
      "Epoch 4806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9979 - val_loss: 5.2037\n",
      "Epoch 4807/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8896 - val_loss: 5.2121\n",
      "Epoch 4808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8504 - val_loss: 5.2779\n",
      "Epoch 4809/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8893 - val_loss: 5.2150\n",
      "Epoch 4810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9286 - val_loss: 5.3852\n",
      "Epoch 4811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0035 - val_loss: 5.2301\n",
      "Epoch 4812/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9454 - val_loss: 5.6582\n",
      "Epoch 4813/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0634 - val_loss: 5.2207\n",
      "Epoch 4814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8922 - val_loss: 5.1862\n",
      "Epoch 4815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8957 - val_loss: 5.2730\n",
      "Epoch 4816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9772 - val_loss: 5.5198\n",
      "Epoch 4817/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9456 - val_loss: 5.3065\n",
      "Epoch 4818/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0592 - val_loss: 5.2442\n",
      "Epoch 4819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9450 - val_loss: 5.2039\n",
      "Epoch 4820/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9251 - val_loss: 5.2434\n",
      "Epoch 4821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9713 - val_loss: 5.1812\n",
      "Epoch 4822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9120 - val_loss: 5.4980\n",
      "Epoch 4823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2293 - val_loss: 5.8555\n",
      "Epoch 4824/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1154 - val_loss: 5.3507\n",
      "Epoch 4825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9164 - val_loss: 5.2194\n",
      "Epoch 4826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8489 - val_loss: 5.2108\n",
      "Epoch 4827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9370 - val_loss: 5.1961\n",
      "Epoch 4828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9731 - val_loss: 5.4848\n",
      "Epoch 4829/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9865 - val_loss: 5.2102\n",
      "Epoch 4830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0400 - val_loss: 5.5072\n",
      "Epoch 4831/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9608 - val_loss: 5.2023\n",
      "Epoch 4832/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9278 - val_loss: 5.2352\n",
      "Epoch 4833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9224 - val_loss: 5.9222\n",
      "Epoch 4834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0869 - val_loss: 5.1966\n",
      "Epoch 4835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0529 - val_loss: 5.2032\n",
      "Epoch 4836/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9112 - val_loss: 5.6628\n",
      "Epoch 4837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0555 - val_loss: 5.4496\n",
      "Epoch 4838/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9764 - val_loss: 5.3677\n",
      "Epoch 4839/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0104 - val_loss: 5.3086\n",
      "Epoch 4840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1598 - val_loss: 5.5821\n",
      "Epoch 4841/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9897 - val_loss: 5.1888\n",
      "Epoch 4842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8583 - val_loss: 5.3709\n",
      "Epoch 4843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8710 - val_loss: 5.2101\n",
      "Epoch 4844/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9894 - val_loss: 5.2513\n",
      "Epoch 4845/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0885 - val_loss: 5.2839\n",
      "Epoch 4846/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9240 - val_loss: 5.3225\n",
      "Epoch 4847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9312 - val_loss: 5.2003\n",
      "Epoch 4848/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.0319 - val_loss: 5.2034\n",
      "Epoch 4849/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8903 - val_loss: 5.2281\n",
      "Epoch 4850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9296 - val_loss: 5.3502\n",
      "Epoch 4851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8980 - val_loss: 5.3249\n",
      "Epoch 4852/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8896 - val_loss: 5.1962\n",
      "Epoch 4853/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0704 - val_loss: 5.3913\n",
      "Epoch 4854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0568 - val_loss: 5.6199\n",
      "Epoch 4855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4281 - val_loss: 5.3230\n",
      "Epoch 4856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0667 - val_loss: 5.2004\n",
      "Epoch 4857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8660 - val_loss: 5.1856\n",
      "Epoch 4858/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0127 - val_loss: 5.2344\n",
      "Epoch 4859/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2612 - val_loss: 5.2462\n",
      "Epoch 4860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9503 - val_loss: 5.2018\n",
      "Epoch 4861/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1216 - val_loss: 5.6592\n",
      "Epoch 4862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1803 - val_loss: 5.2737\n",
      "Epoch 4863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1407 - val_loss: 6.3989\n",
      "Epoch 4864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1319 - val_loss: 5.1993\n",
      "Epoch 4865/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8721 - val_loss: 5.2293\n",
      "Epoch 4866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0657 - val_loss: 5.1932\n",
      "Epoch 4867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8879 - val_loss: 5.2237\n",
      "Epoch 4868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8959 - val_loss: 5.4230\n",
      "Epoch 4869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9080 - val_loss: 5.1867\n",
      "Epoch 4870/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.9246 - val_loss: 5.1843\n",
      "Epoch 4871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0820 - val_loss: 5.1901\n",
      "Epoch 4872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1154 - val_loss: 5.6691\n",
      "Epoch 4873/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0786 - val_loss: 5.3065\n",
      "Epoch 4874/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9314 - val_loss: 5.2968\n",
      "Epoch 4875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8743 - val_loss: 5.1869\n",
      "Epoch 4876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9215 - val_loss: 5.2457\n",
      "Epoch 4877/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8555 - val_loss: 5.3550\n",
      "Epoch 4878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9224 - val_loss: 5.2345\n",
      "Epoch 4879/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0087 - val_loss: 5.2818\n",
      "Epoch 4880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9525 - val_loss: 5.1834\n",
      "Epoch 4881/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0362 - val_loss: 5.3434\n",
      "Epoch 4882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9035 - val_loss: 5.3066\n",
      "Epoch 4883/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8700 - val_loss: 5.3596\n",
      "Epoch 4884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8444 - val_loss: 5.2120\n",
      "Epoch 4885/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8525 - val_loss: 5.3058\n",
      "Epoch 4886/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8848 - val_loss: 5.2875\n",
      "Epoch 4887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0452 - val_loss: 5.4071\n",
      "Epoch 4888/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8716 - val_loss: 5.1952\n",
      "Epoch 4889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8500 - val_loss: 5.2137\n",
      "Epoch 4890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9183 - val_loss: 5.4809\n",
      "Epoch 4891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1977 - val_loss: 5.2378\n",
      "Epoch 4892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1762 - val_loss: 5.4399\n",
      "Epoch 4893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0930 - val_loss: 5.2662\n",
      "Epoch 4894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8985 - val_loss: 5.3084\n",
      "Epoch 4895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9725 - val_loss: 5.1681\n",
      "Epoch 4896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8804 - val_loss: 5.2178\n",
      "Epoch 4897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0459 - val_loss: 5.2098\n",
      "Epoch 4898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0800 - val_loss: 5.4659\n",
      "Epoch 4899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9397 - val_loss: 5.3546\n",
      "Epoch 4900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8842 - val_loss: 5.1982\n",
      "Epoch 4901/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0600 - val_loss: 5.3909\n",
      "Epoch 4902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8818 - val_loss: 5.1919\n",
      "Epoch 4903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9079 - val_loss: 5.1728\n",
      "Epoch 4904/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0113 - val_loss: 5.3647\n",
      "Epoch 4905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0340 - val_loss: 5.1826\n",
      "Epoch 4906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9983 - val_loss: 5.6573\n",
      "Epoch 4907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2971 - val_loss: 5.1728\n",
      "Epoch 4908/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9221 - val_loss: 5.3949\n",
      "Epoch 4909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1765 - val_loss: 5.2864\n",
      "Epoch 4910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8821 - val_loss: 5.2034\n",
      "Epoch 4911/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8905 - val_loss: 5.1729\n",
      "Epoch 4912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9094 - val_loss: 5.2806\n",
      "Epoch 4913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3538 - val_loss: 5.2983\n",
      "Epoch 4914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0218 - val_loss: 5.5148\n",
      "Epoch 4915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9436 - val_loss: 5.5632\n",
      "Epoch 4916/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0053 - val_loss: 5.1910\n",
      "Epoch 4917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8453 - val_loss: 5.2927\n",
      "Epoch 4918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8796 - val_loss: 5.7910\n",
      "Epoch 4919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9654 - val_loss: 5.2699\n",
      "Epoch 4920/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8656 - val_loss: 5.3215\n",
      "Epoch 4921/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9518 - val_loss: 5.1733\n",
      "Epoch 4922/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2684 - val_loss: 5.6873\n",
      "Epoch 4923/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.5003 - val_loss: 5.5980\n",
      "Epoch 4924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0748 - val_loss: 5.1989\n",
      "Epoch 4925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9291 - val_loss: 5.3571\n",
      "Epoch 4926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9622 - val_loss: 5.1663\n",
      "Epoch 4927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8649 - val_loss: 5.4918\n",
      "Epoch 4928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8950 - val_loss: 5.2212\n",
      "Epoch 4929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9057 - val_loss: 5.1735\n",
      "Epoch 4930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8892 - val_loss: 5.2001\n",
      "Epoch 4931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8864 - val_loss: 5.3844\n",
      "Epoch 4932/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1474 - val_loss: 5.3929\n",
      "Epoch 4933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9194 - val_loss: 5.2469\n",
      "Epoch 4934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0001 - val_loss: 5.3077\n",
      "Epoch 4935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9914 - val_loss: 5.1921\n",
      "Epoch 4936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8793 - val_loss: 5.2380\n",
      "Epoch 4937/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9279 - val_loss: 5.2145\n",
      "Epoch 4938/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9616 - val_loss: 5.2193\n",
      "Epoch 4939/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8456 - val_loss: 5.2423\n",
      "Epoch 4940/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9561 - val_loss: 5.2064\n",
      "Epoch 4941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0596 - val_loss: 6.0936\n",
      "Epoch 4942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2007 - val_loss: 5.2116\n",
      "Epoch 4943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9101 - val_loss: 5.2870\n",
      "Epoch 4944/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8901 - val_loss: 5.1789\n",
      "Epoch 4945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9743 - val_loss: 5.1798\n",
      "Epoch 4946/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 5.1202 - val_loss: 5.4213\n",
      "Epoch 4947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1865 - val_loss: 5.2108\n",
      "Epoch 4948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8664 - val_loss: 5.2833\n",
      "Epoch 4949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9162 - val_loss: 5.2458\n",
      "Epoch 4950/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1289 - val_loss: 5.4057\n",
      "Epoch 4951/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1465 - val_loss: 5.2270\n",
      "Epoch 4952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9612 - val_loss: 5.2081\n",
      "Epoch 4953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8842 - val_loss: 5.2073\n",
      "Epoch 4954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8698 - val_loss: 5.3500\n",
      "Epoch 4955/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0676 - val_loss: 5.2027\n",
      "Epoch 4956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9478 - val_loss: 5.1944\n",
      "Epoch 4957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2756 - val_loss: 5.6115\n",
      "Epoch 4958/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1453 - val_loss: 5.1872\n",
      "Epoch 4959/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0625 - val_loss: 5.1966\n",
      "Epoch 4960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8765 - val_loss: 5.1705\n",
      "Epoch 4961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8509 - val_loss: 5.2700\n",
      "Epoch 4962/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8892 - val_loss: 5.1825\n",
      "Epoch 4963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8960 - val_loss: 5.1896\n",
      "Epoch 4964/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0211 - val_loss: 5.1648\n",
      "Epoch 4965/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9514 - val_loss: 5.1630\n",
      "Epoch 4966/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1138 - val_loss: 5.4233\n",
      "Epoch 4967/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0245 - val_loss: 5.2734\n",
      "Epoch 4968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8212 - val_loss: 5.2020\n",
      "Epoch 4969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9047 - val_loss: 5.2200\n",
      "Epoch 4970/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9515 - val_loss: 5.1958\n",
      "Epoch 4971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1035 - val_loss: 5.2577\n",
      "Epoch 4972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1245 - val_loss: 5.1895\n",
      "Epoch 4973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9214 - val_loss: 5.2576\n",
      "Epoch 4974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9618 - val_loss: 5.5466\n",
      "Epoch 4975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9066 - val_loss: 5.3005\n",
      "Epoch 4976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9570 - val_loss: 5.2268\n",
      "Epoch 4977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0033 - val_loss: 5.1729\n",
      "Epoch 4978/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8612 - val_loss: 5.2874\n",
      "Epoch 4979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9214 - val_loss: 5.2799\n",
      "Epoch 4980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9649 - val_loss: 5.3498\n",
      "Epoch 4981/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9909 - val_loss: 5.1604\n",
      "Epoch 4982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0260 - val_loss: 5.5578\n",
      "Epoch 4983/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8798 - val_loss: 5.1848\n",
      "Epoch 4984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8417 - val_loss: 5.2484\n",
      "Epoch 4985/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0126 - val_loss: 5.1758\n",
      "Epoch 4986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8875 - val_loss: 5.4060\n",
      "Epoch 4987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1262 - val_loss: 5.6118\n",
      "Epoch 4988/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0474 - val_loss: 5.1697\n",
      "Epoch 4989/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9708 - val_loss: 5.1738\n",
      "Epoch 4990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9160 - val_loss: 5.1808\n",
      "Epoch 4991/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8664 - val_loss: 5.2321\n",
      "Epoch 4992/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9817 - val_loss: 5.2476\n",
      "Epoch 4993/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8849 - val_loss: 5.2390\n",
      "Epoch 4994/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8546 - val_loss: 5.2359\n",
      "Epoch 4995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8980 - val_loss: 5.2071\n",
      "Epoch 4996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9359 - val_loss: 5.4398\n",
      "Epoch 4997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9983 - val_loss: 6.3208\n",
      "Epoch 4998/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2740 - val_loss: 5.2305\n",
      "Epoch 4999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9275 - val_loss: 5.1734\n",
      "Epoch 5000/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8976 - val_loss: 5.1848\n",
      "Epoch 5001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9215 - val_loss: 5.2754\n",
      "Epoch 5002/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9863 - val_loss: 5.2466\n",
      "Epoch 5003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0137 - val_loss: 5.1541\n",
      "Epoch 5004/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9734 - val_loss: 5.3333\n",
      "Epoch 5005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8823 - val_loss: 5.1712\n",
      "Epoch 5006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0162 - val_loss: 5.1756\n",
      "Epoch 5007/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8424 - val_loss: 5.1936\n",
      "Epoch 5008/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8607 - val_loss: 5.3189\n",
      "Epoch 5009/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8392 - val_loss: 5.1826\n",
      "Epoch 5010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8947 - val_loss: 5.2781\n",
      "Epoch 5011/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8409 - val_loss: 5.2678\n",
      "Epoch 5012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8747 - val_loss: 5.4849\n",
      "Epoch 5013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1623 - val_loss: 5.6111\n",
      "Epoch 5014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1380 - val_loss: 5.5768\n",
      "Epoch 5015/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0409 - val_loss: 5.3601\n",
      "Epoch 5016/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9520 - val_loss: 5.1967\n",
      "Epoch 5017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8598 - val_loss: 5.2314\n",
      "Epoch 5018/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9515 - val_loss: 5.4111\n",
      "Epoch 5019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0071 - val_loss: 5.2224\n",
      "Epoch 5020/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8591 - val_loss: 5.3452\n",
      "Epoch 5021/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.8690 - val_loss: 5.1836\n",
      "Epoch 5022/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8669 - val_loss: 5.1782\n",
      "Epoch 5023/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8782 - val_loss: 5.1804\n",
      "Epoch 5024/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9497 - val_loss: 5.1761\n",
      "Epoch 5025/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8573 - val_loss: 5.2313\n",
      "Epoch 5026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8621 - val_loss: 5.4735\n",
      "Epoch 5027/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2275 - val_loss: 5.4579\n",
      "Epoch 5028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0062 - val_loss: 5.3322\n",
      "Epoch 5029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0219 - val_loss: 5.2767\n",
      "Epoch 5030/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8571 - val_loss: 5.2603\n",
      "Epoch 5031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9302 - val_loss: 5.1696\n",
      "Epoch 5032/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9626 - val_loss: 5.3902\n",
      "Epoch 5033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9287 - val_loss: 5.2722\n",
      "Epoch 5034/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9786 - val_loss: 5.2689\n",
      "Epoch 5035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8288 - val_loss: 5.2143\n",
      "Epoch 5036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8796 - val_loss: 5.2554\n",
      "Epoch 5037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9621 - val_loss: 5.2870\n",
      "Epoch 5038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8627 - val_loss: 5.2819\n",
      "Epoch 5039/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0103 - val_loss: 5.3357\n",
      "Epoch 5040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1471 - val_loss: 5.1965\n",
      "Epoch 5041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8689 - val_loss: 5.2281\n",
      "Epoch 5042/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0753 - val_loss: 5.2929\n",
      "Epoch 5043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8773 - val_loss: 5.1661\n",
      "Epoch 5044/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8850 - val_loss: 5.2906\n",
      "Epoch 5045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8562 - val_loss: 5.2083\n",
      "Epoch 5046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8553 - val_loss: 5.1894\n",
      "Epoch 5047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8482 - val_loss: 5.1647\n",
      "Epoch 5048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8390 - val_loss: 5.1568\n",
      "Epoch 5049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8240 - val_loss: 5.6270\n",
      "Epoch 5050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9324 - val_loss: 5.1557\n",
      "Epoch 5051/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8406 - val_loss: 5.1741\n",
      "Epoch 5052/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8478 - val_loss: 5.1723\n",
      "Epoch 5053/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8467 - val_loss: 5.8327\n",
      "Epoch 5054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1864 - val_loss: 5.8471\n",
      "Epoch 5055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0742 - val_loss: 5.1593\n",
      "Epoch 5056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8736 - val_loss: 5.1733\n",
      "Epoch 5057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8335 - val_loss: 5.1780\n",
      "Epoch 5058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8440 - val_loss: 5.2759\n",
      "Epoch 5059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9319 - val_loss: 5.1816\n",
      "Epoch 5060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8651 - val_loss: 5.1669\n",
      "Epoch 5061/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8980 - val_loss: 5.2128\n",
      "Epoch 5062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8656 - val_loss: 5.2829\n",
      "Epoch 5063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8588 - val_loss: 5.4341\n",
      "Epoch 5064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0021 - val_loss: 5.4687\n",
      "Epoch 5065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9398 - val_loss: 5.1787\n",
      "Epoch 5066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9439 - val_loss: 5.2381\n",
      "Epoch 5067/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8858 - val_loss: 5.1487\n",
      "Epoch 5068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9590 - val_loss: 5.2517\n",
      "Epoch 5069/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1034 - val_loss: 5.1593\n",
      "Epoch 5070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9647 - val_loss: 5.3804\n",
      "Epoch 5071/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9810 - val_loss: 5.2905\n",
      "Epoch 5072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9847 - val_loss: 5.4740\n",
      "Epoch 5073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0557 - val_loss: 5.4302\n",
      "Epoch 5074/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0183 - val_loss: 5.5669\n",
      "Epoch 5075/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1400 - val_loss: 5.3817\n",
      "Epoch 5076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9273 - val_loss: 5.1934\n",
      "Epoch 5077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9795 - val_loss: 5.4016\n",
      "Epoch 5078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9287 - val_loss: 5.1689\n",
      "Epoch 5079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8947 - val_loss: 5.3459\n",
      "Epoch 5080/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8713 - val_loss: 5.1658\n",
      "Epoch 5081/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8654 - val_loss: 5.2536\n",
      "Epoch 5082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9258 - val_loss: 5.3028\n",
      "Epoch 5083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0155 - val_loss: 5.1535\n",
      "Epoch 5084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8904 - val_loss: 5.2429\n",
      "Epoch 5085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8521 - val_loss: 5.1593\n",
      "Epoch 5086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8703 - val_loss: 5.1392\n",
      "Epoch 5087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8526 - val_loss: 5.1571\n",
      "Epoch 5088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0960 - val_loss: 5.3638\n",
      "Epoch 5089/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9952 - val_loss: 5.1930\n",
      "Epoch 5090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8937 - val_loss: 5.2354\n",
      "Epoch 5091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8540 - val_loss: 5.2366\n",
      "Epoch 5092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9017 - val_loss: 5.1837\n",
      "Epoch 5093/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9501 - val_loss: 5.2151\n",
      "Epoch 5094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9902 - val_loss: 5.1554\n",
      "Epoch 5095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8600 - val_loss: 5.2383\n",
      "Epoch 5096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9445 - val_loss: 5.1860\n",
      "Epoch 5097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1059 - val_loss: 5.5389\n",
      "Epoch 5098/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.9841 - val_loss: 5.2494\n",
      "Epoch 5099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9118 - val_loss: 5.2008\n",
      "Epoch 5100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0525 - val_loss: 6.6414\n",
      "Epoch 5101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.6521 - val_loss: 5.2155\n",
      "Epoch 5102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9397 - val_loss: 5.3922\n",
      "Epoch 5103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9586 - val_loss: 5.2474\n",
      "Epoch 5104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0170 - val_loss: 5.2089\n",
      "Epoch 5105/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9151 - val_loss: 5.3881\n",
      "Epoch 5106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8594 - val_loss: 5.1609\n",
      "Epoch 5107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8701 - val_loss: 5.1846\n",
      "Epoch 5108/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8365 - val_loss: 5.1830\n",
      "Epoch 5109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8376 - val_loss: 5.1612\n",
      "Epoch 5110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9836 - val_loss: 5.2103\n",
      "Epoch 5111/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8757 - val_loss: 5.2247\n",
      "Epoch 5112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8907 - val_loss: 5.2135\n",
      "Epoch 5113/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8757 - val_loss: 5.1704\n",
      "Epoch 5114/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8114 - val_loss: 5.3437\n",
      "Epoch 5115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9259 - val_loss: 5.2420\n",
      "Epoch 5116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8608 - val_loss: 5.1476\n",
      "Epoch 5117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8286 - val_loss: 5.5073\n",
      "Epoch 5118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9119 - val_loss: 5.1654\n",
      "Epoch 5119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8730 - val_loss: 5.1920\n",
      "Epoch 5120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9941 - val_loss: 5.2476\n",
      "Epoch 5121/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8447 - val_loss: 5.1758\n",
      "Epoch 5122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8264 - val_loss: 5.1657\n",
      "Epoch 5123/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8492 - val_loss: 5.2517\n",
      "Epoch 5124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8167 - val_loss: 5.1897\n",
      "Epoch 5125/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9257 - val_loss: 5.2675\n",
      "Epoch 5126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8529 - val_loss: 5.2084\n",
      "Epoch 5127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9326 - val_loss: 5.1922\n",
      "Epoch 5128/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8675 - val_loss: 5.2333\n",
      "Epoch 5129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9254 - val_loss: 5.1620\n",
      "Epoch 5130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8703 - val_loss: 5.2385\n",
      "Epoch 5131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8646 - val_loss: 5.3294\n",
      "Epoch 5132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9181 - val_loss: 5.2091\n",
      "Epoch 5133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8348 - val_loss: 5.1985\n",
      "Epoch 5134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8743 - val_loss: 5.4934\n",
      "Epoch 5135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9778 - val_loss: 5.6874\n",
      "Epoch 5136/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.3772 - val_loss: 5.1595\n",
      "Epoch 5137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9648 - val_loss: 5.1946\n",
      "Epoch 5138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9131 - val_loss: 5.1623\n",
      "Epoch 5139/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8155 - val_loss: 5.2026\n",
      "Epoch 5140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8933 - val_loss: 5.1731\n",
      "Epoch 5141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0300 - val_loss: 5.3682\n",
      "Epoch 5142/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9672 - val_loss: 5.1687\n",
      "Epoch 5143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9465 - val_loss: 5.1638\n",
      "Epoch 5144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8840 - val_loss: 5.1864\n",
      "Epoch 5145/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9072 - val_loss: 5.1500\n",
      "Epoch 5146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8545 - val_loss: 5.2534\n",
      "Epoch 5147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2330 - val_loss: 5.2197\n",
      "Epoch 5148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8494 - val_loss: 5.2102\n",
      "Epoch 5149/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9320 - val_loss: 5.4729\n",
      "Epoch 5150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0237 - val_loss: 5.2228\n",
      "Epoch 5151/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8327 - val_loss: 5.1795\n",
      "Epoch 5152/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9131 - val_loss: 5.1605\n",
      "Epoch 5153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9400 - val_loss: 5.3452\n",
      "Epoch 5154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8337 - val_loss: 5.1862\n",
      "Epoch 5155/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8324 - val_loss: 5.1629\n",
      "Epoch 5156/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9531 - val_loss: 5.2183\n",
      "Epoch 5157/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8587 - val_loss: 5.3783\n",
      "Epoch 5158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3819 - val_loss: 5.2796\n",
      "Epoch 5159/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2167 - val_loss: 5.3538\n",
      "Epoch 5160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0389 - val_loss: 5.1857\n",
      "Epoch 5161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9139 - val_loss: 5.1627\n",
      "Epoch 5162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8354 - val_loss: 5.2633\n",
      "Epoch 5163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9430 - val_loss: 5.6346\n",
      "Epoch 5164/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9602 - val_loss: 5.4086\n",
      "Epoch 5165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8569 - val_loss: 5.2468\n",
      "Epoch 5166/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0605 - val_loss: 5.3006\n",
      "Epoch 5167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9225 - val_loss: 5.1876\n",
      "Epoch 5168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9230 - val_loss: 5.1495\n",
      "Epoch 5169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9619 - val_loss: 5.5361\n",
      "Epoch 5170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9605 - val_loss: 5.1645\n",
      "Epoch 5171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9355 - val_loss: 5.2978\n",
      "Epoch 5172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0473 - val_loss: 5.7618\n",
      "Epoch 5173/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2660 - val_loss: 5.1656\n",
      "Epoch 5174/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.9794 - val_loss: 5.3576\n",
      "Epoch 5175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8322 - val_loss: 5.1571\n",
      "Epoch 5176/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8580 - val_loss: 5.1495\n",
      "Epoch 5177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8814 - val_loss: 5.2087\n",
      "Epoch 5178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8339 - val_loss: 5.1854\n",
      "Epoch 5179/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8993 - val_loss: 5.4029\n",
      "Epoch 5180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9810 - val_loss: 5.2384\n",
      "Epoch 5181/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8656 - val_loss: 5.1801\n",
      "Epoch 5182/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8298 - val_loss: 5.3431\n",
      "Epoch 5183/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9021 - val_loss: 5.2010\n",
      "Epoch 5184/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8429 - val_loss: 5.1857\n",
      "Epoch 5185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9763 - val_loss: 5.3085\n",
      "Epoch 5186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8624 - val_loss: 5.1413\n",
      "Epoch 5187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0057 - val_loss: 5.1527\n",
      "Epoch 5188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0532 - val_loss: 5.2150\n",
      "Epoch 5189/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9973 - val_loss: 5.2517\n",
      "Epoch 5190/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9482 - val_loss: 5.1692\n",
      "Epoch 5191/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8315 - val_loss: 5.1737\n",
      "Epoch 5192/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8647 - val_loss: 5.3500\n",
      "Epoch 5193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8828 - val_loss: 5.2377\n",
      "Epoch 5194/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.578 - 0s 46us/step - loss: 4.8668 - val_loss: 5.2258\n",
      "Epoch 5195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8927 - val_loss: 5.2075\n",
      "Epoch 5196/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8846 - val_loss: 5.7174\n",
      "Epoch 5197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9730 - val_loss: 5.2042\n",
      "Epoch 5198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8389 - val_loss: 5.2053\n",
      "Epoch 5199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9437 - val_loss: 5.3658\n",
      "Epoch 5200/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9737 - val_loss: 5.1649\n",
      "Epoch 5201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8359 - val_loss: 5.2490\n",
      "Epoch 5202/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8153 - val_loss: 5.1884\n",
      "Epoch 5203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9080 - val_loss: 5.4653\n",
      "Epoch 5204/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9007 - val_loss: 5.1677\n",
      "Epoch 5205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9471 - val_loss: 5.4303\n",
      "Epoch 5206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2418 - val_loss: 5.1680\n",
      "Epoch 5207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9628 - val_loss: 5.2006\n",
      "Epoch 5208/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9641 - val_loss: 5.2256\n",
      "Epoch 5209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9047 - val_loss: 5.1768\n",
      "Epoch 5210/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8665 - val_loss: 5.2495\n",
      "Epoch 5211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8864 - val_loss: 5.1743\n",
      "Epoch 5212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8440 - val_loss: 5.1990\n",
      "Epoch 5213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8947 - val_loss: 5.1291\n",
      "Epoch 5214/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9165 - val_loss: 5.5819\n",
      "Epoch 5215/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9658 - val_loss: 5.4236\n",
      "Epoch 5216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8562 - val_loss: 5.1531\n",
      "Epoch 5217/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8222 - val_loss: 5.1788\n",
      "Epoch 5218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8880 - val_loss: 5.1744\n",
      "Epoch 5219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8739 - val_loss: 5.1947\n",
      "Epoch 5220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8560 - val_loss: 5.2700\n",
      "Epoch 5221/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1256 - val_loss: 5.6677\n",
      "Epoch 5222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2415 - val_loss: 5.1526\n",
      "Epoch 5223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8435 - val_loss: 5.1521\n",
      "Epoch 5224/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8251 - val_loss: 5.1560\n",
      "Epoch 5225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0765 - val_loss: 5.2671\n",
      "Epoch 5226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0746 - val_loss: 5.7467\n",
      "Epoch 5227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1736 - val_loss: 5.3823\n",
      "Epoch 5228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9712 - val_loss: 5.1614\n",
      "Epoch 5229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8904 - val_loss: 5.1550\n",
      "Epoch 5230/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8570 - val_loss: 5.2365\n",
      "Epoch 5231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8565 - val_loss: 5.1815\n",
      "Epoch 5232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8170 - val_loss: 5.1884\n",
      "Epoch 5233/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9267 - val_loss: 5.2255\n",
      "Epoch 5234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8930 - val_loss: 5.1965\n",
      "Epoch 5235/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9972 - val_loss: 5.2821\n",
      "Epoch 5236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9353 - val_loss: 5.3310\n",
      "Epoch 5237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1784 - val_loss: 5.2439\n",
      "Epoch 5238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9045 - val_loss: 5.1641\n",
      "Epoch 5239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8054 - val_loss: 5.1741\n",
      "Epoch 5240/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8310 - val_loss: 5.1837\n",
      "Epoch 5241/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9396 - val_loss: 5.1791\n",
      "Epoch 5242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9037 - val_loss: 5.2352\n",
      "Epoch 5243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8545 - val_loss: 5.2533\n",
      "Epoch 5244/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0346 - val_loss: 5.1635\n",
      "Epoch 5245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0544 - val_loss: 5.2025\n",
      "Epoch 5246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1607 - val_loss: 5.2884\n",
      "Epoch 5247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0064 - val_loss: 5.1504\n",
      "Epoch 5248/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9925 - val_loss: 5.4652\n",
      "Epoch 5249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0377 - val_loss: 5.1408\n",
      "Epoch 5250/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8600 - val_loss: 5.1877\n",
      "Epoch 5251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9117 - val_loss: 5.1297\n",
      "Epoch 5252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8394 - val_loss: 5.1598\n",
      "Epoch 5253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9064 - val_loss: 5.2054\n",
      "Epoch 5254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9743 - val_loss: 5.1527\n",
      "Epoch 5255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9590 - val_loss: 5.1753\n",
      "Epoch 5256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8659 - val_loss: 5.4032\n",
      "Epoch 5257/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0455 - val_loss: 5.3285\n",
      "Epoch 5258/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0564 - val_loss: 5.3546\n",
      "Epoch 5259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9069 - val_loss: 5.1721\n",
      "Epoch 5260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8774 - val_loss: 5.2202\n",
      "Epoch 5261/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9293 - val_loss: 5.5249\n",
      "Epoch 5262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8598 - val_loss: 5.4937\n",
      "Epoch 5263/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9068 - val_loss: 5.1617\n",
      "Epoch 5264/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9435 - val_loss: 5.3392\n",
      "Epoch 5265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0221 - val_loss: 5.5603\n",
      "Epoch 5266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2388 - val_loss: 5.1468\n",
      "Epoch 5267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0573 - val_loss: 5.5203\n",
      "Epoch 5268/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9555 - val_loss: 5.2808\n",
      "Epoch 5269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8719 - val_loss: 5.1738\n",
      "Epoch 5270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9435 - val_loss: 5.1460\n",
      "Epoch 5271/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.790 - 0s 46us/step - loss: 4.9079 - val_loss: 5.1475\n",
      "Epoch 5272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8241 - val_loss: 5.2582\n",
      "Epoch 5273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8170 - val_loss: 5.1669\n",
      "Epoch 5274/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9388 - val_loss: 5.1767\n",
      "Epoch 5275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8302 - val_loss: 5.1913\n",
      "Epoch 5276/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8302 - val_loss: 5.2577\n",
      "Epoch 5277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8661 - val_loss: 5.1756\n",
      "Epoch 5278/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9019 - val_loss: 5.2029\n",
      "Epoch 5279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9408 - val_loss: 5.1567\n",
      "Epoch 5280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9753 - val_loss: 5.5039\n",
      "Epoch 5281/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0909 - val_loss: 5.1423\n",
      "Epoch 5282/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9098 - val_loss: 5.1458\n",
      "Epoch 5283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8661 - val_loss: 5.3971\n",
      "Epoch 5284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8874 - val_loss: 5.2175\n",
      "Epoch 5285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0404 - val_loss: 5.2066\n",
      "Epoch 5286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8434 - val_loss: 5.1846\n",
      "Epoch 5287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9839 - val_loss: 5.1508\n",
      "Epoch 5288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8436 - val_loss: 5.1712\n",
      "Epoch 5289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8404 - val_loss: 5.1754\n",
      "Epoch 5290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8364 - val_loss: 5.1529\n",
      "Epoch 5291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8961 - val_loss: 5.2181\n",
      "Epoch 5292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8936 - val_loss: 5.1687\n",
      "Epoch 5293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9505 - val_loss: 5.1503\n",
      "Epoch 5294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9737 - val_loss: 5.1682\n",
      "Epoch 5295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8897 - val_loss: 5.1588\n",
      "Epoch 5296/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8500 - val_loss: 5.6295\n",
      "Epoch 5297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0710 - val_loss: 5.2547\n",
      "Epoch 5298/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0653 - val_loss: 5.2635\n",
      "Epoch 5299/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9436 - val_loss: 5.5033\n",
      "Epoch 5300/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8256 - val_loss: 5.1365\n",
      "Epoch 5301/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8405 - val_loss: 5.1920\n",
      "Epoch 5302/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8914 - val_loss: 5.1887\n",
      "Epoch 5303/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0474 - val_loss: 5.1728\n",
      "Epoch 5304/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8827 - val_loss: 5.1807\n",
      "Epoch 5305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8396 - val_loss: 5.4059\n",
      "Epoch 5306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8232 - val_loss: 5.2754\n",
      "Epoch 5307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8466 - val_loss: 5.1521\n",
      "Epoch 5308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9005 - val_loss: 5.2659\n",
      "Epoch 5309/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9199 - val_loss: 5.2554\n",
      "Epoch 5310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7979 - val_loss: 5.1802\n",
      "Epoch 5311/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8674 - val_loss: 5.1655\n",
      "Epoch 5312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9119 - val_loss: 5.2780\n",
      "Epoch 5313/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1352 - val_loss: 5.9104\n",
      "Epoch 5314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1328 - val_loss: 5.1538\n",
      "Epoch 5315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0987 - val_loss: 5.9696\n",
      "Epoch 5316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2007 - val_loss: 5.3245\n",
      "Epoch 5317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8603 - val_loss: 5.1886\n",
      "Epoch 5318/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8344 - val_loss: 5.4119\n",
      "Epoch 5319/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0255 - val_loss: 5.1825\n",
      "Epoch 5320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8366 - val_loss: 5.1629\n",
      "Epoch 5321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8481 - val_loss: 5.1615\n",
      "Epoch 5322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0054 - val_loss: 5.2122\n",
      "Epoch 5323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8608 - val_loss: 5.1510\n",
      "Epoch 5324/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8089 - val_loss: 5.1376\n",
      "Epoch 5325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8170 - val_loss: 5.1626\n",
      "Epoch 5326/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8254 - val_loss: 5.1736\n",
      "Epoch 5327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8567 - val_loss: 5.2415\n",
      "Epoch 5328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9305 - val_loss: 5.2064\n",
      "Epoch 5329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8328 - val_loss: 5.3584\n",
      "Epoch 5330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9598 - val_loss: 5.3288\n",
      "Epoch 5331/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8254 - val_loss: 5.1562\n",
      "Epoch 5332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8141 - val_loss: 5.1667\n",
      "Epoch 5333/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8536 - val_loss: 5.2842\n",
      "Epoch 5334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8140 - val_loss: 5.1400\n",
      "Epoch 5335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0437 - val_loss: 5.5340\n",
      "Epoch 5336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8846 - val_loss: 5.2332\n",
      "Epoch 5337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8431 - val_loss: 5.3396\n",
      "Epoch 5338/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8156 - val_loss: 5.1312\n",
      "Epoch 5339/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7953 - val_loss: 5.1679\n",
      "Epoch 5340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7852 - val_loss: 5.2590\n",
      "Epoch 5341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8298 - val_loss: 5.1711\n",
      "Epoch 5342/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8865 - val_loss: 5.1514\n",
      "Epoch 5343/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8335 - val_loss: 5.4047\n",
      "Epoch 5344/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8983 - val_loss: 5.1559\n",
      "Epoch 5345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8274 - val_loss: 5.2926\n",
      "Epoch 5346/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8611 - val_loss: 5.2150\n",
      "Epoch 5347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8379 - val_loss: 5.3053\n",
      "Epoch 5348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8931 - val_loss: 5.1371\n",
      "Epoch 5349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9268 - val_loss: 5.1584\n",
      "Epoch 5350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8054 - val_loss: 5.2166\n",
      "Epoch 5351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1037 - val_loss: 5.5589\n",
      "Epoch 5352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9188 - val_loss: 5.1883\n",
      "Epoch 5353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8788 - val_loss: 5.2101\n",
      "Epoch 5354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7987 - val_loss: 5.1323\n",
      "Epoch 5355/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8305 - val_loss: 5.1398\n",
      "Epoch 5356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9215 - val_loss: 5.2574\n",
      "Epoch 5357/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8081 - val_loss: 5.1831\n",
      "Epoch 5358/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8989 - val_loss: 5.1516\n",
      "Epoch 5359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8862 - val_loss: 5.3039\n",
      "Epoch 5360/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9907 - val_loss: 5.2908\n",
      "Epoch 5361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8422 - val_loss: 5.3468\n",
      "Epoch 5362/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9069 - val_loss: 5.2996\n",
      "Epoch 5363/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1152 - val_loss: 5.1729\n",
      "Epoch 5364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9757 - val_loss: 5.2047\n",
      "Epoch 5365/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9773 - val_loss: 5.1699\n",
      "Epoch 5366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8865 - val_loss: 5.2141\n",
      "Epoch 5367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8828 - val_loss: 5.1432\n",
      "Epoch 5368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9623 - val_loss: 5.5448\n",
      "Epoch 5369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0108 - val_loss: 5.2102\n",
      "Epoch 5370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9672 - val_loss: 5.1907\n",
      "Epoch 5371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8790 - val_loss: 5.2714\n",
      "Epoch 5372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0443 - val_loss: 5.1689\n",
      "Epoch 5373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8352 - val_loss: 5.1738\n",
      "Epoch 5374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8806 - val_loss: 5.2110\n",
      "Epoch 5375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9221 - val_loss: 5.3091\n",
      "Epoch 5376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1317 - val_loss: 5.7370\n",
      "Epoch 5377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1535 - val_loss: 5.2670\n",
      "Epoch 5378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9512 - val_loss: 5.2202\n",
      "Epoch 5379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8976 - val_loss: 5.1384\n",
      "Epoch 5380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8411 - val_loss: 5.4029\n",
      "Epoch 5381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9353 - val_loss: 5.1573\n",
      "Epoch 5382/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8279 - val_loss: 5.1462\n",
      "Epoch 5383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1172 - val_loss: 6.9811\n",
      "Epoch 5384/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3837 - val_loss: 5.2591\n",
      "Epoch 5385/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.360 - 0s 23us/step - loss: 4.9350 - val_loss: 5.3323\n",
      "Epoch 5386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8597 - val_loss: 5.2536\n",
      "Epoch 5387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0079 - val_loss: 5.1723\n",
      "Epoch 5388/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8307 - val_loss: 5.2500\n",
      "Epoch 5389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8621 - val_loss: 5.2015\n",
      "Epoch 5390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9731 - val_loss: 5.1445\n",
      "Epoch 5391/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8460 - val_loss: 5.1666\n",
      "Epoch 5392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8451 - val_loss: 5.1338\n",
      "Epoch 5393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8253 - val_loss: 5.1193\n",
      "Epoch 5394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8327 - val_loss: 5.1837\n",
      "Epoch 5395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8168 - val_loss: 5.1855\n",
      "Epoch 5396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8189 - val_loss: 5.2486\n",
      "Epoch 5397/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8412 - val_loss: 5.2203\n",
      "Epoch 5398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8025 - val_loss: 5.1412\n",
      "Epoch 5399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7978 - val_loss: 5.3655\n",
      "Epoch 5400/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0083 - val_loss: 5.1550\n",
      "Epoch 5401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8020 - val_loss: 5.2513\n",
      "Epoch 5402/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8832 - val_loss: 5.1683\n",
      "Epoch 5403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8352 - val_loss: 5.1811\n",
      "Epoch 5404/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8257 - val_loss: 5.2419\n",
      "Epoch 5405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8555 - val_loss: 5.1308\n",
      "Epoch 5406/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0654 - val_loss: 5.5396\n",
      "Epoch 5407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0691 - val_loss: 5.1345\n",
      "Epoch 5408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9188 - val_loss: 5.4127\n",
      "Epoch 5409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8531 - val_loss: 5.1685\n",
      "Epoch 5410/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8708 - val_loss: 5.1800\n",
      "Epoch 5411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8833 - val_loss: 5.1435\n",
      "Epoch 5412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8545 - val_loss: 5.4713\n",
      "Epoch 5413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8621 - val_loss: 5.1406\n",
      "Epoch 5414/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8554 - val_loss: 5.2111\n",
      "Epoch 5415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9345 - val_loss: 5.1710\n",
      "Epoch 5416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9172 - val_loss: 5.1171\n",
      "Epoch 5417/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1130 - val_loss: 5.8188\n",
      "Epoch 5418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8895 - val_loss: 5.2227\n",
      "Epoch 5419/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8419 - val_loss: 5.2144\n",
      "Epoch 5420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7805 - val_loss: 5.2218\n",
      "Epoch 5421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9008 - val_loss: 5.3867\n",
      "Epoch 5422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0523 - val_loss: 5.2539\n",
      "Epoch 5423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8910 - val_loss: 5.2506\n",
      "Epoch 5424/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9028 - val_loss: 5.1307\n",
      "Epoch 5425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8128 - val_loss: 5.2744\n",
      "Epoch 5426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8215 - val_loss: 5.7676\n",
      "Epoch 5427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9928 - val_loss: 5.1258\n",
      "Epoch 5428/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7779 - val_loss: 5.1863\n",
      "Epoch 5429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8137 - val_loss: 5.1465\n",
      "Epoch 5430/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8196 - val_loss: 5.2033\n",
      "Epoch 5431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0361 - val_loss: 5.7894\n",
      "Epoch 5432/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0543 - val_loss: 5.1337\n",
      "Epoch 5433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9281 - val_loss: 5.1632\n",
      "Epoch 5434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8367 - val_loss: 5.2852\n",
      "Epoch 5435/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9062 - val_loss: 5.1467\n",
      "Epoch 5436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7901 - val_loss: 5.1588\n",
      "Epoch 5437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9609 - val_loss: 5.2668\n",
      "Epoch 5438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7615 - val_loss: 5.1861\n",
      "Epoch 5439/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9024 - val_loss: 5.2151\n",
      "Epoch 5440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9045 - val_loss: 5.3578\n",
      "Epoch 5441/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8199 - val_loss: 5.2055\n",
      "Epoch 5442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9243 - val_loss: 5.4432\n",
      "Epoch 5443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8665 - val_loss: 5.2469\n",
      "Epoch 5444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7902 - val_loss: 5.1690\n",
      "Epoch 5445/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7996 - val_loss: 5.1289\n",
      "Epoch 5446/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8061 - val_loss: 5.3405\n",
      "Epoch 5447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8246 - val_loss: 5.1441\n",
      "Epoch 5448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8418 - val_loss: 5.2720\n",
      "Epoch 5449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0218 - val_loss: 5.6116\n",
      "Epoch 5450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2468 - val_loss: 5.4153\n",
      "Epoch 5451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9991 - val_loss: 5.1445\n",
      "Epoch 5452/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8229 - val_loss: 5.2721\n",
      "Epoch 5453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9046 - val_loss: 5.1539\n",
      "Epoch 5454/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8132 - val_loss: 5.2165\n",
      "Epoch 5455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8930 - val_loss: 5.3774\n",
      "Epoch 5456/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9373 - val_loss: 5.3065\n",
      "Epoch 5457/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8973 - val_loss: 5.1426\n",
      "Epoch 5458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9195 - val_loss: 5.1869\n",
      "Epoch 5459/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8683 - val_loss: 5.1292\n",
      "Epoch 5460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3252 - val_loss: 5.2874\n",
      "Epoch 5461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8002 - val_loss: 5.1713\n",
      "Epoch 5462/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7646 - val_loss: 5.1419\n",
      "Epoch 5463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7787 - val_loss: 5.3228\n",
      "Epoch 5464/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9060 - val_loss: 5.1383\n",
      "Epoch 5465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8941 - val_loss: 5.1489\n",
      "Epoch 5466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8161 - val_loss: 5.1179\n",
      "Epoch 5467/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9965 - val_loss: 5.2681\n",
      "Epoch 5468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0194 - val_loss: 5.2129\n",
      "Epoch 5469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0100 - val_loss: 5.1340\n",
      "Epoch 5470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8122 - val_loss: 5.1570\n",
      "Epoch 5471/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8068 - val_loss: 5.1699\n",
      "Epoch 5472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9311 - val_loss: 5.5599\n",
      "Epoch 5473/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9480 - val_loss: 5.2813\n",
      "Epoch 5474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8456 - val_loss: 5.1805\n",
      "Epoch 5475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7877 - val_loss: 5.6578\n",
      "Epoch 5476/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2159 - val_loss: 5.2543\n",
      "Epoch 5477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0980 - val_loss: 5.4852\n",
      "Epoch 5478/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8832 - val_loss: 5.4108\n",
      "Epoch 5479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9072 - val_loss: 5.2028\n",
      "Epoch 5480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7896 - val_loss: 5.1212\n",
      "Epoch 5481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8834 - val_loss: 5.1171\n",
      "Epoch 5482/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8068 - val_loss: 5.1238\n",
      "Epoch 5483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7897 - val_loss: 5.1675\n",
      "Epoch 5484/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7812 - val_loss: 5.1026\n",
      "Epoch 5485/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8355 - val_loss: 5.1333\n",
      "Epoch 5486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8193 - val_loss: 5.1641\n",
      "Epoch 5487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7971 - val_loss: 5.1475\n",
      "Epoch 5488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9183 - val_loss: 5.1784\n",
      "Epoch 5489/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8296 - val_loss: 5.4646\n",
      "Epoch 5490/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0054 - val_loss: 5.1947\n",
      "Epoch 5491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8844 - val_loss: 5.4372\n",
      "Epoch 5492/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8938 - val_loss: 5.1688\n",
      "Epoch 5493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9037 - val_loss: 5.2152\n",
      "Epoch 5494/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8895 - val_loss: 5.1696\n",
      "Epoch 5495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9785 - val_loss: 5.2323\n",
      "Epoch 5496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9010 - val_loss: 5.3521\n",
      "Epoch 5497/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9026 - val_loss: 5.1705\n",
      "Epoch 5498/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8047 - val_loss: 5.2015\n",
      "Epoch 5499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0008 - val_loss: 5.5312\n",
      "Epoch 5500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8731 - val_loss: 5.1655\n",
      "Epoch 5501/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9234 - val_loss: 5.2568\n",
      "Epoch 5502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8939 - val_loss: 5.1629\n",
      "Epoch 5503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8384 - val_loss: 5.1212\n",
      "Epoch 5504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7714 - val_loss: 5.2497\n",
      "Epoch 5505/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.162 - 0s 46us/step - loss: 4.9776 - val_loss: 5.3926\n",
      "Epoch 5506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9339 - val_loss: 5.4097\n",
      "Epoch 5507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9007 - val_loss: 5.7235\n",
      "Epoch 5508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1081 - val_loss: 5.4307\n",
      "Epoch 5509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8660 - val_loss: 5.9584\n",
      "Epoch 5510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8475 - val_loss: 5.1258\n",
      "Epoch 5511/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8803 - val_loss: 5.3438\n",
      "Epoch 5512/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9121 - val_loss: 5.2758\n",
      "Epoch 5513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0309 - val_loss: 5.2813\n",
      "Epoch 5514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8420 - val_loss: 5.1218\n",
      "Epoch 5515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9357 - val_loss: 5.1567\n",
      "Epoch 5516/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0530 - val_loss: 5.2830\n",
      "Epoch 5517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9641 - val_loss: 5.1103\n",
      "Epoch 5518/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8432 - val_loss: 5.4407\n",
      "Epoch 5519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8700 - val_loss: 5.1329\n",
      "Epoch 5520/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8275 - val_loss: 5.1402\n",
      "Epoch 5521/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8320 - val_loss: 5.6824\n",
      "Epoch 5522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2938 - val_loss: 5.3486\n",
      "Epoch 5523/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9131 - val_loss: 5.3715\n",
      "Epoch 5524/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0301 - val_loss: 5.2544\n",
      "Epoch 5525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8360 - val_loss: 5.1998\n",
      "Epoch 5526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9341 - val_loss: 5.2063\n",
      "Epoch 5527/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8062 - val_loss: 5.1237\n",
      "Epoch 5528/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8457 - val_loss: 5.1326\n",
      "Epoch 5529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8998 - val_loss: 5.1222\n",
      "Epoch 5530/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8223 - val_loss: 5.1512\n",
      "Epoch 5531/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8212 - val_loss: 5.1503\n",
      "Epoch 5532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9985 - val_loss: 5.2370\n",
      "Epoch 5533/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8025 - val_loss: 5.2504\n",
      "Epoch 5534/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8702 - val_loss: 5.1234\n",
      "Epoch 5535/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8326 - val_loss: 5.3542\n",
      "Epoch 5536/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3121 - val_loss: 5.2740\n",
      "Epoch 5537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0416 - val_loss: 5.3518\n",
      "Epoch 5538/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9652 - val_loss: 5.1705\n",
      "Epoch 5539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8051 - val_loss: 5.1706\n",
      "Epoch 5540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8849 - val_loss: 5.4826\n",
      "Epoch 5541/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8854 - val_loss: 5.1644\n",
      "Epoch 5542/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8857 - val_loss: 5.1689\n",
      "Epoch 5543/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9345 - val_loss: 5.1217\n",
      "Epoch 5544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8342 - val_loss: 5.4456\n",
      "Epoch 5545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0335 - val_loss: 5.2092\n",
      "Epoch 5546/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8059 - val_loss: 5.3610\n",
      "Epoch 5547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0915 - val_loss: 5.2460\n",
      "Epoch 5548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8586 - val_loss: 5.5339\n",
      "Epoch 5549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9902 - val_loss: 5.1420\n",
      "Epoch 5550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7650 - val_loss: 5.1477\n",
      "Epoch 5551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8329 - val_loss: 5.2593\n",
      "Epoch 5552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8486 - val_loss: 5.1365\n",
      "Epoch 5553/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9857 - val_loss: 5.8569\n",
      "Epoch 5554/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.9581 - val_loss: 5.1286\n",
      "Epoch 5555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7744 - val_loss: 5.1810\n",
      "Epoch 5556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7849 - val_loss: 5.1417\n",
      "Epoch 5557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7921 - val_loss: 5.1198\n",
      "Epoch 5558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8553 - val_loss: 5.3212\n",
      "Epoch 5559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8488 - val_loss: 5.3002\n",
      "Epoch 5560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7891 - val_loss: 5.1422\n",
      "Epoch 5561/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7990 - val_loss: 5.2924\n",
      "Epoch 5562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7883 - val_loss: 5.1431\n",
      "Epoch 5563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8367 - val_loss: 5.1563\n",
      "Epoch 5564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8728 - val_loss: 5.7191\n",
      "Epoch 5565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0290 - val_loss: 5.1153\n",
      "Epoch 5566/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9786 - val_loss: 5.1659\n",
      "Epoch 5567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8820 - val_loss: 5.1752\n",
      "Epoch 5568/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9556 - val_loss: 5.1733\n",
      "Epoch 5569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8742 - val_loss: 5.2399\n",
      "Epoch 5570/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0956 - val_loss: 5.4445\n",
      "Epoch 5571/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2830 - val_loss: 5.3044\n",
      "Epoch 5572/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.1428 - val_loss: 5.2378\n",
      "Epoch 5573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8977 - val_loss: 5.2469\n",
      "Epoch 5574/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8946 - val_loss: 5.6618\n",
      "Epoch 5575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0082 - val_loss: 5.2304\n",
      "Epoch 5576/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0239 - val_loss: 5.3733\n",
      "Epoch 5577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1696 - val_loss: 5.1344\n",
      "Epoch 5578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9196 - val_loss: 5.2591\n",
      "Epoch 5579/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7832 - val_loss: 5.1560\n",
      "Epoch 5580/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8196 - val_loss: 5.1387\n",
      "Epoch 5581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8147 - val_loss: 5.1802\n",
      "Epoch 5582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7699 - val_loss: 5.2033\n",
      "Epoch 5583/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8468 - val_loss: 5.3140\n",
      "Epoch 5584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8016 - val_loss: 5.1473\n",
      "Epoch 5585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9012 - val_loss: 5.6388\n",
      "Epoch 5586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0751 - val_loss: 5.5415\n",
      "Epoch 5587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1270 - val_loss: 5.1785\n",
      "Epoch 5588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8113 - val_loss: 5.1147\n",
      "Epoch 5589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8887 - val_loss: 5.1969\n",
      "Epoch 5590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8566 - val_loss: 5.1465\n",
      "Epoch 5591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7785 - val_loss: 5.1360\n",
      "Epoch 5592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8843 - val_loss: 5.7439\n",
      "Epoch 5593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9234 - val_loss: 5.1401\n",
      "Epoch 5594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8593 - val_loss: 5.1717\n",
      "Epoch 5595/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8123 - val_loss: 5.1259\n",
      "Epoch 5596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7758 - val_loss: 5.1505\n",
      "Epoch 5597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9400 - val_loss: 5.1173\n",
      "Epoch 5598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8001 - val_loss: 5.1856\n",
      "Epoch 5599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8316 - val_loss: 5.1430\n",
      "Epoch 5600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7823 - val_loss: 5.3761\n",
      "Epoch 5601/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8170 - val_loss: 5.2419\n",
      "Epoch 5602/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8032 - val_loss: 5.3246\n",
      "Epoch 5603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7638 - val_loss: 5.2495\n",
      "Epoch 5604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8344 - val_loss: 5.2131\n",
      "Epoch 5605/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7488 - val_loss: 5.1520\n",
      "Epoch 5606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7666 - val_loss: 5.1610\n",
      "Epoch 5607/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.902 - 0s 46us/step - loss: 4.8209 - val_loss: 5.1478\n",
      "Epoch 5608/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9084 - val_loss: 5.1803\n",
      "Epoch 5609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7717 - val_loss: 5.2247\n",
      "Epoch 5610/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9004 - val_loss: 5.1837\n",
      "Epoch 5611/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9099 - val_loss: 5.1276\n",
      "Epoch 5612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9053 - val_loss: 5.1690\n",
      "Epoch 5613/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8122 - val_loss: 5.1456\n",
      "Epoch 5614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8042 - val_loss: 5.1473\n",
      "Epoch 5615/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8103 - val_loss: 5.3595\n",
      "Epoch 5616/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8029 - val_loss: 5.2016\n",
      "Epoch 5617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8705 - val_loss: 5.1044\n",
      "Epoch 5618/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7892 - val_loss: 5.1624\n",
      "Epoch 5619/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7942 - val_loss: 5.1593\n",
      "Epoch 5620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0411 - val_loss: 5.8908\n",
      "Epoch 5621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0050 - val_loss: 5.2694\n",
      "Epoch 5622/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8713 - val_loss: 5.1429\n",
      "Epoch 5623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7770 - val_loss: 5.1354\n",
      "Epoch 5624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7976 - val_loss: 5.1264\n",
      "Epoch 5625/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8093 - val_loss: 5.1644\n",
      "Epoch 5626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8643 - val_loss: 5.2121\n",
      "Epoch 5627/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8756 - val_loss: 5.2586\n",
      "Epoch 5628/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9379 - val_loss: 5.6493\n",
      "Epoch 5629/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8578 - val_loss: 5.2360\n",
      "Epoch 5630/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7748 - val_loss: 5.1463\n",
      "Epoch 5631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7791 - val_loss: 5.1024\n",
      "Epoch 5632/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8411 - val_loss: 5.6999\n",
      "Epoch 5633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9541 - val_loss: 5.1038\n",
      "Epoch 5634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9133 - val_loss: 5.1297\n",
      "Epoch 5635/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8765 - val_loss: 5.1211\n",
      "Epoch 5636/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8414 - val_loss: 5.1842\n",
      "Epoch 5637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2347 - val_loss: 5.1381\n",
      "Epoch 5638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8729 - val_loss: 5.1043\n",
      "Epoch 5639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9429 - val_loss: 5.1342\n",
      "Epoch 5640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8387 - val_loss: 5.3588\n",
      "Epoch 5641/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9531 - val_loss: 5.1467\n",
      "Epoch 5642/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8174 - val_loss: 5.2984\n",
      "Epoch 5643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8150 - val_loss: 5.2546\n",
      "Epoch 5644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8469 - val_loss: 5.1218\n",
      "Epoch 5645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7794 - val_loss: 5.1621\n",
      "Epoch 5646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7847 - val_loss: 5.1483\n",
      "Epoch 5647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7833 - val_loss: 5.1202\n",
      "Epoch 5648/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8929 - val_loss: 5.1985\n",
      "Epoch 5649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8162 - val_loss: 5.3451\n",
      "Epoch 5650/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9161 - val_loss: 5.1294\n",
      "Epoch 5651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8990 - val_loss: 5.3155\n",
      "Epoch 5652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9655 - val_loss: 5.1527\n",
      "Epoch 5653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9737 - val_loss: 5.1906\n",
      "Epoch 5654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8400 - val_loss: 5.3954\n",
      "Epoch 5655/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7959 - val_loss: 5.1120\n",
      "Epoch 5656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7597 - val_loss: 5.3576\n",
      "Epoch 5657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9456 - val_loss: 5.1045\n",
      "Epoch 5658/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8382 - val_loss: 5.1163\n",
      "Epoch 5659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8215 - val_loss: 5.2348\n",
      "Epoch 5660/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0217 - val_loss: 5.7942\n",
      "Epoch 5661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9756 - val_loss: 5.3365\n",
      "Epoch 5662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9137 - val_loss: 5.1466\n",
      "Epoch 5663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8130 - val_loss: 5.1157\n",
      "Epoch 5664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9220 - val_loss: 5.1227\n",
      "Epoch 5665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8395 - val_loss: 5.1715\n",
      "Epoch 5666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9335 - val_loss: 5.1454\n",
      "Epoch 5667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8955 - val_loss: 5.1924\n",
      "Epoch 5668/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9750 - val_loss: 5.8463\n",
      "Epoch 5669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9671 - val_loss: 5.1509\n",
      "Epoch 5670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7739 - val_loss: 5.3511\n",
      "Epoch 5671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8173 - val_loss: 5.1432\n",
      "Epoch 5672/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7643 - val_loss: 5.2074\n",
      "Epoch 5673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7937 - val_loss: 5.1702\n",
      "Epoch 5674/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7481 - val_loss: 5.1205\n",
      "Epoch 5675/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8525 - val_loss: 5.3764\n",
      "Epoch 5676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8169 - val_loss: 5.2546\n",
      "Epoch 5677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9027 - val_loss: 5.3398\n",
      "Epoch 5678/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0350 - val_loss: 5.1037\n",
      "Epoch 5679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9560 - val_loss: 5.1295\n",
      "Epoch 5680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0471 - val_loss: 5.1073\n",
      "Epoch 5681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8071 - val_loss: 5.1976\n",
      "Epoch 5682/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8256 - val_loss: 5.1640\n",
      "Epoch 5683/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0633 - val_loss: 5.4571\n",
      "Epoch 5684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9587 - val_loss: 5.1551\n",
      "Epoch 5685/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9076 - val_loss: 5.1088\n",
      "Epoch 5686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8990 - val_loss: 5.1487\n",
      "Epoch 5687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1711 - val_loss: 5.2163\n",
      "Epoch 5688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9306 - val_loss: 5.1401\n",
      "Epoch 5689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7631 - val_loss: 5.2336\n",
      "Epoch 5690/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7801 - val_loss: 5.1690\n",
      "Epoch 5691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7868 - val_loss: 5.1870\n",
      "Epoch 5692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9193 - val_loss: 5.7168\n",
      "Epoch 5693/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8088 - val_loss: 5.1997\n",
      "Epoch 5694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0005 - val_loss: 5.1233\n",
      "Epoch 5695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8338 - val_loss: 5.1414\n",
      "Epoch 5696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8531 - val_loss: 5.1739\n",
      "Epoch 5697/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8251 - val_loss: 5.1672\n",
      "Epoch 5698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8131 - val_loss: 5.3254\n",
      "Epoch 5699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7658 - val_loss: 5.1460\n",
      "Epoch 5700/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7838 - val_loss: 5.2159\n",
      "Epoch 5701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7575 - val_loss: 5.3693\n",
      "Epoch 5702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9526 - val_loss: 5.2528\n",
      "Epoch 5703/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9634 - val_loss: 5.4185\n",
      "Epoch 5704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1985 - val_loss: 5.1288\n",
      "Epoch 5705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8437 - val_loss: 5.1153\n",
      "Epoch 5706/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7876 - val_loss: 5.2870\n",
      "Epoch 5707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8216 - val_loss: 5.1221\n",
      "Epoch 5708/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7762 - val_loss: 5.1419\n",
      "Epoch 5709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8867 - val_loss: 5.4640\n",
      "Epoch 5710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0119 - val_loss: 5.1684\n",
      "Epoch 5711/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7898 - val_loss: 5.1373\n",
      "Epoch 5712/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7986 - val_loss: 5.1449\n",
      "Epoch 5713/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7833 - val_loss: 5.1421\n",
      "Epoch 5714/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7755 - val_loss: 5.1315\n",
      "Epoch 5715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7743 - val_loss: 5.2042\n",
      "Epoch 5716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7663 - val_loss: 5.1403\n",
      "Epoch 5717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7642 - val_loss: 5.1489\n",
      "Epoch 5718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7877 - val_loss: 5.2235\n",
      "Epoch 5719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1928 - val_loss: 5.2953\n",
      "Epoch 5720/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8651 - val_loss: 5.1305\n",
      "Epoch 5721/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8607 - val_loss: 5.8484\n",
      "Epoch 5722/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1265 - val_loss: 5.4545\n",
      "Epoch 5723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3061 - val_loss: 5.2379\n",
      "Epoch 5724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9213 - val_loss: 5.1228\n",
      "Epoch 5725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8115 - val_loss: 5.1409\n",
      "Epoch 5726/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7831 - val_loss: 5.1051\n",
      "Epoch 5727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7536 - val_loss: 5.1112\n",
      "Epoch 5728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8274 - val_loss: 5.4201\n",
      "Epoch 5729/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9283 - val_loss: 5.1117\n",
      "Epoch 5730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8270 - val_loss: 5.1674\n",
      "Epoch 5731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8608 - val_loss: 5.7818\n",
      "Epoch 5732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9934 - val_loss: 5.1261\n",
      "Epoch 5733/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7913 - val_loss: 5.0941\n",
      "Epoch 5734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7920 - val_loss: 5.2130\n",
      "Epoch 5735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8236 - val_loss: 5.2058\n",
      "Epoch 5736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7848 - val_loss: 5.2336\n",
      "Epoch 5737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8698 - val_loss: 5.3579\n",
      "Epoch 5738/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8708 - val_loss: 5.1398\n",
      "Epoch 5739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7838 - val_loss: 5.3036\n",
      "Epoch 5740/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0256 - val_loss: 5.7405\n",
      "Epoch 5741/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9898 - val_loss: 5.1516\n",
      "Epoch 5742/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8012 - val_loss: 5.2586\n",
      "Epoch 5743/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9909 - val_loss: 5.5037\n",
      "Epoch 5744/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1886 - val_loss: 5.1970\n",
      "Epoch 5745/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7984 - val_loss: 5.1613\n",
      "Epoch 5746/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0157 - val_loss: 5.1135\n",
      "Epoch 5747/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0218 - val_loss: 5.2824\n",
      "Epoch 5748/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8183 - val_loss: 5.1061\n",
      "Epoch 5749/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7679 - val_loss: 5.0850\n",
      "Epoch 5750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8103 - val_loss: 5.3751\n",
      "Epoch 5751/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8485 - val_loss: 5.1676\n",
      "Epoch 5752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8226 - val_loss: 5.0928\n",
      "Epoch 5753/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7702 - val_loss: 5.0896\n",
      "Epoch 5754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8266 - val_loss: 5.2778\n",
      "Epoch 5755/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0846 - val_loss: 5.0946\n",
      "Epoch 5756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9640 - val_loss: 5.6999\n",
      "Epoch 5757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0312 - val_loss: 5.4147\n",
      "Epoch 5758/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1029 - val_loss: 5.1714\n",
      "Epoch 5759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7894 - val_loss: 5.8374\n",
      "Epoch 5760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9246 - val_loss: 5.1049\n",
      "Epoch 5761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8239 - val_loss: 5.0695\n",
      "Epoch 5762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8752 - val_loss: 5.3481\n",
      "Epoch 5763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8280 - val_loss: 5.1290\n",
      "Epoch 5764/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9530 - val_loss: 5.1187\n",
      "Epoch 5765/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7984 - val_loss: 5.2044\n",
      "Epoch 5766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7808 - val_loss: 5.5076\n",
      "Epoch 5767/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8803 - val_loss: 5.3106\n",
      "Epoch 5768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8168 - val_loss: 5.2771\n",
      "Epoch 5769/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9324 - val_loss: 5.2079\n",
      "Epoch 5770/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8438 - val_loss: 5.1301\n",
      "Epoch 5771/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8932 - val_loss: 5.1686\n",
      "Epoch 5772/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8202 - val_loss: 5.1063\n",
      "Epoch 5773/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9339 - val_loss: 5.2532\n",
      "Epoch 5774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7945 - val_loss: 5.1327\n",
      "Epoch 5775/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7627 - val_loss: 5.1116\n",
      "Epoch 5776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8444 - val_loss: 5.1237\n",
      "Epoch 5777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8247 - val_loss: 5.1461\n",
      "Epoch 5778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1706 - val_loss: 5.2436\n",
      "Epoch 5779/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8200 - val_loss: 5.1413\n",
      "Epoch 5780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7472 - val_loss: 5.1008\n",
      "Epoch 5781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8419 - val_loss: 5.2372\n",
      "Epoch 5782/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8136 - val_loss: 5.0848\n",
      "Epoch 5783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8336 - val_loss: 5.0970\n",
      "Epoch 5784/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8151 - val_loss: 5.1015\n",
      "Epoch 5785/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8389 - val_loss: 5.1283\n",
      "Epoch 5786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8144 - val_loss: 5.2180\n",
      "Epoch 5787/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7642 - val_loss: 5.1785\n",
      "Epoch 5788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9337 - val_loss: 5.1843\n",
      "Epoch 5789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8335 - val_loss: 5.2171\n",
      "Epoch 5790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8405 - val_loss: 5.2034\n",
      "Epoch 5791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7979 - val_loss: 5.1103\n",
      "Epoch 5792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8676 - val_loss: 5.1799\n",
      "Epoch 5793/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8686 - val_loss: 5.3268\n",
      "Epoch 5794/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8855 - val_loss: 5.2632\n",
      "Epoch 5795/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9572 - val_loss: 5.1183\n",
      "Epoch 5796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9566 - val_loss: 5.2279\n",
      "Epoch 5797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8415 - val_loss: 5.1465\n",
      "Epoch 5798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7942 - val_loss: 5.2597\n",
      "Epoch 5799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0121 - val_loss: 5.3124\n",
      "Epoch 5800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7892 - val_loss: 5.1662\n",
      "Epoch 5801/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8403 - val_loss: 5.3880\n",
      "Epoch 5802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8947 - val_loss: 5.1706\n",
      "Epoch 5803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8167 - val_loss: 5.1153\n",
      "Epoch 5804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7589 - val_loss: 5.1179\n",
      "Epoch 5805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8021 - val_loss: 5.1029\n",
      "Epoch 5806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8013 - val_loss: 5.1235\n",
      "Epoch 5807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8039 - val_loss: 5.1392\n",
      "Epoch 5808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8321 - val_loss: 5.1327\n",
      "Epoch 5809/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7699 - val_loss: 5.1269\n",
      "Epoch 5810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7751 - val_loss: 5.1692\n",
      "Epoch 5811/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8461 - val_loss: 5.1293\n",
      "Epoch 5812/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7337 - val_loss: 5.3025\n",
      "Epoch 5813/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7986 - val_loss: 5.1111\n",
      "Epoch 5814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7606 - val_loss: 5.2102\n",
      "Epoch 5815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7736 - val_loss: 5.3005\n",
      "Epoch 5816/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8614 - val_loss: 5.3391\n",
      "Epoch 5817/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8111 - val_loss: 5.1058\n",
      "Epoch 5818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8752 - val_loss: 5.3910\n",
      "Epoch 5819/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8170 - val_loss: 5.3143\n",
      "Epoch 5820/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8645 - val_loss: 5.1618\n",
      "Epoch 5821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8833 - val_loss: 5.2582\n",
      "Epoch 5822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7686 - val_loss: 5.6116\n",
      "Epoch 5823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9143 - val_loss: 5.6337\n",
      "Epoch 5824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9826 - val_loss: 5.1086\n",
      "Epoch 5825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8920 - val_loss: 5.1692\n",
      "Epoch 5826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7953 - val_loss: 5.1855\n",
      "Epoch 5827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8196 - val_loss: 5.2136\n",
      "Epoch 5828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7790 - val_loss: 5.1418\n",
      "Epoch 5829/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7685 - val_loss: 5.0941\n",
      "Epoch 5830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0719 - val_loss: 5.0803\n",
      "Epoch 5831/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9815 - val_loss: 5.1031\n",
      "Epoch 5832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8394 - val_loss: 5.0788\n",
      "Epoch 5833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7921 - val_loss: 5.0765\n",
      "Epoch 5834/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8606 - val_loss: 5.6057\n",
      "Epoch 5835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1502 - val_loss: 5.1453\n",
      "Epoch 5836/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7497 - val_loss: 5.1400\n",
      "Epoch 5837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7374 - val_loss: 5.0904\n",
      "Epoch 5838/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7803 - val_loss: 5.1111\n",
      "Epoch 5839/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7729 - val_loss: 5.1145\n",
      "Epoch 5840/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8524 - val_loss: 5.1154\n",
      "Epoch 5841/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9053 - val_loss: 5.1071\n",
      "Epoch 5842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7871 - val_loss: 5.0805\n",
      "Epoch 5843/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9437 - val_loss: 5.4045\n",
      "Epoch 5844/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8581 - val_loss: 5.1367\n",
      "Epoch 5845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7311 - val_loss: 5.6244\n",
      "Epoch 5846/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9322 - val_loss: 6.0883\n",
      "Epoch 5847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0307 - val_loss: 5.1313\n",
      "Epoch 5848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7693 - val_loss: 5.2164\n",
      "Epoch 5849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7815 - val_loss: 5.1743\n",
      "Epoch 5850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7542 - val_loss: 5.1282\n",
      "Epoch 5851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9338 - val_loss: 5.1212\n",
      "Epoch 5852/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7340 - val_loss: 5.3210\n",
      "Epoch 5853/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8161 - val_loss: 5.1207\n",
      "Epoch 5854/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8253 - val_loss: 5.0988\n",
      "Epoch 5855/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8469 - val_loss: 5.1651\n",
      "Epoch 5856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7565 - val_loss: 5.1315\n",
      "Epoch 5857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8356 - val_loss: 5.0856\n",
      "Epoch 5858/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.9846 - val_loss: 5.2012\n",
      "Epoch 5859/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8779 - val_loss: 5.0914\n",
      "Epoch 5860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7906 - val_loss: 5.1044\n",
      "Epoch 5861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8280 - val_loss: 5.3086\n",
      "Epoch 5862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7797 - val_loss: 5.0934\n",
      "Epoch 5863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8334 - val_loss: 5.1215\n",
      "Epoch 5864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7462 - val_loss: 5.1336\n",
      "Epoch 5865/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7965 - val_loss: 5.1107\n",
      "Epoch 5866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8377 - val_loss: 5.1628\n",
      "Epoch 5867/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.881 - 0s 46us/step - loss: 4.9345 - val_loss: 5.1094\n",
      "Epoch 5868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7762 - val_loss: 5.1216\n",
      "Epoch 5869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7740 - val_loss: 5.2990\n",
      "Epoch 5870/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8553 - val_loss: 5.1080\n",
      "Epoch 5871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8560 - val_loss: 5.4957\n",
      "Epoch 5872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9362 - val_loss: 5.5987\n",
      "Epoch 5873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9290 - val_loss: 5.0964\n",
      "Epoch 5874/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8647 - val_loss: 5.1450\n",
      "Epoch 5875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7871 - val_loss: 5.0949\n",
      "Epoch 5876/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8877 - val_loss: 5.1830\n",
      "Epoch 5877/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9674 - val_loss: 5.1410\n",
      "Epoch 5878/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8563 - val_loss: 5.1296\n",
      "Epoch 5879/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7807 - val_loss: 5.2404\n",
      "Epoch 5880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7726 - val_loss: 5.2600\n",
      "Epoch 5881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7651 - val_loss: 5.0917\n",
      "Epoch 5882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7805 - val_loss: 5.2410\n",
      "Epoch 5883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8223 - val_loss: 5.2723\n",
      "Epoch 5884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8420 - val_loss: 5.4168\n",
      "Epoch 5885/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8259 - val_loss: 5.1512\n",
      "Epoch 5886/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9305 - val_loss: 5.1176\n",
      "Epoch 5887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7721 - val_loss: 5.2703\n",
      "Epoch 5888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7761 - val_loss: 5.1133\n",
      "Epoch 5889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8324 - val_loss: 5.1841\n",
      "Epoch 5890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7670 - val_loss: 5.1394\n",
      "Epoch 5891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7586 - val_loss: 5.3840\n",
      "Epoch 5892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9196 - val_loss: 5.1095\n",
      "Epoch 5893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9754 - val_loss: 5.4752\n",
      "Epoch 5894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7795 - val_loss: 5.1340\n",
      "Epoch 5895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8011 - val_loss: 5.3769\n",
      "Epoch 5896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7982 - val_loss: 5.1988\n",
      "Epoch 5897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7517 - val_loss: 5.1386\n",
      "Epoch 5898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7944 - val_loss: 5.0964\n",
      "Epoch 5899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7841 - val_loss: 5.0701\n",
      "Epoch 5900/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8469 - val_loss: 5.1827\n",
      "Epoch 5901/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0797 - val_loss: 5.4963\n",
      "Epoch 5902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0904 - val_loss: 5.2222\n",
      "Epoch 5903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8195 - val_loss: 5.1206\n",
      "Epoch 5904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8022 - val_loss: 5.1069\n",
      "Epoch 5905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8001 - val_loss: 5.2717\n",
      "Epoch 5906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8251 - val_loss: 5.1017\n",
      "Epoch 5907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8179 - val_loss: 5.2069\n",
      "Epoch 5908/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7697 - val_loss: 5.1948\n",
      "Epoch 5909/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8074 - val_loss: 5.1706\n",
      "Epoch 5910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1522 - val_loss: 5.0665\n",
      "Epoch 5911/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7329 - val_loss: 5.1573\n",
      "Epoch 5912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8565 - val_loss: 5.4767\n",
      "Epoch 5913/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9892 - val_loss: 5.1197\n",
      "Epoch 5914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8497 - val_loss: 5.5033\n",
      "Epoch 5915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9828 - val_loss: 5.2581\n",
      "Epoch 5916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7921 - val_loss: 5.1738\n",
      "Epoch 5917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7904 - val_loss: 5.0756\n",
      "Epoch 5918/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7486 - val_loss: 5.1110\n",
      "Epoch 5919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7690 - val_loss: 5.1447\n",
      "Epoch 5920/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8799 - val_loss: 5.4921\n",
      "Epoch 5921/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8898 - val_loss: 5.0891\n",
      "Epoch 5922/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8094 - val_loss: 5.3592\n",
      "Epoch 5923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8049 - val_loss: 5.1107\n",
      "Epoch 5924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8252 - val_loss: 5.2923\n",
      "Epoch 5925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7972 - val_loss: 5.1969\n",
      "Epoch 5926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9848 - val_loss: 5.1500\n",
      "Epoch 5927/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9222 - val_loss: 5.0776\n",
      "Epoch 5928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0632 - val_loss: 5.1959\n",
      "Epoch 5929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9217 - val_loss: 5.0965\n",
      "Epoch 5930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7808 - val_loss: 5.0877\n",
      "Epoch 5931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7419 - val_loss: 5.1551\n",
      "Epoch 5932/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7187 - val_loss: 5.1377\n",
      "Epoch 5933/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7163 - val_loss: 5.0737\n",
      "Epoch 5934/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8424 - val_loss: 5.1656\n",
      "Epoch 5935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8366 - val_loss: 5.1263\n",
      "Epoch 5936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7952 - val_loss: 5.2627\n",
      "Epoch 5937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9025 - val_loss: 5.1749\n",
      "Epoch 5938/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7928 - val_loss: 5.1361\n",
      "Epoch 5939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0027 - val_loss: 5.1396\n",
      "Epoch 5940/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7857 - val_loss: 5.0938\n",
      "Epoch 5941/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7609 - val_loss: 5.2585\n",
      "Epoch 5942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9004 - val_loss: 5.7272\n",
      "Epoch 5943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9532 - val_loss: 5.1503\n",
      "Epoch 5944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8347 - val_loss: 5.1125\n",
      "Epoch 5945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8026 - val_loss: 5.2875\n",
      "Epoch 5946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9936 - val_loss: 5.0766\n",
      "Epoch 5947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8363 - val_loss: 5.1051\n",
      "Epoch 5948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7633 - val_loss: 5.1312\n",
      "Epoch 5949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7729 - val_loss: 5.1194\n",
      "Epoch 5950/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7267 - val_loss: 5.2258\n",
      "Epoch 5951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8259 - val_loss: 5.1904\n",
      "Epoch 5952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8615 - val_loss: 5.2955\n",
      "Epoch 5953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7632 - val_loss: 5.1181\n",
      "Epoch 5954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8723 - val_loss: 5.2091\n",
      "Epoch 5955/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9800 - val_loss: 5.1611\n",
      "Epoch 5956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9018 - val_loss: 5.3225\n",
      "Epoch 5957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9841 - val_loss: 5.6780\n",
      "Epoch 5958/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8742 - val_loss: 5.1218\n",
      "Epoch 5959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0714 - val_loss: 5.1097\n",
      "Epoch 5960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8192 - val_loss: 5.0880\n",
      "Epoch 5961/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8584 - val_loss: 5.5173\n",
      "Epoch 5962/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0574 - val_loss: 5.3219\n",
      "Epoch 5963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8851 - val_loss: 5.0836\n",
      "Epoch 5964/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.7759 - val_loss: 5.0727\n",
      "Epoch 5965/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.9569 - val_loss: 5.1270\n",
      "Epoch 5966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0366 - val_loss: 5.0768\n",
      "Epoch 5967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8585 - val_loss: 5.0730\n",
      "Epoch 5968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7886 - val_loss: 5.1421\n",
      "Epoch 5969/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0597 - val_loss: 5.2106\n",
      "Epoch 5970/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7785 - val_loss: 5.0752\n",
      "Epoch 5971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7814 - val_loss: 5.3599\n",
      "Epoch 5972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9574 - val_loss: 5.1420\n",
      "Epoch 5973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8678 - val_loss: 5.0665\n",
      "Epoch 5974/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7249 - val_loss: 5.0820\n",
      "Epoch 5975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7488 - val_loss: 5.1179\n",
      "Epoch 5976/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7961 - val_loss: 5.0953\n",
      "Epoch 5977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8366 - val_loss: 5.0941\n",
      "Epoch 5978/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7836 - val_loss: 5.0797\n",
      "Epoch 5979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7984 - val_loss: 5.0840\n",
      "Epoch 5980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8039 - val_loss: 5.2516\n",
      "Epoch 5981/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1123 - val_loss: 5.2309\n",
      "Epoch 5982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2682 - val_loss: 5.5855\n",
      "Epoch 5983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8072 - val_loss: 5.0959\n",
      "Epoch 5984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8274 - val_loss: 5.1684\n",
      "Epoch 5985/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7867 - val_loss: 5.1075\n",
      "Epoch 5986/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7859 - val_loss: 5.2736\n",
      "Epoch 5987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8541 - val_loss: 5.1248\n",
      "Epoch 5988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8383 - val_loss: 5.6571\n",
      "Epoch 5989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9677 - val_loss: 5.1096\n",
      "Epoch 5990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8335 - val_loss: 5.1032\n",
      "Epoch 5991/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8248 - val_loss: 5.1016\n",
      "Epoch 5992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9580 - val_loss: 5.3443\n",
      "Epoch 5993/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8837 - val_loss: 5.4110\n",
      "Epoch 5994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8393 - val_loss: 5.1590\n",
      "Epoch 5995/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8225 - val_loss: 5.3765\n",
      "Epoch 5996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7898 - val_loss: 5.4400\n",
      "Epoch 5997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8546 - val_loss: 5.0900\n",
      "Epoch 5998/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8246 - val_loss: 5.2305\n",
      "Epoch 5999/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0088 - val_loss: 5.2124\n",
      "Epoch 6000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9763 - val_loss: 5.1819\n",
      "Epoch 6001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7742 - val_loss: 5.1324\n",
      "Epoch 6002/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9177 - val_loss: 5.0928\n",
      "Epoch 6003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7341 - val_loss: 5.0808\n",
      "Epoch 6004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8242 - val_loss: 5.1169\n",
      "Epoch 6005/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7944 - val_loss: 5.1147\n",
      "Epoch 6006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7935 - val_loss: 5.2489\n",
      "Epoch 6007/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9020 - val_loss: 5.3325\n",
      "Epoch 6008/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0633 - val_loss: 5.2612\n",
      "Epoch 6009/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7515 - val_loss: 5.2816\n",
      "Epoch 6010/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.9828 - val_loss: 5.5770\n",
      "Epoch 6011/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8191 - val_loss: 5.1779\n",
      "Epoch 6012/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7536 - val_loss: 5.1891\n",
      "Epoch 6013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9646 - val_loss: 5.5784\n",
      "Epoch 6014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1255 - val_loss: 5.6263\n",
      "Epoch 6015/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1196 - val_loss: 5.1480\n",
      "Epoch 6016/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8909 - val_loss: 5.4521\n",
      "Epoch 6017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0619 - val_loss: 5.1928\n",
      "Epoch 6018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7726 - val_loss: 5.0841\n",
      "Epoch 6019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8190 - val_loss: 5.0963\n",
      "Epoch 6020/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8272 - val_loss: 5.1103\n",
      "Epoch 6021/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8005 - val_loss: 5.1278\n",
      "Epoch 6022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7396 - val_loss: 5.1225\n",
      "Epoch 6023/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7699 - val_loss: 5.0938\n",
      "Epoch 6024/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8328 - val_loss: 5.2882\n",
      "Epoch 6025/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8244 - val_loss: 5.1104\n",
      "Epoch 6026/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8032 - val_loss: 5.2614\n",
      "Epoch 6027/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7401 - val_loss: 5.0804\n",
      "Epoch 6028/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8885 - val_loss: 5.1605\n",
      "Epoch 6029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9605 - val_loss: 6.2178\n",
      "Epoch 6030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3503 - val_loss: 5.0672\n",
      "Epoch 6031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1631 - val_loss: 5.5241\n",
      "Epoch 6032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9532 - val_loss: 5.2530\n",
      "Epoch 6033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0321 - val_loss: 5.8044\n",
      "Epoch 6034/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9677 - val_loss: 5.2092\n",
      "Epoch 6035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8752 - val_loss: 5.1168\n",
      "Epoch 6036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7956 - val_loss: 5.0712\n",
      "Epoch 6037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7816 - val_loss: 5.3190\n",
      "Epoch 6038/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8655 - val_loss: 5.0862\n",
      "Epoch 6039/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7751 - val_loss: 5.6107\n",
      "Epoch 6040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9196 - val_loss: 5.1190\n",
      "Epoch 6041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8750 - val_loss: 5.2027\n",
      "Epoch 6042/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8619 - val_loss: 5.1182\n",
      "Epoch 6043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7571 - val_loss: 5.0978\n",
      "Epoch 6044/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7314 - val_loss: 5.1446\n",
      "Epoch 6045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8043 - val_loss: 5.0827\n",
      "Epoch 6046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8454 - val_loss: 5.0795\n",
      "Epoch 6047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8082 - val_loss: 5.1113\n",
      "Epoch 6048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9338 - val_loss: 5.1716\n",
      "Epoch 6049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9341 - val_loss: 5.1306\n",
      "Epoch 6050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9669 - val_loss: 5.4434\n",
      "Epoch 6051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7659 - val_loss: 5.1368\n",
      "Epoch 6052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7419 - val_loss: 5.1038\n",
      "Epoch 6053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8443 - val_loss: 5.2266\n",
      "Epoch 6054/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9581 - val_loss: 5.2261\n",
      "Epoch 6055/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9667 - val_loss: 5.3664\n",
      "Epoch 6056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2578 - val_loss: 5.3483\n",
      "Epoch 6057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8945 - val_loss: 5.0773\n",
      "Epoch 6058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7573 - val_loss: 5.0777\n",
      "Epoch 6059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8115 - val_loss: 5.2714\n",
      "Epoch 6060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8092 - val_loss: 5.2884\n",
      "Epoch 6061/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8239 - val_loss: 5.4140\n",
      "Epoch 6062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9169 - val_loss: 5.1561\n",
      "Epoch 6063/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9313 - val_loss: 5.4271\n",
      "Epoch 6064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8747 - val_loss: 5.1238\n",
      "Epoch 6065/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8496 - val_loss: 5.1638\n",
      "Epoch 6066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9059 - val_loss: 5.9719\n",
      "Epoch 6067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1242 - val_loss: 5.1636\n",
      "Epoch 6068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9159 - val_loss: 5.4204\n",
      "Epoch 6069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0188 - val_loss: 5.1124\n",
      "Epoch 6070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8489 - val_loss: 5.2517\n",
      "Epoch 6071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8652 - val_loss: 5.0913\n",
      "Epoch 6072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7386 - val_loss: 5.0943\n",
      "Epoch 6073/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8548 - val_loss: 5.3829\n",
      "Epoch 6074/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8262 - val_loss: 5.0843\n",
      "Epoch 6075/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8947 - val_loss: 5.0931\n",
      "Epoch 6076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7683 - val_loss: 5.1099\n",
      "Epoch 6077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7709 - val_loss: 5.3397\n",
      "Epoch 6078/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9149 - val_loss: 5.6704\n",
      "Epoch 6079/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3234 - val_loss: 5.1635\n",
      "Epoch 6080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9115 - val_loss: 5.4450\n",
      "Epoch 6081/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7647 - val_loss: 5.1331\n",
      "Epoch 6082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8677 - val_loss: 5.0999\n",
      "Epoch 6083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7958 - val_loss: 5.1927\n",
      "Epoch 6084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7871 - val_loss: 5.2803\n",
      "Epoch 6085/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8145 - val_loss: 5.0827\n",
      "Epoch 6086/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7755 - val_loss: 5.1580\n",
      "Epoch 6087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8845 - val_loss: 5.0968\n",
      "Epoch 6088/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7635 - val_loss: 5.1862\n",
      "Epoch 6089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7382 - val_loss: 5.0745\n",
      "Epoch 6090/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7319 - val_loss: 5.1575\n",
      "Epoch 6091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7352 - val_loss: 5.1168\n",
      "Epoch 6092/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8024 - val_loss: 5.1769\n",
      "Epoch 6093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8104 - val_loss: 5.3051\n",
      "Epoch 6094/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8464 - val_loss: 5.0852\n",
      "Epoch 6095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9269 - val_loss: 5.5227\n",
      "Epoch 6096/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9447 - val_loss: 5.1091\n",
      "Epoch 6097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8361 - val_loss: 5.1628\n",
      "Epoch 6098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8738 - val_loss: 5.1610\n",
      "Epoch 6099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7878 - val_loss: 5.0981\n",
      "Epoch 6100/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7813 - val_loss: 5.1077\n",
      "Epoch 6101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7926 - val_loss: 5.2210\n",
      "Epoch 6102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8692 - val_loss: 5.5962\n",
      "Epoch 6103/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8702 - val_loss: 5.0709\n",
      "Epoch 6104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6996 - val_loss: 5.4583\n",
      "Epoch 6105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9712 - val_loss: 5.2678\n",
      "Epoch 6106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9915 - val_loss: 5.0873\n",
      "Epoch 6107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9056 - val_loss: 5.2944\n",
      "Epoch 6108/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.8923 - val_loss: 5.1606\n",
      "Epoch 6109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7425 - val_loss: 5.0527\n",
      "Epoch 6110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7297 - val_loss: 5.0755\n",
      "Epoch 6111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8131 - val_loss: 5.4701\n",
      "Epoch 6112/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8589 - val_loss: 5.7115\n",
      "Epoch 6113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4006 - val_loss: 5.0627\n",
      "Epoch 6114/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8163 - val_loss: 5.0706\n",
      "Epoch 6115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8096 - val_loss: 5.1283\n",
      "Epoch 6116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9025 - val_loss: 5.3107\n",
      "Epoch 6117/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9089 - val_loss: 5.2538\n",
      "Epoch 6118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8674 - val_loss: 5.1782\n",
      "Epoch 6119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7798 - val_loss: 5.0950\n",
      "Epoch 6120/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.7706 - val_loss: 5.0938\n",
      "Epoch 6121/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7310 - val_loss: 5.0823\n",
      "Epoch 6122/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8058 - val_loss: 5.1524\n",
      "Epoch 6123/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9775 - val_loss: 5.2936\n",
      "Epoch 6124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7347 - val_loss: 5.0712\n",
      "Epoch 6125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8019 - val_loss: 5.0782\n",
      "Epoch 6126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8240 - val_loss: 5.1733\n",
      "Epoch 6127/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8629 - val_loss: 5.0921\n",
      "Epoch 6128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7429 - val_loss: 5.1527\n",
      "Epoch 6129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7039 - val_loss: 5.0715\n",
      "Epoch 6130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7118 - val_loss: 5.0914\n",
      "Epoch 6131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7500 - val_loss: 5.4167\n",
      "Epoch 6132/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0967 - val_loss: 5.2479\n",
      "Epoch 6133/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9507 - val_loss: 5.1264\n",
      "Epoch 6134/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7243 - val_loss: 5.0950\n",
      "Epoch 6135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7584 - val_loss: 5.0880\n",
      "Epoch 6136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7779 - val_loss: 5.0688\n",
      "Epoch 6137/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7497 - val_loss: 5.0779\n",
      "Epoch 6138/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8903 - val_loss: 5.5122\n",
      "Epoch 6139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0074 - val_loss: 5.1023\n",
      "Epoch 6140/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.833 - 0s 46us/step - loss: 4.9607 - val_loss: 5.0948\n",
      "Epoch 6141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7756 - val_loss: 5.1114\n",
      "Epoch 6142/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9556 - val_loss: 5.0545\n",
      "Epoch 6143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8702 - val_loss: 5.0716\n",
      "Epoch 6144/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8648 - val_loss: 5.1186\n",
      "Epoch 6145/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7752 - val_loss: 5.3405\n",
      "Epoch 6146/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.042 - 0s 46us/step - loss: 4.8529 - val_loss: 5.1008\n",
      "Epoch 6147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8821 - val_loss: 5.2424\n",
      "Epoch 6148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7026 - val_loss: 5.2186\n",
      "Epoch 6149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7848 - val_loss: 5.0762\n",
      "Epoch 6150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7938 - val_loss: 5.2503\n",
      "Epoch 6151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8393 - val_loss: 5.1721\n",
      "Epoch 6152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8381 - val_loss: 5.0945\n",
      "Epoch 6153/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7883 - val_loss: 5.0769\n",
      "Epoch 6154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8535 - val_loss: 5.2701\n",
      "Epoch 6155/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7190 - val_loss: 5.1079\n",
      "Epoch 6156/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7702 - val_loss: 5.0723\n",
      "Epoch 6157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8879 - val_loss: 5.2758\n",
      "Epoch 6158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7425 - val_loss: 5.6175\n",
      "Epoch 6159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3541 - val_loss: 5.0691\n",
      "Epoch 6160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9951 - val_loss: 5.1386\n",
      "Epoch 6161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0063 - val_loss: 5.1533\n",
      "Epoch 6162/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8399 - val_loss: 5.1061\n",
      "Epoch 6163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7115 - val_loss: 5.1065\n",
      "Epoch 6164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8438 - val_loss: 5.1189\n",
      "Epoch 6165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8000 - val_loss: 5.1614\n",
      "Epoch 6166/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0329 - val_loss: 5.1137\n",
      "Epoch 6167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7510 - val_loss: 5.0894\n",
      "Epoch 6168/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.8469 - val_loss: 5.1109\n",
      "Epoch 6169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3573 - val_loss: 5.3867\n",
      "Epoch 6170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6980 - val_loss: 5.1760\n",
      "Epoch 6171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9066 - val_loss: 5.9060\n",
      "Epoch 6172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8949 - val_loss: 5.0741\n",
      "Epoch 6173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7211 - val_loss: 5.0758\n",
      "Epoch 6174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7953 - val_loss: 5.3517\n",
      "Epoch 6175/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9197 - val_loss: 5.1412\n",
      "Epoch 6176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7572 - val_loss: 5.5444\n",
      "Epoch 6177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8279 - val_loss: 5.1465\n",
      "Epoch 6178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7100 - val_loss: 5.4194\n",
      "Epoch 6179/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8218 - val_loss: 5.0732\n",
      "Epoch 6180/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8150 - val_loss: 5.1049\n",
      "Epoch 6181/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7831 - val_loss: 5.1089\n",
      "Epoch 6182/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8787 - val_loss: 5.0548\n",
      "Epoch 6183/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7520 - val_loss: 5.0803\n",
      "Epoch 6184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8332 - val_loss: 5.0940\n",
      "Epoch 6185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7436 - val_loss: 5.1113\n",
      "Epoch 6186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7804 - val_loss: 5.2089\n",
      "Epoch 6187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7183 - val_loss: 5.2222\n",
      "Epoch 6188/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7464 - val_loss: 5.1821\n",
      "Epoch 6189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9230 - val_loss: 5.0709\n",
      "Epoch 6190/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7438 - val_loss: 5.0925\n",
      "Epoch 6191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7424 - val_loss: 5.1932\n",
      "Epoch 6192/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0527 - val_loss: 5.1307\n",
      "Epoch 6193/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8283 - val_loss: 5.3167\n",
      "Epoch 6194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7613 - val_loss: 5.1812\n",
      "Epoch 6195/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8147 - val_loss: 5.2175\n",
      "Epoch 6196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7923 - val_loss: 5.0708\n",
      "Epoch 6197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8216 - val_loss: 5.0425\n",
      "Epoch 6198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8419 - val_loss: 5.0606\n",
      "Epoch 6199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7615 - val_loss: 5.1857\n",
      "Epoch 6200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7566 - val_loss: 5.0722\n",
      "Epoch 6201/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7393 - val_loss: 5.0647\n",
      "Epoch 6202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7532 - val_loss: 5.0853\n",
      "Epoch 6203/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9022 - val_loss: 5.0964\n",
      "Epoch 6204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0962 - val_loss: 5.6490\n",
      "Epoch 6205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0095 - val_loss: 5.0801\n",
      "Epoch 6206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7684 - val_loss: 5.1034\n",
      "Epoch 6207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7902 - val_loss: 5.0585\n",
      "Epoch 6208/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7377 - val_loss: 5.2469\n",
      "Epoch 6209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9238 - val_loss: 5.0649\n",
      "Epoch 6210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9400 - val_loss: 5.0738\n",
      "Epoch 6211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8992 - val_loss: 5.0941\n",
      "Epoch 6212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7852 - val_loss: 5.0804\n",
      "Epoch 6213/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7459 - val_loss: 5.2002\n",
      "Epoch 6214/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7308 - val_loss: 5.3914\n",
      "Epoch 6215/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8694 - val_loss: 5.1077\n",
      "Epoch 6216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7835 - val_loss: 5.0863\n",
      "Epoch 6217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8277 - val_loss: 5.2101\n",
      "Epoch 6218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7706 - val_loss: 5.1730\n",
      "Epoch 6219/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8078 - val_loss: 5.0870\n",
      "Epoch 6220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7335 - val_loss: 5.1842\n",
      "Epoch 6221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7595 - val_loss: 5.2823\n",
      "Epoch 6222/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8369 - val_loss: 5.0411\n",
      "Epoch 6223/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7609 - val_loss: 5.0845\n",
      "Epoch 6224/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7745 - val_loss: 5.0439\n",
      "Epoch 6225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7319 - val_loss: 5.0701\n",
      "Epoch 6226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7720 - val_loss: 5.1144\n",
      "Epoch 6227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7665 - val_loss: 5.0647\n",
      "Epoch 6228/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7417 - val_loss: 5.1196\n",
      "Epoch 6229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7808 - val_loss: 5.0799\n",
      "Epoch 6230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7821 - val_loss: 5.2166\n",
      "Epoch 6231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8620 - val_loss: 5.1363\n",
      "Epoch 6232/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9168 - val_loss: 5.0541\n",
      "Epoch 6233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7457 - val_loss: 5.0511\n",
      "Epoch 6234/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9663 - val_loss: 5.2070\n",
      "Epoch 6235/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1640 - val_loss: 5.1661\n",
      "Epoch 6236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7429 - val_loss: 5.0467\n",
      "Epoch 6237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7919 - val_loss: 5.0447\n",
      "Epoch 6238/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7798 - val_loss: 5.2682\n",
      "Epoch 6239/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1094 - val_loss: 5.2113\n",
      "Epoch 6240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8779 - val_loss: 5.0950\n",
      "Epoch 6241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7886 - val_loss: 5.2193\n",
      "Epoch 6242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3071 - val_loss: 5.7929\n",
      "Epoch 6243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7554 - val_loss: 5.1355\n",
      "Epoch 6244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8803 - val_loss: 5.0871\n",
      "Epoch 6245/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8116 - val_loss: 5.2558\n",
      "Epoch 6246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8870 - val_loss: 5.0666\n",
      "Epoch 6247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0887 - val_loss: 5.5249\n",
      "Epoch 6248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0896 - val_loss: 5.0678\n",
      "Epoch 6249/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8403 - val_loss: 5.0609\n",
      "Epoch 6250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7240 - val_loss: 5.1677\n",
      "Epoch 6251/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7307 - val_loss: 5.1087\n",
      "Epoch 6252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9530 - val_loss: 5.2496\n",
      "Epoch 6253/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7350 - val_loss: 5.0892\n",
      "Epoch 6254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7267 - val_loss: 5.1530\n",
      "Epoch 6255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8569 - val_loss: 5.0775\n",
      "Epoch 6256/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6900 - val_loss: 5.0710\n",
      "Epoch 6257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8368 - val_loss: 5.1993\n",
      "Epoch 6258/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7493 - val_loss: 5.2867\n",
      "Epoch 6259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7714 - val_loss: 5.0668\n",
      "Epoch 6260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7236 - val_loss: 5.0616\n",
      "Epoch 6261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8509 - val_loss: 5.0707\n",
      "Epoch 6262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7557 - val_loss: 5.1361\n",
      "Epoch 6263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7070 - val_loss: 5.0904\n",
      "Epoch 6264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7352 - val_loss: 5.0737\n",
      "Epoch 6265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7244 - val_loss: 5.1230\n",
      "Epoch 6266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7161 - val_loss: 5.0793\n",
      "Epoch 6267/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7417 - val_loss: 5.0704\n",
      "Epoch 6268/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7140 - val_loss: 5.0942\n",
      "Epoch 6269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7651 - val_loss: 5.1038\n",
      "Epoch 6270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7786 - val_loss: 5.5186\n",
      "Epoch 6271/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9316 - val_loss: 5.0859\n",
      "Epoch 6272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7667 - val_loss: 5.2892\n",
      "Epoch 6273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8194 - val_loss: 5.2457\n",
      "Epoch 6274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7601 - val_loss: 5.0783\n",
      "Epoch 6275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7965 - val_loss: 5.0908\n",
      "Epoch 6276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0009 - val_loss: 5.4032\n",
      "Epoch 6277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8150 - val_loss: 5.3120\n",
      "Epoch 6278/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9271 - val_loss: 5.1610\n",
      "Epoch 6279/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.7942 - val_loss: 5.0627\n",
      "Epoch 6280/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7467 - val_loss: 5.0512\n",
      "Epoch 6281/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8222 - val_loss: 5.2654\n",
      "Epoch 6282/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9592 - val_loss: 5.2295\n",
      "Epoch 6283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0215 - val_loss: 5.1086\n",
      "Epoch 6284/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8031 - val_loss: 5.1387\n",
      "Epoch 6285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8763 - val_loss: 5.1128\n",
      "Epoch 6286/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8917 - val_loss: 5.0402\n",
      "Epoch 6287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7984 - val_loss: 5.2131\n",
      "Epoch 6288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8140 - val_loss: 5.9663\n",
      "Epoch 6289/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.3922 - val_loss: 5.0679\n",
      "Epoch 6290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8831 - val_loss: 5.0988\n",
      "Epoch 6291/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7871 - val_loss: 5.0613\n",
      "Epoch 6292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9334 - val_loss: 5.2091\n",
      "Epoch 6293/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8665 - val_loss: 5.0622\n",
      "Epoch 6294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7115 - val_loss: 5.1990\n",
      "Epoch 6295/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0158 - val_loss: 5.2225\n",
      "Epoch 6296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9429 - val_loss: 5.0656\n",
      "Epoch 6297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8865 - val_loss: 5.0800\n",
      "Epoch 6298/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7412 - val_loss: 5.0694\n",
      "Epoch 6299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7575 - val_loss: 5.1304\n",
      "Epoch 6300/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7488 - val_loss: 5.0710\n",
      "Epoch 6301/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6937 - val_loss: 5.1477\n",
      "Epoch 6302/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7044 - val_loss: 5.0852\n",
      "Epoch 6303/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7621 - val_loss: 5.1925\n",
      "Epoch 6304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7293 - val_loss: 5.0552\n",
      "Epoch 6305/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8464 - val_loss: 5.0263\n",
      "Epoch 6306/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8105 - val_loss: 5.1715\n",
      "Epoch 6307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8521 - val_loss: 5.0323\n",
      "Epoch 6308/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7693 - val_loss: 5.0665\n",
      "Epoch 6309/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7507 - val_loss: 5.0615\n",
      "Epoch 6310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8523 - val_loss: 5.1128\n",
      "Epoch 6311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0088 - val_loss: 5.2126\n",
      "Epoch 6312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8977 - val_loss: 5.1877\n",
      "Epoch 6313/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8339 - val_loss: 5.0960\n",
      "Epoch 6314/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 23us/step - loss: 4.8101 - val_loss: 5.0697\n",
      "Epoch 6315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8138 - val_loss: 5.0738\n",
      "Epoch 6316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7233 - val_loss: 5.0737\n",
      "Epoch 6317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8157 - val_loss: 5.6633\n",
      "Epoch 6318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9736 - val_loss: 5.0881\n",
      "Epoch 6319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8364 - val_loss: 5.0301\n",
      "Epoch 6320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8248 - val_loss: 5.1024\n",
      "Epoch 6321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7317 - val_loss: 5.1982\n",
      "Epoch 6322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0915 - val_loss: 5.1588\n",
      "Epoch 6323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8413 - val_loss: 5.0525\n",
      "Epoch 6324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7327 - val_loss: 5.1112\n",
      "Epoch 6325/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9249 - val_loss: 5.8246\n",
      "Epoch 6326/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8112 - val_loss: 5.1014\n",
      "Epoch 6327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7202 - val_loss: 5.1183\n",
      "Epoch 6328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7406 - val_loss: 5.0699\n",
      "Epoch 6329/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8005 - val_loss: 6.5791\n",
      "Epoch 6330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1897 - val_loss: 5.1035\n",
      "Epoch 6331/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7838 - val_loss: 5.0592\n",
      "Epoch 6332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6842 - val_loss: 5.4991\n",
      "Epoch 6333/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9085 - val_loss: 5.1565\n",
      "Epoch 6334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0908 - val_loss: 5.1126\n",
      "Epoch 6335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7691 - val_loss: 5.1277\n",
      "Epoch 6336/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7509 - val_loss: 5.0614\n",
      "Epoch 6337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7387 - val_loss: 5.0956\n",
      "Epoch 6338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7237 - val_loss: 5.3865\n",
      "Epoch 6339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8505 - val_loss: 5.0583\n",
      "Epoch 6340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7211 - val_loss: 5.0437\n",
      "Epoch 6341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7431 - val_loss: 5.2295\n",
      "Epoch 6342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9449 - val_loss: 5.1051\n",
      "Epoch 6343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7147 - val_loss: 5.0586\n",
      "Epoch 6344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8306 - val_loss: 5.1716\n",
      "Epoch 6345/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7455 - val_loss: 5.1018\n",
      "Epoch 6346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8747 - val_loss: 5.1168\n",
      "Epoch 6347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8626 - val_loss: 5.2012\n",
      "Epoch 6348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7936 - val_loss: 5.0679\n",
      "Epoch 6349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8319 - val_loss: 5.1975\n",
      "Epoch 6350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7799 - val_loss: 5.1351\n",
      "Epoch 6351/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8872 - val_loss: 5.2664\n",
      "Epoch 6352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8080 - val_loss: 5.0569\n",
      "Epoch 6353/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6825 - val_loss: 5.0590\n",
      "Epoch 6354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7047 - val_loss: 5.3170\n",
      "Epoch 6355/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7957 - val_loss: 5.0566\n",
      "Epoch 6356/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7683 - val_loss: 5.1942\n",
      "Epoch 6357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7358 - val_loss: 5.0555\n",
      "Epoch 6358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8702 - val_loss: 5.0302\n",
      "Epoch 6359/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7261 - val_loss: 5.0405\n",
      "Epoch 6360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7851 - val_loss: 5.0513\n",
      "Epoch 6361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7741 - val_loss: 5.1343\n",
      "Epoch 6362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8356 - val_loss: 5.1308\n",
      "Epoch 6363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8092 - val_loss: 5.0334\n",
      "Epoch 6364/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7864 - val_loss: 5.1729\n",
      "Epoch 6365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8875 - val_loss: 5.0986\n",
      "Epoch 6366/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8252 - val_loss: 5.0363\n",
      "Epoch 6367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7251 - val_loss: 5.0242\n",
      "Epoch 6368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7724 - val_loss: 5.0819\n",
      "Epoch 6369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7099 - val_loss: 5.0561\n",
      "Epoch 6370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8129 - val_loss: 5.1280\n",
      "Epoch 6371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8181 - val_loss: 5.0543\n",
      "Epoch 6372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7479 - val_loss: 5.1263\n",
      "Epoch 6373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7204 - val_loss: 5.1908\n",
      "Epoch 6374/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9293 - val_loss: 5.1985\n",
      "Epoch 6375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0898 - val_loss: 5.1105\n",
      "Epoch 6376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7904 - val_loss: 5.1589\n",
      "Epoch 6377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9390 - val_loss: 5.5279\n",
      "Epoch 6378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8979 - val_loss: 5.1585\n",
      "Epoch 6379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7621 - val_loss: 5.1964\n",
      "Epoch 6380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0403 - val_loss: 5.2230\n",
      "Epoch 6381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0585 - val_loss: 5.3966\n",
      "Epoch 6382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8747 - val_loss: 5.0945\n",
      "Epoch 6383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9115 - val_loss: 5.2959\n",
      "Epoch 6384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9160 - val_loss: 5.1208\n",
      "Epoch 6385/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7079 - val_loss: 5.1698\n",
      "Epoch 6386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6964 - val_loss: 5.0692\n",
      "Epoch 6387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7477 - val_loss: 5.0682\n",
      "Epoch 6388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7738 - val_loss: 5.0705\n",
      "Epoch 6389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7348 - val_loss: 5.0835\n",
      "Epoch 6390/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7471 - val_loss: 5.0564\n",
      "Epoch 6391/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7227 - val_loss: 5.0421\n",
      "Epoch 6392/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7469 - val_loss: 5.6285\n",
      "Epoch 6393/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9852 - val_loss: 5.1870\n",
      "Epoch 6394/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8057 - val_loss: 5.1334\n",
      "Epoch 6395/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9425 - val_loss: 5.0255\n",
      "Epoch 6396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7394 - val_loss: 5.1134\n",
      "Epoch 6397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8011 - val_loss: 5.3526\n",
      "Epoch 6398/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9514 - val_loss: 5.1428\n",
      "Epoch 6399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0640 - val_loss: 5.9287\n",
      "Epoch 6400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0827 - val_loss: 5.5452\n",
      "Epoch 6401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0146 - val_loss: 5.0800\n",
      "Epoch 6402/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9089 - val_loss: 5.1259\n",
      "Epoch 6403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8137 - val_loss: 5.0555\n",
      "Epoch 6404/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7096 - val_loss: 5.0712\n",
      "Epoch 6405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7881 - val_loss: 5.0732\n",
      "Epoch 6406/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7515 - val_loss: 5.2881\n",
      "Epoch 6407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7305 - val_loss: 5.1170\n",
      "Epoch 6408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7162 - val_loss: 5.1209\n",
      "Epoch 6409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7495 - val_loss: 5.0537\n",
      "Epoch 6410/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7856 - val_loss: 5.0343\n",
      "Epoch 6411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7463 - val_loss: 5.0814\n",
      "Epoch 6412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6873 - val_loss: 5.3364\n",
      "Epoch 6413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9565 - val_loss: 5.2265\n",
      "Epoch 6414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2420 - val_loss: 5.2535\n",
      "Epoch 6415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2143 - val_loss: 5.0252\n",
      "Epoch 6416/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7738 - val_loss: 5.3397\n",
      "Epoch 6417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8264 - val_loss: 5.2841\n",
      "Epoch 6418/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7198 - val_loss: 5.0827\n",
      "Epoch 6419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7280 - val_loss: 5.0608\n",
      "Epoch 6420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8368 - val_loss: 5.0608\n",
      "Epoch 6421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7584 - val_loss: 5.0463\n",
      "Epoch 6422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7154 - val_loss: 5.1488\n",
      "Epoch 6423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7764 - val_loss: 5.0400\n",
      "Epoch 6424/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7635 - val_loss: 5.3210\n",
      "Epoch 6425/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7873 - val_loss: 5.1254\n",
      "Epoch 6426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9960 - val_loss: 5.2803\n",
      "Epoch 6427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8042 - val_loss: 5.0130\n",
      "Epoch 6428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7776 - val_loss: 5.3964\n",
      "Epoch 6429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8188 - val_loss: 5.0381\n",
      "Epoch 6430/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0676 - val_loss: 5.2155\n",
      "Epoch 6431/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7978 - val_loss: 5.3205\n",
      "Epoch 6432/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9140 - val_loss: 5.0619\n",
      "Epoch 6433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7829 - val_loss: 5.2259\n",
      "Epoch 6434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9586 - val_loss: 5.7363\n",
      "Epoch 6435/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7666 - val_loss: 5.0917\n",
      "Epoch 6436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8373 - val_loss: 5.1367\n",
      "Epoch 6437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9595 - val_loss: 5.7326\n",
      "Epoch 6438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1289 - val_loss: 5.1556\n",
      "Epoch 6439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9646 - val_loss: 5.0720\n",
      "Epoch 6440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1733 - val_loss: 5.1009\n",
      "Epoch 6441/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7904 - val_loss: 5.0501\n",
      "Epoch 6442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8600 - val_loss: 5.4600\n",
      "Epoch 6443/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8149 - val_loss: 5.0783\n",
      "Epoch 6444/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7640 - val_loss: 5.0367\n",
      "Epoch 6445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7136 - val_loss: 5.0540\n",
      "Epoch 6446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8727 - val_loss: 5.2454\n",
      "Epoch 6447/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7459 - val_loss: 5.0437\n",
      "Epoch 6448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7078 - val_loss: 5.3890\n",
      "Epoch 6449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9939 - val_loss: 5.0726\n",
      "Epoch 6450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7286 - val_loss: 5.0551\n",
      "Epoch 6451/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7058 - val_loss: 5.0537\n",
      "Epoch 6452/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6845 - val_loss: 5.0531\n",
      "Epoch 6453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7334 - val_loss: 5.0419\n",
      "Epoch 6454/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0374 - val_loss: 5.9130\n",
      "Epoch 6455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0528 - val_loss: 5.0450\n",
      "Epoch 6456/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7317 - val_loss: 5.3120\n",
      "Epoch 6457/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8830 - val_loss: 5.1228\n",
      "Epoch 6458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9304 - val_loss: 5.0548\n",
      "Epoch 6459/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8364 - val_loss: 5.0685\n",
      "Epoch 6460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8801 - val_loss: 5.5302\n",
      "Epoch 6461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8564 - val_loss: 5.2018\n",
      "Epoch 6462/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8652 - val_loss: 5.0395\n",
      "Epoch 6463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7260 - val_loss: 5.2422\n",
      "Epoch 6464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7302 - val_loss: 5.1289\n",
      "Epoch 6465/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7071 - val_loss: 5.1350\n",
      "Epoch 6466/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7211 - val_loss: 5.0271\n",
      "Epoch 6467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8542 - val_loss: 5.0828\n",
      "Epoch 6468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6913 - val_loss: 5.0524\n",
      "Epoch 6469/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7378 - val_loss: 5.1986\n",
      "Epoch 6470/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9420 - val_loss: 5.1306\n",
      "Epoch 6471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8083 - val_loss: 5.0549\n",
      "Epoch 6472/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.7818 - val_loss: 5.1468\n",
      "Epoch 6473/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.8196 - val_loss: 5.0397\n",
      "Epoch 6474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8244 - val_loss: 5.3143\n",
      "Epoch 6475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8812 - val_loss: 5.9695\n",
      "Epoch 6476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8647 - val_loss: 5.1582\n",
      "Epoch 6477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7695 - val_loss: 5.1456\n",
      "Epoch 6478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7484 - val_loss: 5.1403\n",
      "Epoch 6479/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7075 - val_loss: 5.0656\n",
      "Epoch 6480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7140 - val_loss: 5.0890\n",
      "Epoch 6481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7342 - val_loss: 5.0543\n",
      "Epoch 6482/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7721 - val_loss: 5.1111\n",
      "Epoch 6483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7468 - val_loss: 5.1620\n",
      "Epoch 6484/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8370 - val_loss: 5.0807\n",
      "Epoch 6485/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7967 - val_loss: 5.3380\n",
      "Epoch 6486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7425 - val_loss: 5.2697\n",
      "Epoch 6487/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7886 - val_loss: 5.0687\n",
      "Epoch 6488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8002 - val_loss: 5.0509\n",
      "Epoch 6489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7565 - val_loss: 5.0912\n",
      "Epoch 6490/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9198 - val_loss: 5.3562\n",
      "Epoch 6491/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0042 - val_loss: 5.0504\n",
      "Epoch 6492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7572 - val_loss: 5.1440\n",
      "Epoch 6493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7565 - val_loss: 5.1318\n",
      "Epoch 6494/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8022 - val_loss: 5.0475\n",
      "Epoch 6495/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6806 - val_loss: 5.2870\n",
      "Epoch 6496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7598 - val_loss: 5.2328\n",
      "Epoch 6497/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7911 - val_loss: 5.1829\n",
      "Epoch 6498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8135 - val_loss: 5.0386\n",
      "Epoch 6499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6889 - val_loss: 5.0368\n",
      "Epoch 6500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7523 - val_loss: 5.0624\n",
      "Epoch 6501/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8311 - val_loss: 5.1822\n",
      "Epoch 6502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8715 - val_loss: 5.3159\n",
      "Epoch 6503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8931 - val_loss: 5.0004\n",
      "Epoch 6504/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7045 - val_loss: 5.3357\n",
      "Epoch 6505/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7912 - val_loss: 5.0492\n",
      "Epoch 6506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7548 - val_loss: 5.3539\n",
      "Epoch 6507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7498 - val_loss: 5.4235\n",
      "Epoch 6508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9041 - val_loss: 5.0418\n",
      "Epoch 6509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7223 - val_loss: 5.1326\n",
      "Epoch 6510/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6977 - val_loss: 5.0579\n",
      "Epoch 6511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7722 - val_loss: 5.0453\n",
      "Epoch 6512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8530 - val_loss: 5.0330\n",
      "Epoch 6513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6945 - val_loss: 5.2144\n",
      "Epoch 6514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7756 - val_loss: 5.0988\n",
      "Epoch 6515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7451 - val_loss: 5.0460\n",
      "Epoch 6516/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7483 - val_loss: 5.1699\n",
      "Epoch 6517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7650 - val_loss: 5.2657\n",
      "Epoch 6518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8161 - val_loss: 5.3697\n",
      "Epoch 6519/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7257 - val_loss: 5.2796\n",
      "Epoch 6520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8639 - val_loss: 5.1838\n",
      "Epoch 6521/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8572 - val_loss: 5.0443\n",
      "Epoch 6522/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7325 - val_loss: 5.2813\n",
      "Epoch 6523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8504 - val_loss: 5.0228\n",
      "Epoch 6524/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7179 - val_loss: 5.0465\n",
      "Epoch 6525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7053 - val_loss: 5.0782\n",
      "Epoch 6526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8082 - val_loss: 5.3023\n",
      "Epoch 6527/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7323 - val_loss: 5.1175\n",
      "Epoch 6528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0841 - val_loss: 5.8395\n",
      "Epoch 6529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8725 - val_loss: 5.1748\n",
      "Epoch 6530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7582 - val_loss: 5.0404\n",
      "Epoch 6531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6877 - val_loss: 5.0202\n",
      "Epoch 6532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8002 - val_loss: 5.0803\n",
      "Epoch 6533/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7549 - val_loss: 5.0383\n",
      "Epoch 6534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7876 - val_loss: 5.0356\n",
      "Epoch 6535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7138 - val_loss: 5.0709\n",
      "Epoch 6536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8287 - val_loss: 5.1115\n",
      "Epoch 6537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8174 - val_loss: 5.2057\n",
      "Epoch 6538/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6864 - val_loss: 5.0577\n",
      "Epoch 6539/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.7684 - val_loss: 5.0649\n",
      "Epoch 6540/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7528 - val_loss: 5.0504\n",
      "Epoch 6541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7253 - val_loss: 5.0384\n",
      "Epoch 6542/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7007 - val_loss: 5.2415\n",
      "Epoch 6543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7250 - val_loss: 5.2433\n",
      "Epoch 6544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8575 - val_loss: 6.0335\n",
      "Epoch 6545/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0886 - val_loss: 5.0855\n",
      "Epoch 6546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9907 - val_loss: 5.2180\n",
      "Epoch 6547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9274 - val_loss: 5.0414\n",
      "Epoch 6548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7359 - val_loss: 5.0440\n",
      "Epoch 6549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8332 - val_loss: 5.0525\n",
      "Epoch 6550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8790 - val_loss: 5.0439\n",
      "Epoch 6551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7508 - val_loss: 5.0574\n",
      "Epoch 6552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8771 - val_loss: 5.6315\n",
      "Epoch 6553/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9159 - val_loss: 5.4769\n",
      "Epoch 6554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8453 - val_loss: 5.0655\n",
      "Epoch 6555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7822 - val_loss: 5.1257\n",
      "Epoch 6556/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7594 - val_loss: 5.1068\n",
      "Epoch 6557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8741 - val_loss: 5.3669\n",
      "Epoch 6558/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0232 - val_loss: 5.0499\n",
      "Epoch 6559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8337 - val_loss: 5.3810\n",
      "Epoch 6560/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7372 - val_loss: 5.0178\n",
      "Epoch 6561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8671 - val_loss: 5.2184\n",
      "Epoch 6562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8580 - val_loss: 5.0467\n",
      "Epoch 6563/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7082 - val_loss: 5.0710\n",
      "Epoch 6564/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8005 - val_loss: 5.5408\n",
      "Epoch 6565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7685 - val_loss: 5.0617\n",
      "Epoch 6566/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7133 - val_loss: 5.1534\n",
      "Epoch 6567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7202 - val_loss: 5.1040\n",
      "Epoch 6568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7518 - val_loss: 5.2009\n",
      "Epoch 6569/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0420 - val_loss: 5.3266\n",
      "Epoch 6570/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7707 - val_loss: 5.0540\n",
      "Epoch 6571/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9278 - val_loss: 5.0415\n",
      "Epoch 6572/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7386 - val_loss: 5.0352\n",
      "Epoch 6573/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7456 - val_loss: 5.0499\n",
      "Epoch 6574/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8230 - val_loss: 5.6556\n",
      "Epoch 6575/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9132 - val_loss: 5.0719\n",
      "Epoch 6576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8094 - val_loss: 5.0264\n",
      "Epoch 6577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0245 - val_loss: 5.1994\n",
      "Epoch 6578/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0033 - val_loss: 5.0476\n",
      "Epoch 6579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8435 - val_loss: 5.0384\n",
      "Epoch 6580/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8916 - val_loss: 5.0666\n",
      "Epoch 6581/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7759 - val_loss: 5.0861\n",
      "Epoch 6582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7714 - val_loss: 5.1808\n",
      "Epoch 6583/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8256 - val_loss: 5.4241\n",
      "Epoch 6584/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7396 - val_loss: 5.0390\n",
      "Epoch 6585/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7673 - val_loss: 5.2186\n",
      "Epoch 6586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8207 - val_loss: 5.0627\n",
      "Epoch 6587/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7476 - val_loss: 5.0983\n",
      "Epoch 6588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7781 - val_loss: 5.0684\n",
      "Epoch 6589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7215 - val_loss: 5.0113\n",
      "Epoch 6590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7983 - val_loss: 5.5486\n",
      "Epoch 6591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0514 - val_loss: 5.3232\n",
      "Epoch 6592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9594 - val_loss: 5.2882\n",
      "Epoch 6593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1079 - val_loss: 5.2666\n",
      "Epoch 6594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8521 - val_loss: 5.0705\n",
      "Epoch 6595/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8656 - val_loss: 5.0302\n",
      "Epoch 6596/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7952 - val_loss: 5.1029\n",
      "Epoch 6597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7422 - val_loss: 5.0621\n",
      "Epoch 6598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8243 - val_loss: 5.1115\n",
      "Epoch 6599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7757 - val_loss: 5.0446\n",
      "Epoch 6600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7714 - val_loss: 5.1978\n",
      "Epoch 6601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7922 - val_loss: 5.0373\n",
      "Epoch 6602/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7475 - val_loss: 5.0746\n",
      "Epoch 6603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7469 - val_loss: 5.2904\n",
      "Epoch 6604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7613 - val_loss: 5.0275\n",
      "Epoch 6605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7306 - val_loss: 5.1265\n",
      "Epoch 6606/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7439 - val_loss: 5.0357\n",
      "Epoch 6607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7657 - val_loss: 5.0275\n",
      "Epoch 6608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7408 - val_loss: 5.0352\n",
      "Epoch 6609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6555 - val_loss: 5.1163\n",
      "Epoch 6610/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7196 - val_loss: 5.0320\n",
      "Epoch 6611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7806 - val_loss: 5.0991\n",
      "Epoch 6612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7997 - val_loss: 5.1215\n",
      "Epoch 6613/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7715 - val_loss: 5.0486\n",
      "Epoch 6614/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6847 - val_loss: 5.0660\n",
      "Epoch 6615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8683 - val_loss: 5.0936\n",
      "Epoch 6616/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8493 - val_loss: 5.0427\n",
      "Epoch 6617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7197 - val_loss: 5.4929\n",
      "Epoch 6618/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7853 - val_loss: 5.0614\n",
      "Epoch 6619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0303 - val_loss: 5.3952\n",
      "Epoch 6620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1598 - val_loss: 5.2794\n",
      "Epoch 6621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9137 - val_loss: 5.3432\n",
      "Epoch 6622/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2944 - val_loss: 5.2940\n",
      "Epoch 6623/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8450 - val_loss: 5.1780\n",
      "Epoch 6624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8300 - val_loss: 5.0190\n",
      "Epoch 6625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7114 - val_loss: 5.0652\n",
      "Epoch 6626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6795 - val_loss: 5.0162\n",
      "Epoch 6627/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7568 - val_loss: 5.0659\n",
      "Epoch 6628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7139 - val_loss: 5.0588\n",
      "Epoch 6629/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8093 - val_loss: 5.2747\n",
      "Epoch 6630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0137 - val_loss: 5.0808\n",
      "Epoch 6631/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1707 - val_loss: 5.1368\n",
      "Epoch 6632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7473 - val_loss: 5.2816\n",
      "Epoch 6633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7313 - val_loss: 5.0418\n",
      "Epoch 6634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7758 - val_loss: 5.1758\n",
      "Epoch 6635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7745 - val_loss: 5.0263\n",
      "Epoch 6636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6757 - val_loss: 5.0413\n",
      "Epoch 6637/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0863 - val_loss: 5.2699\n",
      "Epoch 6638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8423 - val_loss: 5.0152\n",
      "Epoch 6639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9412 - val_loss: 5.1844\n",
      "Epoch 6640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7397 - val_loss: 5.0491\n",
      "Epoch 6641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7851 - val_loss: 5.0476\n",
      "Epoch 6642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9103 - val_loss: 5.1515\n",
      "Epoch 6643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7784 - val_loss: 5.0832\n",
      "Epoch 6644/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6898 - val_loss: 5.3177\n",
      "Epoch 6645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8802 - val_loss: 5.0191\n",
      "Epoch 6646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9563 - val_loss: 5.1423\n",
      "Epoch 6647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8519 - val_loss: 5.0250\n",
      "Epoch 6648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7050 - val_loss: 5.0285\n",
      "Epoch 6649/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7204 - val_loss: 5.0195\n",
      "Epoch 6650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7484 - val_loss: 5.0798\n",
      "Epoch 6651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7664 - val_loss: 5.0267\n",
      "Epoch 6652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8011 - val_loss: 4.9967\n",
      "Epoch 6653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7201 - val_loss: 5.0633\n",
      "Epoch 6654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8197 - val_loss: 5.0187\n",
      "Epoch 6655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7015 - val_loss: 5.0279\n",
      "Epoch 6656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7129 - val_loss: 5.0241\n",
      "Epoch 6657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7462 - val_loss: 5.0089\n",
      "Epoch 6658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7805 - val_loss: 5.0158\n",
      "Epoch 6659/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7110 - val_loss: 5.0288\n",
      "Epoch 6660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7559 - val_loss: 5.1690\n",
      "Epoch 6661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6891 - val_loss: 5.0245\n",
      "Epoch 6662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7536 - val_loss: 5.0279\n",
      "Epoch 6663/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7714 - val_loss: 5.1271\n",
      "Epoch 6664/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7251 - val_loss: 5.0356\n",
      "Epoch 6665/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8207 - val_loss: 5.1050\n",
      "Epoch 6666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8085 - val_loss: 5.1178\n",
      "Epoch 6667/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6614 - val_loss: 5.0448\n",
      "Epoch 6668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7439 - val_loss: 5.0131\n",
      "Epoch 6669/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6886 - val_loss: 5.0884\n",
      "Epoch 6670/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7107 - val_loss: 5.2096\n",
      "Epoch 6671/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7321 - val_loss: 5.0440\n",
      "Epoch 6672/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9263 - val_loss: 5.1043\n",
      "Epoch 6673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0826 - val_loss: 5.0597\n",
      "Epoch 6674/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7463 - val_loss: 5.2030\n",
      "Epoch 6675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8055 - val_loss: 5.0383\n",
      "Epoch 6676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9064 - val_loss: 5.0487\n",
      "Epoch 6677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7203 - val_loss: 5.0285\n",
      "Epoch 6678/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6967 - val_loss: 4.9987\n",
      "Epoch 6679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6382 - val_loss: 5.2573\n",
      "Epoch 6680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7363 - val_loss: 5.0793\n",
      "Epoch 6681/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6978 - val_loss: 5.0390\n",
      "Epoch 6682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7682 - val_loss: 5.2429\n",
      "Epoch 6683/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8227 - val_loss: 5.0706\n",
      "Epoch 6684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7389 - val_loss: 5.0601\n",
      "Epoch 6685/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7723 - val_loss: 5.7423\n",
      "Epoch 6686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8793 - val_loss: 5.5850\n",
      "Epoch 6687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8649 - val_loss: 5.0129\n",
      "Epoch 6688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7227 - val_loss: 5.0469\n",
      "Epoch 6689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7753 - val_loss: 4.9994\n",
      "Epoch 6690/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9424 - val_loss: 5.6150\n",
      "Epoch 6691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9504 - val_loss: 5.0913\n",
      "Epoch 6692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7890 - val_loss: 5.0538\n",
      "Epoch 6693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6864 - val_loss: 5.1159\n",
      "Epoch 6694/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7864 - val_loss: 5.0676\n",
      "Epoch 6695/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6941 - val_loss: 5.0346\n",
      "Epoch 6696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8413 - val_loss: 5.0655\n",
      "Epoch 6697/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7057 - val_loss: 5.0171\n",
      "Epoch 6698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7249 - val_loss: 5.2199\n",
      "Epoch 6699/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1362 - val_loss: 5.3723\n",
      "Epoch 6700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8048 - val_loss: 5.1153\n",
      "Epoch 6701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7084 - val_loss: 5.0707\n",
      "Epoch 6702/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8091 - val_loss: 5.5562\n",
      "Epoch 6703/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9873 - val_loss: 5.2774\n",
      "Epoch 6704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7489 - val_loss: 5.0476\n",
      "Epoch 6705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6771 - val_loss: 5.0551\n",
      "Epoch 6706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7530 - val_loss: 5.2527\n",
      "Epoch 6707/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9542 - val_loss: 5.3472\n",
      "Epoch 6708/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8403 - val_loss: 5.1009\n",
      "Epoch 6709/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8582 - val_loss: 5.6023\n",
      "Epoch 6710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8125 - val_loss: 5.0612\n",
      "Epoch 6711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7410 - val_loss: 5.0395\n",
      "Epoch 6712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9387 - val_loss: 5.3068\n",
      "Epoch 6713/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7870 - val_loss: 5.0207\n",
      "Epoch 6714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7716 - val_loss: 5.0994\n",
      "Epoch 6715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6844 - val_loss: 5.0274\n",
      "Epoch 6716/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7036 - val_loss: 5.0387\n",
      "Epoch 6717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7297 - val_loss: 5.0162\n",
      "Epoch 6718/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7351 - val_loss: 5.1165\n",
      "Epoch 6719/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8346 - val_loss: 5.5459\n",
      "Epoch 6720/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7835 - val_loss: 5.0676\n",
      "Epoch 6721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7044 - val_loss: 5.1333\n",
      "Epoch 6722/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7071 - val_loss: 5.0218\n",
      "Epoch 6723/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7313 - val_loss: 5.0772\n",
      "Epoch 6724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7717 - val_loss: 5.2401\n",
      "Epoch 6725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7572 - val_loss: 5.0725\n",
      "Epoch 6726/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8983 - val_loss: 5.0607\n",
      "Epoch 6727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7587 - val_loss: 5.0476\n",
      "Epoch 6728/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8317 - val_loss: 5.1420\n",
      "Epoch 6729/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7826 - val_loss: 5.0537\n",
      "Epoch 6730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6870 - val_loss: 5.0427\n",
      "Epoch 6731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8058 - val_loss: 5.0685\n",
      "Epoch 6732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8312 - val_loss: 5.1291\n",
      "Epoch 6733/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8172 - val_loss: 5.0760\n",
      "Epoch 6734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8222 - val_loss: 5.0550\n",
      "Epoch 6735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7580 - val_loss: 5.0784\n",
      "Epoch 6736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7647 - val_loss: 5.0699\n",
      "Epoch 6737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7193 - val_loss: 5.0813\n",
      "Epoch 6738/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7557 - val_loss: 5.2120\n",
      "Epoch 6739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7882 - val_loss: 5.0036\n",
      "Epoch 6740/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6928 - val_loss: 5.0549\n",
      "Epoch 6741/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7843 - val_loss: 5.0304\n",
      "Epoch 6742/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7009 - val_loss: 5.1048\n",
      "Epoch 6743/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7691 - val_loss: 5.0554\n",
      "Epoch 6744/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6987 - val_loss: 5.0798\n",
      "Epoch 6745/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7037 - val_loss: 5.1030\n",
      "Epoch 6746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7966 - val_loss: 5.0927\n",
      "Epoch 6747/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8944 - val_loss: 5.6381\n",
      "Epoch 6748/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8943 - val_loss: 5.0288\n",
      "Epoch 6749/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7088 - val_loss: 5.0953\n",
      "Epoch 6750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6967 - val_loss: 5.3650\n",
      "Epoch 6751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7347 - val_loss: 5.2834\n",
      "Epoch 6752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7701 - val_loss: 5.2091\n",
      "Epoch 6753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7110 - val_loss: 5.0706\n",
      "Epoch 6754/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8210 - val_loss: 5.0056\n",
      "Epoch 6755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6636 - val_loss: 5.1975\n",
      "Epoch 6756/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7620 - val_loss: 5.0704\n",
      "Epoch 6757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7865 - val_loss: 5.0703\n",
      "Epoch 6758/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9073 - val_loss: 5.0241\n",
      "Epoch 6759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0603 - val_loss: 5.0047\n",
      "Epoch 6760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7441 - val_loss: 5.0449\n",
      "Epoch 6761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9137 - val_loss: 5.0408\n",
      "Epoch 6762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8257 - val_loss: 5.3859\n",
      "Epoch 6763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0516 - val_loss: 5.1899\n",
      "Epoch 6764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7802 - val_loss: 5.1166\n",
      "Epoch 6765/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7121 - val_loss: 4.9910\n",
      "Epoch 6766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8522 - val_loss: 5.0996\n",
      "Epoch 6767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6731 - val_loss: 5.0268\n",
      "Epoch 6768/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7337 - val_loss: 5.0411\n",
      "Epoch 6769/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7061 - val_loss: 5.0497\n",
      "Epoch 6770/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6901 - val_loss: 5.0505\n",
      "Epoch 6771/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8493 - val_loss: 5.2908\n",
      "Epoch 6772/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8309 - val_loss: 5.0782\n",
      "Epoch 6773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8255 - val_loss: 5.3985\n",
      "Epoch 6774/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7947 - val_loss: 5.0250\n",
      "Epoch 6775/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7303 - val_loss: 5.1198\n",
      "Epoch 6776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8615 - val_loss: 5.2997\n",
      "Epoch 6777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7501 - val_loss: 5.1418\n",
      "Epoch 6778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8894 - val_loss: 4.9969\n",
      "Epoch 6779/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7636 - val_loss: 5.1304\n",
      "Epoch 6780/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8150 - val_loss: 5.1377\n",
      "Epoch 6781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9664 - val_loss: 4.9931\n",
      "Epoch 6782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8315 - val_loss: 5.0457\n",
      "Epoch 6783/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7051 - val_loss: 5.3496\n",
      "Epoch 6784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7996 - val_loss: 5.3001\n",
      "Epoch 6785/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8925 - val_loss: 5.0619\n",
      "Epoch 6786/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 7.493 - 0s 46us/step - loss: 4.8665 - val_loss: 5.0449\n",
      "Epoch 6787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7970 - val_loss: 5.0923\n",
      "Epoch 6788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7635 - val_loss: 5.7023\n",
      "Epoch 6789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7969 - val_loss: 5.0525\n",
      "Epoch 6790/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.258 - 0s 46us/step - loss: 4.6929 - val_loss: 5.1181\n",
      "Epoch 6791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7321 - val_loss: 5.0309\n",
      "Epoch 6792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6910 - val_loss: 5.0150\n",
      "Epoch 6793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7093 - val_loss: 5.0311\n",
      "Epoch 6794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7036 - val_loss: 5.0525\n",
      "Epoch 6795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6983 - val_loss: 5.1088\n",
      "Epoch 6796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7881 - val_loss: 5.0299\n",
      "Epoch 6797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6856 - val_loss: 5.0230\n",
      "Epoch 6798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8549 - val_loss: 5.0516\n",
      "Epoch 6799/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6974 - val_loss: 5.2845\n",
      "Epoch 6800/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9045 - val_loss: 5.4090\n",
      "Epoch 6801/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8707 - val_loss: 5.1985\n",
      "Epoch 6802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8451 - val_loss: 5.2915\n",
      "Epoch 6803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8289 - val_loss: 5.0377\n",
      "Epoch 6804/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7520 - val_loss: 5.2391\n",
      "Epoch 6805/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7367 - val_loss: 5.0171\n",
      "Epoch 6806/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7874 - val_loss: 5.0715\n",
      "Epoch 6807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6863 - val_loss: 5.0411\n",
      "Epoch 6808/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7105 - val_loss: 5.0134\n",
      "Epoch 6809/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7149 - val_loss: 5.0425\n",
      "Epoch 6810/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6937 - val_loss: 5.0295\n",
      "Epoch 6811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7032 - val_loss: 5.0268\n",
      "Epoch 6812/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0113 - val_loss: 5.0323\n",
      "Epoch 6813/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6664 - val_loss: 5.1473\n",
      "Epoch 6814/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7836 - val_loss: 5.1627\n",
      "Epoch 6815/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7108 - val_loss: 5.1655\n",
      "Epoch 6816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7048 - val_loss: 5.0150\n",
      "Epoch 6817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7303 - val_loss: 5.0555\n",
      "Epoch 6818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8111 - val_loss: 5.2883\n",
      "Epoch 6819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8767 - val_loss: 5.0132\n",
      "Epoch 6820/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1334 - val_loss: 5.0580\n",
      "Epoch 6821/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8406 - val_loss: 5.1846\n",
      "Epoch 6822/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7319 - val_loss: 5.0528\n",
      "Epoch 6823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7352 - val_loss: 5.0145\n",
      "Epoch 6824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7000 - val_loss: 5.1418\n",
      "Epoch 6825/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7683 - val_loss: 5.0144\n",
      "Epoch 6826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7376 - val_loss: 5.0453\n",
      "Epoch 6827/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6889 - val_loss: 5.0100\n",
      "Epoch 6828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8471 - val_loss: 5.0986\n",
      "Epoch 6829/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7687 - val_loss: 5.0307\n",
      "Epoch 6830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7453 - val_loss: 5.0164\n",
      "Epoch 6831/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9044 - val_loss: 5.1077\n",
      "Epoch 6832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7615 - val_loss: 5.1013\n",
      "Epoch 6833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6737 - val_loss: 5.2287\n",
      "Epoch 6834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7108 - val_loss: 5.0755\n",
      "Epoch 6835/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7103 - val_loss: 5.0775\n",
      "Epoch 6836/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6844 - val_loss: 5.0304\n",
      "Epoch 6837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7058 - val_loss: 5.0309\n",
      "Epoch 6838/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7705 - val_loss: 5.0130\n",
      "Epoch 6839/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6693 - val_loss: 5.0237\n",
      "Epoch 6840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7305 - val_loss: 5.0266\n",
      "Epoch 6841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7731 - val_loss: 5.0297\n",
      "Epoch 6842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8946 - val_loss: 5.0477\n",
      "Epoch 6843/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9003 - val_loss: 5.0455\n",
      "Epoch 6844/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7978 - val_loss: 5.1223\n",
      "Epoch 6845/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.7005 - val_loss: 5.0430\n",
      "Epoch 6846/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6739 - val_loss: 5.3019\n",
      "Epoch 6847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7922 - val_loss: 5.1547\n",
      "Epoch 6848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6944 - val_loss: 5.0254\n",
      "Epoch 6849/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7135 - val_loss: 5.0735\n",
      "Epoch 6850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8451 - val_loss: 5.1344\n",
      "Epoch 6851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9656 - val_loss: 5.0155\n",
      "Epoch 6852/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7868 - val_loss: 5.0709\n",
      "Epoch 6853/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6620 - val_loss: 5.0153\n",
      "Epoch 6854/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6700 - val_loss: 5.0805\n",
      "Epoch 6855/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8638 - val_loss: 5.2746\n",
      "Epoch 6856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7947 - val_loss: 5.6166\n",
      "Epoch 6857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8234 - val_loss: 5.0218\n",
      "Epoch 6858/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7295 - val_loss: 5.0466\n",
      "Epoch 6859/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7958 - val_loss: 5.0405\n",
      "Epoch 6860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7942 - val_loss: 5.4855\n",
      "Epoch 6861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9386 - val_loss: 5.1278\n",
      "Epoch 6862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7189 - val_loss: 5.0325\n",
      "Epoch 6863/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6908 - val_loss: 5.0968\n",
      "Epoch 6864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9340 - val_loss: 5.0524\n",
      "Epoch 6865/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9545 - val_loss: 5.1721\n",
      "Epoch 6866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7880 - val_loss: 5.4832\n",
      "Epoch 6867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8513 - val_loss: 5.0362\n",
      "Epoch 6868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7410 - val_loss: 5.0079\n",
      "Epoch 6869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8000 - val_loss: 4.9883\n",
      "Epoch 6870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8292 - val_loss: 5.0444\n",
      "Epoch 6871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6692 - val_loss: 5.1197\n",
      "Epoch 6872/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7705 - val_loss: 5.1387\n",
      "Epoch 6873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6657 - val_loss: 5.0949\n",
      "Epoch 6874/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8061 - val_loss: 4.9830\n",
      "Epoch 6875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7661 - val_loss: 5.1144\n",
      "Epoch 6876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9037 - val_loss: 5.0876\n",
      "Epoch 6877/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7933 - val_loss: 5.1234\n",
      "Epoch 6878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8240 - val_loss: 5.1203\n",
      "Epoch 6879/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9880 - val_loss: 5.0681\n",
      "Epoch 6880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7074 - val_loss: 5.0409\n",
      "Epoch 6881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0615 - val_loss: 5.1218\n",
      "Epoch 6882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7171 - val_loss: 5.2592\n",
      "Epoch 6883/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8807 - val_loss: 5.0437\n",
      "Epoch 6884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9187 - val_loss: 5.1110\n",
      "Epoch 6885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7372 - val_loss: 5.2494\n",
      "Epoch 6886/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7377 - val_loss: 5.1915\n",
      "Epoch 6887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6920 - val_loss: 5.1015\n",
      "Epoch 6888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7393 - val_loss: 5.0994\n",
      "Epoch 6889/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7366 - val_loss: 5.4112\n",
      "Epoch 6890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7461 - val_loss: 5.2286\n",
      "Epoch 6891/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7423 - val_loss: 5.8753\n",
      "Epoch 6892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8526 - val_loss: 5.0593\n",
      "Epoch 6893/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7036 - val_loss: 5.0521\n",
      "Epoch 6894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8276 - val_loss: 5.0710\n",
      "Epoch 6895/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9856 - val_loss: 5.0034\n",
      "Epoch 6896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7140 - val_loss: 5.0654\n",
      "Epoch 6897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6987 - val_loss: 5.0900\n",
      "Epoch 6898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7287 - val_loss: 5.0299\n",
      "Epoch 6899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6610 - val_loss: 5.0347\n",
      "Epoch 6900/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7373 - val_loss: 5.0396\n",
      "Epoch 6901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7089 - val_loss: 5.0301\n",
      "Epoch 6902/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7176 - val_loss: 5.0298\n",
      "Epoch 6903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9404 - val_loss: 5.0091\n",
      "Epoch 6904/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0207 - val_loss: 5.2273\n",
      "Epoch 6905/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7428 - val_loss: 5.0868\n",
      "Epoch 6906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7663 - val_loss: 5.0397\n",
      "Epoch 6907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6907 - val_loss: 5.0675\n",
      "Epoch 6908/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7288 - val_loss: 5.2157\n",
      "Epoch 6909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7993 - val_loss: 5.0163\n",
      "Epoch 6910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6829 - val_loss: 5.2872\n",
      "Epoch 6911/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7300 - val_loss: 5.0612\n",
      "Epoch 6912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6592 - val_loss: 4.9859\n",
      "Epoch 6913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7004 - val_loss: 5.0205\n",
      "Epoch 6914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8172 - val_loss: 5.0182\n",
      "Epoch 6915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6911 - val_loss: 5.0720\n",
      "Epoch 6916/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7943 - val_loss: 5.1329\n",
      "Epoch 6917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7665 - val_loss: 5.1441\n",
      "Epoch 6918/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.0842 - val_loss: 5.0253\n",
      "Epoch 6919/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9198 - val_loss: 5.0553\n",
      "Epoch 6920/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8469 - val_loss: 5.0335\n",
      "Epoch 6921/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7432 - val_loss: 5.0217\n",
      "Epoch 6922/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7323 - val_loss: 5.1335\n",
      "Epoch 6923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7207 - val_loss: 5.0120\n",
      "Epoch 6924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7788 - val_loss: 5.2035\n",
      "Epoch 6925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7150 - val_loss: 5.0264\n",
      "Epoch 6926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9358 - val_loss: 4.9984\n",
      "Epoch 6927/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8101 - val_loss: 5.0364\n",
      "Epoch 6928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7341 - val_loss: 5.0015\n",
      "Epoch 6929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6345 - val_loss: 5.1712\n",
      "Epoch 6930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8158 - val_loss: 4.9858\n",
      "Epoch 6931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6860 - val_loss: 5.0199\n",
      "Epoch 6932/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7337 - val_loss: 5.0387\n",
      "Epoch 6933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9699 - val_loss: 4.9995\n",
      "Epoch 6934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7395 - val_loss: 5.0355\n",
      "Epoch 6935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6631 - val_loss: 5.1501\n",
      "Epoch 6936/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8744 - val_loss: 5.0152\n",
      "Epoch 6937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8875 - val_loss: 5.0098\n",
      "Epoch 6938/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7332 - val_loss: 5.0640\n",
      "Epoch 6939/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7621 - val_loss: 5.0782\n",
      "Epoch 6940/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8089 - val_loss: 5.2602\n",
      "Epoch 6941/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7026 - val_loss: 5.2314\n",
      "Epoch 6942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7343 - val_loss: 5.3411\n",
      "Epoch 6943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7942 - val_loss: 5.1897\n",
      "Epoch 6944/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6936 - val_loss: 5.0193\n",
      "Epoch 6945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6714 - val_loss: 5.0596\n",
      "Epoch 6946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7007 - val_loss: 5.3426\n",
      "Epoch 6947/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7581 - val_loss: 5.0879\n",
      "Epoch 6948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8175 - val_loss: 5.0214\n",
      "Epoch 6949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8324 - val_loss: 5.0174\n",
      "Epoch 6950/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7454 - val_loss: 5.0966\n",
      "Epoch 6951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7486 - val_loss: 5.0689\n",
      "Epoch 6952/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7231 - val_loss: 4.9978\n",
      "Epoch 6953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7525 - val_loss: 5.0057\n",
      "Epoch 6954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0809 - val_loss: 5.0691\n",
      "Epoch 6955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8519 - val_loss: 5.0879\n",
      "Epoch 6956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9422 - val_loss: 5.0512\n",
      "Epoch 6957/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8262 - val_loss: 5.0237\n",
      "Epoch 6958/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7750 - val_loss: 5.4960\n",
      "Epoch 6959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8812 - val_loss: 5.0353\n",
      "Epoch 6960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7619 - val_loss: 5.0237\n",
      "Epoch 6961/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8420 - val_loss: 5.1096\n",
      "Epoch 6962/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8733 - val_loss: 5.0413\n",
      "Epoch 6963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7991 - val_loss: 5.0298\n",
      "Epoch 6964/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8083 - val_loss: 5.2913\n",
      "Epoch 6965/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7204 - val_loss: 5.0012\n",
      "Epoch 6966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6752 - val_loss: 5.0576\n",
      "Epoch 6967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6904 - val_loss: 5.0020\n",
      "Epoch 6968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7135 - val_loss: 5.2956\n",
      "Epoch 6969/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9777 - val_loss: 5.0913\n",
      "Epoch 6970/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9472 - val_loss: 5.1273\n",
      "Epoch 6971/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7555 - val_loss: 5.0497\n",
      "Epoch 6972/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8132 - val_loss: 4.9990\n",
      "Epoch 6973/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6706 - val_loss: 5.0609\n",
      "Epoch 6974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6731 - val_loss: 5.0232\n",
      "Epoch 6975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6983 - val_loss: 5.1561\n",
      "Epoch 6976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8474 - val_loss: 5.0493\n",
      "Epoch 6977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7287 - val_loss: 5.3981\n",
      "Epoch 6978/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7526 - val_loss: 5.0171\n",
      "Epoch 6979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6899 - val_loss: 5.1033\n",
      "Epoch 6980/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6955 - val_loss: 5.0872\n",
      "Epoch 6981/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7518 - val_loss: 5.0465\n",
      "Epoch 6982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6771 - val_loss: 5.0432\n",
      "Epoch 6983/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7157 - val_loss: 5.0525\n",
      "Epoch 6984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7015 - val_loss: 5.0166\n",
      "Epoch 6985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6543 - val_loss: 5.0334\n",
      "Epoch 6986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6928 - val_loss: 5.4949\n",
      "Epoch 6987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7924 - val_loss: 5.3617\n",
      "Epoch 6988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8682 - val_loss: 5.0273\n",
      "Epoch 6989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8189 - val_loss: 5.0641\n",
      "Epoch 6990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7434 - val_loss: 5.0552\n",
      "Epoch 6991/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.917 - 0s 46us/step - loss: 4.7122 - val_loss: 5.0106\n",
      "Epoch 6992/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7837 - val_loss: 5.1777\n",
      "Epoch 6993/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8623 - val_loss: 6.1264\n",
      "Epoch 6994/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0339 - val_loss: 5.5360\n",
      "Epoch 6995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8864 - val_loss: 5.0367\n",
      "Epoch 6996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7245 - val_loss: 5.0495\n",
      "Epoch 6997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7862 - val_loss: 5.0179\n",
      "Epoch 6998/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7128 - val_loss: 5.0292\n",
      "Epoch 6999/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6935 - val_loss: 5.0290\n",
      "Epoch 7000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7937 - val_loss: 5.1056\n",
      "Epoch 7001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6670 - val_loss: 4.9845\n",
      "Epoch 7002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7183 - val_loss: 5.0240\n",
      "Epoch 7003/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8674 - val_loss: 5.1529\n",
      "Epoch 7004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8077 - val_loss: 5.2816\n",
      "Epoch 7005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8248 - val_loss: 5.0333\n",
      "Epoch 7006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7498 - val_loss: 4.9641\n",
      "Epoch 7007/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8461 - val_loss: 5.4053\n",
      "Epoch 7008/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8962 - val_loss: 4.9765\n",
      "Epoch 7009/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9142 - val_loss: 5.0181\n",
      "Epoch 7010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7894 - val_loss: 5.0045\n",
      "Epoch 7011/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8288 - val_loss: 5.1029\n",
      "Epoch 7012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7102 - val_loss: 5.0438\n",
      "Epoch 7013/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7356 - val_loss: 5.0296\n",
      "Epoch 7014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7089 - val_loss: 5.2167\n",
      "Epoch 7015/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7258 - val_loss: 5.1670\n",
      "Epoch 7016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8252 - val_loss: 5.1763\n",
      "Epoch 7017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7463 - val_loss: 5.0946\n",
      "Epoch 7018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7122 - val_loss: 5.0771\n",
      "Epoch 7019/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6835 - val_loss: 5.3792\n",
      "Epoch 7020/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2229 - val_loss: 5.3618\n",
      "Epoch 7021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8070 - val_loss: 5.0026\n",
      "Epoch 7022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1170 - val_loss: 5.6878\n",
      "Epoch 7023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0474 - val_loss: 5.0054\n",
      "Epoch 7024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6873 - val_loss: 5.0085\n",
      "Epoch 7025/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7292 - val_loss: 5.1750\n",
      "Epoch 7026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7513 - val_loss: 5.3488\n",
      "Epoch 7027/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9702 - val_loss: 5.2151\n",
      "Epoch 7028/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7405 - val_loss: 5.2115\n",
      "Epoch 7029/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9749 - val_loss: 6.1836\n",
      "Epoch 7030/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8615 - val_loss: 5.2301\n",
      "Epoch 7031/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8527 - val_loss: 5.0043\n",
      "Epoch 7032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7121 - val_loss: 5.0748\n",
      "Epoch 7033/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7391 - val_loss: 5.0254\n",
      "Epoch 7034/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7449 - val_loss: 5.0060\n",
      "Epoch 7035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7818 - val_loss: 5.2145\n",
      "Epoch 7036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8298 - val_loss: 5.0693\n",
      "Epoch 7037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8258 - val_loss: 5.0034\n",
      "Epoch 7038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8701 - val_loss: 5.4264\n",
      "Epoch 7039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8349 - val_loss: 5.0961\n",
      "Epoch 7040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7053 - val_loss: 5.0866\n",
      "Epoch 7041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6883 - val_loss: 5.3161\n",
      "Epoch 7042/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7235 - val_loss: 5.0654\n",
      "Epoch 7043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6699 - val_loss: 5.1065\n",
      "Epoch 7044/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6635 - val_loss: 5.1282\n",
      "Epoch 7045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7228 - val_loss: 4.9939\n",
      "Epoch 7046/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6865 - val_loss: 5.0134\n",
      "Epoch 7047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7517 - val_loss: 5.0322\n",
      "Epoch 7048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8009 - val_loss: 5.4521\n",
      "Epoch 7049/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9126 - val_loss: 5.2607\n",
      "Epoch 7050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9331 - val_loss: 5.0820\n",
      "Epoch 7051/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7927 - val_loss: 5.0867\n",
      "Epoch 7052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8174 - val_loss: 5.2785\n",
      "Epoch 7053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7486 - val_loss: 5.2307\n",
      "Epoch 7054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7521 - val_loss: 4.9900\n",
      "Epoch 7055/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7646 - val_loss: 4.9729\n",
      "Epoch 7056/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6811 - val_loss: 5.3731\n",
      "Epoch 7057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7774 - val_loss: 5.0120\n",
      "Epoch 7058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7538 - val_loss: 5.1109\n",
      "Epoch 7059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6897 - val_loss: 4.9877\n",
      "Epoch 7060/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7491 - val_loss: 5.0032\n",
      "Epoch 7061/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7811 - val_loss: 4.9819\n",
      "Epoch 7062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7582 - val_loss: 5.0505\n",
      "Epoch 7063/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7299 - val_loss: 4.9797\n",
      "Epoch 7064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6689 - val_loss: 5.0934\n",
      "Epoch 7065/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7880 - val_loss: 5.0563\n",
      "Epoch 7066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6856 - val_loss: 5.0365\n",
      "Epoch 7067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6519 - val_loss: 5.0823\n",
      "Epoch 7068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7161 - val_loss: 5.0676\n",
      "Epoch 7069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6635 - val_loss: 4.9775\n",
      "Epoch 7070/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6865 - val_loss: 4.9923\n",
      "Epoch 7071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6970 - val_loss: 5.0146\n",
      "Epoch 7072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7093 - val_loss: 5.0292\n",
      "Epoch 7073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6941 - val_loss: 5.0603\n",
      "Epoch 7074/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8319 - val_loss: 5.2109\n",
      "Epoch 7075/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7790 - val_loss: 5.0196\n",
      "Epoch 7076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7513 - val_loss: 5.1716\n",
      "Epoch 7077/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7540 - val_loss: 5.0314\n",
      "Epoch 7078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6738 - val_loss: 5.2752\n",
      "Epoch 7079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7164 - val_loss: 5.0555\n",
      "Epoch 7080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7167 - val_loss: 5.0042\n",
      "Epoch 7081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6759 - val_loss: 5.0815\n",
      "Epoch 7082/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9355 - val_loss: 5.0187\n",
      "Epoch 7083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6660 - val_loss: 5.0490\n",
      "Epoch 7084/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7884 - val_loss: 4.9709\n",
      "Epoch 7085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6686 - val_loss: 4.9976\n",
      "Epoch 7086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6398 - val_loss: 5.1729\n",
      "Epoch 7087/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.7387 - val_loss: 5.0262\n",
      "Epoch 7088/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8302 - val_loss: 4.9725\n",
      "Epoch 7089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8764 - val_loss: 4.9806\n",
      "Epoch 7090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7400 - val_loss: 5.1610\n",
      "Epoch 7091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7889 - val_loss: 5.0454\n",
      "Epoch 7092/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7379 - val_loss: 4.9938\n",
      "Epoch 7093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7277 - val_loss: 6.0276\n",
      "Epoch 7094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9669 - val_loss: 5.1196\n",
      "Epoch 7095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9748 - val_loss: 5.2982\n",
      "Epoch 7096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7360 - val_loss: 4.9899\n",
      "Epoch 7097/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7711 - val_loss: 5.0034\n",
      "Epoch 7098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6779 - val_loss: 5.0794\n",
      "Epoch 7099/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7069 - val_loss: 5.0290\n",
      "Epoch 7100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6709 - val_loss: 5.0012\n",
      "Epoch 7101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6721 - val_loss: 5.0265\n",
      "Epoch 7102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7530 - val_loss: 5.0754\n",
      "Epoch 7103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6910 - val_loss: 5.0161\n",
      "Epoch 7104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8340 - val_loss: 5.0324\n",
      "Epoch 7105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6916 - val_loss: 5.0659\n",
      "Epoch 7106/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6758 - val_loss: 5.0692\n",
      "Epoch 7107/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7376 - val_loss: 4.9973\n",
      "Epoch 7108/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7851 - val_loss: 5.1566\n",
      "Epoch 7109/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7072 - val_loss: 4.9841\n",
      "Epoch 7110/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6577 - val_loss: 5.0015\n",
      "Epoch 7111/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6471 - val_loss: 5.1264\n",
      "Epoch 7112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8765 - val_loss: 5.1242\n",
      "Epoch 7113/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8098 - val_loss: 5.4835\n",
      "Epoch 7114/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7594 - val_loss: 4.9922\n",
      "Epoch 7115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7813 - val_loss: 5.4100\n",
      "Epoch 7116/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8697 - val_loss: 5.0085\n",
      "Epoch 7117/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6874 - val_loss: 5.0137\n",
      "Epoch 7118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6704 - val_loss: 5.2242\n",
      "Epoch 7119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7755 - val_loss: 4.9996\n",
      "Epoch 7120/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6964 - val_loss: 5.0665\n",
      "Epoch 7121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7060 - val_loss: 4.9929\n",
      "Epoch 7122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6705 - val_loss: 5.2478\n",
      "Epoch 7123/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8582 - val_loss: 5.5736\n",
      "Epoch 7124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2183 - val_loss: 6.0168\n",
      "Epoch 7125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0017 - val_loss: 5.2954\n",
      "Epoch 7126/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0719 - val_loss: 5.0006\n",
      "Epoch 7127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7462 - val_loss: 5.0133\n",
      "Epoch 7128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6773 - val_loss: 5.0407\n",
      "Epoch 7129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7977 - val_loss: 5.2706\n",
      "Epoch 7130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0428 - val_loss: 5.1236\n",
      "Epoch 7131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9469 - val_loss: 5.0055\n",
      "Epoch 7132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6761 - val_loss: 5.1243\n",
      "Epoch 7133/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7002 - val_loss: 5.1425\n",
      "Epoch 7134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7241 - val_loss: 5.0673\n",
      "Epoch 7135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6617 - val_loss: 5.0262\n",
      "Epoch 7136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9766 - val_loss: 5.0145\n",
      "Epoch 7137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7903 - val_loss: 5.3055\n",
      "Epoch 7138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8064 - val_loss: 5.3037\n",
      "Epoch 7139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7755 - val_loss: 5.0449\n",
      "Epoch 7140/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8939 - val_loss: 4.9963\n",
      "Epoch 7141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8326 - val_loss: 4.9664\n",
      "Epoch 7142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7692 - val_loss: 4.9762\n",
      "Epoch 7143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6557 - val_loss: 5.1014\n",
      "Epoch 7144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6896 - val_loss: 5.0094\n",
      "Epoch 7145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6502 - val_loss: 5.2555\n",
      "Epoch 7146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7240 - val_loss: 5.1942\n",
      "Epoch 7147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7498 - val_loss: 5.3515\n",
      "Epoch 7148/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8365 - val_loss: 5.1485\n",
      "Epoch 7149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7411 - val_loss: 5.1585\n",
      "Epoch 7150/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7032 - val_loss: 5.0152\n",
      "Epoch 7151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6737 - val_loss: 4.9798\n",
      "Epoch 7152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8456 - val_loss: 5.0224\n",
      "Epoch 7153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8221 - val_loss: 5.2951\n",
      "Epoch 7154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8730 - val_loss: 4.9979\n",
      "Epoch 7155/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.7796 - val_loss: 5.0509\n",
      "Epoch 7156/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.7804 - val_loss: 5.0691\n",
      "Epoch 7157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7534 - val_loss: 5.0411\n",
      "Epoch 7158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7072 - val_loss: 4.9760\n",
      "Epoch 7159/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6508 - val_loss: 5.1304\n",
      "Epoch 7160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6922 - val_loss: 4.9694\n",
      "Epoch 7161/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6474 - val_loss: 5.0012\n",
      "Epoch 7162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6857 - val_loss: 4.9934\n",
      "Epoch 7163/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6700 - val_loss: 5.0235\n",
      "Epoch 7164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7543 - val_loss: 5.0639\n",
      "Epoch 7165/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6534 - val_loss: 5.0139\n",
      "Epoch 7166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6725 - val_loss: 5.0274\n",
      "Epoch 7167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7766 - val_loss: 5.0134\n",
      "Epoch 7168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7313 - val_loss: 4.9965\n",
      "Epoch 7169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6895 - val_loss: 4.9887\n",
      "Epoch 7170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7612 - val_loss: 4.9555\n",
      "Epoch 7171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6833 - val_loss: 4.9730\n",
      "Epoch 7172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7718 - val_loss: 5.2126\n",
      "Epoch 7173/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8377 - val_loss: 4.9969\n",
      "Epoch 7174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6710 - val_loss: 5.1818\n",
      "Epoch 7175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6727 - val_loss: 5.1908\n",
      "Epoch 7176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7974 - val_loss: 5.2002\n",
      "Epoch 7177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7129 - val_loss: 4.9909\n",
      "Epoch 7178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7087 - val_loss: 5.3885\n",
      "Epoch 7179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9418 - val_loss: 5.0367\n",
      "Epoch 7180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6637 - val_loss: 5.0464\n",
      "Epoch 7181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7080 - val_loss: 4.9981\n",
      "Epoch 7182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7949 - val_loss: 4.9795\n",
      "Epoch 7183/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7948 - val_loss: 5.0447\n",
      "Epoch 7184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6829 - val_loss: 5.0020\n",
      "Epoch 7185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6863 - val_loss: 5.0118\n",
      "Epoch 7186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6919 - val_loss: 5.2906\n",
      "Epoch 7187/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9123 - val_loss: 5.0053\n",
      "Epoch 7188/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7563 - val_loss: 5.1889\n",
      "Epoch 7189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7940 - val_loss: 4.9991\n",
      "Epoch 7190/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7679 - val_loss: 5.0454\n",
      "Epoch 7191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6566 - val_loss: 5.0114\n",
      "Epoch 7192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9061 - val_loss: 4.9848\n",
      "Epoch 7193/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7797 - val_loss: 5.0114\n",
      "Epoch 7194/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6454 - val_loss: 4.9989\n",
      "Epoch 7195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6711 - val_loss: 5.1045\n",
      "Epoch 7196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7315 - val_loss: 5.0505\n",
      "Epoch 7197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6955 - val_loss: 4.9980\n",
      "Epoch 7198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6535 - val_loss: 5.0129\n",
      "Epoch 7199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7148 - val_loss: 5.0065\n",
      "Epoch 7200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7576 - val_loss: 5.0453\n",
      "Epoch 7201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7757 - val_loss: 5.1010\n",
      "Epoch 7202/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6564 - val_loss: 5.1904\n",
      "Epoch 7203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7607 - val_loss: 5.1216\n",
      "Epoch 7204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6576 - val_loss: 5.5160\n",
      "Epoch 7205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8559 - val_loss: 5.0136\n",
      "Epoch 7206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0994 - val_loss: 6.1334\n",
      "Epoch 7207/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8370 - val_loss: 5.0416\n",
      "Epoch 7208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7927 - val_loss: 4.9876\n",
      "Epoch 7209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9390 - val_loss: 4.9668\n",
      "Epoch 7210/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9030 - val_loss: 5.2176\n",
      "Epoch 7211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6676 - val_loss: 4.9777\n",
      "Epoch 7212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7720 - val_loss: 5.2012\n",
      "Epoch 7213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8426 - val_loss: 5.0074\n",
      "Epoch 7214/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7149 - val_loss: 5.1947\n",
      "Epoch 7215/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7455 - val_loss: 5.2806\n",
      "Epoch 7216/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.8362 - val_loss: 5.0849\n",
      "Epoch 7217/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7467 - val_loss: 5.0958\n",
      "Epoch 7218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8051 - val_loss: 5.0585\n",
      "Epoch 7219/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7600 - val_loss: 5.0716\n",
      "Epoch 7220/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6921 - val_loss: 5.2243\n",
      "Epoch 7221/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2669 - val_loss: 5.6932\n",
      "Epoch 7222/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9574 - val_loss: 5.3808\n",
      "Epoch 7223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9075 - val_loss: 5.1674\n",
      "Epoch 7224/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7311 - val_loss: 4.9759\n",
      "Epoch 7225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6876 - val_loss: 5.0735\n",
      "Epoch 7226/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7307 - val_loss: 5.0960\n",
      "Epoch 7227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6946 - val_loss: 5.0040\n",
      "Epoch 7228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6859 - val_loss: 5.1436\n",
      "Epoch 7229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8085 - val_loss: 5.0022\n",
      "Epoch 7230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8279 - val_loss: 5.1080\n",
      "Epoch 7231/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6539 - val_loss: 4.9665\n",
      "Epoch 7232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7068 - val_loss: 5.7703\n",
      "Epoch 7233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8331 - val_loss: 5.0774\n",
      "Epoch 7234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7495 - val_loss: 4.9638\n",
      "Epoch 7235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6751 - val_loss: 4.9920\n",
      "Epoch 7236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8264 - val_loss: 4.9914\n",
      "Epoch 7237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6828 - val_loss: 5.4260\n",
      "Epoch 7238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8333 - val_loss: 5.0746\n",
      "Epoch 7239/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7458 - val_loss: 4.9958\n",
      "Epoch 7240/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7314 - val_loss: 5.1373\n",
      "Epoch 7241/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8910 - val_loss: 4.9663\n",
      "Epoch 7242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6679 - val_loss: 5.0265\n",
      "Epoch 7243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6894 - val_loss: 4.9560\n",
      "Epoch 7244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8137 - val_loss: 5.1683\n",
      "Epoch 7245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8235 - val_loss: 5.1114\n",
      "Epoch 7246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7069 - val_loss: 4.9829\n",
      "Epoch 7247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6683 - val_loss: 4.9850\n",
      "Epoch 7248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6917 - val_loss: 5.3991\n",
      "Epoch 7249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7868 - val_loss: 5.4866\n",
      "Epoch 7250/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8047 - val_loss: 5.0092\n",
      "Epoch 7251/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7485 - val_loss: 5.0666\n",
      "Epoch 7252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7996 - val_loss: 5.1816\n",
      "Epoch 7253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7185 - val_loss: 4.9963\n",
      "Epoch 7254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7793 - val_loss: 5.0886\n",
      "Epoch 7255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6297 - val_loss: 4.9858\n",
      "Epoch 7256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6592 - val_loss: 4.9816\n",
      "Epoch 7257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6458 - val_loss: 5.0209\n",
      "Epoch 7258/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7895 - val_loss: 5.0531\n",
      "Epoch 7259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8227 - val_loss: 5.4917\n",
      "Epoch 7260/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9473 - val_loss: 5.0421\n",
      "Epoch 7261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8840 - val_loss: 5.0136\n",
      "Epoch 7262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7653 - val_loss: 4.9951\n",
      "Epoch 7263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8042 - val_loss: 5.0065\n",
      "Epoch 7264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8039 - val_loss: 5.0117\n",
      "Epoch 7265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7369 - val_loss: 4.9884\n",
      "Epoch 7266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6454 - val_loss: 4.9800\n",
      "Epoch 7267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6682 - val_loss: 4.9594\n",
      "Epoch 7268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0404 - val_loss: 4.9930\n",
      "Epoch 7269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8730 - val_loss: 5.0262\n",
      "Epoch 7270/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7305 - val_loss: 5.0080\n",
      "Epoch 7271/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7260 - val_loss: 4.9673\n",
      "Epoch 7272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7097 - val_loss: 5.0107\n",
      "Epoch 7273/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6380 - val_loss: 5.0182\n",
      "Epoch 7274/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7923 - val_loss: 5.1351\n",
      "Epoch 7275/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7187 - val_loss: 5.0448\n",
      "Epoch 7276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7982 - val_loss: 5.2080\n",
      "Epoch 7277/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6486 - val_loss: 4.9678\n",
      "Epoch 7278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7010 - val_loss: 5.1997\n",
      "Epoch 7279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9169 - val_loss: 5.4924\n",
      "Epoch 7280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7655 - val_loss: 5.0226\n",
      "Epoch 7281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7040 - val_loss: 4.9635\n",
      "Epoch 7282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7018 - val_loss: 5.1280\n",
      "Epoch 7283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0140 - val_loss: 5.1384\n",
      "Epoch 7284/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8872 - val_loss: 4.9777\n",
      "Epoch 7285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7195 - val_loss: 5.1732\n",
      "Epoch 7286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6703 - val_loss: 4.9576\n",
      "Epoch 7287/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6512 - val_loss: 4.9714\n",
      "Epoch 7288/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8423 - val_loss: 4.9849\n",
      "Epoch 7289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0108 - val_loss: 5.4429\n",
      "Epoch 7290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9784 - val_loss: 5.1579\n",
      "Epoch 7291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0065 - val_loss: 5.4280\n",
      "Epoch 7292/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7666 - val_loss: 5.0289\n",
      "Epoch 7293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7109 - val_loss: 5.2940\n",
      "Epoch 7294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8858 - val_loss: 5.1571\n",
      "Epoch 7295/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8796 - val_loss: 5.2804\n",
      "Epoch 7296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7428 - val_loss: 5.0459\n",
      "Epoch 7297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8623 - val_loss: 5.1029\n",
      "Epoch 7298/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6517 - val_loss: 5.4125\n",
      "Epoch 7299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0046 - val_loss: 5.4253\n",
      "Epoch 7300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1210 - val_loss: 5.1321\n",
      "Epoch 7301/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7022 - val_loss: 5.1025\n",
      "Epoch 7302/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7246 - val_loss: 4.9952\n",
      "Epoch 7303/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7006 - val_loss: 5.0068\n",
      "Epoch 7304/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7508 - val_loss: 5.5381\n",
      "Epoch 7305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7797 - val_loss: 4.9777\n",
      "Epoch 7306/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6777 - val_loss: 4.9633\n",
      "Epoch 7307/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6686 - val_loss: 5.2896\n",
      "Epoch 7308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8281 - val_loss: 5.6900\n",
      "Epoch 7309/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9333 - val_loss: 5.0115\n",
      "Epoch 7310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7162 - val_loss: 4.9829\n",
      "Epoch 7311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7008 - val_loss: 5.2667\n",
      "Epoch 7312/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9892 - val_loss: 5.0106\n",
      "Epoch 7313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8567 - val_loss: 5.0660\n",
      "Epoch 7314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7760 - val_loss: 4.9816\n",
      "Epoch 7315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8573 - val_loss: 5.6284\n",
      "Epoch 7316/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6842 - val_loss: 5.1824\n",
      "Epoch 7317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8085 - val_loss: 4.9740\n",
      "Epoch 7318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7466 - val_loss: 4.9840\n",
      "Epoch 7319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8419 - val_loss: 5.0633\n",
      "Epoch 7320/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8321 - val_loss: 4.9894\n",
      "Epoch 7321/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7062 - val_loss: 5.0261\n",
      "Epoch 7322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7457 - val_loss: 4.9712\n",
      "Epoch 7323/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7726 - val_loss: 5.0965\n",
      "Epoch 7324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6596 - val_loss: 5.0035\n",
      "Epoch 7325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6690 - val_loss: 4.9963\n",
      "Epoch 7326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6359 - val_loss: 5.0312\n",
      "Epoch 7327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7607 - val_loss: 5.2058\n",
      "Epoch 7328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6655 - val_loss: 5.1994\n",
      "Epoch 7329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7202 - val_loss: 5.0713\n",
      "Epoch 7330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7072 - val_loss: 4.9945\n",
      "Epoch 7331/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7236 - val_loss: 5.2959\n",
      "Epoch 7332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0167 - val_loss: 5.0000\n",
      "Epoch 7333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7150 - val_loss: 5.1048\n",
      "Epoch 7334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8784 - val_loss: 4.9695\n",
      "Epoch 7335/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6657 - val_loss: 4.9708\n",
      "Epoch 7336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7414 - val_loss: 5.1293\n",
      "Epoch 7337/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8727 - val_loss: 5.1015\n",
      "Epoch 7338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7119 - val_loss: 5.4359\n",
      "Epoch 7339/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7645 - val_loss: 5.0312\n",
      "Epoch 7340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8332 - val_loss: 4.9453\n",
      "Epoch 7341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7572 - val_loss: 5.0155\n",
      "Epoch 7342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6985 - val_loss: 5.0204\n",
      "Epoch 7343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7742 - val_loss: 4.9907\n",
      "Epoch 7344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8074 - val_loss: 4.9776\n",
      "Epoch 7345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6913 - val_loss: 5.0299\n",
      "Epoch 7346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8540 - val_loss: 5.0288\n",
      "Epoch 7347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7373 - val_loss: 4.9985\n",
      "Epoch 7348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6671 - val_loss: 5.0994\n",
      "Epoch 7349/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7587 - val_loss: 5.1769\n",
      "Epoch 7350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8609 - val_loss: 5.3725\n",
      "Epoch 7351/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8168 - val_loss: 4.9782\n",
      "Epoch 7352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6725 - val_loss: 5.1010\n",
      "Epoch 7353/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7352 - val_loss: 5.0307\n",
      "Epoch 7354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7267 - val_loss: 5.2900\n",
      "Epoch 7355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7150 - val_loss: 5.0224\n",
      "Epoch 7356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6467 - val_loss: 5.1560\n",
      "Epoch 7357/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7836 - val_loss: 5.2403\n",
      "Epoch 7358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7095 - val_loss: 5.1821\n",
      "Epoch 7359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7590 - val_loss: 5.0836\n",
      "Epoch 7360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7477 - val_loss: 5.1937\n",
      "Epoch 7361/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8860 - val_loss: 6.0858\n",
      "Epoch 7362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0406 - val_loss: 5.0777\n",
      "Epoch 7363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7147 - val_loss: 4.9918\n",
      "Epoch 7364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7666 - val_loss: 5.0299\n",
      "Epoch 7365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6589 - val_loss: 4.9729\n",
      "Epoch 7366/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7056 - val_loss: 5.1544\n",
      "Epoch 7367/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2512 - val_loss: 5.1467\n",
      "Epoch 7368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6981 - val_loss: 5.0005\n",
      "Epoch 7369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7136 - val_loss: 4.9965\n",
      "Epoch 7370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9499 - val_loss: 4.9902\n",
      "Epoch 7371/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7321 - val_loss: 5.0265\n",
      "Epoch 7372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8175 - val_loss: 5.0968\n",
      "Epoch 7373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9370 - val_loss: 5.4938\n",
      "Epoch 7374/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7718 - val_loss: 4.9789\n",
      "Epoch 7375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7750 - val_loss: 4.9466\n",
      "Epoch 7376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7979 - val_loss: 5.6609\n",
      "Epoch 7377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8113 - val_loss: 5.0086\n",
      "Epoch 7378/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6878 - val_loss: 5.0022\n",
      "Epoch 7379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6850 - val_loss: 4.9715\n",
      "Epoch 7380/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7102 - val_loss: 5.1751\n",
      "Epoch 7381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7209 - val_loss: 5.0812\n",
      "Epoch 7382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7127 - val_loss: 5.0081\n",
      "Epoch 7383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7177 - val_loss: 5.4138\n",
      "Epoch 7384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7663 - val_loss: 4.9655\n",
      "Epoch 7385/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8596 - val_loss: 5.1077\n",
      "Epoch 7386/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8998 - val_loss: 5.0234\n",
      "Epoch 7387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7543 - val_loss: 5.0831\n",
      "Epoch 7388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6555 - val_loss: 5.1218\n",
      "Epoch 7389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9925 - val_loss: 5.0232\n",
      "Epoch 7390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7637 - val_loss: 4.9975\n",
      "Epoch 7391/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7366 - val_loss: 5.2860\n",
      "Epoch 7392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8771 - val_loss: 4.9885\n",
      "Epoch 7393/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7848 - val_loss: 4.9894\n",
      "Epoch 7394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7606 - val_loss: 4.9673\n",
      "Epoch 7395/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6751 - val_loss: 5.0891\n",
      "Epoch 7396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6571 - val_loss: 4.9755\n",
      "Epoch 7397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6825 - val_loss: 4.9649\n",
      "Epoch 7398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7471 - val_loss: 4.9831\n",
      "Epoch 7399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6643 - val_loss: 4.9743\n",
      "Epoch 7400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7024 - val_loss: 4.9965\n",
      "Epoch 7401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6975 - val_loss: 4.9875\n",
      "Epoch 7402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6984 - val_loss: 4.9778\n",
      "Epoch 7403/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6532 - val_loss: 5.0641\n",
      "Epoch 7404/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6436 - val_loss: 5.0215\n",
      "Epoch 7405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6775 - val_loss: 5.0674\n",
      "Epoch 7406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8095 - val_loss: 5.3939\n",
      "Epoch 7407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8815 - val_loss: 5.3246\n",
      "Epoch 7408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6572 - val_loss: 4.9949\n",
      "Epoch 7409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7065 - val_loss: 5.0300\n",
      "Epoch 7410/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7220 - val_loss: 5.1583\n",
      "Epoch 7411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7391 - val_loss: 4.9502\n",
      "Epoch 7412/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7037 - val_loss: 5.0108\n",
      "Epoch 7413/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7162 - val_loss: 5.0244\n",
      "Epoch 7414/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7005 - val_loss: 5.2694\n",
      "Epoch 7415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8548 - val_loss: 4.9729\n",
      "Epoch 7416/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6452 - val_loss: 5.0316\n",
      "Epoch 7417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7602 - val_loss: 4.9661\n",
      "Epoch 7418/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6662 - val_loss: 4.9686\n",
      "Epoch 7419/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7426 - val_loss: 5.8008\n",
      "Epoch 7420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9859 - val_loss: 6.0140\n",
      "Epoch 7421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4610 - val_loss: 5.0120\n",
      "Epoch 7422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9184 - val_loss: 5.3071\n",
      "Epoch 7423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8460 - val_loss: 4.9981\n",
      "Epoch 7424/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6761 - val_loss: 5.0994\n",
      "Epoch 7425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6829 - val_loss: 5.0172\n",
      "Epoch 7426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7371 - val_loss: 5.2780\n",
      "Epoch 7427/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6999 - val_loss: 4.9927\n",
      "Epoch 7428/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8237 - val_loss: 4.9649\n",
      "Epoch 7429/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6687 - val_loss: 5.0902\n",
      "Epoch 7430/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7583 - val_loss: 5.1127\n",
      "Epoch 7431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7387 - val_loss: 5.4854\n",
      "Epoch 7432/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7166 - val_loss: 5.0461\n",
      "Epoch 7433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6858 - val_loss: 4.9680\n",
      "Epoch 7434/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6610 - val_loss: 4.9711\n",
      "Epoch 7435/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8492 - val_loss: 5.0513\n",
      "Epoch 7436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6621 - val_loss: 4.9588\n",
      "Epoch 7437/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6911 - val_loss: 5.0060\n",
      "Epoch 7438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6384 - val_loss: 5.0169\n",
      "Epoch 7439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8734 - val_loss: 5.0660\n",
      "Epoch 7440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7298 - val_loss: 4.9503\n",
      "Epoch 7441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7266 - val_loss: 4.9884\n",
      "Epoch 7442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6476 - val_loss: 4.9811\n",
      "Epoch 7443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7146 - val_loss: 5.0767\n",
      "Epoch 7444/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7785 - val_loss: 5.0397\n",
      "Epoch 7445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6548 - val_loss: 4.9615\n",
      "Epoch 7446/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7023 - val_loss: 5.2070\n",
      "Epoch 7447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9567 - val_loss: 5.4269\n",
      "Epoch 7448/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7504 - val_loss: 4.9896\n",
      "Epoch 7449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7429 - val_loss: 4.9933\n",
      "Epoch 7450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7589 - val_loss: 5.0147\n",
      "Epoch 7451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7623 - val_loss: 5.0340\n",
      "Epoch 7452/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8931 - val_loss: 5.0151\n",
      "Epoch 7453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7837 - val_loss: 5.3223\n",
      "Epoch 7454/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8262 - val_loss: 5.0600\n",
      "Epoch 7455/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6543 - val_loss: 4.9847\n",
      "Epoch 7456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8618 - val_loss: 4.9790\n",
      "Epoch 7457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6568 - val_loss: 5.0006\n",
      "Epoch 7458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7307 - val_loss: 5.0055\n",
      "Epoch 7459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6772 - val_loss: 5.0512\n",
      "Epoch 7460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6733 - val_loss: 5.0526\n",
      "Epoch 7461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7394 - val_loss: 5.0028\n",
      "Epoch 7462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7096 - val_loss: 5.1412\n",
      "Epoch 7463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6887 - val_loss: 5.0321\n",
      "Epoch 7464/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7158 - val_loss: 4.9983\n",
      "Epoch 7465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8444 - val_loss: 5.0828\n",
      "Epoch 7466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7760 - val_loss: 4.9926\n",
      "Epoch 7467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7183 - val_loss: 5.0739\n",
      "Epoch 7468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7247 - val_loss: 4.9919\n",
      "Epoch 7469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8084 - val_loss: 4.9859\n",
      "Epoch 7470/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9634 - val_loss: 5.0014\n",
      "Epoch 7471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8105 - val_loss: 4.9869\n",
      "Epoch 7472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6633 - val_loss: 5.0542\n",
      "Epoch 7473/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6660 - val_loss: 4.9886\n",
      "Epoch 7474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7204 - val_loss: 5.0117\n",
      "Epoch 7475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6665 - val_loss: 5.2727\n",
      "Epoch 7476/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6518 - val_loss: 4.9549\n",
      "Epoch 7477/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6551 - val_loss: 5.0002\n",
      "Epoch 7478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7843 - val_loss: 5.1250\n",
      "Epoch 7479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7060 - val_loss: 5.1761\n",
      "Epoch 7480/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7049 - val_loss: 5.0227\n",
      "Epoch 7481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7818 - val_loss: 5.2527\n",
      "Epoch 7482/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7123 - val_loss: 4.9961\n",
      "Epoch 7483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7136 - val_loss: 4.9793\n",
      "Epoch 7484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7823 - val_loss: 5.0622\n",
      "Epoch 7485/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8796 - val_loss: 4.9370\n",
      "Epoch 7486/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6469 - val_loss: 4.9916\n",
      "Epoch 7487/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7088 - val_loss: 5.0032\n",
      "Epoch 7488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7328 - val_loss: 5.0590\n",
      "Epoch 7489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8191 - val_loss: 4.9836\n",
      "Epoch 7490/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6281 - val_loss: 4.9788\n",
      "Epoch 7491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8717 - val_loss: 4.9584\n",
      "Epoch 7492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8099 - val_loss: 4.9967\n",
      "Epoch 7493/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6486 - val_loss: 4.9820\n",
      "Epoch 7494/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8362 - val_loss: 5.0095\n",
      "Epoch 7495/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9307 - val_loss: 5.1218\n",
      "Epoch 7496/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7894 - val_loss: 5.1066\n",
      "Epoch 7497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7993 - val_loss: 5.3003\n",
      "Epoch 7498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7972 - val_loss: 5.1002\n",
      "Epoch 7499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6586 - val_loss: 4.9903\n",
      "Epoch 7500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7372 - val_loss: 5.1232\n",
      "Epoch 7501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8686 - val_loss: 5.0009\n",
      "Epoch 7502/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6736 - val_loss: 5.0123\n",
      "Epoch 7503/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6939 - val_loss: 4.9813\n",
      "Epoch 7504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7489 - val_loss: 5.0185\n",
      "Epoch 7505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6690 - val_loss: 4.9530\n",
      "Epoch 7506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6380 - val_loss: 5.2763\n",
      "Epoch 7507/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6999 - val_loss: 4.9719\n",
      "Epoch 7508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6883 - val_loss: 5.0895\n",
      "Epoch 7509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8536 - val_loss: 5.0702\n",
      "Epoch 7510/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7028 - val_loss: 5.0059\n",
      "Epoch 7511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6291 - val_loss: 4.9844\n",
      "Epoch 7512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7136 - val_loss: 4.9984\n",
      "Epoch 7513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6882 - val_loss: 5.1284\n",
      "Epoch 7514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7590 - val_loss: 5.0416\n",
      "Epoch 7515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7206 - val_loss: 4.9913\n",
      "Epoch 7516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6792 - val_loss: 5.1723\n",
      "Epoch 7517/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7950 - val_loss: 4.9584\n",
      "Epoch 7518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7010 - val_loss: 5.0376\n",
      "Epoch 7519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6326 - val_loss: 4.9872\n",
      "Epoch 7520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7213 - val_loss: 4.9804\n",
      "Epoch 7521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7629 - val_loss: 4.9758\n",
      "Epoch 7522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6905 - val_loss: 4.9649\n",
      "Epoch 7523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7350 - val_loss: 4.9440\n",
      "Epoch 7524/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7844 - val_loss: 4.9814\n",
      "Epoch 7525/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6333 - val_loss: 4.9675\n",
      "Epoch 7526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7793 - val_loss: 4.9667\n",
      "Epoch 7527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7615 - val_loss: 4.9780\n",
      "Epoch 7528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6845 - val_loss: 5.0359\n",
      "Epoch 7529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6852 - val_loss: 5.0413\n",
      "Epoch 7530/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7108 - val_loss: 4.9934\n",
      "Epoch 7531/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6820 - val_loss: 4.9858\n",
      "Epoch 7532/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6521 - val_loss: 5.1282\n",
      "Epoch 7533/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7099 - val_loss: 4.9885\n",
      "Epoch 7534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6669 - val_loss: 5.0169\n",
      "Epoch 7535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6754 - val_loss: 4.9915\n",
      "Epoch 7536/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6351 - val_loss: 5.0206\n",
      "Epoch 7537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6583 - val_loss: 4.9976\n",
      "Epoch 7538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6673 - val_loss: 5.0567\n",
      "Epoch 7539/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6897 - val_loss: 4.9785\n",
      "Epoch 7540/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7282 - val_loss: 5.4151\n",
      "Epoch 7541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8052 - val_loss: 5.2388\n",
      "Epoch 7542/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9359 - val_loss: 4.9831\n",
      "Epoch 7543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7315 - val_loss: 5.0267\n",
      "Epoch 7544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7380 - val_loss: 4.9679\n",
      "Epoch 7545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8213 - val_loss: 4.9265\n",
      "Epoch 7546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7486 - val_loss: 4.9574\n",
      "Epoch 7547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7523 - val_loss: 4.9664\n",
      "Epoch 7548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6954 - val_loss: 4.9875\n",
      "Epoch 7549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6442 - val_loss: 4.9954\n",
      "Epoch 7550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6611 - val_loss: 5.0142\n",
      "Epoch 7551/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6457 - val_loss: 4.9658\n",
      "Epoch 7552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6562 - val_loss: 5.1564\n",
      "Epoch 7553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7850 - val_loss: 5.3562\n",
      "Epoch 7554/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7628 - val_loss: 4.9741\n",
      "Epoch 7555/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.6740 - val_loss: 4.9756\n",
      "Epoch 7556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8025 - val_loss: 5.1883\n",
      "Epoch 7557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8954 - val_loss: 4.9738\n",
      "Epoch 7558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7370 - val_loss: 4.9559\n",
      "Epoch 7559/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6502 - val_loss: 5.2026\n",
      "Epoch 7560/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7041 - val_loss: 5.3407\n",
      "Epoch 7561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7622 - val_loss: 5.0217\n",
      "Epoch 7562/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7954 - val_loss: 5.0238\n",
      "Epoch 7563/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6693 - val_loss: 5.5490\n",
      "Epoch 7564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9953 - val_loss: 5.1045\n",
      "Epoch 7565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9583 - val_loss: 5.1163\n",
      "Epoch 7566/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6872 - val_loss: 4.9805\n",
      "Epoch 7567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6625 - val_loss: 4.9778\n",
      "Epoch 7568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8573 - val_loss: 5.2277\n",
      "Epoch 7569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8585 - val_loss: 5.1315\n",
      "Epoch 7570/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7635 - val_loss: 5.0300\n",
      "Epoch 7571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8626 - val_loss: 4.9835\n",
      "Epoch 7572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7104 - val_loss: 4.9656\n",
      "Epoch 7573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6420 - val_loss: 4.9770\n",
      "Epoch 7574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7270 - val_loss: 4.9740\n",
      "Epoch 7575/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7211 - val_loss: 4.9924\n",
      "Epoch 7576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7755 - val_loss: 4.9739\n",
      "Epoch 7577/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6893 - val_loss: 4.9500\n",
      "Epoch 7578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6402 - val_loss: 4.9554\n",
      "Epoch 7579/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6580 - val_loss: 4.9454\n",
      "Epoch 7580/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7315 - val_loss: 4.9801\n",
      "Epoch 7581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6979 - val_loss: 4.9657\n",
      "Epoch 7582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6172 - val_loss: 5.0982\n",
      "Epoch 7583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9034 - val_loss: 5.1588\n",
      "Epoch 7584/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9760 - val_loss: 5.0075\n",
      "Epoch 7585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6677 - val_loss: 4.9946\n",
      "Epoch 7586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6826 - val_loss: 5.3065\n",
      "Epoch 7587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7052 - val_loss: 4.9720\n",
      "Epoch 7588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7386 - val_loss: 5.0072\n",
      "Epoch 7589/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7269 - val_loss: 4.9939\n",
      "Epoch 7590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6283 - val_loss: 5.1288\n",
      "Epoch 7591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8935 - val_loss: 5.1119\n",
      "Epoch 7592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7816 - val_loss: 5.2407\n",
      "Epoch 7593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7731 - val_loss: 5.0344\n",
      "Epoch 7594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9540 - val_loss: 4.9859\n",
      "Epoch 7595/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8840 - val_loss: 5.0076\n",
      "Epoch 7596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7417 - val_loss: 4.9753\n",
      "Epoch 7597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7055 - val_loss: 4.9448\n",
      "Epoch 7598/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7010 - val_loss: 5.0205\n",
      "Epoch 7599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7199 - val_loss: 4.9710\n",
      "Epoch 7600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7655 - val_loss: 4.9970\n",
      "Epoch 7601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6410 - val_loss: 5.1184\n",
      "Epoch 7602/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6597 - val_loss: 4.9894\n",
      "Epoch 7603/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7308 - val_loss: 4.9760\n",
      "Epoch 7604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6842 - val_loss: 5.0534\n",
      "Epoch 7605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7302 - val_loss: 5.0143\n",
      "Epoch 7606/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6167 - val_loss: 5.2590\n",
      "Epoch 7607/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6706 - val_loss: 4.9938\n",
      "Epoch 7608/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8217 - val_loss: 4.9854\n",
      "Epoch 7609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8719 - val_loss: 4.9642\n",
      "Epoch 7610/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8278 - val_loss: 5.0229\n",
      "Epoch 7611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8112 - val_loss: 5.1449\n",
      "Epoch 7612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9085 - val_loss: 5.4217\n",
      "Epoch 7613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7863 - val_loss: 4.9619\n",
      "Epoch 7614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6689 - val_loss: 4.9703\n",
      "Epoch 7615/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7556 - val_loss: 4.9999\n",
      "Epoch 7616/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6718 - val_loss: 4.9773\n",
      "Epoch 7617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6927 - val_loss: 5.0847\n",
      "Epoch 7618/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6619 - val_loss: 4.9793\n",
      "Epoch 7619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7191 - val_loss: 4.9696\n",
      "Epoch 7620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6725 - val_loss: 5.1932\n",
      "Epoch 7621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8205 - val_loss: 5.0800\n",
      "Epoch 7622/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6836 - val_loss: 4.9846\n",
      "Epoch 7623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7438 - val_loss: 5.0351\n",
      "Epoch 7624/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8764 - val_loss: 4.9857\n",
      "Epoch 7625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0300 - val_loss: 5.1762\n",
      "Epoch 7626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7042 - val_loss: 5.0193\n",
      "Epoch 7627/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8213 - val_loss: 4.9871\n",
      "Epoch 7628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8226 - val_loss: 5.0476\n",
      "Epoch 7629/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7423 - val_loss: 5.1162\n",
      "Epoch 7630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6532 - val_loss: 4.9815\n",
      "Epoch 7631/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6957 - val_loss: 4.9492\n",
      "Epoch 7632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7121 - val_loss: 4.9734\n",
      "Epoch 7633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6528 - val_loss: 5.1263\n",
      "Epoch 7634/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6486 - val_loss: 4.9795\n",
      "Epoch 7635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6971 - val_loss: 4.9959\n",
      "Epoch 7636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7347 - val_loss: 5.1119\n",
      "Epoch 7637/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7853 - val_loss: 5.1982\n",
      "Epoch 7638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6866 - val_loss: 5.0150\n",
      "Epoch 7639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7163 - val_loss: 5.0851\n",
      "Epoch 7640/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.027 - 0s 46us/step - loss: 4.6961 - val_loss: 5.0637\n",
      "Epoch 7641/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6931 - val_loss: 5.0027\n",
      "Epoch 7642/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7356 - val_loss: 5.0530\n",
      "Epoch 7643/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6785 - val_loss: 4.9478\n",
      "Epoch 7644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6284 - val_loss: 5.1454\n",
      "Epoch 7645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8201 - val_loss: 5.0859\n",
      "Epoch 7646/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6893 - val_loss: 4.9973\n",
      "Epoch 7647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7578 - val_loss: 4.9643\n",
      "Epoch 7648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8033 - val_loss: 4.9607\n",
      "Epoch 7649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6536 - val_loss: 5.1367\n",
      "Epoch 7650/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6498 - val_loss: 5.1218\n",
      "Epoch 7651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7552 - val_loss: 4.9796\n",
      "Epoch 7652/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6303 - val_loss: 5.0939\n",
      "Epoch 7653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6902 - val_loss: 4.9861\n",
      "Epoch 7654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6279 - val_loss: 5.0321\n",
      "Epoch 7655/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7560 - val_loss: 5.1677\n",
      "Epoch 7656/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8668 - val_loss: 4.9966\n",
      "Epoch 7657/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.678 - 0s 46us/step - loss: 4.7142 - val_loss: 5.0157\n",
      "Epoch 7658/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6973 - val_loss: 5.0935\n",
      "Epoch 7659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6143 - val_loss: 4.9589\n",
      "Epoch 7660/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6786 - val_loss: 5.1294\n",
      "Epoch 7661/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7836 - val_loss: 4.9708\n",
      "Epoch 7662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6840 - val_loss: 4.9533\n",
      "Epoch 7663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6293 - val_loss: 4.9679\n",
      "Epoch 7664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6523 - val_loss: 4.9664\n",
      "Epoch 7665/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6417 - val_loss: 4.9921\n",
      "Epoch 7666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8259 - val_loss: 5.0129\n",
      "Epoch 7667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6706 - val_loss: 4.9844\n",
      "Epoch 7668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6383 - val_loss: 5.0336\n",
      "Epoch 7669/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7844 - val_loss: 4.9766\n",
      "Epoch 7670/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7045 - val_loss: 5.0684\n",
      "Epoch 7671/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7173 - val_loss: 5.0076\n",
      "Epoch 7672/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6858 - val_loss: 5.0938\n",
      "Epoch 7673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7670 - val_loss: 5.4433\n",
      "Epoch 7674/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8616 - val_loss: 4.9604\n",
      "Epoch 7675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6881 - val_loss: 5.2452\n",
      "Epoch 7676/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8995 - val_loss: 4.9686\n",
      "Epoch 7677/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7208 - val_loss: 5.3988\n",
      "Epoch 7678/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2998 - val_loss: 5.4100\n",
      "Epoch 7679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9261 - val_loss: 4.9642\n",
      "Epoch 7680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6297 - val_loss: 4.9722\n",
      "Epoch 7681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8058 - val_loss: 4.9459\n",
      "Epoch 7682/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6652 - val_loss: 4.9733\n",
      "Epoch 7683/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6809 - val_loss: 5.1868\n",
      "Epoch 7684/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 5.1068 - val_loss: 5.0566\n",
      "Epoch 7685/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0480 - val_loss: 5.1459\n",
      "Epoch 7686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6915 - val_loss: 4.9444\n",
      "Epoch 7687/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6507 - val_loss: 5.0869\n",
      "Epoch 7688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6541 - val_loss: 4.9587\n",
      "Epoch 7689/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.6549 - val_loss: 4.9910\n",
      "Epoch 7690/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.7431 - val_loss: 4.9707\n",
      "Epoch 7691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6710 - val_loss: 5.1491\n",
      "Epoch 7692/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.598 - 0s 46us/step - loss: 4.7069 - val_loss: 4.9809\n",
      "Epoch 7693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6406 - val_loss: 5.0159\n",
      "Epoch 7694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6476 - val_loss: 4.9798\n",
      "Epoch 7695/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6607 - val_loss: 5.4867\n",
      "Epoch 7696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9803 - val_loss: 5.0394\n",
      "Epoch 7697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6849 - val_loss: 4.9707\n",
      "Epoch 7698/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6631 - val_loss: 4.9832\n",
      "Epoch 7699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6554 - val_loss: 5.1030\n",
      "Epoch 7700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6529 - val_loss: 4.9607\n",
      "Epoch 7701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6958 - val_loss: 5.1077\n",
      "Epoch 7702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6654 - val_loss: 5.0781\n",
      "Epoch 7703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6807 - val_loss: 5.0620\n",
      "Epoch 7704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7505 - val_loss: 4.9351\n",
      "Epoch 7705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7071 - val_loss: 5.0520\n",
      "Epoch 7706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8721 - val_loss: 4.9281\n",
      "Epoch 7707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6512 - val_loss: 5.1128\n",
      "Epoch 7708/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7856 - val_loss: 5.2880\n",
      "Epoch 7709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7664 - val_loss: 5.0471\n",
      "Epoch 7710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7076 - val_loss: 5.0211\n",
      "Epoch 7711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7409 - val_loss: 5.0250\n",
      "Epoch 7712/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6463 - val_loss: 4.9925\n",
      "Epoch 7713/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6107 - val_loss: 4.9979\n",
      "Epoch 7714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6369 - val_loss: 5.0140\n",
      "Epoch 7715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6678 - val_loss: 4.9610\n",
      "Epoch 7716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7644 - val_loss: 5.8736\n",
      "Epoch 7717/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.8193 - val_loss: 4.9569\n",
      "Epoch 7718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8099 - val_loss: 4.9419\n",
      "Epoch 7719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7007 - val_loss: 5.0899\n",
      "Epoch 7720/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7049 - val_loss: 4.9427\n",
      "Epoch 7721/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6859 - val_loss: 5.0264\n",
      "Epoch 7722/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7998 - val_loss: 5.0879\n",
      "Epoch 7723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7355 - val_loss: 5.0240\n",
      "Epoch 7724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7060 - val_loss: 4.9626\n",
      "Epoch 7725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6900 - val_loss: 4.9548\n",
      "Epoch 7726/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9373 - val_loss: 5.0329\n",
      "Epoch 7727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8397 - val_loss: 5.1216\n",
      "Epoch 7728/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7613 - val_loss: 5.0203\n",
      "Epoch 7729/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6827 - val_loss: 5.2680\n",
      "Epoch 7730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6650 - val_loss: 5.0665\n",
      "Epoch 7731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7251 - val_loss: 4.9608\n",
      "Epoch 7732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9485 - val_loss: 5.3753\n",
      "Epoch 7733/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8254 - val_loss: 4.9818\n",
      "Epoch 7734/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7511 - val_loss: 4.9692\n",
      "Epoch 7735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0560 - val_loss: 5.5935\n",
      "Epoch 7736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8546 - val_loss: 4.9437\n",
      "Epoch 7737/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7852 - val_loss: 5.4540\n",
      "Epoch 7738/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8719 - val_loss: 5.0806\n",
      "Epoch 7739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6784 - val_loss: 5.2162\n",
      "Epoch 7740/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7820 - val_loss: 5.0292\n",
      "Epoch 7741/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7494 - val_loss: 5.0662\n",
      "Epoch 7742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6858 - val_loss: 4.9449\n",
      "Epoch 7743/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6495 - val_loss: 4.9549\n",
      "Epoch 7744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6360 - val_loss: 5.0041\n",
      "Epoch 7745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7069 - val_loss: 5.0201\n",
      "Epoch 7746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7071 - val_loss: 5.3270\n",
      "Epoch 7747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8380 - val_loss: 4.9565\n",
      "Epoch 7748/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7464 - val_loss: 5.1176\n",
      "Epoch 7749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7036 - val_loss: 5.1763\n",
      "Epoch 7750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8643 - val_loss: 4.9586\n",
      "Epoch 7751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7282 - val_loss: 4.9476\n",
      "Epoch 7752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7938 - val_loss: 4.9489\n",
      "Epoch 7753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6316 - val_loss: 5.1010\n",
      "Epoch 7754/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6151 - val_loss: 4.9802\n",
      "Epoch 7755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7036 - val_loss: 5.1963\n",
      "Epoch 7756/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7822 - val_loss: 4.9880\n",
      "Epoch 7757/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7962 - val_loss: 5.0141\n",
      "Epoch 7758/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7348 - val_loss: 4.9745\n",
      "Epoch 7759/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6873 - val_loss: 4.9764\n",
      "Epoch 7760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6243 - val_loss: 4.9922\n",
      "Epoch 7761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8767 - val_loss: 4.9711\n",
      "Epoch 7762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7152 - val_loss: 5.0411\n",
      "Epoch 7763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6302 - val_loss: 4.9883\n",
      "Epoch 7764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7219 - val_loss: 5.0221\n",
      "Epoch 7765/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7657 - val_loss: 5.1905\n",
      "Epoch 7766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7166 - val_loss: 5.0215\n",
      "Epoch 7767/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7393 - val_loss: 4.9766\n",
      "Epoch 7768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7025 - val_loss: 4.9608\n",
      "Epoch 7769/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6623 - val_loss: 4.9642\n",
      "Epoch 7770/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7653 - val_loss: 4.9658\n",
      "Epoch 7771/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6582 - val_loss: 5.0029\n",
      "Epoch 7772/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7677 - val_loss: 5.0171\n",
      "Epoch 7773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7082 - val_loss: 5.0197\n",
      "Epoch 7774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6244 - val_loss: 4.9572\n",
      "Epoch 7775/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7196 - val_loss: 5.5188\n",
      "Epoch 7776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7935 - val_loss: 4.9478\n",
      "Epoch 7777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7660 - val_loss: 5.2218\n",
      "Epoch 7778/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6656 - val_loss: 4.9747\n",
      "Epoch 7779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6873 - val_loss: 4.9511\n",
      "Epoch 7780/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6344 - val_loss: 5.0930\n",
      "Epoch 7781/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6601 - val_loss: 4.9606\n",
      "Epoch 7782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6660 - val_loss: 4.9397\n",
      "Epoch 7783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6510 - val_loss: 5.0358\n",
      "Epoch 7784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6958 - val_loss: 4.9429\n",
      "Epoch 7785/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6554 - val_loss: 5.3230\n",
      "Epoch 7786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6912 - val_loss: 5.1010\n",
      "Epoch 7787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7364 - val_loss: 4.9581\n",
      "Epoch 7788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6108 - val_loss: 4.9936\n",
      "Epoch 7789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6355 - val_loss: 4.9776\n",
      "Epoch 7790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6844 - val_loss: 5.1024\n",
      "Epoch 7791/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7064 - val_loss: 5.1263\n",
      "Epoch 7792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6972 - val_loss: 4.9870\n",
      "Epoch 7793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6948 - val_loss: 4.9940\n",
      "Epoch 7794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7560 - val_loss: 5.0220\n",
      "Epoch 7795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6948 - val_loss: 5.1271\n",
      "Epoch 7796/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7215 - val_loss: 5.2921\n",
      "Epoch 7797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7454 - val_loss: 4.9851\n",
      "Epoch 7798/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.540 - 0s 46us/step - loss: 4.7219 - val_loss: 4.9629\n",
      "Epoch 7799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6436 - val_loss: 4.9435\n",
      "Epoch 7800/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7077 - val_loss: 4.9566\n",
      "Epoch 7801/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8934 - val_loss: 5.0992\n",
      "Epoch 7802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7119 - val_loss: 4.9337\n",
      "Epoch 7803/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7470 - val_loss: 4.9564\n",
      "Epoch 7804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9421 - val_loss: 4.9561\n",
      "Epoch 7805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7164 - val_loss: 5.0902\n",
      "Epoch 7806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7336 - val_loss: 4.9850\n",
      "Epoch 7807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7157 - val_loss: 4.9774\n",
      "Epoch 7808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6683 - val_loss: 4.9676\n",
      "Epoch 7809/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7025 - val_loss: 4.9690\n",
      "Epoch 7810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6351 - val_loss: 4.9326\n",
      "Epoch 7811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6968 - val_loss: 5.1148\n",
      "Epoch 7812/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6373 - val_loss: 4.9885\n",
      "Epoch 7813/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6049 - val_loss: 4.9883\n",
      "Epoch 7814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6294 - val_loss: 5.1473\n",
      "Epoch 7815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7611 - val_loss: 5.3153\n",
      "Epoch 7816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.4596 - val_loss: 5.1545\n",
      "Epoch 7817/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6995 - val_loss: 5.1738\n",
      "Epoch 7818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6491 - val_loss: 4.9261\n",
      "Epoch 7819/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6745 - val_loss: 4.9358\n",
      "Epoch 7820/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7477 - val_loss: 4.9512\n",
      "Epoch 7821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6405 - val_loss: 4.9701\n",
      "Epoch 7822/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7826 - val_loss: 5.1699\n",
      "Epoch 7823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9321 - val_loss: 5.0978\n",
      "Epoch 7824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6425 - val_loss: 4.9566\n",
      "Epoch 7825/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7442 - val_loss: 4.9306\n",
      "Epoch 7826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6132 - val_loss: 4.9833\n",
      "Epoch 7827/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8864 - val_loss: 4.9102\n",
      "Epoch 7828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7835 - val_loss: 5.0544\n",
      "Epoch 7829/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7307 - val_loss: 5.6716\n",
      "Epoch 7830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7625 - val_loss: 5.0015\n",
      "Epoch 7831/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6496 - val_loss: 4.9453\n",
      "Epoch 7832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6212 - val_loss: 4.9687\n",
      "Epoch 7833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6478 - val_loss: 4.9444\n",
      "Epoch 7834/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6298 - val_loss: 5.0364\n",
      "Epoch 7835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6658 - val_loss: 4.9823\n",
      "Epoch 7836/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6789 - val_loss: 4.9559\n",
      "Epoch 7837/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8961 - val_loss: 4.9503\n",
      "Epoch 7838/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7618 - val_loss: 5.0204\n",
      "Epoch 7839/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6965 - val_loss: 4.9805\n",
      "Epoch 7840/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6270 - val_loss: 4.9556\n",
      "Epoch 7841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6496 - val_loss: 4.9880\n",
      "Epoch 7842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6610 - val_loss: 5.2650\n",
      "Epoch 7843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7861 - val_loss: 4.9529\n",
      "Epoch 7844/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6855 - val_loss: 4.9748\n",
      "Epoch 7845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7278 - val_loss: 4.9542\n",
      "Epoch 7846/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0561 - val_loss: 5.0834\n",
      "Epoch 7847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7868 - val_loss: 5.7515\n",
      "Epoch 7848/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7866 - val_loss: 5.0460\n",
      "Epoch 7849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6964 - val_loss: 5.2431\n",
      "Epoch 7850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7384 - val_loss: 4.9255\n",
      "Epoch 7851/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6961 - val_loss: 5.1654\n",
      "Epoch 7852/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6530 - val_loss: 5.0797\n",
      "Epoch 7853/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8530 - val_loss: 5.8619\n",
      "Epoch 7854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8629 - val_loss: 5.0417\n",
      "Epoch 7855/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6460 - val_loss: 4.9779\n",
      "Epoch 7856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7015 - val_loss: 4.9713\n",
      "Epoch 7857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6167 - val_loss: 5.1153\n",
      "Epoch 7858/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7215 - val_loss: 4.9629\n",
      "Epoch 7859/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6902 - val_loss: 4.9638\n",
      "Epoch 7860/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.7748 - val_loss: 5.2113\n",
      "Epoch 7861/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7313 - val_loss: 4.9462\n",
      "Epoch 7862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6115 - val_loss: 4.9663\n",
      "Epoch 7863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6930 - val_loss: 4.9756\n",
      "Epoch 7864/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6776 - val_loss: 5.0046\n",
      "Epoch 7865/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6815 - val_loss: 5.0326\n",
      "Epoch 7866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7336 - val_loss: 5.1841\n",
      "Epoch 7867/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6440 - val_loss: 4.9586\n",
      "Epoch 7868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6504 - val_loss: 4.9577\n",
      "Epoch 7869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6420 - val_loss: 4.9364\n",
      "Epoch 7870/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7257 - val_loss: 5.2912\n",
      "Epoch 7871/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9222 - val_loss: 5.3064\n",
      "Epoch 7872/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.6191 - val_loss: 4.9281\n",
      "Epoch 7873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7974 - val_loss: 5.0432\n",
      "Epoch 7874/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6785 - val_loss: 4.9871\n",
      "Epoch 7875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6671 - val_loss: 4.9344\n",
      "Epoch 7876/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6217 - val_loss: 5.0043\n",
      "Epoch 7877/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6276 - val_loss: 4.9615\n",
      "Epoch 7878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7483 - val_loss: 4.9648\n",
      "Epoch 7879/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7587 - val_loss: 5.0388\n",
      "Epoch 7880/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7457 - val_loss: 5.0136\n",
      "Epoch 7881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6959 - val_loss: 5.0838\n",
      "Epoch 7882/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6415 - val_loss: 5.0682\n",
      "Epoch 7883/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6474 - val_loss: 5.1191\n",
      "Epoch 7884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7302 - val_loss: 4.9699\n",
      "Epoch 7885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7009 - val_loss: 5.0292\n",
      "Epoch 7886/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6828 - val_loss: 4.9526\n",
      "Epoch 7887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6529 - val_loss: 4.9918\n",
      "Epoch 7888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7674 - val_loss: 5.3303\n",
      "Epoch 7889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7010 - val_loss: 4.9466\n",
      "Epoch 7890/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7838 - val_loss: 4.9928\n",
      "Epoch 7891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6633 - val_loss: 4.9918\n",
      "Epoch 7892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6705 - val_loss: 4.9375\n",
      "Epoch 7893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6410 - val_loss: 5.1201\n",
      "Epoch 7894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9981 - val_loss: 4.9771\n",
      "Epoch 7895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7610 - val_loss: 4.9225\n",
      "Epoch 7896/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6499 - val_loss: 5.1249\n",
      "Epoch 7897/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6466 - val_loss: 5.1324\n",
      "Epoch 7898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8086 - val_loss: 5.0141\n",
      "Epoch 7899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6970 - val_loss: 4.9405\n",
      "Epoch 7900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8347 - val_loss: 4.9418\n",
      "Epoch 7901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8494 - val_loss: 5.6421\n",
      "Epoch 7902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8241 - val_loss: 5.0570\n",
      "Epoch 7903/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6451 - val_loss: 5.2849\n",
      "Epoch 7904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6123 - val_loss: 5.0195\n",
      "Epoch 7905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7895 - val_loss: 4.9290\n",
      "Epoch 7906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6463 - val_loss: 5.0572\n",
      "Epoch 7907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8120 - val_loss: 5.4857\n",
      "Epoch 7908/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0944 - val_loss: 4.9892\n",
      "Epoch 7909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6663 - val_loss: 4.9666\n",
      "Epoch 7910/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7098 - val_loss: 4.9565\n",
      "Epoch 7911/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6488 - val_loss: 4.9304\n",
      "Epoch 7912/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6731 - val_loss: 4.9246\n",
      "Epoch 7913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6985 - val_loss: 4.9534\n",
      "Epoch 7914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6523 - val_loss: 5.1944\n",
      "Epoch 7915/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6941 - val_loss: 4.9906\n",
      "Epoch 7916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6512 - val_loss: 4.9410\n",
      "Epoch 7917/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8235 - val_loss: 5.1087\n",
      "Epoch 7918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9454 - val_loss: 4.9678\n",
      "Epoch 7919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7923 - val_loss: 4.9321\n",
      "Epoch 7920/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6195 - val_loss: 5.0660\n",
      "Epoch 7921/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8751 - val_loss: 4.9297\n",
      "Epoch 7922/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7139 - val_loss: 5.4376\n",
      "Epoch 7923/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7541 - val_loss: 5.0339\n",
      "Epoch 7924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7503 - val_loss: 4.9629\n",
      "Epoch 7925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7777 - val_loss: 5.0658\n",
      "Epoch 7926/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6267 - val_loss: 4.9503\n",
      "Epoch 7927/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.135 - 0s 46us/step - loss: 4.6172 - val_loss: 4.9545\n",
      "Epoch 7928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8769 - val_loss: 5.2911\n",
      "Epoch 7929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7125 - val_loss: 4.9508\n",
      "Epoch 7930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6676 - val_loss: 5.0555\n",
      "Epoch 7931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6815 - val_loss: 5.1179\n",
      "Epoch 7932/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6294 - val_loss: 4.9431\n",
      "Epoch 7933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7227 - val_loss: 5.0581\n",
      "Epoch 7934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7678 - val_loss: 5.0773\n",
      "Epoch 7935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7453 - val_loss: 5.0374\n",
      "Epoch 7936/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6314 - val_loss: 5.1281\n",
      "Epoch 7937/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7219 - val_loss: 5.0408\n",
      "Epoch 7938/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6835 - val_loss: 4.9561\n",
      "Epoch 7939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6496 - val_loss: 4.9699\n",
      "Epoch 7940/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6595 - val_loss: 4.9519\n",
      "Epoch 7941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7109 - val_loss: 5.4916\n",
      "Epoch 7942/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6872 - val_loss: 5.0398\n",
      "Epoch 7943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6694 - val_loss: 5.2642\n",
      "Epoch 7944/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2930 - val_loss: 4.9554\n",
      "Epoch 7945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8714 - val_loss: 5.5748\n",
      "Epoch 7946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7279 - val_loss: 4.9716\n",
      "Epoch 7947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7858 - val_loss: 5.0987\n",
      "Epoch 7948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7541 - val_loss: 5.1734\n",
      "Epoch 7949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7037 - val_loss: 4.9424\n",
      "Epoch 7950/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6801 - val_loss: 4.9828\n",
      "Epoch 7951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8210 - val_loss: 4.9376\n",
      "Epoch 7952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7100 - val_loss: 4.9476\n",
      "Epoch 7953/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6562 - val_loss: 5.3419\n",
      "Epoch 7954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7108 - val_loss: 5.4068\n",
      "Epoch 7955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7192 - val_loss: 4.9528\n",
      "Epoch 7956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6470 - val_loss: 4.9485\n",
      "Epoch 7957/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6634 - val_loss: 4.9751\n",
      "Epoch 7958/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7184 - val_loss: 5.3925\n",
      "Epoch 7959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6893 - val_loss: 4.9488\n",
      "Epoch 7960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6190 - val_loss: 5.0184\n",
      "Epoch 7961/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7276 - val_loss: 5.0084\n",
      "Epoch 7962/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7836 - val_loss: 5.0824\n",
      "Epoch 7963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8545 - val_loss: 5.4345\n",
      "Epoch 7964/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7484 - val_loss: 5.1037\n",
      "Epoch 7965/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8595 - val_loss: 5.1161\n",
      "Epoch 7966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9275 - val_loss: 5.0785\n",
      "Epoch 7967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7607 - val_loss: 4.9919\n",
      "Epoch 7968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7272 - val_loss: 4.9387\n",
      "Epoch 7969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7921 - val_loss: 5.1188\n",
      "Epoch 7970/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6848 - val_loss: 5.1237\n",
      "Epoch 7971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6819 - val_loss: 4.9831\n",
      "Epoch 7972/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6438 - val_loss: 5.0576\n",
      "Epoch 7973/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6121 - val_loss: 5.0612\n",
      "Epoch 7974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7140 - val_loss: 4.9549\n",
      "Epoch 7975/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7215 - val_loss: 4.9662\n",
      "Epoch 7976/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6729 - val_loss: 5.2820\n",
      "Epoch 7977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6488 - val_loss: 4.9361\n",
      "Epoch 7978/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6646 - val_loss: 5.2540\n",
      "Epoch 7979/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8017 - val_loss: 5.2671\n",
      "Epoch 7980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7549 - val_loss: 4.9378\n",
      "Epoch 7981/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6781 - val_loss: 4.9534\n",
      "Epoch 7982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6175 - val_loss: 4.9363\n",
      "Epoch 7983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6712 - val_loss: 4.9087\n",
      "Epoch 7984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6390 - val_loss: 4.9776\n",
      "Epoch 7985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6588 - val_loss: 5.2675\n",
      "Epoch 7986/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7512 - val_loss: 4.9577\n",
      "Epoch 7987/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8321 - val_loss: 4.9324\n",
      "Epoch 7988/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9342 - val_loss: 5.2041\n",
      "Epoch 7989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8245 - val_loss: 5.4904\n",
      "Epoch 7990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7003 - val_loss: 4.9537\n",
      "Epoch 7991/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7103 - val_loss: 4.9398\n",
      "Epoch 7992/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6562 - val_loss: 5.0435\n",
      "Epoch 7993/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.897 - 0s 46us/step - loss: 4.6691 - val_loss: 5.0054\n",
      "Epoch 7994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6299 - val_loss: 4.9430\n",
      "Epoch 7995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7062 - val_loss: 4.9502\n",
      "Epoch 7996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7167 - val_loss: 4.9366\n",
      "Epoch 7997/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6361 - val_loss: 5.0149\n",
      "Epoch 7998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7057 - val_loss: 5.3419\n",
      "Epoch 7999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9238 - val_loss: 5.1128\n",
      "Epoch 8000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7099 - val_loss: 5.0695\n",
      "Epoch 8001/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7712 - val_loss: 4.9508\n",
      "Epoch 8002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6285 - val_loss: 4.9675\n",
      "Epoch 8003/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6887 - val_loss: 5.0326\n",
      "Epoch 8004/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7157 - val_loss: 4.9643\n",
      "Epoch 8005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6197 - val_loss: 5.1334\n",
      "Epoch 8006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7717 - val_loss: 5.9694\n",
      "Epoch 8007/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9127 - val_loss: 5.4136\n",
      "Epoch 8008/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0350 - val_loss: 5.0322\n",
      "Epoch 8009/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7679 - val_loss: 5.1685\n",
      "Epoch 8010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6411 - val_loss: 5.0060\n",
      "Epoch 8011/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6710 - val_loss: 5.0912\n",
      "Epoch 8012/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6687 - val_loss: 4.9550\n",
      "Epoch 8013/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6181 - val_loss: 4.9500\n",
      "Epoch 8014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7177 - val_loss: 5.0267\n",
      "Epoch 8015/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6424 - val_loss: 5.1991\n",
      "Epoch 8016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2972 - val_loss: 5.1036\n",
      "Epoch 8017/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1881 - val_loss: 5.2055\n",
      "Epoch 8018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8481 - val_loss: 6.0172\n",
      "Epoch 8019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1158 - val_loss: 4.9624\n",
      "Epoch 8020/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7701 - val_loss: 5.0578\n",
      "Epoch 8021/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9516 - val_loss: 5.2703\n",
      "Epoch 8022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8245 - val_loss: 5.6330\n",
      "Epoch 8023/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8613 - val_loss: 4.9602\n",
      "Epoch 8024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6762 - val_loss: 4.9285\n",
      "Epoch 8025/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6666 - val_loss: 4.9740\n",
      "Epoch 8026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7355 - val_loss: 4.9752\n",
      "Epoch 8027/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5943 - val_loss: 5.1038\n",
      "Epoch 8028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7042 - val_loss: 5.2057\n",
      "Epoch 8029/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7936 - val_loss: 4.9651\n",
      "Epoch 8030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7025 - val_loss: 5.0448\n",
      "Epoch 8031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7078 - val_loss: 4.9301\n",
      "Epoch 8032/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7919 - val_loss: 5.1177\n",
      "Epoch 8033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6933 - val_loss: 4.9428\n",
      "Epoch 8034/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6435 - val_loss: 4.9754\n",
      "Epoch 8035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7369 - val_loss: 4.9585\n",
      "Epoch 8036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7524 - val_loss: 5.2995\n",
      "Epoch 8037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7627 - val_loss: 5.1262\n",
      "Epoch 8038/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7205 - val_loss: 4.9877\n",
      "Epoch 8039/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6472 - val_loss: 5.2106\n",
      "Epoch 8040/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7117 - val_loss: 5.0588\n",
      "Epoch 8041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8272 - val_loss: 4.9702\n",
      "Epoch 8042/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6260 - val_loss: 4.9977\n",
      "Epoch 8043/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6755 - val_loss: 4.9584\n",
      "Epoch 8044/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6040 - val_loss: 4.9730\n",
      "Epoch 8045/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7115 - val_loss: 4.9370\n",
      "Epoch 8046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6552 - val_loss: 4.9815\n",
      "Epoch 8047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6808 - val_loss: 5.0866\n",
      "Epoch 8048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7033 - val_loss: 5.0119\n",
      "Epoch 8049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7293 - val_loss: 4.9437\n",
      "Epoch 8050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7746 - val_loss: 4.9446\n",
      "Epoch 8051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7193 - val_loss: 5.1728\n",
      "Epoch 8052/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6552 - val_loss: 4.9537\n",
      "Epoch 8053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6683 - val_loss: 4.9516\n",
      "Epoch 8054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6244 - val_loss: 4.9367\n",
      "Epoch 8055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6269 - val_loss: 4.9288\n",
      "Epoch 8056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6709 - val_loss: 4.9497\n",
      "Epoch 8057/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7242 - val_loss: 4.9369\n",
      "Epoch 8058/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6499 - val_loss: 5.1359\n",
      "Epoch 8059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7997 - val_loss: 5.4695\n",
      "Epoch 8060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6814 - val_loss: 5.0574\n",
      "Epoch 8061/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6104 - val_loss: 4.9256\n",
      "Epoch 8062/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6581 - val_loss: 4.9321\n",
      "Epoch 8063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8701 - val_loss: 4.9589\n",
      "Epoch 8064/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8908 - val_loss: 4.9336\n",
      "Epoch 8065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9713 - val_loss: 4.9416\n",
      "Epoch 8066/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6976 - val_loss: 5.0542\n",
      "Epoch 8067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6798 - val_loss: 5.3403\n",
      "Epoch 8068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8111 - val_loss: 4.9836\n",
      "Epoch 8069/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6216 - val_loss: 4.9559\n",
      "Epoch 8070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6463 - val_loss: 4.9492\n",
      "Epoch 8071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7478 - val_loss: 5.0341\n",
      "Epoch 8072/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7230 - val_loss: 4.9640\n",
      "Epoch 8073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6038 - val_loss: 5.4320\n",
      "Epoch 8074/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9228 - val_loss: 4.9386\n",
      "Epoch 8075/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6454 - val_loss: 4.9115\n",
      "Epoch 8076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6669 - val_loss: 4.9865\n",
      "Epoch 8077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6579 - val_loss: 4.9437\n",
      "Epoch 8078/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7550 - val_loss: 5.6423\n",
      "Epoch 8079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7812 - val_loss: 4.9509\n",
      "Epoch 8080/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6445 - val_loss: 4.9370\n",
      "Epoch 8081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6097 - val_loss: 4.9819\n",
      "Epoch 8082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6210 - val_loss: 4.9425\n",
      "Epoch 8083/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6264 - val_loss: 4.9577\n",
      "Epoch 8084/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6262 - val_loss: 5.1175\n",
      "Epoch 8085/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7090 - val_loss: 5.1340\n",
      "Epoch 8086/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6323 - val_loss: 4.9764\n",
      "Epoch 8087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6235 - val_loss: 4.9513\n",
      "Epoch 8088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6219 - val_loss: 5.0115\n",
      "Epoch 8089/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6844 - val_loss: 4.9259\n",
      "Epoch 8090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6819 - val_loss: 4.9573\n",
      "Epoch 8091/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6564 - val_loss: 4.9524\n",
      "Epoch 8092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7217 - val_loss: 5.3224\n",
      "Epoch 8093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7235 - val_loss: 4.9917\n",
      "Epoch 8094/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7372 - val_loss: 5.4831\n",
      "Epoch 8095/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7898 - val_loss: 4.9575\n",
      "Epoch 8096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7942 - val_loss: 4.9375\n",
      "Epoch 8097/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7140 - val_loss: 4.9380\n",
      "Epoch 8098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7139 - val_loss: 5.0107\n",
      "Epoch 8099/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9491 - val_loss: 5.0455\n",
      "Epoch 8100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6640 - val_loss: 4.9736\n",
      "Epoch 8101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6214 - val_loss: 5.0121\n",
      "Epoch 8102/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.949 - 0s 46us/step - loss: 4.6405 - val_loss: 4.9755\n",
      "Epoch 8103/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6530 - val_loss: 4.9536\n",
      "Epoch 8104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7272 - val_loss: 5.0427\n",
      "Epoch 8105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6388 - val_loss: 4.9875\n",
      "Epoch 8106/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6680 - val_loss: 5.0452\n",
      "Epoch 8107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7364 - val_loss: 4.9696\n",
      "Epoch 8108/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6478 - val_loss: 4.9411\n",
      "Epoch 8109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6643 - val_loss: 4.9559\n",
      "Epoch 8110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6710 - val_loss: 5.0147\n",
      "Epoch 8111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6368 - val_loss: 5.0316\n",
      "Epoch 8112/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6660 - val_loss: 5.1791\n",
      "Epoch 8113/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8084 - val_loss: 5.0191\n",
      "Epoch 8114/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9836 - val_loss: 5.0449\n",
      "Epoch 8115/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7722 - val_loss: 5.1226\n",
      "Epoch 8116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6305 - val_loss: 4.9387\n",
      "Epoch 8117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6723 - val_loss: 5.0722\n",
      "Epoch 8118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6900 - val_loss: 4.9441\n",
      "Epoch 8119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7812 - val_loss: 6.4138\n",
      "Epoch 8120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9945 - val_loss: 5.1133\n",
      "Epoch 8121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6619 - val_loss: 4.9487\n",
      "Epoch 8122/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6847 - val_loss: 4.9527\n",
      "Epoch 8123/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6003 - val_loss: 4.9866\n",
      "Epoch 8124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6302 - val_loss: 4.9354\n",
      "Epoch 8125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7602 - val_loss: 4.9855\n",
      "Epoch 8126/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6852 - val_loss: 4.9698\n",
      "Epoch 8127/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6273 - val_loss: 4.9864\n",
      "Epoch 8128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6928 - val_loss: 5.8902\n",
      "Epoch 8129/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7424 - val_loss: 4.9404\n",
      "Epoch 8130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6949 - val_loss: 4.9656\n",
      "Epoch 8131/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6423 - val_loss: 4.9596\n",
      "Epoch 8132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7255 - val_loss: 5.0964\n",
      "Epoch 8133/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6959 - val_loss: 4.9504\n",
      "Epoch 8134/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7162 - val_loss: 5.5647\n",
      "Epoch 8135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7770 - val_loss: 5.0414\n",
      "Epoch 8136/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7644 - val_loss: 4.9900\n",
      "Epoch 8137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6300 - val_loss: 4.9314\n",
      "Epoch 8138/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8305 - val_loss: 5.2341\n",
      "Epoch 8139/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7813 - val_loss: 4.9730\n",
      "Epoch 8140/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7757 - val_loss: 5.0168\n",
      "Epoch 8141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7301 - val_loss: 4.9730\n",
      "Epoch 8142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6228 - val_loss: 4.9166\n",
      "Epoch 8143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6447 - val_loss: 4.9938\n",
      "Epoch 8144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6501 - val_loss: 5.0309\n",
      "Epoch 8145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6879 - val_loss: 5.5234\n",
      "Epoch 8146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7973 - val_loss: 4.9284\n",
      "Epoch 8147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6768 - val_loss: 4.9584\n",
      "Epoch 8148/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7301 - val_loss: 5.0866\n",
      "Epoch 8149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7254 - val_loss: 4.9382\n",
      "Epoch 8150/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5985 - val_loss: 4.9398\n",
      "Epoch 8151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7023 - val_loss: 4.9239\n",
      "Epoch 8152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7277 - val_loss: 4.9604\n",
      "Epoch 8153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5976 - val_loss: 4.9822\n",
      "Epoch 8154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6931 - val_loss: 5.0384\n",
      "Epoch 8155/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7592 - val_loss: 5.2130\n",
      "Epoch 8156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9865 - val_loss: 4.9435\n",
      "Epoch 8157/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7505 - val_loss: 5.6694\n",
      "Epoch 8158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7271 - val_loss: 5.0136\n",
      "Epoch 8159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7186 - val_loss: 4.8957\n",
      "Epoch 8160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6837 - val_loss: 4.9380\n",
      "Epoch 8161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7933 - val_loss: 5.0185\n",
      "Epoch 8162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7027 - val_loss: 4.9324\n",
      "Epoch 8163/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5877 - val_loss: 5.0266\n",
      "Epoch 8164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6280 - val_loss: 5.0157\n",
      "Epoch 8165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6733 - val_loss: 4.9674\n",
      "Epoch 8166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6389 - val_loss: 5.2815\n",
      "Epoch 8167/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6805 - val_loss: 4.9747\n",
      "Epoch 8168/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6779 - val_loss: 5.0895\n",
      "Epoch 8169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2533 - val_loss: 4.9275\n",
      "Epoch 8170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0319 - val_loss: 5.5951\n",
      "Epoch 8171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7413 - val_loss: 4.9177\n",
      "Epoch 8172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6700 - val_loss: 4.9015\n",
      "Epoch 8173/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6807 - val_loss: 4.9848\n",
      "Epoch 8174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7005 - val_loss: 4.9729\n",
      "Epoch 8175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6268 - val_loss: 4.9957\n",
      "Epoch 8176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8429 - val_loss: 4.9307\n",
      "Epoch 8177/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6872 - val_loss: 4.9233\n",
      "Epoch 8178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7566 - val_loss: 5.2020\n",
      "Epoch 8179/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7418 - val_loss: 5.0121\n",
      "Epoch 8180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6797 - val_loss: 5.0376\n",
      "Epoch 8181/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7589 - val_loss: 4.9433\n",
      "Epoch 8182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6256 - val_loss: 4.9144\n",
      "Epoch 8183/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6527 - val_loss: 5.0504\n",
      "Epoch 8184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6540 - val_loss: 4.9351\n",
      "Epoch 8185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7160 - val_loss: 4.8977\n",
      "Epoch 8186/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6740 - val_loss: 4.9942\n",
      "Epoch 8187/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9279 - val_loss: 5.2250\n",
      "Epoch 8188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6536 - val_loss: 4.9987\n",
      "Epoch 8189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6932 - val_loss: 4.9470\n",
      "Epoch 8190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6983 - val_loss: 5.2896\n",
      "Epoch 8191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8651 - val_loss: 4.9648\n",
      "Epoch 8192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6201 - val_loss: 5.0406\n",
      "Epoch 8193/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6227 - val_loss: 4.9883\n",
      "Epoch 8194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6155 - val_loss: 4.9745\n",
      "Epoch 8195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9212 - val_loss: 5.4275\n",
      "Epoch 8196/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9567 - val_loss: 4.9335\n",
      "Epoch 8197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6511 - val_loss: 5.0949\n",
      "Epoch 8198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6482 - val_loss: 4.9494\n",
      "Epoch 8199/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6888 - val_loss: 4.9503\n",
      "Epoch 8200/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6613 - val_loss: 5.1465\n",
      "Epoch 8201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6972 - val_loss: 5.2049\n",
      "Epoch 8202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8206 - val_loss: 4.9735\n",
      "Epoch 8203/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6702 - val_loss: 5.0562\n",
      "Epoch 8204/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7760 - val_loss: 4.9921\n",
      "Epoch 8205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8722 - val_loss: 5.0879\n",
      "Epoch 8206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7652 - val_loss: 4.9570\n",
      "Epoch 8207/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6400 - val_loss: 4.9865\n",
      "Epoch 8208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8043 - val_loss: 5.0783\n",
      "Epoch 8209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6326 - val_loss: 4.9274\n",
      "Epoch 8210/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6611 - val_loss: 4.9263\n",
      "Epoch 8211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7505 - val_loss: 4.9476\n",
      "Epoch 8212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6631 - val_loss: 4.9991\n",
      "Epoch 8213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7054 - val_loss: 4.9489\n",
      "Epoch 8214/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6817 - val_loss: 5.2674\n",
      "Epoch 8215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8376 - val_loss: 5.0257\n",
      "Epoch 8216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6755 - val_loss: 5.2436\n",
      "Epoch 8217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6439 - val_loss: 5.2234\n",
      "Epoch 8218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0906 - val_loss: 5.7852\n",
      "Epoch 8219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8046 - val_loss: 4.9590\n",
      "Epoch 8220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5894 - val_loss: 5.1763\n",
      "Epoch 8221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7965 - val_loss: 5.1913\n",
      "Epoch 8222/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.381 - 0s 46us/step - loss: 4.7977 - val_loss: 5.0776\n",
      "Epoch 8223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6313 - val_loss: 5.0574\n",
      "Epoch 8224/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6044 - val_loss: 4.9103\n",
      "Epoch 8225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6647 - val_loss: 5.0066\n",
      "Epoch 8226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8450 - val_loss: 5.0641\n",
      "Epoch 8227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0543 - val_loss: 6.1424\n",
      "Epoch 8228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7632 - val_loss: 5.3177\n",
      "Epoch 8229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7903 - val_loss: 4.9771\n",
      "Epoch 8230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6292 - val_loss: 5.0504\n",
      "Epoch 8231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6302 - val_loss: 5.1643\n",
      "Epoch 8232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8057 - val_loss: 5.1091\n",
      "Epoch 8233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6742 - val_loss: 4.9092\n",
      "Epoch 8234/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6914 - val_loss: 4.9449\n",
      "Epoch 8235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6629 - val_loss: 5.0248\n",
      "Epoch 8236/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6811 - val_loss: 4.9267\n",
      "Epoch 8237/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6461 - val_loss: 5.0762\n",
      "Epoch 8238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7109 - val_loss: 4.9721\n",
      "Epoch 8239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6512 - val_loss: 5.4264\n",
      "Epoch 8240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6880 - val_loss: 5.0199\n",
      "Epoch 8241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6036 - val_loss: 4.9201\n",
      "Epoch 8242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5857 - val_loss: 5.2203\n",
      "Epoch 8243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6844 - val_loss: 5.0777\n",
      "Epoch 8244/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6236 - val_loss: 5.0339\n",
      "Epoch 8245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6308 - val_loss: 5.1278\n",
      "Epoch 8246/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5917 - val_loss: 4.9029\n",
      "Epoch 8247/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6435 - val_loss: 5.0568\n",
      "Epoch 8248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6284 - val_loss: 5.0004\n",
      "Epoch 8249/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6906 - val_loss: 5.0159\n",
      "Epoch 8250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7241 - val_loss: 5.0546\n",
      "Epoch 8251/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0418 - val_loss: 4.9775\n",
      "Epoch 8252/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7883 - val_loss: 5.5280\n",
      "Epoch 8253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7629 - val_loss: 5.1879\n",
      "Epoch 8254/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6267 - val_loss: 5.0030\n",
      "Epoch 8255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6460 - val_loss: 4.9449\n",
      "Epoch 8256/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7317 - val_loss: 5.0123\n",
      "Epoch 8257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7144 - val_loss: 4.9174\n",
      "Epoch 8258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6540 - val_loss: 5.0191\n",
      "Epoch 8259/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7011 - val_loss: 4.9082\n",
      "Epoch 8260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7141 - val_loss: 5.2182\n",
      "Epoch 8261/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6506 - val_loss: 5.0328\n",
      "Epoch 8262/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8887 - val_loss: 4.9297\n",
      "Epoch 8263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6697 - val_loss: 4.9626\n",
      "Epoch 8264/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6040 - val_loss: 4.9332\n",
      "Epoch 8265/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7569 - val_loss: 4.9591\n",
      "Epoch 8266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6587 - val_loss: 4.9245\n",
      "Epoch 8267/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7265 - val_loss: 5.2716\n",
      "Epoch 8268/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6262 - val_loss: 4.9972\n",
      "Epoch 8269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6259 - val_loss: 4.9432\n",
      "Epoch 8270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7096 - val_loss: 4.9801\n",
      "Epoch 8271/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6563 - val_loss: 5.0041\n",
      "Epoch 8272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0732 - val_loss: 5.0708\n",
      "Epoch 8273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8280 - val_loss: 5.0594\n",
      "Epoch 8274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7753 - val_loss: 5.0729\n",
      "Epoch 8275/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6900 - val_loss: 4.9117\n",
      "Epoch 8276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6664 - val_loss: 4.9348\n",
      "Epoch 8277/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6078 - val_loss: 5.5736\n",
      "Epoch 8278/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0144 - val_loss: 4.9927\n",
      "Epoch 8279/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6288 - val_loss: 4.9548\n",
      "Epoch 8280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6179 - val_loss: 4.9622\n",
      "Epoch 8281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8275 - val_loss: 5.4861\n",
      "Epoch 8282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7976 - val_loss: 4.9390\n",
      "Epoch 8283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6333 - val_loss: 4.9308\n",
      "Epoch 8284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7655 - val_loss: 5.2870\n",
      "Epoch 8285/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8727 - val_loss: 4.9332\n",
      "Epoch 8286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8006 - val_loss: 5.0054\n",
      "Epoch 8287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7134 - val_loss: 4.9142\n",
      "Epoch 8288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7058 - val_loss: 4.9407\n",
      "Epoch 8289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6678 - val_loss: 4.9669\n",
      "Epoch 8290/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6883 - val_loss: 4.9400\n",
      "Epoch 8291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6588 - val_loss: 4.9646\n",
      "Epoch 8292/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6708 - val_loss: 4.9230\n",
      "Epoch 8293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6426 - val_loss: 5.0254\n",
      "Epoch 8294/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6734 - val_loss: 4.9556\n",
      "Epoch 8295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5988 - val_loss: 4.9815\n",
      "Epoch 8296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5906 - val_loss: 4.9460\n",
      "Epoch 8297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7502 - val_loss: 4.9331\n",
      "Epoch 8298/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6088 - val_loss: 4.9319\n",
      "Epoch 8299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7711 - val_loss: 4.9505\n",
      "Epoch 8300/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7568 - val_loss: 4.9462\n",
      "Epoch 8301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6340 - val_loss: 5.1634\n",
      "Epoch 8302/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7384 - val_loss: 5.1009\n",
      "Epoch 8303/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7178 - val_loss: 4.9380\n",
      "Epoch 8304/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6178 - val_loss: 4.9508\n",
      "Epoch 8305/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6628 - val_loss: 5.0069\n",
      "Epoch 8306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6736 - val_loss: 4.9511\n",
      "Epoch 8307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8582 - val_loss: 4.9212\n",
      "Epoch 8308/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7207 - val_loss: 4.9291\n",
      "Epoch 8309/20000\n",
      "685/685 [==============================] - 0s 137us/step - loss: 4.6844 - val_loss: 4.9217\n",
      "Epoch 8310/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6284 - val_loss: 4.9545\n",
      "Epoch 8311/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6101 - val_loss: 4.9446\n",
      "Epoch 8312/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6409 - val_loss: 5.2662\n",
      "Epoch 8313/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7036 - val_loss: 5.2001\n",
      "Epoch 8314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7109 - val_loss: 5.4570\n",
      "Epoch 8315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8508 - val_loss: 4.9663\n",
      "Epoch 8316/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6529 - val_loss: 5.2345\n",
      "Epoch 8317/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7642 - val_loss: 4.9757\n",
      "Epoch 8318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6809 - val_loss: 5.1968\n",
      "Epoch 8319/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7945 - val_loss: 4.9097\n",
      "Epoch 8320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5799 - val_loss: 4.9207\n",
      "Epoch 8321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8577 - val_loss: 5.1671\n",
      "Epoch 8322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9417 - val_loss: 5.3807\n",
      "Epoch 8323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7674 - val_loss: 5.1436\n",
      "Epoch 8324/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6264 - val_loss: 5.2674\n",
      "Epoch 8325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7489 - val_loss: 4.9693\n",
      "Epoch 8326/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7410 - val_loss: 5.0626\n",
      "Epoch 8327/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6172 - val_loss: 4.9592\n",
      "Epoch 8328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6671 - val_loss: 4.9260\n",
      "Epoch 8329/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7009 - val_loss: 4.9335\n",
      "Epoch 8330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5972 - val_loss: 4.9142\n",
      "Epoch 8331/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7030 - val_loss: 5.0641\n",
      "Epoch 8332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5921 - val_loss: 4.9211\n",
      "Epoch 8333/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7068 - val_loss: 4.9231\n",
      "Epoch 8334/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8682 - val_loss: 5.4504\n",
      "Epoch 8335/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8702 - val_loss: 4.9393\n",
      "Epoch 8336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6088 - val_loss: 4.9455\n",
      "Epoch 8337/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6046 - val_loss: 4.9498\n",
      "Epoch 8338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7448 - val_loss: 4.9676\n",
      "Epoch 8339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7867 - val_loss: 5.0066\n",
      "Epoch 8340/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7495 - val_loss: 4.8910\n",
      "Epoch 8341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7136 - val_loss: 4.9388\n",
      "Epoch 8342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9181 - val_loss: 4.9409\n",
      "Epoch 8343/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6620 - val_loss: 5.2129\n",
      "Epoch 8344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7540 - val_loss: 5.0263\n",
      "Epoch 8345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6396 - val_loss: 5.0561\n",
      "Epoch 8346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6925 - val_loss: 5.1962\n",
      "Epoch 8347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6750 - val_loss: 5.1701\n",
      "Epoch 8348/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6431 - val_loss: 5.3450\n",
      "Epoch 8349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6663 - val_loss: 5.0640\n",
      "Epoch 8350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6523 - val_loss: 4.9480\n",
      "Epoch 8351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7459 - val_loss: 5.0238\n",
      "Epoch 8352/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6543 - val_loss: 4.9988\n",
      "Epoch 8353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6251 - val_loss: 4.9104\n",
      "Epoch 8354/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6423 - val_loss: 5.1012\n",
      "Epoch 8355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9205 - val_loss: 4.9006\n",
      "Epoch 8356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6430 - val_loss: 5.2131\n",
      "Epoch 8357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9127 - val_loss: 5.2332\n",
      "Epoch 8358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6264 - val_loss: 5.0948\n",
      "Epoch 8359/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7773 - val_loss: 5.0933\n",
      "Epoch 8360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7154 - val_loss: 5.1795\n",
      "Epoch 8361/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0174 - val_loss: 4.9536\n",
      "Epoch 8362/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7622 - val_loss: 4.9398\n",
      "Epoch 8363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6222 - val_loss: 5.0947\n",
      "Epoch 8364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7716 - val_loss: 5.1409\n",
      "Epoch 8365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6588 - val_loss: 4.9396\n",
      "Epoch 8366/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6211 - val_loss: 5.0871\n",
      "Epoch 8367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7038 - val_loss: 5.1261\n",
      "Epoch 8368/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6224 - val_loss: 5.1478\n",
      "Epoch 8369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7703 - val_loss: 5.0637\n",
      "Epoch 8370/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6971 - val_loss: 5.1382\n",
      "Epoch 8371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6936 - val_loss: 4.9303\n",
      "Epoch 8372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6823 - val_loss: 4.9163\n",
      "Epoch 8373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7002 - val_loss: 5.0405\n",
      "Epoch 8374/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6479 - val_loss: 4.8938\n",
      "Epoch 8375/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6666 - val_loss: 4.9238\n",
      "Epoch 8376/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7695 - val_loss: 4.9153\n",
      "Epoch 8377/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7407 - val_loss: 5.0555\n",
      "Epoch 8378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6604 - val_loss: 4.9656\n",
      "Epoch 8379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6270 - val_loss: 4.9911\n",
      "Epoch 8380/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8597 - val_loss: 4.9524\n",
      "Epoch 8381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8601 - val_loss: 5.0828\n",
      "Epoch 8382/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7991 - val_loss: 5.0612\n",
      "Epoch 8383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6760 - val_loss: 4.9443\n",
      "Epoch 8384/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6742 - val_loss: 4.9592\n",
      "Epoch 8385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5987 - val_loss: 5.1449\n",
      "Epoch 8386/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7439 - val_loss: 4.9745\n",
      "Epoch 8387/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7879 - val_loss: 5.0876\n",
      "Epoch 8388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6627 - val_loss: 4.9052\n",
      "Epoch 8389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6415 - val_loss: 5.0366\n",
      "Epoch 8390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6515 - val_loss: 4.9905\n",
      "Epoch 8391/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6755 - val_loss: 4.9368\n",
      "Epoch 8392/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6472 - val_loss: 4.9454\n",
      "Epoch 8393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7759 - val_loss: 5.1786\n",
      "Epoch 8394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8470 - val_loss: 5.0083\n",
      "Epoch 8395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8624 - val_loss: 4.9705\n",
      "Epoch 8396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7967 - val_loss: 4.9578\n",
      "Epoch 8397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6772 - val_loss: 5.1720\n",
      "Epoch 8398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9363 - val_loss: 4.9247\n",
      "Epoch 8399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7679 - val_loss: 4.9663\n",
      "Epoch 8400/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7272 - val_loss: 5.0439\n",
      "Epoch 8401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7276 - val_loss: 4.9577\n",
      "Epoch 8402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6255 - val_loss: 4.9557\n",
      "Epoch 8403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6346 - val_loss: 5.1227\n",
      "Epoch 8404/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7509 - val_loss: 5.2866\n",
      "Epoch 8405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7858 - val_loss: 5.0456\n",
      "Epoch 8406/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7514 - val_loss: 4.9883\n",
      "Epoch 8407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7176 - val_loss: 4.9530\n",
      "Epoch 8408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6345 - val_loss: 4.9192\n",
      "Epoch 8409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7150 - val_loss: 4.9411\n",
      "Epoch 8410/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7789 - val_loss: 5.1510\n",
      "Epoch 8411/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7158 - val_loss: 4.9312\n",
      "Epoch 8412/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7043 - val_loss: 4.9525\n",
      "Epoch 8413/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6228 - val_loss: 4.9168\n",
      "Epoch 8414/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6680 - val_loss: 5.4417\n",
      "Epoch 8415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8249 - val_loss: 4.9082\n",
      "Epoch 8416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6921 - val_loss: 4.9670\n",
      "Epoch 8417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6667 - val_loss: 5.1272\n",
      "Epoch 8418/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7358 - val_loss: 4.9428\n",
      "Epoch 8419/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6252 - val_loss: 4.9234\n",
      "Epoch 8420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6614 - val_loss: 4.9245\n",
      "Epoch 8421/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5949 - val_loss: 5.2139\n",
      "Epoch 8422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7940 - val_loss: 4.9739\n",
      "Epoch 8423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6002 - val_loss: 4.9534\n",
      "Epoch 8424/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6534 - val_loss: 4.9218\n",
      "Epoch 8425/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6526 - val_loss: 4.9306\n",
      "Epoch 8426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7047 - val_loss: 4.9435\n",
      "Epoch 8427/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6319 - val_loss: 4.9203\n",
      "Epoch 8428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5916 - val_loss: 4.9377\n",
      "Epoch 8429/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7263 - val_loss: 4.9168\n",
      "Epoch 8430/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7006 - val_loss: 4.9809\n",
      "Epoch 8431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6569 - val_loss: 4.8977\n",
      "Epoch 8432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8051 - val_loss: 4.9957\n",
      "Epoch 8433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8228 - val_loss: 5.0401\n",
      "Epoch 8434/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7006 - val_loss: 4.9945\n",
      "Epoch 8435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8100 - val_loss: 5.0049\n",
      "Epoch 8436/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5829 - val_loss: 5.0733\n",
      "Epoch 8437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6310 - val_loss: 4.9561\n",
      "Epoch 8438/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9417 - val_loss: 4.9924\n",
      "Epoch 8439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7506 - val_loss: 5.3061\n",
      "Epoch 8440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7280 - val_loss: 4.9744\n",
      "Epoch 8441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6756 - val_loss: 5.1423\n",
      "Epoch 8442/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8301 - val_loss: 4.9593\n",
      "Epoch 8443/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6515 - val_loss: 4.9531\n",
      "Epoch 8444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6931 - val_loss: 5.1045\n",
      "Epoch 8445/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8263 - val_loss: 5.0742\n",
      "Epoch 8446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6400 - val_loss: 4.9688\n",
      "Epoch 8447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6572 - val_loss: 4.9840\n",
      "Epoch 8448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6910 - val_loss: 4.9203\n",
      "Epoch 8449/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6131 - val_loss: 4.9048\n",
      "Epoch 8450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7251 - val_loss: 5.1697\n",
      "Epoch 8451/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7010 - val_loss: 5.0148\n",
      "Epoch 8452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5968 - val_loss: 4.9076\n",
      "Epoch 8453/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7113 - val_loss: 5.1142\n",
      "Epoch 8454/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6634 - val_loss: 5.1126\n",
      "Epoch 8455/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8457 - val_loss: 5.1845\n",
      "Epoch 8456/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6799 - val_loss: 4.9612\n",
      "Epoch 8457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7022 - val_loss: 4.9397\n",
      "Epoch 8458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8750 - val_loss: 4.9959\n",
      "Epoch 8459/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6625 - val_loss: 4.9068\n",
      "Epoch 8460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0709 - val_loss: 6.1445\n",
      "Epoch 8461/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7187 - val_loss: 5.0423\n",
      "Epoch 8462/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8170 - val_loss: 5.2177\n",
      "Epoch 8463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7837 - val_loss: 4.9111\n",
      "Epoch 8464/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6111 - val_loss: 4.9519\n",
      "Epoch 8465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6079 - val_loss: 4.9055\n",
      "Epoch 8466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6988 - val_loss: 4.9882\n",
      "Epoch 8467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6369 - val_loss: 4.9777\n",
      "Epoch 8468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7443 - val_loss: 5.2691\n",
      "Epoch 8469/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.9982 - val_loss: 5.4020\n",
      "Epoch 8470/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0521 - val_loss: 4.9205\n",
      "Epoch 8471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9142 - val_loss: 4.8969\n",
      "Epoch 8472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6191 - val_loss: 4.9040\n",
      "Epoch 8473/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6424 - val_loss: 4.9373\n",
      "Epoch 8474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6576 - val_loss: 4.8950\n",
      "Epoch 8475/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7011 - val_loss: 4.9408\n",
      "Epoch 8476/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7992 - val_loss: 5.2600\n",
      "Epoch 8477/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6754 - val_loss: 4.9232\n",
      "Epoch 8478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9379 - val_loss: 5.6619\n",
      "Epoch 8479/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8023 - val_loss: 4.9324\n",
      "Epoch 8480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6085 - val_loss: 4.9258\n",
      "Epoch 8481/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7241 - val_loss: 5.0596\n",
      "Epoch 8482/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7703 - val_loss: 4.9108\n",
      "Epoch 8483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8693 - val_loss: 5.0906\n",
      "Epoch 8484/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6636 - val_loss: 4.9268\n",
      "Epoch 8485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6285 - val_loss: 4.8936\n",
      "Epoch 8486/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6170 - val_loss: 4.9242\n",
      "Epoch 8487/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6043 - val_loss: 4.9471\n",
      "Epoch 8488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6462 - val_loss: 4.9109\n",
      "Epoch 8489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8812 - val_loss: 4.9623\n",
      "Epoch 8490/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6074 - val_loss: 5.1034\n",
      "Epoch 8491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7542 - val_loss: 5.1405\n",
      "Epoch 8492/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6970 - val_loss: 4.8901\n",
      "Epoch 8493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6425 - val_loss: 4.9242\n",
      "Epoch 8494/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6481 - val_loss: 5.1173\n",
      "Epoch 8495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6087 - val_loss: 4.9669\n",
      "Epoch 8496/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6852 - val_loss: 4.9061\n",
      "Epoch 8497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7066 - val_loss: 4.9951\n",
      "Epoch 8498/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6386 - val_loss: 4.9325\n",
      "Epoch 8499/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5979 - val_loss: 4.9224\n",
      "Epoch 8500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6977 - val_loss: 4.9959\n",
      "Epoch 8501/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6701 - val_loss: 5.0015\n",
      "Epoch 8502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6659 - val_loss: 4.8833\n",
      "Epoch 8503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5916 - val_loss: 4.9486\n",
      "Epoch 8504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6129 - val_loss: 4.9217\n",
      "Epoch 8505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7913 - val_loss: 4.9622\n",
      "Epoch 8506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6559 - val_loss: 5.4704\n",
      "Epoch 8507/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7748 - val_loss: 5.0391\n",
      "Epoch 8508/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6642 - val_loss: 4.9674\n",
      "Epoch 8509/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5938 - val_loss: 4.9037\n",
      "Epoch 8510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6265 - val_loss: 4.9443\n",
      "Epoch 8511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6289 - val_loss: 5.0571\n",
      "Epoch 8512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6047 - val_loss: 5.6264\n",
      "Epoch 8513/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7732 - val_loss: 4.9578\n",
      "Epoch 8514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6567 - val_loss: 4.9211\n",
      "Epoch 8515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8624 - val_loss: 4.9423\n",
      "Epoch 8516/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6054 - val_loss: 5.1989\n",
      "Epoch 8517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7058 - val_loss: 4.9189\n",
      "Epoch 8518/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8441 - val_loss: 5.3146\n",
      "Epoch 8519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6279 - val_loss: 4.9908\n",
      "Epoch 8520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6424 - val_loss: 5.0075\n",
      "Epoch 8521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8049 - val_loss: 5.2435\n",
      "Epoch 8522/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8100 - val_loss: 5.2323\n",
      "Epoch 8523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6014 - val_loss: 4.9131\n",
      "Epoch 8524/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6609 - val_loss: 5.0805\n",
      "Epoch 8525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6286 - val_loss: 4.9127\n",
      "Epoch 8526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5841 - val_loss: 5.2332\n",
      "Epoch 8527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6953 - val_loss: 5.0061\n",
      "Epoch 8528/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6625 - val_loss: 5.0348\n",
      "Epoch 8529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6003 - val_loss: 4.9261\n",
      "Epoch 8530/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7908 - val_loss: 5.4164\n",
      "Epoch 8531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8042 - val_loss: 5.1294\n",
      "Epoch 8532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7966 - val_loss: 4.9153\n",
      "Epoch 8533/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7333 - val_loss: 4.8993\n",
      "Epoch 8534/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6751 - val_loss: 4.9823\n",
      "Epoch 8535/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6832 - val_loss: 4.9014\n",
      "Epoch 8536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6578 - val_loss: 5.0229\n",
      "Epoch 8537/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8642 - val_loss: 5.1760\n",
      "Epoch 8538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6416 - val_loss: 4.9109\n",
      "Epoch 8539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6790 - val_loss: 4.9544\n",
      "Epoch 8540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9141 - val_loss: 5.6361\n",
      "Epoch 8541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6976 - val_loss: 5.0301\n",
      "Epoch 8542/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7026 - val_loss: 5.3176\n",
      "Epoch 8543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6706 - val_loss: 5.0939\n",
      "Epoch 8544/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6063 - val_loss: 4.9255\n",
      "Epoch 8545/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6037 - val_loss: 4.9290\n",
      "Epoch 8546/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9008 - val_loss: 5.1506\n",
      "Epoch 8547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6400 - val_loss: 4.9796\n",
      "Epoch 8548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7296 - val_loss: 5.0035\n",
      "Epoch 8549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6598 - val_loss: 5.1312\n",
      "Epoch 8550/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7685 - val_loss: 4.9213\n",
      "Epoch 8551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7604 - val_loss: 5.1126\n",
      "Epoch 8552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7559 - val_loss: 5.0183\n",
      "Epoch 8553/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7969 - val_loss: 5.0607\n",
      "Epoch 8554/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6447 - val_loss: 4.9388\n",
      "Epoch 8555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8931 - val_loss: 4.9821\n",
      "Epoch 8556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7167 - val_loss: 4.8936\n",
      "Epoch 8557/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6581 - val_loss: 4.9156\n",
      "Epoch 8558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7076 - val_loss: 5.2722\n",
      "Epoch 8559/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7973 - val_loss: 4.9379\n",
      "Epoch 8560/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6690 - val_loss: 4.9691\n",
      "Epoch 8561/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6582 - val_loss: 5.2162\n",
      "Epoch 8562/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7912 - val_loss: 5.4547\n",
      "Epoch 8563/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8329 - val_loss: 5.4585\n",
      "Epoch 8564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7001 - val_loss: 5.1077\n",
      "Epoch 8565/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7645 - val_loss: 4.9788\n",
      "Epoch 8566/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6789 - val_loss: 4.9846\n",
      "Epoch 8567/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7140 - val_loss: 5.7681\n",
      "Epoch 8568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6812 - val_loss: 4.9404\n",
      "Epoch 8569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6182 - val_loss: 4.9274\n",
      "Epoch 8570/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7559 - val_loss: 4.9221\n",
      "Epoch 8571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6423 - val_loss: 4.9411\n",
      "Epoch 8572/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7803 - val_loss: 5.2525\n",
      "Epoch 8573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9534 - val_loss: 5.2613\n",
      "Epoch 8574/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6283 - val_loss: 5.0134\n",
      "Epoch 8575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6261 - val_loss: 5.1839\n",
      "Epoch 8576/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7029 - val_loss: 5.0144\n",
      "Epoch 8577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6680 - val_loss: 5.1932\n",
      "Epoch 8578/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8016 - val_loss: 6.4611\n",
      "Epoch 8579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7571 - val_loss: 4.9241\n",
      "Epoch 8580/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6197 - val_loss: 4.8882\n",
      "Epoch 8581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8630 - val_loss: 4.9602\n",
      "Epoch 8582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6388 - val_loss: 5.4022\n",
      "Epoch 8583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6827 - val_loss: 4.9052\n",
      "Epoch 8584/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8375 - val_loss: 5.0257\n",
      "Epoch 8585/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2005 - val_loss: 5.1125\n",
      "Epoch 8586/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7365 - val_loss: 4.9482\n",
      "Epoch 8587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6431 - val_loss: 4.9626\n",
      "Epoch 8588/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7017 - val_loss: 4.9076\n",
      "Epoch 8589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7142 - val_loss: 4.9477\n",
      "Epoch 8590/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6737 - val_loss: 4.9392\n",
      "Epoch 8591/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7073 - val_loss: 4.9001\n",
      "Epoch 8592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6164 - val_loss: 4.9902\n",
      "Epoch 8593/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6155 - val_loss: 4.9312\n",
      "Epoch 8594/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5659 - val_loss: 4.9414\n",
      "Epoch 8595/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6570 - val_loss: 4.9248\n",
      "Epoch 8596/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9276 - val_loss: 5.0575\n",
      "Epoch 8597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9678 - val_loss: 5.0295\n",
      "Epoch 8598/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6753 - val_loss: 4.9499\n",
      "Epoch 8599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7963 - val_loss: 5.0720\n",
      "Epoch 8600/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6125 - val_loss: 4.9273\n",
      "Epoch 8601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6149 - val_loss: 5.0722\n",
      "Epoch 8602/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6470 - val_loss: 5.2464\n",
      "Epoch 8603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7189 - val_loss: 4.9642\n",
      "Epoch 8604/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6146 - val_loss: 4.9575\n",
      "Epoch 8605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6054 - val_loss: 5.0603\n",
      "Epoch 8606/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6151 - val_loss: 5.0450\n",
      "Epoch 8607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7033 - val_loss: 5.2633\n",
      "Epoch 8608/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7858 - val_loss: 5.1702\n",
      "Epoch 8609/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6024 - val_loss: 4.9881\n",
      "Epoch 8610/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6564 - val_loss: 4.9290\n",
      "Epoch 8611/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6565 - val_loss: 4.8811\n",
      "Epoch 8612/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6147 - val_loss: 5.1085\n",
      "Epoch 8613/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6859 - val_loss: 4.9558\n",
      "Epoch 8614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6166 - val_loss: 4.9309\n",
      "Epoch 8615/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7466 - val_loss: 4.9074\n",
      "Epoch 8616/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9204 - val_loss: 5.0194\n",
      "Epoch 8617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8161 - val_loss: 5.0414\n",
      "Epoch 8618/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7247 - val_loss: 4.9252\n",
      "Epoch 8619/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6656 - val_loss: 4.9143\n",
      "Epoch 8620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5812 - val_loss: 5.0280\n",
      "Epoch 8621/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6471 - val_loss: 4.9554\n",
      "Epoch 8622/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6504 - val_loss: 4.8822\n",
      "Epoch 8623/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6719 - val_loss: 5.6790\n",
      "Epoch 8624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7443 - val_loss: 4.9373\n",
      "Epoch 8625/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6091 - val_loss: 4.9624\n",
      "Epoch 8626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6886 - val_loss: 4.8926\n",
      "Epoch 8627/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6388 - val_loss: 4.9204\n",
      "Epoch 8628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6335 - val_loss: 4.9307\n",
      "Epoch 8629/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6100 - val_loss: 5.2546\n",
      "Epoch 8630/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0808 - val_loss: 5.1042\n",
      "Epoch 8631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7426 - val_loss: 5.0004\n",
      "Epoch 8632/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8588 - val_loss: 4.9614\n",
      "Epoch 8633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7028 - val_loss: 4.9419\n",
      "Epoch 8634/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5979 - val_loss: 5.1229\n",
      "Epoch 8635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7946 - val_loss: 5.0319\n",
      "Epoch 8636/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6248 - val_loss: 5.0555\n",
      "Epoch 8637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6701 - val_loss: 5.0462\n",
      "Epoch 8638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6816 - val_loss: 4.9099\n",
      "Epoch 8639/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6439 - val_loss: 4.9752\n",
      "Epoch 8640/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7454 - val_loss: 4.9668\n",
      "Epoch 8641/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6256 - val_loss: 5.1193\n",
      "Epoch 8642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8174 - val_loss: 4.9297\n",
      "Epoch 8643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5853 - val_loss: 4.9169\n",
      "Epoch 8644/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5617 - val_loss: 4.9270\n",
      "Epoch 8645/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6236 - val_loss: 5.2383\n",
      "Epoch 8646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7933 - val_loss: 4.9355\n",
      "Epoch 8647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6068 - val_loss: 4.9010\n",
      "Epoch 8648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7017 - val_loss: 5.0025\n",
      "Epoch 8649/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6912 - val_loss: 4.9372\n",
      "Epoch 8650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6846 - val_loss: 4.9196\n",
      "Epoch 8651/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6638 - val_loss: 5.0604\n",
      "Epoch 8652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6314 - val_loss: 4.9122\n",
      "Epoch 8653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6136 - val_loss: 4.9065\n",
      "Epoch 8654/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6256 - val_loss: 5.0887\n",
      "Epoch 8655/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8173 - val_loss: 4.8961\n",
      "Epoch 8656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6884 - val_loss: 5.0079\n",
      "Epoch 8657/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6823 - val_loss: 5.0858\n",
      "Epoch 8658/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9102 - val_loss: 4.9457\n",
      "Epoch 8659/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2009 - val_loss: 5.0829\n",
      "Epoch 8660/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8558 - val_loss: 4.8933\n",
      "Epoch 8661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7058 - val_loss: 5.2764\n",
      "Epoch 8662/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7006 - val_loss: 4.9200\n",
      "Epoch 8663/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.672 - 0s 68us/step - loss: 4.6032 - val_loss: 5.0140\n",
      "Epoch 8664/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7633 - val_loss: 5.1373\n",
      "Epoch 8665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6136 - val_loss: 4.9087\n",
      "Epoch 8666/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6858 - val_loss: 5.1554\n",
      "Epoch 8667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7942 - val_loss: 4.9052\n",
      "Epoch 8668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6828 - val_loss: 4.9056\n",
      "Epoch 8669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5833 - val_loss: 4.9086\n",
      "Epoch 8670/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6670 - val_loss: 5.2005\n",
      "Epoch 8671/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7531 - val_loss: 4.9186\n",
      "Epoch 8672/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7249 - val_loss: 5.0484\n",
      "Epoch 8673/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6707 - val_loss: 4.9200\n",
      "Epoch 8674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5952 - val_loss: 4.8879\n",
      "Epoch 8675/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6162 - val_loss: 4.9286\n",
      "Epoch 8676/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7363 - val_loss: 4.9127\n",
      "Epoch 8677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5982 - val_loss: 4.9181\n",
      "Epoch 8678/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6192 - val_loss: 6.1680\n",
      "Epoch 8679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9856 - val_loss: 4.9024\n",
      "Epoch 8680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6191 - val_loss: 4.9167\n",
      "Epoch 8681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5880 - val_loss: 4.9954\n",
      "Epoch 8682/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6595 - val_loss: 4.9932\n",
      "Epoch 8683/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7114 - val_loss: 4.9411\n",
      "Epoch 8684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6561 - val_loss: 4.8997\n",
      "Epoch 8685/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6060 - val_loss: 4.8962\n",
      "Epoch 8686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6416 - val_loss: 4.9192\n",
      "Epoch 8687/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6420 - val_loss: 4.9608\n",
      "Epoch 8688/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7051 - val_loss: 5.0852\n",
      "Epoch 8689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7579 - val_loss: 4.9269\n",
      "Epoch 8690/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6292 - val_loss: 4.9932\n",
      "Epoch 8691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5924 - val_loss: 5.0019\n",
      "Epoch 8692/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6225 - val_loss: 5.0033\n",
      "Epoch 8693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6695 - val_loss: 4.9837\n",
      "Epoch 8694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7431 - val_loss: 4.9325\n",
      "Epoch 8695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6504 - val_loss: 4.9153\n",
      "Epoch 8696/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6631 - val_loss: 4.9409\n",
      "Epoch 8697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6494 - val_loss: 4.9356\n",
      "Epoch 8698/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8017 - val_loss: 5.0081\n",
      "Epoch 8699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6366 - val_loss: 4.9434\n",
      "Epoch 8700/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6511 - val_loss: 4.9407\n",
      "Epoch 8701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7411 - val_loss: 5.1190\n",
      "Epoch 8702/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8285 - val_loss: 5.1240\n",
      "Epoch 8703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7573 - val_loss: 4.9431\n",
      "Epoch 8704/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6255 - val_loss: 5.2165\n",
      "Epoch 8705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7066 - val_loss: 4.9363\n",
      "Epoch 8706/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7051 - val_loss: 4.9431\n",
      "Epoch 8707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5841 - val_loss: 5.4163\n",
      "Epoch 8708/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0034 - val_loss: 5.1294\n",
      "Epoch 8709/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6793 - val_loss: 4.9569\n",
      "Epoch 8710/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6701 - val_loss: 4.9281\n",
      "Epoch 8711/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6146 - val_loss: 4.9032\n",
      "Epoch 8712/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6019 - val_loss: 4.8977\n",
      "Epoch 8713/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5702 - val_loss: 4.9345\n",
      "Epoch 8714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5635 - val_loss: 4.9363\n",
      "Epoch 8715/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7244 - val_loss: 4.9061\n",
      "Epoch 8716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6949 - val_loss: 5.0110\n",
      "Epoch 8717/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6385 - val_loss: 4.9082\n",
      "Epoch 8718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5978 - val_loss: 5.0918\n",
      "Epoch 8719/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8791 - val_loss: 4.8825\n",
      "Epoch 8720/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0195 - val_loss: 5.6008\n",
      "Epoch 8721/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8296 - val_loss: 4.9234\n",
      "Epoch 8722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5917 - val_loss: 4.9007\n",
      "Epoch 8723/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6054 - val_loss: 4.9156\n",
      "Epoch 8724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6003 - val_loss: 5.3331\n",
      "Epoch 8725/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2100 - val_loss: 4.9385\n",
      "Epoch 8726/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7295 - val_loss: 4.9096\n",
      "Epoch 8727/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6906 - val_loss: 4.9432\n",
      "Epoch 8728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6960 - val_loss: 5.0344\n",
      "Epoch 8729/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8760 - val_loss: 4.8918\n",
      "Epoch 8730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6619 - val_loss: 4.9064\n",
      "Epoch 8731/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7338 - val_loss: 4.9076\n",
      "Epoch 8732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6009 - val_loss: 4.8977\n",
      "Epoch 8733/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6370 - val_loss: 5.1005\n",
      "Epoch 8734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6652 - val_loss: 5.0837\n",
      "Epoch 8735/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6622 - val_loss: 4.9008\n",
      "Epoch 8736/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6503 - val_loss: 5.0989\n",
      "Epoch 8737/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8158 - val_loss: 4.9994\n",
      "Epoch 8738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6364 - val_loss: 5.2778\n",
      "Epoch 8739/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5996 - val_loss: 4.9521\n",
      "Epoch 8740/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6235 - val_loss: 4.9928\n",
      "Epoch 8741/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6632 - val_loss: 4.8892\n",
      "Epoch 8742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5974 - val_loss: 4.9266\n",
      "Epoch 8743/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6620 - val_loss: 4.9992\n",
      "Epoch 8744/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7111 - val_loss: 4.9638\n",
      "Epoch 8745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6313 - val_loss: 5.1378\n",
      "Epoch 8746/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7246 - val_loss: 6.3969\n",
      "Epoch 8747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7331 - val_loss: 5.1536\n",
      "Epoch 8748/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6803 - val_loss: 5.0104\n",
      "Epoch 8749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6299 - val_loss: 5.0575\n",
      "Epoch 8750/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7278 - val_loss: 4.9415\n",
      "Epoch 8751/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6994 - val_loss: 5.4904\n",
      "Epoch 8752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8384 - val_loss: 4.9827\n",
      "Epoch 8753/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6941 - val_loss: 4.9435\n",
      "Epoch 8754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6293 - val_loss: 4.9816\n",
      "Epoch 8755/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6519 - val_loss: 5.1321\n",
      "Epoch 8756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7494 - val_loss: 4.9728\n",
      "Epoch 8757/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6914 - val_loss: 4.9373\n",
      "Epoch 8758/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6510 - val_loss: 4.9802\n",
      "Epoch 8759/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7833 - val_loss: 4.9059\n",
      "Epoch 8760/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7785 - val_loss: 4.8949\n",
      "Epoch 8761/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6705 - val_loss: 5.5132\n",
      "Epoch 8762/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7334 - val_loss: 5.1283\n",
      "Epoch 8763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7365 - val_loss: 5.1100\n",
      "Epoch 8764/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7756 - val_loss: 5.1062\n",
      "Epoch 8765/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7345 - val_loss: 5.0942\n",
      "Epoch 8766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7161 - val_loss: 4.8826\n",
      "Epoch 8767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7169 - val_loss: 5.2494\n",
      "Epoch 8768/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7119 - val_loss: 4.9644\n",
      "Epoch 8769/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7005 - val_loss: 5.2181\n",
      "Epoch 8770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8817 - val_loss: 4.9201\n",
      "Epoch 8771/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5758 - val_loss: 4.9063\n",
      "Epoch 8772/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8186 - val_loss: 4.9312\n",
      "Epoch 8773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7463 - val_loss: 4.9325\n",
      "Epoch 8774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6388 - val_loss: 4.9514\n",
      "Epoch 8775/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6099 - val_loss: 4.8942\n",
      "Epoch 8776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6101 - val_loss: 5.0425\n",
      "Epoch 8777/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5883 - val_loss: 4.9963\n",
      "Epoch 8778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7867 - val_loss: 5.0130\n",
      "Epoch 8779/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6738 - val_loss: 5.0794\n",
      "Epoch 8780/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.669 - 0s 68us/step - loss: 4.5979 - val_loss: 4.9112\n",
      "Epoch 8781/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5945 - val_loss: 4.9364\n",
      "Epoch 8782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6753 - val_loss: 5.4703\n",
      "Epoch 8783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8294 - val_loss: 4.9106\n",
      "Epoch 8784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9389 - val_loss: 5.1430\n",
      "Epoch 8785/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7940 - val_loss: 4.9191\n",
      "Epoch 8786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5863 - val_loss: 4.9847\n",
      "Epoch 8787/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5956 - val_loss: 4.9602\n",
      "Epoch 8788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6400 - val_loss: 5.2537\n",
      "Epoch 8789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6730 - val_loss: 4.9222\n",
      "Epoch 8790/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6431 - val_loss: 4.9271\n",
      "Epoch 8791/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7063 - val_loss: 4.9328\n",
      "Epoch 8792/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6071 - val_loss: 5.0895\n",
      "Epoch 8793/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6137 - val_loss: 4.9281\n",
      "Epoch 8794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6464 - val_loss: 4.9575\n",
      "Epoch 8795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7910 - val_loss: 4.9070\n",
      "Epoch 8796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6694 - val_loss: 5.0571\n",
      "Epoch 8797/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6879 - val_loss: 4.9453\n",
      "Epoch 8798/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7239 - val_loss: 4.8997\n",
      "Epoch 8799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6196 - val_loss: 5.0028\n",
      "Epoch 8800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6254 - val_loss: 4.9404\n",
      "Epoch 8801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6262 - val_loss: 4.9172\n",
      "Epoch 8802/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5554 - val_loss: 4.9121\n",
      "Epoch 8803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5701 - val_loss: 5.1394\n",
      "Epoch 8804/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6989 - val_loss: 5.0757\n",
      "Epoch 8805/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.408 - 0s 46us/step - loss: 4.5957 - val_loss: 4.9177\n",
      "Epoch 8806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6720 - val_loss: 5.1006\n",
      "Epoch 8807/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9394 - val_loss: 4.9072\n",
      "Epoch 8808/20000\n",
      "685/685 [==============================] - 0s 137us/step - loss: 4.5910 - val_loss: 4.9161\n",
      "Epoch 8809/20000\n",
      "685/685 [==============================] - 0s 137us/step - loss: 4.6613 - val_loss: 5.0133\n",
      "Epoch 8810/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.6748 - val_loss: 4.9402\n",
      "Epoch 8811/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6323 - val_loss: 4.9847\n",
      "Epoch 8812/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7417 - val_loss: 5.2583\n",
      "Epoch 8813/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6699 - val_loss: 5.0845\n",
      "Epoch 8814/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5714 - val_loss: 5.0885\n",
      "Epoch 8815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7934 - val_loss: 4.8820\n",
      "Epoch 8816/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6647 - val_loss: 4.8878\n",
      "Epoch 8817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5844 - val_loss: 4.9079\n",
      "Epoch 8818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7721 - val_loss: 4.9072\n",
      "Epoch 8819/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8326 - val_loss: 4.9548\n",
      "Epoch 8820/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7464 - val_loss: 4.9546\n",
      "Epoch 8821/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6258 - val_loss: 4.9424\n",
      "Epoch 8822/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6159 - val_loss: 4.9617\n",
      "Epoch 8823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5839 - val_loss: 4.9416\n",
      "Epoch 8824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6995 - val_loss: 4.9170\n",
      "Epoch 8825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6276 - val_loss: 4.9197\n",
      "Epoch 8826/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6023 - val_loss: 4.9055\n",
      "Epoch 8827/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6140 - val_loss: 4.9709\n",
      "Epoch 8828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5838 - val_loss: 4.9374\n",
      "Epoch 8829/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6417 - val_loss: 5.2531\n",
      "Epoch 8830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7491 - val_loss: 5.3000\n",
      "Epoch 8831/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6674 - val_loss: 4.9207\n",
      "Epoch 8832/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6429 - val_loss: 4.9087\n",
      "Epoch 8833/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6680 - val_loss: 4.8889\n",
      "Epoch 8834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6099 - val_loss: 5.4544\n",
      "Epoch 8835/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7536 - val_loss: 4.9087\n",
      "Epoch 8836/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7580 - val_loss: 4.9162\n",
      "Epoch 8837/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6817 - val_loss: 4.9151\n",
      "Epoch 8838/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5925 - val_loss: 4.9117\n",
      "Epoch 8839/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6368 - val_loss: 4.9773\n",
      "Epoch 8840/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6614 - val_loss: 4.9197\n",
      "Epoch 8841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6769 - val_loss: 4.9043\n",
      "Epoch 8842/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7170 - val_loss: 4.9478\n",
      "Epoch 8843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6282 - val_loss: 5.0990\n",
      "Epoch 8844/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6537 - val_loss: 4.9296\n",
      "Epoch 8845/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6716 - val_loss: 4.8961\n",
      "Epoch 8846/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6278 - val_loss: 5.2857\n",
      "Epoch 8847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6221 - val_loss: 4.9475\n",
      "Epoch 8848/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6414 - val_loss: 5.0361\n",
      "Epoch 8849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6655 - val_loss: 5.0980\n",
      "Epoch 8850/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7848 - val_loss: 5.2523\n",
      "Epoch 8851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8662 - val_loss: 5.2157\n",
      "Epoch 8852/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1441 - val_loss: 4.9946\n",
      "Epoch 8853/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6557 - val_loss: 5.1184\n",
      "Epoch 8854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7429 - val_loss: 5.1021\n",
      "Epoch 8855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7519 - val_loss: 5.0253\n",
      "Epoch 8856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6472 - val_loss: 5.1387\n",
      "Epoch 8857/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6262 - val_loss: 4.9837\n",
      "Epoch 8858/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6718 - val_loss: 4.9165\n",
      "Epoch 8859/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5869 - val_loss: 4.9529\n",
      "Epoch 8860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5910 - val_loss: 4.9154\n",
      "Epoch 8861/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7190 - val_loss: 4.9075\n",
      "Epoch 8862/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7247 - val_loss: 5.3245\n",
      "Epoch 8863/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7948 - val_loss: 5.5174\n",
      "Epoch 8864/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8011 - val_loss: 5.0342\n",
      "Epoch 8865/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8336 - val_loss: 4.9573\n",
      "Epoch 8866/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7302 - val_loss: 4.9180\n",
      "Epoch 8867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5929 - val_loss: 4.9232\n",
      "Epoch 8868/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6673 - val_loss: 4.9089\n",
      "Epoch 8869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6947 - val_loss: 4.9027\n",
      "Epoch 8870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7814 - val_loss: 5.1971\n",
      "Epoch 8871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6857 - val_loss: 4.9234\n",
      "Epoch 8872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6883 - val_loss: 5.0720\n",
      "Epoch 8873/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6564 - val_loss: 5.4858\n",
      "Epoch 8874/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7487 - val_loss: 5.2559\n",
      "Epoch 8875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6584 - val_loss: 4.9491\n",
      "Epoch 8876/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5857 - val_loss: 4.9766\n",
      "Epoch 8877/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5980 - val_loss: 4.9718\n",
      "Epoch 8878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6806 - val_loss: 4.8767\n",
      "Epoch 8879/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6534 - val_loss: 6.0007\n",
      "Epoch 8880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0009 - val_loss: 5.0715\n",
      "Epoch 8881/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7022 - val_loss: 4.9652\n",
      "Epoch 8882/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7676 - val_loss: 5.0839\n",
      "Epoch 8883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5879 - val_loss: 5.0326\n",
      "Epoch 8884/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7009 - val_loss: 4.9161\n",
      "Epoch 8885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6434 - val_loss: 4.8973\n",
      "Epoch 8886/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8260 - val_loss: 5.6364\n",
      "Epoch 8887/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6904 - val_loss: 4.9345\n",
      "Epoch 8888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6139 - val_loss: 5.0359\n",
      "Epoch 8889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6674 - val_loss: 4.9734\n",
      "Epoch 8890/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8653 - val_loss: 4.9063\n",
      "Epoch 8891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6425 - val_loss: 4.9024\n",
      "Epoch 8892/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5674 - val_loss: 4.9801\n",
      "Epoch 8893/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6485 - val_loss: 4.9287\n",
      "Epoch 8894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6455 - val_loss: 5.0549\n",
      "Epoch 8895/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7443 - val_loss: 4.9303\n",
      "Epoch 8896/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7215 - val_loss: 4.8924\n",
      "Epoch 8897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6902 - val_loss: 4.9209\n",
      "Epoch 8898/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5869 - val_loss: 5.0581\n",
      "Epoch 8899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7751 - val_loss: 5.8407\n",
      "Epoch 8900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0833 - val_loss: 5.3064\n",
      "Epoch 8901/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9768 - val_loss: 5.3152\n",
      "Epoch 8902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7214 - val_loss: 5.0906\n",
      "Epoch 8903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6458 - val_loss: 4.9843\n",
      "Epoch 8904/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5938 - val_loss: 4.9052\n",
      "Epoch 8905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6522 - val_loss: 5.0000\n",
      "Epoch 8906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6080 - val_loss: 4.9614\n",
      "Epoch 8907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6023 - val_loss: 4.9590\n",
      "Epoch 8908/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7353 - val_loss: 4.8984\n",
      "Epoch 8909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6797 - val_loss: 5.0365\n",
      "Epoch 8910/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6439 - val_loss: 5.1642\n",
      "Epoch 8911/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6521 - val_loss: 4.9082\n",
      "Epoch 8912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7108 - val_loss: 4.9016\n",
      "Epoch 8913/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6769 - val_loss: 4.8954\n",
      "Epoch 8914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6870 - val_loss: 4.9974\n",
      "Epoch 8915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6519 - val_loss: 5.4799\n",
      "Epoch 8916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6766 - val_loss: 4.9078\n",
      "Epoch 8917/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6380 - val_loss: 4.8997\n",
      "Epoch 8918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5809 - val_loss: 4.9140\n",
      "Epoch 8919/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6347 - val_loss: 4.9422\n",
      "Epoch 8920/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6515 - val_loss: 4.9538\n",
      "Epoch 8921/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5785 - val_loss: 4.9455\n",
      "Epoch 8922/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6717 - val_loss: 4.9159\n",
      "Epoch 8923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6543 - val_loss: 4.9253\n",
      "Epoch 8924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6908 - val_loss: 4.9325\n",
      "Epoch 8925/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5751 - val_loss: 4.9148\n",
      "Epoch 8926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7740 - val_loss: 5.9577\n",
      "Epoch 8927/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0192 - val_loss: 4.9257\n",
      "Epoch 8928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5764 - val_loss: 5.0004\n",
      "Epoch 8929/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7789 - val_loss: 5.0911\n",
      "Epoch 8930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6652 - val_loss: 5.1977\n",
      "Epoch 8931/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7320 - val_loss: 4.9550\n",
      "Epoch 8932/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6101 - val_loss: 4.9874\n",
      "Epoch 8933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6591 - val_loss: 5.1030\n",
      "Epoch 8934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8569 - val_loss: 4.9257\n",
      "Epoch 8935/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6151 - val_loss: 5.0516\n",
      "Epoch 8936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5897 - val_loss: 5.0569\n",
      "Epoch 8937/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7166 - val_loss: 4.9138\n",
      "Epoch 8938/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5766 - val_loss: 5.6519\n",
      "Epoch 8939/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1393 - val_loss: 4.9307\n",
      "Epoch 8940/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7490 - val_loss: 4.9380\n",
      "Epoch 8941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6745 - val_loss: 4.8782\n",
      "Epoch 8942/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6048 - val_loss: 4.8766\n",
      "Epoch 8943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6245 - val_loss: 4.9058\n",
      "Epoch 8944/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6201 - val_loss: 5.9278\n",
      "Epoch 8945/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1111 - val_loss: 5.0191\n",
      "Epoch 8946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7555 - val_loss: 4.9312\n",
      "Epoch 8947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9211 - val_loss: 4.9090\n",
      "Epoch 8948/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8353 - val_loss: 4.8995\n",
      "Epoch 8949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6420 - val_loss: 5.0299\n",
      "Epoch 8950/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5808 - val_loss: 5.1180\n",
      "Epoch 8951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5955 - val_loss: 5.0024\n",
      "Epoch 8952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6657 - val_loss: 4.9555\n",
      "Epoch 8953/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7490 - val_loss: 4.9480\n",
      "Epoch 8954/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9364 - val_loss: 5.5502\n",
      "Epoch 8955/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7486 - val_loss: 5.0486\n",
      "Epoch 8956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6770 - val_loss: 4.8952\n",
      "Epoch 8957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8166 - val_loss: 4.9814\n",
      "Epoch 8958/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5832 - val_loss: 4.9101\n",
      "Epoch 8959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9172 - val_loss: 5.2994\n",
      "Epoch 8960/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0488 - val_loss: 5.1736\n",
      "Epoch 8961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7573 - val_loss: 4.9024\n",
      "Epoch 8962/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6015 - val_loss: 4.9233\n",
      "Epoch 8963/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8260 - val_loss: 5.4845\n",
      "Epoch 8964/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9237 - val_loss: 5.1636\n",
      "Epoch 8965/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6916 - val_loss: 4.8864\n",
      "Epoch 8966/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7444 - val_loss: 4.8946\n",
      "Epoch 8967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8307 - val_loss: 4.8966\n",
      "Epoch 8968/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7062 - val_loss: 5.0178\n",
      "Epoch 8969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7108 - val_loss: 5.0565\n",
      "Epoch 8970/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6123 - val_loss: 4.9969\n",
      "Epoch 8971/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6419 - val_loss: 4.8849\n",
      "Epoch 8972/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7779 - val_loss: 4.8912\n",
      "Epoch 8973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6257 - val_loss: 4.9298\n",
      "Epoch 8974/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6130 - val_loss: 4.9613\n",
      "Epoch 8975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8775 - val_loss: 4.9158\n",
      "Epoch 8976/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6184 - val_loss: 4.8972\n",
      "Epoch 8977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5371 - val_loss: 4.9790\n",
      "Epoch 8978/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9235 - val_loss: 4.9433\n",
      "Epoch 8979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6468 - val_loss: 4.9090\n",
      "Epoch 8980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6799 - val_loss: 5.3657\n",
      "Epoch 8981/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7713 - val_loss: 4.8963\n",
      "Epoch 8982/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7024 - val_loss: 5.1122\n",
      "Epoch 8983/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7795 - val_loss: 4.9089\n",
      "Epoch 8984/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6134 - val_loss: 4.9020\n",
      "Epoch 8985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5822 - val_loss: 4.8899\n",
      "Epoch 8986/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5497 - val_loss: 4.9078\n",
      "Epoch 8987/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6331 - val_loss: 5.1977\n",
      "Epoch 8988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7328 - val_loss: 5.0396\n",
      "Epoch 8989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6279 - val_loss: 4.9565\n",
      "Epoch 8990/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6657 - val_loss: 5.0444\n",
      "Epoch 8991/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6769 - val_loss: 5.0566\n",
      "Epoch 8992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6617 - val_loss: 5.1437\n",
      "Epoch 8993/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6926 - val_loss: 5.0370\n",
      "Epoch 8994/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6501 - val_loss: 5.7111\n",
      "Epoch 8995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9107 - val_loss: 4.9320\n",
      "Epoch 8996/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6453 - val_loss: 4.9237\n",
      "Epoch 8997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5934 - val_loss: 5.0108\n",
      "Epoch 8998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6333 - val_loss: 4.9514\n",
      "Epoch 8999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7713 - val_loss: 4.9663\n",
      "Epoch 9000/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6784 - val_loss: 4.9595\n",
      "Epoch 9001/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6168 - val_loss: 4.9227\n",
      "Epoch 9002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6465 - val_loss: 4.9964\n",
      "Epoch 9003/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7132 - val_loss: 4.9605\n",
      "Epoch 9004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7184 - val_loss: 5.4966\n",
      "Epoch 9005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6560 - val_loss: 4.8998\n",
      "Epoch 9006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5960 - val_loss: 4.8878\n",
      "Epoch 9007/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6612 - val_loss: 5.0099\n",
      "Epoch 9008/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6743 - val_loss: 4.8956\n",
      "Epoch 9009/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5957 - val_loss: 4.8942\n",
      "Epoch 9010/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8611 - val_loss: 4.9967\n",
      "Epoch 9011/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9975 - val_loss: 4.8821\n",
      "Epoch 9012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6517 - val_loss: 4.8947\n",
      "Epoch 9013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6042 - val_loss: 5.1241\n",
      "Epoch 9014/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6649 - val_loss: 5.3051\n",
      "Epoch 9015/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6989 - val_loss: 5.1017\n",
      "Epoch 9016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6147 - val_loss: 5.2610\n",
      "Epoch 9017/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7869 - val_loss: 5.1449\n",
      "Epoch 9018/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6467 - val_loss: 4.9107\n",
      "Epoch 9019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5824 - val_loss: 5.0520\n",
      "Epoch 9020/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5953 - val_loss: 4.9950\n",
      "Epoch 9021/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6181 - val_loss: 5.1744\n",
      "Epoch 9022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7901 - val_loss: 5.4450\n",
      "Epoch 9023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9394 - val_loss: 4.9195\n",
      "Epoch 9024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5855 - val_loss: 5.0110\n",
      "Epoch 9025/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6067 - val_loss: 5.2767\n",
      "Epoch 9026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6978 - val_loss: 5.0811\n",
      "Epoch 9027/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6213 - val_loss: 4.8673\n",
      "Epoch 9028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7030 - val_loss: 4.8841\n",
      "Epoch 9029/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6077 - val_loss: 5.1057\n",
      "Epoch 9030/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6438 - val_loss: 4.9223\n",
      "Epoch 9031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7030 - val_loss: 4.9211\n",
      "Epoch 9032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5939 - val_loss: 4.9341\n",
      "Epoch 9033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5644 - val_loss: 5.0589\n",
      "Epoch 9034/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6745 - val_loss: 5.0597\n",
      "Epoch 9035/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8591 - val_loss: 5.3464\n",
      "Epoch 9036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6266 - val_loss: 4.9315\n",
      "Epoch 9037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6224 - val_loss: 5.0921\n",
      "Epoch 9038/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5595 - val_loss: 5.0396\n",
      "Epoch 9039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5997 - val_loss: 5.0178\n",
      "Epoch 9040/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6335 - val_loss: 5.2368\n",
      "Epoch 9041/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6524 - val_loss: 4.9422\n",
      "Epoch 9042/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7149 - val_loss: 4.8713\n",
      "Epoch 9043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6394 - val_loss: 5.0277\n",
      "Epoch 9044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5919 - val_loss: 5.1058\n",
      "Epoch 9045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6352 - val_loss: 4.9946\n",
      "Epoch 9046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6399 - val_loss: 4.8830\n",
      "Epoch 9047/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7331 - val_loss: 4.9635\n",
      "Epoch 9048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5653 - val_loss: 5.1542\n",
      "Epoch 9049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6864 - val_loss: 5.1137\n",
      "Epoch 9050/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6896 - val_loss: 5.0460\n",
      "Epoch 9051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6603 - val_loss: 4.8979\n",
      "Epoch 9052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6060 - val_loss: 5.2208\n",
      "Epoch 9053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6930 - val_loss: 5.4414\n",
      "Epoch 9054/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6255 - val_loss: 4.9090\n",
      "Epoch 9055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5942 - val_loss: 5.0701\n",
      "Epoch 9056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8125 - val_loss: 4.9107\n",
      "Epoch 9057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6249 - val_loss: 5.0338\n",
      "Epoch 9058/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7728 - val_loss: 4.9265\n",
      "Epoch 9059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6037 - val_loss: 4.9021\n",
      "Epoch 9060/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6809 - val_loss: 5.0012\n",
      "Epoch 9061/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7558 - val_loss: 5.4909\n",
      "Epoch 9062/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6967 - val_loss: 4.9577\n",
      "Epoch 9063/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5797 - val_loss: 4.8875\n",
      "Epoch 9064/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7096 - val_loss: 5.5812\n",
      "Epoch 9065/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9336 - val_loss: 5.1063\n",
      "Epoch 9066/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7529 - val_loss: 5.1478\n",
      "Epoch 9067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6574 - val_loss: 4.9485\n",
      "Epoch 9068/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5778 - val_loss: 4.9797\n",
      "Epoch 9069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7231 - val_loss: 5.2475\n",
      "Epoch 9070/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7479 - val_loss: 4.8768\n",
      "Epoch 9071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5620 - val_loss: 5.0816\n",
      "Epoch 9072/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7130 - val_loss: 4.8977\n",
      "Epoch 9073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6477 - val_loss: 4.8907\n",
      "Epoch 9074/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7045 - val_loss: 4.8901\n",
      "Epoch 9075/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6053 - val_loss: 4.8996\n",
      "Epoch 9076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6041 - val_loss: 4.8954\n",
      "Epoch 9077/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5621 - val_loss: 5.1385\n",
      "Epoch 9078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7455 - val_loss: 5.0305\n",
      "Epoch 9079/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6231 - val_loss: 5.0285\n",
      "Epoch 9080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5802 - val_loss: 4.8953\n",
      "Epoch 9081/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6587 - val_loss: 5.4617\n",
      "Epoch 9082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9124 - val_loss: 5.1494\n",
      "Epoch 9083/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8246 - val_loss: 5.6684\n",
      "Epoch 9084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7116 - val_loss: 4.9054\n",
      "Epoch 9085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6209 - val_loss: 4.9513\n",
      "Epoch 9086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5899 - val_loss: 4.8775\n",
      "Epoch 9087/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5715 - val_loss: 4.8845\n",
      "Epoch 9088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5672 - val_loss: 4.9108\n",
      "Epoch 9089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5961 - val_loss: 5.0884\n",
      "Epoch 9090/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6292 - val_loss: 4.9314\n",
      "Epoch 9091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5816 - val_loss: 5.0784\n",
      "Epoch 9092/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6052 - val_loss: 4.8672\n",
      "Epoch 9093/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6478 - val_loss: 4.9148\n",
      "Epoch 9094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6640 - val_loss: 4.9138\n",
      "Epoch 9095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7654 - val_loss: 5.0362\n",
      "Epoch 9096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6308 - val_loss: 4.9881\n",
      "Epoch 9097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6123 - val_loss: 4.9160\n",
      "Epoch 9098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7627 - val_loss: 5.1076\n",
      "Epoch 9099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8570 - val_loss: 4.9311\n",
      "Epoch 9100/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6287 - val_loss: 4.9197\n",
      "Epoch 9101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6197 - val_loss: 4.9428\n",
      "Epoch 9102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6067 - val_loss: 4.9475\n",
      "Epoch 9103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7441 - val_loss: 5.1178\n",
      "Epoch 9104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7939 - val_loss: 5.0381\n",
      "Epoch 9105/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0666 - val_loss: 4.8997\n",
      "Epoch 9106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6406 - val_loss: 5.0177\n",
      "Epoch 9107/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5773 - val_loss: 5.0045\n",
      "Epoch 9108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6813 - val_loss: 4.9580\n",
      "Epoch 9109/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6470 - val_loss: 4.9045\n",
      "Epoch 9110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5606 - val_loss: 4.9310\n",
      "Epoch 9111/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6199 - val_loss: 4.8919\n",
      "Epoch 9112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6144 - val_loss: 4.9221\n",
      "Epoch 9113/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6400 - val_loss: 4.9284\n",
      "Epoch 9114/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5897 - val_loss: 4.9118\n",
      "Epoch 9115/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7759 - val_loss: 4.8895\n",
      "Epoch 9116/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7868 - val_loss: 4.9219\n",
      "Epoch 9117/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6296 - val_loss: 4.8878\n",
      "Epoch 9118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5681 - val_loss: 5.0054\n",
      "Epoch 9119/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7030 - val_loss: 4.8939\n",
      "Epoch 9120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5979 - val_loss: 4.9739\n",
      "Epoch 9121/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6602 - val_loss: 5.0788\n",
      "Epoch 9122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6713 - val_loss: 5.6316\n",
      "Epoch 9123/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7685 - val_loss: 5.0062\n",
      "Epoch 9124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6816 - val_loss: 4.8982\n",
      "Epoch 9125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7382 - val_loss: 5.0640\n",
      "Epoch 9126/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7921 - val_loss: 5.1197\n",
      "Epoch 9127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5912 - val_loss: 4.8755\n",
      "Epoch 9128/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8194 - val_loss: 4.9190\n",
      "Epoch 9129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6908 - val_loss: 4.8777\n",
      "Epoch 9130/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6118 - val_loss: 4.8782\n",
      "Epoch 9131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6610 - val_loss: 4.8915\n",
      "Epoch 9132/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6070 - val_loss: 4.8741\n",
      "Epoch 9133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5828 - val_loss: 5.1818\n",
      "Epoch 9134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6627 - val_loss: 4.9075\n",
      "Epoch 9135/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6438 - val_loss: 4.8873\n",
      "Epoch 9136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6171 - val_loss: 4.9119\n",
      "Epoch 9137/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6682 - val_loss: 5.2565\n",
      "Epoch 9138/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6600 - val_loss: 4.9962\n",
      "Epoch 9139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6608 - val_loss: 4.9846\n",
      "Epoch 9140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7042 - val_loss: 5.0044\n",
      "Epoch 9141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6654 - val_loss: 5.1978\n",
      "Epoch 9142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7589 - val_loss: 5.1762\n",
      "Epoch 9143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7227 - val_loss: 4.9517\n",
      "Epoch 9144/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6387 - val_loss: 4.9846\n",
      "Epoch 9145/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.6514 - val_loss: 4.8833\n",
      "Epoch 9146/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5981 - val_loss: 5.1081\n",
      "Epoch 9147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8609 - val_loss: 5.0921\n",
      "Epoch 9148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6761 - val_loss: 4.9869\n",
      "Epoch 9149/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6536 - val_loss: 4.9049\n",
      "Epoch 9150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6181 - val_loss: 4.9371\n",
      "Epoch 9151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8279 - val_loss: 4.8641\n",
      "Epoch 9152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6175 - val_loss: 4.8836\n",
      "Epoch 9153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6078 - val_loss: 5.0686\n",
      "Epoch 9154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6654 - val_loss: 4.9482\n",
      "Epoch 9155/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6205 - val_loss: 4.9274\n",
      "Epoch 9156/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6097 - val_loss: 4.9198\n",
      "Epoch 9157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5873 - val_loss: 5.1434\n",
      "Epoch 9158/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6414 - val_loss: 4.9110\n",
      "Epoch 9159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6155 - val_loss: 4.9389\n",
      "Epoch 9160/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6282 - val_loss: 5.0278\n",
      "Epoch 9161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6533 - val_loss: 4.9512\n",
      "Epoch 9162/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9185 - val_loss: 4.8985\n",
      "Epoch 9163/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7215 - val_loss: 4.8963\n",
      "Epoch 9164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6575 - val_loss: 4.8860\n",
      "Epoch 9165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6070 - val_loss: 4.9160\n",
      "Epoch 9166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5802 - val_loss: 5.1018\n",
      "Epoch 9167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6332 - val_loss: 5.1473\n",
      "Epoch 9168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7277 - val_loss: 5.3668\n",
      "Epoch 9169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6212 - val_loss: 4.9693\n",
      "Epoch 9170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7180 - val_loss: 5.2247\n",
      "Epoch 9171/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7232 - val_loss: 4.8990\n",
      "Epoch 9172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7177 - val_loss: 4.8919\n",
      "Epoch 9173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6079 - val_loss: 4.9107\n",
      "Epoch 9174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7845 - val_loss: 4.9147\n",
      "Epoch 9175/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7306 - val_loss: 4.9145\n",
      "Epoch 9176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7360 - val_loss: 4.9011\n",
      "Epoch 9177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6088 - val_loss: 4.9379\n",
      "Epoch 9178/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6105 - val_loss: 4.9949\n",
      "Epoch 9179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6251 - val_loss: 4.9154\n",
      "Epoch 9180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6458 - val_loss: 4.9154\n",
      "Epoch 9181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6329 - val_loss: 5.1412\n",
      "Epoch 9182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5900 - val_loss: 4.8908\n",
      "Epoch 9183/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6772 - val_loss: 5.6063\n",
      "Epoch 9184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8779 - val_loss: 4.9622\n",
      "Epoch 9185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6354 - val_loss: 5.2651\n",
      "Epoch 9186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6320 - val_loss: 4.9855\n",
      "Epoch 9187/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6211 - val_loss: 4.8673\n",
      "Epoch 9188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8740 - val_loss: 4.8949\n",
      "Epoch 9189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9357 - val_loss: 5.2610\n",
      "Epoch 9190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6707 - val_loss: 4.9285\n",
      "Epoch 9191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5977 - val_loss: 5.5977\n",
      "Epoch 9192/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7200 - val_loss: 4.9090\n",
      "Epoch 9193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6043 - val_loss: 4.9129\n",
      "Epoch 9194/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6275 - val_loss: 4.9537\n",
      "Epoch 9195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6750 - val_loss: 4.9373\n",
      "Epoch 9196/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6438 - val_loss: 5.0329\n",
      "Epoch 9197/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7956 - val_loss: 5.0126\n",
      "Epoch 9198/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6101 - val_loss: 4.9868\n",
      "Epoch 9199/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6185 - val_loss: 5.1382\n",
      "Epoch 9200/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.421 - 0s 46us/step - loss: 4.5991 - val_loss: 4.9177\n",
      "Epoch 9201/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5992 - val_loss: 5.0459\n",
      "Epoch 9202/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6174 - val_loss: 4.9273\n",
      "Epoch 9203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7451 - val_loss: 5.1870\n",
      "Epoch 9204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9880 - val_loss: 4.9003\n",
      "Epoch 9205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5734 - val_loss: 4.8776\n",
      "Epoch 9206/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6469 - val_loss: 4.9192\n",
      "Epoch 9207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6336 - val_loss: 4.9043\n",
      "Epoch 9208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5786 - val_loss: 4.9246\n",
      "Epoch 9209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6494 - val_loss: 4.9163\n",
      "Epoch 9210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5998 - val_loss: 4.9614\n",
      "Epoch 9211/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6780 - val_loss: 5.0948\n",
      "Epoch 9212/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6406 - val_loss: 4.9039\n",
      "Epoch 9213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6065 - val_loss: 4.9621\n",
      "Epoch 9214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6059 - val_loss: 5.1431\n",
      "Epoch 9215/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6953 - val_loss: 5.0142\n",
      "Epoch 9216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5940 - val_loss: 4.9511\n",
      "Epoch 9217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6909 - val_loss: 5.1103\n",
      "Epoch 9218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5891 - val_loss: 4.9305\n",
      "Epoch 9219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7018 - val_loss: 4.8785\n",
      "Epoch 9220/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6191 - val_loss: 4.8759\n",
      "Epoch 9221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6133 - val_loss: 4.8781\n",
      "Epoch 9222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6083 - val_loss: 4.8687\n",
      "Epoch 9223/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6239 - val_loss: 4.9017\n",
      "Epoch 9224/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0720 - val_loss: 5.3878\n",
      "Epoch 9225/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7403 - val_loss: 5.0553\n",
      "Epoch 9226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6309 - val_loss: 5.0130\n",
      "Epoch 9227/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6117 - val_loss: 4.8757\n",
      "Epoch 9228/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6750 - val_loss: 5.1143\n",
      "Epoch 9229/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5977 - val_loss: 5.0950\n",
      "Epoch 9230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5965 - val_loss: 4.9271\n",
      "Epoch 9231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7192 - val_loss: 4.9264\n",
      "Epoch 9232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6852 - val_loss: 5.0830\n",
      "Epoch 9233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5994 - val_loss: 4.8950\n",
      "Epoch 9234/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6527 - val_loss: 5.0188\n",
      "Epoch 9235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6224 - val_loss: 4.8976\n",
      "Epoch 9236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8836 - val_loss: 4.9425\n",
      "Epoch 9237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7790 - val_loss: 4.8853\n",
      "Epoch 9238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5943 - val_loss: 4.8772\n",
      "Epoch 9239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5908 - val_loss: 5.0151\n",
      "Epoch 9240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7169 - val_loss: 4.9815\n",
      "Epoch 9241/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8585 - val_loss: 4.8725\n",
      "Epoch 9242/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6412 - val_loss: 4.8928\n",
      "Epoch 9243/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6169 - val_loss: 5.3860\n",
      "Epoch 9244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6828 - val_loss: 4.8786\n",
      "Epoch 9245/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7643 - val_loss: 4.8605\n",
      "Epoch 9246/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6701 - val_loss: 4.9255\n",
      "Epoch 9247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6706 - val_loss: 4.9398\n",
      "Epoch 9248/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6510 - val_loss: 4.9479\n",
      "Epoch 9249/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5562 - val_loss: 4.9434\n",
      "Epoch 9250/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6754 - val_loss: 5.0080\n",
      "Epoch 9251/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7077 - val_loss: 5.2089\n",
      "Epoch 9252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7897 - val_loss: 5.1791\n",
      "Epoch 9253/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6690 - val_loss: 4.9492\n",
      "Epoch 9254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6398 - val_loss: 4.9688\n",
      "Epoch 9255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6455 - val_loss: 4.8567\n",
      "Epoch 9256/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5755 - val_loss: 4.9069\n",
      "Epoch 9257/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7113 - val_loss: 4.9181\n",
      "Epoch 9258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6413 - val_loss: 5.0771\n",
      "Epoch 9259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5630 - val_loss: 5.2712\n",
      "Epoch 9260/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6765 - val_loss: 4.9619\n",
      "Epoch 9261/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6206 - val_loss: 4.9136\n",
      "Epoch 9262/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5562 - val_loss: 4.9067\n",
      "Epoch 9263/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6087 - val_loss: 4.9774\n",
      "Epoch 9264/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6547 - val_loss: 4.8865\n",
      "Epoch 9265/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7120 - val_loss: 4.8732\n",
      "Epoch 9266/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7150 - val_loss: 4.9983\n",
      "Epoch 9267/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6439 - val_loss: 4.9129\n",
      "Epoch 9268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6395 - val_loss: 4.9596\n",
      "Epoch 9269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6679 - val_loss: 4.9916\n",
      "Epoch 9270/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5850 - val_loss: 4.9183\n",
      "Epoch 9271/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6046 - val_loss: 4.8766\n",
      "Epoch 9272/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6244 - val_loss: 4.9115\n",
      "Epoch 9273/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6177 - val_loss: 4.8998\n",
      "Epoch 9274/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5937 - val_loss: 4.8948\n",
      "Epoch 9275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5988 - val_loss: 5.0112\n",
      "Epoch 9276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7279 - val_loss: 5.0040\n",
      "Epoch 9277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7084 - val_loss: 5.1935\n",
      "Epoch 9278/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6156 - val_loss: 5.0132\n",
      "Epoch 9279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6051 - val_loss: 5.0245\n",
      "Epoch 9280/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6581 - val_loss: 4.8909\n",
      "Epoch 9281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6502 - val_loss: 4.9814\n",
      "Epoch 9282/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7276 - val_loss: 4.9116\n",
      "Epoch 9283/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6585 - val_loss: 5.2963\n",
      "Epoch 9284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6959 - val_loss: 4.9556\n",
      "Epoch 9285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7106 - val_loss: 4.8640\n",
      "Epoch 9286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6005 - val_loss: 4.8832\n",
      "Epoch 9287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5857 - val_loss: 4.8942\n",
      "Epoch 9288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6134 - val_loss: 4.9454\n",
      "Epoch 9289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5772 - val_loss: 4.9130\n",
      "Epoch 9290/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6320 - val_loss: 4.8624\n",
      "Epoch 9291/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5795 - val_loss: 4.8976\n",
      "Epoch 9292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5696 - val_loss: 4.8998\n",
      "Epoch 9293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6019 - val_loss: 5.0887\n",
      "Epoch 9294/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5980 - val_loss: 4.9218\n",
      "Epoch 9295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6095 - val_loss: 4.8928\n",
      "Epoch 9296/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5894 - val_loss: 4.9226\n",
      "Epoch 9297/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7296 - val_loss: 5.0438\n",
      "Epoch 9298/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7249 - val_loss: 4.9297\n",
      "Epoch 9299/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7301 - val_loss: 4.9588\n",
      "Epoch 9300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8121 - val_loss: 5.3137\n",
      "Epoch 9301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7597 - val_loss: 4.8709\n",
      "Epoch 9302/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5769 - val_loss: 4.9368\n",
      "Epoch 9303/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.7371 - val_loss: 4.8734\n",
      "Epoch 9304/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5695 - val_loss: 4.8654\n",
      "Epoch 9305/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6355 - val_loss: 4.9153\n",
      "Epoch 9306/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5679 - val_loss: 4.9678\n",
      "Epoch 9307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6516 - val_loss: 4.9153\n",
      "Epoch 9308/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6446 - val_loss: 4.9015\n",
      "Epoch 9309/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6410 - val_loss: 4.9118\n",
      "Epoch 9310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6726 - val_loss: 4.9706\n",
      "Epoch 9311/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7108 - val_loss: 4.9084\n",
      "Epoch 9312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6725 - val_loss: 5.2596\n",
      "Epoch 9313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7272 - val_loss: 5.0378\n",
      "Epoch 9314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5902 - val_loss: 4.9084\n",
      "Epoch 9315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6310 - val_loss: 5.1180\n",
      "Epoch 9316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6522 - val_loss: 5.0436\n",
      "Epoch 9317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6044 - val_loss: 4.9036\n",
      "Epoch 9318/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5769 - val_loss: 5.0216\n",
      "Epoch 9319/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.9990 - val_loss: 4.9269\n",
      "Epoch 9320/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0760 - val_loss: 5.2454\n",
      "Epoch 9321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7921 - val_loss: 4.9876\n",
      "Epoch 9322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6867 - val_loss: 5.2541\n",
      "Epoch 9323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7567 - val_loss: 4.9986\n",
      "Epoch 9324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5909 - val_loss: 4.9460\n",
      "Epoch 9325/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5930 - val_loss: 5.9160\n",
      "Epoch 9326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1030 - val_loss: 4.8972\n",
      "Epoch 9327/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.3478 - val_loss: 5.3962\n",
      "Epoch 9328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7459 - val_loss: 5.0340\n",
      "Epoch 9329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6103 - val_loss: 4.8821\n",
      "Epoch 9330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6397 - val_loss: 4.9863\n",
      "Epoch 9331/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5721 - val_loss: 4.9196\n",
      "Epoch 9332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6874 - val_loss: 5.0856\n",
      "Epoch 9333/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8924 - val_loss: 5.5892\n",
      "Epoch 9334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6408 - val_loss: 4.8822\n",
      "Epoch 9335/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7008 - val_loss: 4.8671\n",
      "Epoch 9336/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6633 - val_loss: 4.9614\n",
      "Epoch 9337/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6854 - val_loss: 5.2556\n",
      "Epoch 9338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6529 - val_loss: 4.9428\n",
      "Epoch 9339/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5973 - val_loss: 4.9331\n",
      "Epoch 9340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6142 - val_loss: 4.9527\n",
      "Epoch 9341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6124 - val_loss: 4.9489\n",
      "Epoch 9342/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6967 - val_loss: 4.9904\n",
      "Epoch 9343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5903 - val_loss: 4.8867\n",
      "Epoch 9344/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5728 - val_loss: 4.9614\n",
      "Epoch 9345/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6996 - val_loss: 4.8736\n",
      "Epoch 9346/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9912 - val_loss: 5.1629\n",
      "Epoch 9347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6139 - val_loss: 5.3127\n",
      "Epoch 9348/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7860 - val_loss: 4.9389\n",
      "Epoch 9349/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5676 - val_loss: 4.9380\n",
      "Epoch 9350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7584 - val_loss: 4.8700\n",
      "Epoch 9351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7653 - val_loss: 4.9077\n",
      "Epoch 9352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7138 - val_loss: 4.8555\n",
      "Epoch 9353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6323 - val_loss: 4.9933\n",
      "Epoch 9354/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6020 - val_loss: 4.9185\n",
      "Epoch 9355/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5604 - val_loss: 4.8782\n",
      "Epoch 9356/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5930 - val_loss: 5.1295\n",
      "Epoch 9357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5717 - val_loss: 4.9763\n",
      "Epoch 9358/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6246 - val_loss: 4.8767\n",
      "Epoch 9359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7616 - val_loss: 5.1820\n",
      "Epoch 9360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6515 - val_loss: 4.9061\n",
      "Epoch 9361/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6790 - val_loss: 5.0831\n",
      "Epoch 9362/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6766 - val_loss: 4.9132\n",
      "Epoch 9363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7890 - val_loss: 4.8863\n",
      "Epoch 9364/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6427 - val_loss: 4.9019\n",
      "Epoch 9365/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5857 - val_loss: 4.8633\n",
      "Epoch 9366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8513 - val_loss: 5.1104\n",
      "Epoch 9367/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 7.268 - 0s 46us/step - loss: 4.8015 - val_loss: 4.8990\n",
      "Epoch 9368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5830 - val_loss: 5.3550\n",
      "Epoch 9369/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7100 - val_loss: 4.9180\n",
      "Epoch 9370/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7366 - val_loss: 4.9182\n",
      "Epoch 9371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5995 - val_loss: 4.9151\n",
      "Epoch 9372/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6236 - val_loss: 4.9024\n",
      "Epoch 9373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5923 - val_loss: 5.1935\n",
      "Epoch 9374/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5749 - val_loss: 4.9553\n",
      "Epoch 9375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7557 - val_loss: 4.8736\n",
      "Epoch 9376/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5677 - val_loss: 4.9673\n",
      "Epoch 9377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6744 - val_loss: 4.8761\n",
      "Epoch 9378/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7025 - val_loss: 4.9325\n",
      "Epoch 9379/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.6628 - val_loss: 4.8660\n",
      "Epoch 9380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5737 - val_loss: 4.9063\n",
      "Epoch 9381/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6045 - val_loss: 4.9534\n",
      "Epoch 9382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6177 - val_loss: 5.1653\n",
      "Epoch 9383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6377 - val_loss: 4.9526\n",
      "Epoch 9384/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7317 - val_loss: 5.0235\n",
      "Epoch 9385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6057 - val_loss: 4.8690\n",
      "Epoch 9386/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5884 - val_loss: 5.0283\n",
      "Epoch 9387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7665 - val_loss: 4.9961\n",
      "Epoch 9388/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6812 - val_loss: 4.9558\n",
      "Epoch 9389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5900 - val_loss: 4.8984\n",
      "Epoch 9390/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5907 - val_loss: 5.0005\n",
      "Epoch 9391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6993 - val_loss: 5.1106\n",
      "Epoch 9392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9859 - val_loss: 4.8794\n",
      "Epoch 9393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9857 - val_loss: 5.0600\n",
      "Epoch 9394/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7116 - val_loss: 4.9231\n",
      "Epoch 9395/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6165 - val_loss: 4.9589\n",
      "Epoch 9396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6485 - val_loss: 5.1555\n",
      "Epoch 9397/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6524 - val_loss: 4.8965\n",
      "Epoch 9398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6885 - val_loss: 5.1190\n",
      "Epoch 9399/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7248 - val_loss: 4.8732\n",
      "Epoch 9400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5938 - val_loss: 4.8925\n",
      "Epoch 9401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7827 - val_loss: 4.9626\n",
      "Epoch 9402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5519 - val_loss: 4.8946\n",
      "Epoch 9403/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6913 - val_loss: 4.8554\n",
      "Epoch 9404/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6396 - val_loss: 4.8786\n",
      "Epoch 9405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6808 - val_loss: 5.0098\n",
      "Epoch 9406/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7227 - val_loss: 5.0639\n",
      "Epoch 9407/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6218 - val_loss: 4.8787\n",
      "Epoch 9408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6994 - val_loss: 5.0206\n",
      "Epoch 9409/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6415 - val_loss: 5.0069\n",
      "Epoch 9410/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5983 - val_loss: 4.9005\n",
      "Epoch 9411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5457 - val_loss: 4.9948\n",
      "Epoch 9412/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7560 - val_loss: 4.9484\n",
      "Epoch 9413/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9513 - val_loss: 5.0141\n",
      "Epoch 9414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7824 - val_loss: 4.8768\n",
      "Epoch 9415/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7186 - val_loss: 5.3785\n",
      "Epoch 9416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9795 - val_loss: 4.8769\n",
      "Epoch 9417/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5699 - val_loss: 4.8967\n",
      "Epoch 9418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6480 - val_loss: 4.9192\n",
      "Epoch 9419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5454 - val_loss: 4.9807\n",
      "Epoch 9420/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7993 - val_loss: 4.9119\n",
      "Epoch 9421/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7690 - val_loss: 4.9237\n",
      "Epoch 9422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7794 - val_loss: 4.9763\n",
      "Epoch 9423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8635 - val_loss: 5.3881\n",
      "Epoch 9424/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6921 - val_loss: 4.9585\n",
      "Epoch 9425/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6155 - val_loss: 4.8878\n",
      "Epoch 9426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5751 - val_loss: 4.8857\n",
      "Epoch 9427/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7185 - val_loss: 5.1000\n",
      "Epoch 9428/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5944 - val_loss: 4.8597\n",
      "Epoch 9429/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6131 - val_loss: 5.1694\n",
      "Epoch 9430/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.8764 - val_loss: 4.9267\n",
      "Epoch 9431/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6980 - val_loss: 4.9250\n",
      "Epoch 9432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5784 - val_loss: 5.0891\n",
      "Epoch 9433/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5918 - val_loss: 4.8928\n",
      "Epoch 9434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5760 - val_loss: 4.9237\n",
      "Epoch 9435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6442 - val_loss: 5.2913\n",
      "Epoch 9436/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0780 - val_loss: 4.8898\n",
      "Epoch 9437/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6510 - val_loss: 4.8692\n",
      "Epoch 9438/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5571 - val_loss: 4.9055\n",
      "Epoch 9439/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5600 - val_loss: 4.9215\n",
      "Epoch 9440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5830 - val_loss: 4.9706\n",
      "Epoch 9441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5687 - val_loss: 4.9104\n",
      "Epoch 9442/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5600 - val_loss: 4.8887\n",
      "Epoch 9443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6271 - val_loss: 4.8837\n",
      "Epoch 9444/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8354 - val_loss: 5.0033\n",
      "Epoch 9445/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7220 - val_loss: 5.2869\n",
      "Epoch 9446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6243 - val_loss: 4.9328\n",
      "Epoch 9447/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5744 - val_loss: 4.9110\n",
      "Epoch 9448/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5705 - val_loss: 4.9085\n",
      "Epoch 9449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8023 - val_loss: 5.5703\n",
      "Epoch 9450/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6477 - val_loss: 5.0507\n",
      "Epoch 9451/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6707 - val_loss: 5.4115\n",
      "Epoch 9452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6307 - val_loss: 5.1931\n",
      "Epoch 9453/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6415 - val_loss: 5.0286\n",
      "Epoch 9454/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7064 - val_loss: 5.4633\n",
      "Epoch 9455/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6724 - val_loss: 4.8955\n",
      "Epoch 9456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6250 - val_loss: 4.8843\n",
      "Epoch 9457/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5826 - val_loss: 4.9672\n",
      "Epoch 9458/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7081 - val_loss: 5.0405\n",
      "Epoch 9459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8815 - val_loss: 4.9871\n",
      "Epoch 9460/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6645 - val_loss: 4.8896\n",
      "Epoch 9461/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5684 - val_loss: 5.4768\n",
      "Epoch 9462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8145 - val_loss: 4.9356\n",
      "Epoch 9463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8038 - val_loss: 4.9398\n",
      "Epoch 9464/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6290 - val_loss: 4.9519\n",
      "Epoch 9465/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6437 - val_loss: 5.1028\n",
      "Epoch 9466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7722 - val_loss: 5.1540\n",
      "Epoch 9467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6455 - val_loss: 4.9971\n",
      "Epoch 9468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6438 - val_loss: 4.9024\n",
      "Epoch 9469/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6045 - val_loss: 5.0114\n",
      "Epoch 9470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6512 - val_loss: 5.0320\n",
      "Epoch 9471/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6288 - val_loss: 4.9097\n",
      "Epoch 9472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5639 - val_loss: 4.9301\n",
      "Epoch 9473/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6318 - val_loss: 4.8927\n",
      "Epoch 9474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7285 - val_loss: 4.9471\n",
      "Epoch 9475/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7264 - val_loss: 4.9221\n",
      "Epoch 9476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6800 - val_loss: 4.9539\n",
      "Epoch 9477/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5695 - val_loss: 4.9102\n",
      "Epoch 9478/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5783 - val_loss: 4.9010\n",
      "Epoch 9479/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8356 - val_loss: 5.1631\n",
      "Epoch 9480/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8270 - val_loss: 5.0837\n",
      "Epoch 9481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8964 - val_loss: 5.2765\n",
      "Epoch 9482/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6639 - val_loss: 4.8613\n",
      "Epoch 9483/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6040 - val_loss: 5.1393\n",
      "Epoch 9484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6439 - val_loss: 4.9752\n",
      "Epoch 9485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6706 - val_loss: 5.1981\n",
      "Epoch 9486/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6581 - val_loss: 4.9487\n",
      "Epoch 9487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6016 - val_loss: 4.8740\n",
      "Epoch 9488/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6856 - val_loss: 4.8815\n",
      "Epoch 9489/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6938 - val_loss: 4.8478\n",
      "Epoch 9490/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6441 - val_loss: 5.5839\n",
      "Epoch 9491/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6540 - val_loss: 4.8985\n",
      "Epoch 9492/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6213 - val_loss: 4.9068\n",
      "Epoch 9493/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8042 - val_loss: 4.8779\n",
      "Epoch 9494/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6405 - val_loss: 4.9034\n",
      "Epoch 9495/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6588 - val_loss: 4.9402\n",
      "Epoch 9496/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7055 - val_loss: 5.5656\n",
      "Epoch 9497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0552 - val_loss: 4.9268\n",
      "Epoch 9498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7788 - val_loss: 4.8944\n",
      "Epoch 9499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5602 - val_loss: 4.8912\n",
      "Epoch 9500/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6051 - val_loss: 4.8804\n",
      "Epoch 9501/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5646 - val_loss: 5.2446\n",
      "Epoch 9502/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7214 - val_loss: 5.0062\n",
      "Epoch 9503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7667 - val_loss: 4.8987\n",
      "Epoch 9504/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6575 - val_loss: 5.5082\n",
      "Epoch 9505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8997 - val_loss: 5.1621\n",
      "Epoch 9506/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8637 - val_loss: 5.3461\n",
      "Epoch 9507/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8641 - val_loss: 4.8922\n",
      "Epoch 9508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5717 - val_loss: 4.8753\n",
      "Epoch 9509/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6233 - val_loss: 5.0455\n",
      "Epoch 9510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7290 - val_loss: 4.8840\n",
      "Epoch 9511/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5607 - val_loss: 4.8834\n",
      "Epoch 9512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7398 - val_loss: 4.8545\n",
      "Epoch 9513/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5966 - val_loss: 5.1206\n",
      "Epoch 9514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5683 - val_loss: 4.8869\n",
      "Epoch 9515/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6230 - val_loss: 4.9813\n",
      "Epoch 9516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7054 - val_loss: 4.9891\n",
      "Epoch 9517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6700 - val_loss: 5.2054\n",
      "Epoch 9518/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6035 - val_loss: 4.8840\n",
      "Epoch 9519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6727 - val_loss: 4.9073\n",
      "Epoch 9520/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5844 - val_loss: 5.1135\n",
      "Epoch 9521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6264 - val_loss: 4.8781\n",
      "Epoch 9522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5649 - val_loss: 4.8904\n",
      "Epoch 9523/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8773 - val_loss: 4.9596\n",
      "Epoch 9524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6611 - val_loss: 4.8846\n",
      "Epoch 9525/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6209 - val_loss: 5.1301\n",
      "Epoch 9526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6325 - val_loss: 4.8759\n",
      "Epoch 9527/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6488 - val_loss: 5.0234\n",
      "Epoch 9528/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6956 - val_loss: 4.8710\n",
      "Epoch 9529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6058 - val_loss: 4.9035\n",
      "Epoch 9530/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6029 - val_loss: 4.9245\n",
      "Epoch 9531/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5786 - val_loss: 4.8868\n",
      "Epoch 9532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5972 - val_loss: 4.9015\n",
      "Epoch 9533/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5903 - val_loss: 4.9343\n",
      "Epoch 9534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6603 - val_loss: 4.8685\n",
      "Epoch 9535/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7813 - val_loss: 5.5483\n",
      "Epoch 9536/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7697 - val_loss: 5.1604\n",
      "Epoch 9537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8836 - val_loss: 5.3879\n",
      "Epoch 9538/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1948 - val_loss: 5.4550\n",
      "Epoch 9539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7110 - val_loss: 4.9955\n",
      "Epoch 9540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5738 - val_loss: 4.9045\n",
      "Epoch 9541/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6176 - val_loss: 4.9559\n",
      "Epoch 9542/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6046 - val_loss: 4.9739\n",
      "Epoch 9543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5870 - val_loss: 4.9820\n",
      "Epoch 9544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7476 - val_loss: 4.9190\n",
      "Epoch 9545/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5765 - val_loss: 4.9802\n",
      "Epoch 9546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8823 - val_loss: 4.9799\n",
      "Epoch 9547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6480 - val_loss: 4.8787\n",
      "Epoch 9548/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5901 - val_loss: 4.9115\n",
      "Epoch 9549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6409 - val_loss: 4.8947\n",
      "Epoch 9550/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5720 - val_loss: 4.8817\n",
      "Epoch 9551/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6227 - val_loss: 4.9867\n",
      "Epoch 9552/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6011 - val_loss: 4.8827\n",
      "Epoch 9553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5506 - val_loss: 5.0657\n",
      "Epoch 9554/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9916 - val_loss: 5.2181\n",
      "Epoch 9555/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1438 - val_loss: 4.9789\n",
      "Epoch 9556/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6317 - val_loss: 5.1204\n",
      "Epoch 9557/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6842 - val_loss: 4.8981\n",
      "Epoch 9558/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5869 - val_loss: 5.0445\n",
      "Epoch 9559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5667 - val_loss: 4.8653\n",
      "Epoch 9560/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5593 - val_loss: 4.9007\n",
      "Epoch 9561/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6001 - val_loss: 4.8912\n",
      "Epoch 9562/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7462 - val_loss: 4.8908\n",
      "Epoch 9563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7199 - val_loss: 5.0032\n",
      "Epoch 9564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7292 - val_loss: 4.9019\n",
      "Epoch 9565/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8024 - val_loss: 5.0464\n",
      "Epoch 9566/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5679 - val_loss: 5.0003\n",
      "Epoch 9567/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5821 - val_loss: 5.0105\n",
      "Epoch 9568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5626 - val_loss: 5.1618\n",
      "Epoch 9569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5684 - val_loss: 4.8893\n",
      "Epoch 9570/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5873 - val_loss: 4.8625\n",
      "Epoch 9571/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7062 - val_loss: 4.9601\n",
      "Epoch 9572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6234 - val_loss: 4.9874\n",
      "Epoch 9573/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6310 - val_loss: 4.9856\n",
      "Epoch 9574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5634 - val_loss: 4.8928\n",
      "Epoch 9575/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6339 - val_loss: 5.0178\n",
      "Epoch 9576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6216 - val_loss: 5.0333\n",
      "Epoch 9577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6829 - val_loss: 5.1251\n",
      "Epoch 9578/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2584 - val_loss: 6.4371\n",
      "Epoch 9579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8377 - val_loss: 4.9253\n",
      "Epoch 9580/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6019 - val_loss: 5.1867\n",
      "Epoch 9581/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7000 - val_loss: 4.9882\n",
      "Epoch 9582/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5655 - val_loss: 4.9165\n",
      "Epoch 9583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6029 - val_loss: 4.9676\n",
      "Epoch 9584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5852 - val_loss: 4.9274\n",
      "Epoch 9585/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5876 - val_loss: 4.8853\n",
      "Epoch 9586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8029 - val_loss: 4.9086\n",
      "Epoch 9587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8699 - val_loss: 6.2559\n",
      "Epoch 9588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8803 - val_loss: 5.0858\n",
      "Epoch 9589/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7269 - val_loss: 4.8679\n",
      "Epoch 9590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5907 - val_loss: 4.8577\n",
      "Epoch 9591/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5799 - val_loss: 4.9002\n",
      "Epoch 9592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5563 - val_loss: 5.1018\n",
      "Epoch 9593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5861 - val_loss: 4.8837\n",
      "Epoch 9594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7738 - val_loss: 4.9296\n",
      "Epoch 9595/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6305 - val_loss: 5.1880\n",
      "Epoch 9596/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6162 - val_loss: 4.8927\n",
      "Epoch 9597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6603 - val_loss: 5.5519\n",
      "Epoch 9598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7174 - val_loss: 5.1293\n",
      "Epoch 9599/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.872 - 0s 68us/step - loss: 4.5761 - val_loss: 4.8600\n",
      "Epoch 9600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6162 - val_loss: 5.0116\n",
      "Epoch 9601/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6046 - val_loss: 4.9072\n",
      "Epoch 9602/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6117 - val_loss: 4.8860\n",
      "Epoch 9603/20000\n",
      "685/685 [==============================] - 0s 137us/step - loss: 4.6954 - val_loss: 4.9443\n",
      "Epoch 9604/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5493 - val_loss: 4.8950\n",
      "Epoch 9605/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5831 - val_loss: 4.9391\n",
      "Epoch 9606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9898 - val_loss: 4.8837\n",
      "Epoch 9607/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7175 - val_loss: 5.2216\n",
      "Epoch 9608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9080 - val_loss: 5.2438\n",
      "Epoch 9609/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8069 - val_loss: 4.9950\n",
      "Epoch 9610/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6209 - val_loss: 4.8957\n",
      "Epoch 9611/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0080 - val_loss: 5.3395\n",
      "Epoch 9612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8124 - val_loss: 5.0687\n",
      "Epoch 9613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9961 - val_loss: 4.8661\n",
      "Epoch 9614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7432 - val_loss: 5.0020\n",
      "Epoch 9615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7954 - val_loss: 4.8987\n",
      "Epoch 9616/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7450 - val_loss: 4.8709\n",
      "Epoch 9617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5444 - val_loss: 4.8812\n",
      "Epoch 9618/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5803 - val_loss: 4.9104\n",
      "Epoch 9619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5703 - val_loss: 4.8739\n",
      "Epoch 9620/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6629 - val_loss: 4.9470\n",
      "Epoch 9621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5931 - val_loss: 4.9138\n",
      "Epoch 9622/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6403 - val_loss: 4.8820\n",
      "Epoch 9623/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6468 - val_loss: 4.8595\n",
      "Epoch 9624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5682 - val_loss: 4.9765\n",
      "Epoch 9625/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5837 - val_loss: 5.0422\n",
      "Epoch 9626/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6588 - val_loss: 4.9871\n",
      "Epoch 9627/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7114 - val_loss: 4.9783\n",
      "Epoch 9628/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6994 - val_loss: 4.9495\n",
      "Epoch 9629/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7109 - val_loss: 5.4397\n",
      "Epoch 9630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6559 - val_loss: 4.8690\n",
      "Epoch 9631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5580 - val_loss: 5.1582\n",
      "Epoch 9632/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8465 - val_loss: 4.9102\n",
      "Epoch 9633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7792 - val_loss: 5.0821\n",
      "Epoch 9634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8178 - val_loss: 4.8767\n",
      "Epoch 9635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6866 - val_loss: 5.4880\n",
      "Epoch 9636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7531 - val_loss: 5.1368\n",
      "Epoch 9637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6915 - val_loss: 4.8976\n",
      "Epoch 9638/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5789 - val_loss: 4.9102\n",
      "Epoch 9639/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5982 - val_loss: 4.8897\n",
      "Epoch 9640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5539 - val_loss: 4.8790\n",
      "Epoch 9641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5811 - val_loss: 5.0717\n",
      "Epoch 9642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6343 - val_loss: 4.8950\n",
      "Epoch 9643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5771 - val_loss: 5.0620\n",
      "Epoch 9644/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7107 - val_loss: 4.9126\n",
      "Epoch 9645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7237 - val_loss: 4.9233\n",
      "Epoch 9646/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5597 - val_loss: 4.8819\n",
      "Epoch 9647/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6927 - val_loss: 5.0415\n",
      "Epoch 9648/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7968 - val_loss: 5.0218\n",
      "Epoch 9649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8199 - val_loss: 4.8863\n",
      "Epoch 9650/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5540 - val_loss: 4.8496\n",
      "Epoch 9651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6109 - val_loss: 5.0567\n",
      "Epoch 9652/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6093 - val_loss: 4.8762\n",
      "Epoch 9653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5973 - val_loss: 4.8962\n",
      "Epoch 9654/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8531 - val_loss: 5.4502\n",
      "Epoch 9655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6839 - val_loss: 4.9610\n",
      "Epoch 9656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7161 - val_loss: 4.8549\n",
      "Epoch 9657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6080 - val_loss: 4.8638\n",
      "Epoch 9658/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6875 - val_loss: 4.8927\n",
      "Epoch 9659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5817 - val_loss: 5.2544\n",
      "Epoch 9660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6124 - val_loss: 4.8788\n",
      "Epoch 9661/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8924 - val_loss: 5.0478\n",
      "Epoch 9662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7619 - val_loss: 4.9194\n",
      "Epoch 9663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6709 - val_loss: 4.8552\n",
      "Epoch 9664/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5773 - val_loss: 4.9197\n",
      "Epoch 9665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6457 - val_loss: 4.9599\n",
      "Epoch 9666/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6586 - val_loss: 5.1227\n",
      "Epoch 9667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6575 - val_loss: 5.0301\n",
      "Epoch 9668/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6271 - val_loss: 4.8866\n",
      "Epoch 9669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6439 - val_loss: 4.8567\n",
      "Epoch 9670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5926 - val_loss: 4.8610\n",
      "Epoch 9671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5617 - val_loss: 4.8588\n",
      "Epoch 9672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6412 - val_loss: 4.9261\n",
      "Epoch 9673/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7601 - val_loss: 5.1072\n",
      "Epoch 9674/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0915 - val_loss: 4.8956\n",
      "Epoch 9675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7093 - val_loss: 4.9467\n",
      "Epoch 9676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6356 - val_loss: 4.9554\n",
      "Epoch 9677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6503 - val_loss: 5.1200\n",
      "Epoch 9678/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5541 - val_loss: 4.8653\n",
      "Epoch 9679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6126 - val_loss: 5.0349\n",
      "Epoch 9680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5837 - val_loss: 4.9528\n",
      "Epoch 9681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8259 - val_loss: 5.1182\n",
      "Epoch 9682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8020 - val_loss: 4.8870\n",
      "Epoch 9683/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6257 - val_loss: 4.9101\n",
      "Epoch 9684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6273 - val_loss: 4.8830\n",
      "Epoch 9685/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5781 - val_loss: 4.9515\n",
      "Epoch 9686/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6258 - val_loss: 4.9632\n",
      "Epoch 9687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5797 - val_loss: 4.9680\n",
      "Epoch 9688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5430 - val_loss: 4.9384\n",
      "Epoch 9689/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5540 - val_loss: 5.0950\n",
      "Epoch 9690/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7392 - val_loss: 4.9915\n",
      "Epoch 9691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6504 - val_loss: 4.9057\n",
      "Epoch 9692/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6693 - val_loss: 4.8981\n",
      "Epoch 9693/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6195 - val_loss: 4.8870\n",
      "Epoch 9694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6043 - val_loss: 4.8841\n",
      "Epoch 9695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6105 - val_loss: 4.8953\n",
      "Epoch 9696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6327 - val_loss: 4.8759\n",
      "Epoch 9697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6332 - val_loss: 4.8690\n",
      "Epoch 9698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7279 - val_loss: 5.0978\n",
      "Epoch 9699/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6144 - val_loss: 4.9079\n",
      "Epoch 9700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6035 - val_loss: 4.9169\n",
      "Epoch 9701/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6582 - val_loss: 5.0644\n",
      "Epoch 9702/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9095 - val_loss: 5.0608\n",
      "Epoch 9703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5777 - val_loss: 4.8701\n",
      "Epoch 9704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6833 - val_loss: 5.2222\n",
      "Epoch 9705/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6974 - val_loss: 4.9471\n",
      "Epoch 9706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6812 - val_loss: 5.0012\n",
      "Epoch 9707/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6682 - val_loss: 4.8530\n",
      "Epoch 9708/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7527 - val_loss: 4.8616\n",
      "Epoch 9709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6250 - val_loss: 4.9249\n",
      "Epoch 9710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6008 - val_loss: 4.9052\n",
      "Epoch 9711/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6143 - val_loss: 5.0551\n",
      "Epoch 9712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6002 - val_loss: 4.9085\n",
      "Epoch 9713/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6169 - val_loss: 5.1633\n",
      "Epoch 9714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5953 - val_loss: 4.9112\n",
      "Epoch 9715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6351 - val_loss: 4.8837\n",
      "Epoch 9716/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7097 - val_loss: 4.8750\n",
      "Epoch 9717/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5713 - val_loss: 5.1839\n",
      "Epoch 9718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6328 - val_loss: 4.8654\n",
      "Epoch 9719/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6715 - val_loss: 5.1433\n",
      "Epoch 9720/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0944 - val_loss: 4.8947\n",
      "Epoch 9721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8739 - val_loss: 4.8819\n",
      "Epoch 9722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6197 - val_loss: 4.8868\n",
      "Epoch 9723/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5781 - val_loss: 5.0664\n",
      "Epoch 9724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6017 - val_loss: 4.9060\n",
      "Epoch 9725/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6063 - val_loss: 5.0192\n",
      "Epoch 9726/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5748 - val_loss: 4.8968\n",
      "Epoch 9727/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7454 - val_loss: 4.8712\n",
      "Epoch 9728/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6747 - val_loss: 4.8797\n",
      "Epoch 9729/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5908 - val_loss: 4.9030\n",
      "Epoch 9730/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6930 - val_loss: 4.8657\n",
      "Epoch 9731/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8911 - val_loss: 4.8926\n",
      "Epoch 9732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7712 - val_loss: 4.8676\n",
      "Epoch 9733/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5893 - val_loss: 4.9264\n",
      "Epoch 9734/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5963 - val_loss: 4.8388\n",
      "Epoch 9735/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5799 - val_loss: 4.9117\n",
      "Epoch 9736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6924 - val_loss: 4.9405\n",
      "Epoch 9737/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.5810 - val_loss: 5.0854\n",
      "Epoch 9738/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6370 - val_loss: 4.8868\n",
      "Epoch 9739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7876 - val_loss: 5.2348\n",
      "Epoch 9740/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6813 - val_loss: 4.8653\n",
      "Epoch 9741/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6649 - val_loss: 4.9348\n",
      "Epoch 9742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5385 - val_loss: 4.8741\n",
      "Epoch 9743/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5915 - val_loss: 4.9275\n",
      "Epoch 9744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5811 - val_loss: 4.8941\n",
      "Epoch 9745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7525 - val_loss: 4.8950\n",
      "Epoch 9746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5445 - val_loss: 4.9199\n",
      "Epoch 9747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6016 - val_loss: 4.8477\n",
      "Epoch 9748/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5690 - val_loss: 4.9526\n",
      "Epoch 9749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5668 - val_loss: 4.9106\n",
      "Epoch 9750/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6312 - val_loss: 4.8782\n",
      "Epoch 9751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5946 - val_loss: 4.8746\n",
      "Epoch 9752/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5644 - val_loss: 4.8923\n",
      "Epoch 9753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7276 - val_loss: 4.8404\n",
      "Epoch 9754/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5988 - val_loss: 5.2146\n",
      "Epoch 9755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8132 - val_loss: 5.0901\n",
      "Epoch 9756/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5980 - val_loss: 4.8847\n",
      "Epoch 9757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5885 - val_loss: 5.1801\n",
      "Epoch 9758/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6215 - val_loss: 4.9068\n",
      "Epoch 9759/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5681 - val_loss: 4.9366\n",
      "Epoch 9760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5648 - val_loss: 4.9589\n",
      "Epoch 9761/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6548 - val_loss: 4.8527\n",
      "Epoch 9762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5504 - val_loss: 4.8851\n",
      "Epoch 9763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6093 - val_loss: 5.1060\n",
      "Epoch 9764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6126 - val_loss: 4.9450\n",
      "Epoch 9765/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6047 - val_loss: 5.7538\n",
      "Epoch 9766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9118 - val_loss: 5.0116\n",
      "Epoch 9767/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6187 - val_loss: 4.9502\n",
      "Epoch 9768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5591 - val_loss: 5.2051\n",
      "Epoch 9769/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6728 - val_loss: 4.8847\n",
      "Epoch 9770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6355 - val_loss: 4.8775\n",
      "Epoch 9771/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6164 - val_loss: 4.9972\n",
      "Epoch 9772/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7096 - val_loss: 4.9153\n",
      "Epoch 9773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6193 - val_loss: 4.9554\n",
      "Epoch 9774/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5873 - val_loss: 4.8847\n",
      "Epoch 9775/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9872 - val_loss: 5.1021\n",
      "Epoch 9776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.2866 - val_loss: 4.9842\n",
      "Epoch 9777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7562 - val_loss: 4.8870\n",
      "Epoch 9778/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5445 - val_loss: 5.0453\n",
      "Epoch 9779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6909 - val_loss: 4.9940\n",
      "Epoch 9780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6077 - val_loss: 4.8807\n",
      "Epoch 9781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5705 - val_loss: 4.9015\n",
      "Epoch 9782/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5815 - val_loss: 4.9900\n",
      "Epoch 9783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8872 - val_loss: 4.8878\n",
      "Epoch 9784/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6626 - val_loss: 5.0123\n",
      "Epoch 9785/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6196 - val_loss: 4.9028\n",
      "Epoch 9786/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5689 - val_loss: 4.8915\n",
      "Epoch 9787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5938 - val_loss: 5.0510\n",
      "Epoch 9788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7232 - val_loss: 5.0069\n",
      "Epoch 9789/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5863 - val_loss: 5.4040\n",
      "Epoch 9790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6121 - val_loss: 4.9149\n",
      "Epoch 9791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7062 - val_loss: 5.0371\n",
      "Epoch 9792/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7146 - val_loss: 4.9671\n",
      "Epoch 9793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5866 - val_loss: 4.8534\n",
      "Epoch 9794/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5378 - val_loss: 4.8819\n",
      "Epoch 9795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5849 - val_loss: 4.8495\n",
      "Epoch 9796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5636 - val_loss: 4.8678\n",
      "Epoch 9797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6494 - val_loss: 4.9603\n",
      "Epoch 9798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6753 - val_loss: 5.0567\n",
      "Epoch 9799/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7358 - val_loss: 4.9171\n",
      "Epoch 9800/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5538 - val_loss: 4.8673\n",
      "Epoch 9801/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6403 - val_loss: 5.1005\n",
      "Epoch 9802/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6853 - val_loss: 4.9249\n",
      "Epoch 9803/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5710 - val_loss: 4.9179\n",
      "Epoch 9804/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7153 - val_loss: 4.8678\n",
      "Epoch 9805/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5433 - val_loss: 5.0587\n",
      "Epoch 9806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6112 - val_loss: 4.8753\n",
      "Epoch 9807/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6375 - val_loss: 4.9129\n",
      "Epoch 9808/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6765 - val_loss: 4.8858\n",
      "Epoch 9809/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6294 - val_loss: 4.9385\n",
      "Epoch 9810/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5965 - val_loss: 4.9788\n",
      "Epoch 9811/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5276 - val_loss: 4.9435\n",
      "Epoch 9812/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5396 - val_loss: 4.8847\n",
      "Epoch 9813/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6638 - val_loss: 4.8645\n",
      "Epoch 9814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5944 - val_loss: 4.8785\n",
      "Epoch 9815/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7102 - val_loss: 4.9023\n",
      "Epoch 9816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8157 - val_loss: 5.0469\n",
      "Epoch 9817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9267 - val_loss: 4.8960\n",
      "Epoch 9818/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5635 - val_loss: 4.9421\n",
      "Epoch 9819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5778 - val_loss: 4.8871\n",
      "Epoch 9820/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7474 - val_loss: 4.9251\n",
      "Epoch 9821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8123 - val_loss: 6.1191\n",
      "Epoch 9822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8386 - val_loss: 4.8658\n",
      "Epoch 9823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5932 - val_loss: 4.8940\n",
      "Epoch 9824/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6451 - val_loss: 5.0491\n",
      "Epoch 9825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6768 - val_loss: 5.3308\n",
      "Epoch 9826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6735 - val_loss: 4.9243\n",
      "Epoch 9827/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.7035 - val_loss: 4.8722\n",
      "Epoch 9828/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5746 - val_loss: 4.8232\n",
      "Epoch 9829/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5816 - val_loss: 4.8683\n",
      "Epoch 9830/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6149 - val_loss: 4.9485\n",
      "Epoch 9831/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7240 - val_loss: 4.8844\n",
      "Epoch 9832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7533 - val_loss: 4.9978\n",
      "Epoch 9833/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6507 - val_loss: 4.9872\n",
      "Epoch 9834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5991 - val_loss: 4.9108\n",
      "Epoch 9835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6110 - val_loss: 4.8682\n",
      "Epoch 9836/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6068 - val_loss: 4.8905\n",
      "Epoch 9837/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.6256 - val_loss: 4.8993\n",
      "Epoch 9838/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6022 - val_loss: 4.8479\n",
      "Epoch 9839/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6928 - val_loss: 4.9641\n",
      "Epoch 9840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5913 - val_loss: 4.8810\n",
      "Epoch 9841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5816 - val_loss: 4.8647\n",
      "Epoch 9842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6650 - val_loss: 5.0462\n",
      "Epoch 9843/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6869 - val_loss: 4.8744\n",
      "Epoch 9844/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.677 - 0s 46us/step - loss: 4.7726 - val_loss: 5.7320\n",
      "Epoch 9845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9211 - val_loss: 4.8700\n",
      "Epoch 9846/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8080 - val_loss: 4.9905\n",
      "Epoch 9847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7107 - val_loss: 4.9165\n",
      "Epoch 9848/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7905 - val_loss: 4.8604\n",
      "Epoch 9849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5587 - val_loss: 4.8676\n",
      "Epoch 9850/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6374 - val_loss: 4.9337\n",
      "Epoch 9851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6045 - val_loss: 4.8848\n",
      "Epoch 9852/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6486 - val_loss: 4.8898\n",
      "Epoch 9853/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5669 - val_loss: 5.1379\n",
      "Epoch 9854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6067 - val_loss: 4.9122\n",
      "Epoch 9855/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5855 - val_loss: 4.8670\n",
      "Epoch 9856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5637 - val_loss: 4.8812\n",
      "Epoch 9857/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6577 - val_loss: 4.8765\n",
      "Epoch 9858/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8654 - val_loss: 4.9038\n",
      "Epoch 9859/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6255 - val_loss: 4.8703\n",
      "Epoch 9860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6147 - val_loss: 4.8793\n",
      "Epoch 9861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5685 - val_loss: 5.1031\n",
      "Epoch 9862/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6055 - val_loss: 4.9011\n",
      "Epoch 9863/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5510 - val_loss: 5.0272\n",
      "Epoch 9864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6260 - val_loss: 5.2993\n",
      "Epoch 9865/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9734 - val_loss: 5.2056\n",
      "Epoch 9866/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6172 - val_loss: 5.4025\n",
      "Epoch 9867/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6949 - val_loss: 4.9014\n",
      "Epoch 9868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5627 - val_loss: 4.9353\n",
      "Epoch 9869/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6216 - val_loss: 4.8898\n",
      "Epoch 9870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7536 - val_loss: 5.3825\n",
      "Epoch 9871/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7862 - val_loss: 5.9801\n",
      "Epoch 9872/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1858 - val_loss: 4.9243\n",
      "Epoch 9873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6374 - val_loss: 4.9367\n",
      "Epoch 9874/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5839 - val_loss: 4.8716\n",
      "Epoch 9875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5309 - val_loss: 4.8726\n",
      "Epoch 9876/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5534 - val_loss: 4.9084\n",
      "Epoch 9877/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6251 - val_loss: 4.8623\n",
      "Epoch 9878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6423 - val_loss: 4.8827\n",
      "Epoch 9879/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5932 - val_loss: 4.9618\n",
      "Epoch 9880/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5711 - val_loss: 4.8745\n",
      "Epoch 9881/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6321 - val_loss: 5.2340\n",
      "Epoch 9882/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6145 - val_loss: 4.8994\n",
      "Epoch 9883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6902 - val_loss: 4.9936\n",
      "Epoch 9884/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6520 - val_loss: 4.8547\n",
      "Epoch 9885/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5988 - val_loss: 4.8813\n",
      "Epoch 9886/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5951 - val_loss: 4.8692\n",
      "Epoch 9887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5942 - val_loss: 4.8679\n",
      "Epoch 9888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7664 - val_loss: 5.1653\n",
      "Epoch 9889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6883 - val_loss: 4.9233\n",
      "Epoch 9890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6264 - val_loss: 5.0298\n",
      "Epoch 9891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5786 - val_loss: 4.8559\n",
      "Epoch 9892/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5872 - val_loss: 4.8937\n",
      "Epoch 9893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8059 - val_loss: 4.8463\n",
      "Epoch 9894/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5864 - val_loss: 4.8636\n",
      "Epoch 9895/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6079 - val_loss: 4.8757\n",
      "Epoch 9896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6228 - val_loss: 5.1081\n",
      "Epoch 9897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6532 - val_loss: 5.1002\n",
      "Epoch 9898/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7733 - val_loss: 5.2956\n",
      "Epoch 9899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8007 - val_loss: 5.0093\n",
      "Epoch 9900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6531 - val_loss: 4.8517\n",
      "Epoch 9901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7408 - val_loss: 4.9940\n",
      "Epoch 9902/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5965 - val_loss: 4.8500\n",
      "Epoch 9903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6298 - val_loss: 4.9046\n",
      "Epoch 9904/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6276 - val_loss: 4.9599\n",
      "Epoch 9905/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6723 - val_loss: 5.0372\n",
      "Epoch 9906/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8248 - val_loss: 5.1688\n",
      "Epoch 9907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5799 - val_loss: 4.8848\n",
      "Epoch 9908/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5786 - val_loss: 4.9354\n",
      "Epoch 9909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6021 - val_loss: 4.8523\n",
      "Epoch 9910/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6896 - val_loss: 4.9479\n",
      "Epoch 9911/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9641 - val_loss: 4.8902\n",
      "Epoch 9912/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6383 - val_loss: 4.8552\n",
      "Epoch 9913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7344 - val_loss: 6.1342\n",
      "Epoch 9914/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8436 - val_loss: 4.8416\n",
      "Epoch 9915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5828 - val_loss: 4.8982\n",
      "Epoch 9916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6182 - val_loss: 4.8677\n",
      "Epoch 9917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5446 - val_loss: 5.0577\n",
      "Epoch 9918/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5603 - val_loss: 4.9860\n",
      "Epoch 9919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6125 - val_loss: 5.3385\n",
      "Epoch 9920/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6457 - val_loss: 4.8923\n",
      "Epoch 9921/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.6184 - val_loss: 4.9068\n",
      "Epoch 9922/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6429 - val_loss: 4.8711\n",
      "Epoch 9923/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7127 - val_loss: 5.1426\n",
      "Epoch 9924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6984 - val_loss: 4.8955\n",
      "Epoch 9925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5680 - val_loss: 4.9318\n",
      "Epoch 9926/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5670 - val_loss: 4.9189\n",
      "Epoch 9927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5830 - val_loss: 4.8727\n",
      "Epoch 9928/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8412 - val_loss: 4.9644\n",
      "Epoch 9929/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9573 - val_loss: 5.0987\n",
      "Epoch 9930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5725 - val_loss: 4.8812\n",
      "Epoch 9931/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5878 - val_loss: 5.1803\n",
      "Epoch 9932/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6043 - val_loss: 5.2734\n",
      "Epoch 9933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5923 - val_loss: 4.8640\n",
      "Epoch 9934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6419 - val_loss: 4.9747\n",
      "Epoch 9935/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5602 - val_loss: 4.8722\n",
      "Epoch 9936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5712 - val_loss: 4.8681\n",
      "Epoch 9937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6226 - val_loss: 4.8832\n",
      "Epoch 9938/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5922 - val_loss: 4.9797\n",
      "Epoch 9939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6236 - val_loss: 4.8853\n",
      "Epoch 9940/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7859 - val_loss: 4.9260\n",
      "Epoch 9941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7287 - val_loss: 5.3923\n",
      "Epoch 9942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6331 - val_loss: 4.9725\n",
      "Epoch 9943/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5970 - val_loss: 4.9120\n",
      "Epoch 9944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5325 - val_loss: 4.9060\n",
      "Epoch 9945/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5727 - val_loss: 4.9541\n",
      "Epoch 9946/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5947 - val_loss: 5.1662\n",
      "Epoch 9947/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6367 - val_loss: 5.3393\n",
      "Epoch 9948/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7999 - val_loss: 5.1244\n",
      "Epoch 9949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7745 - val_loss: 5.0469\n",
      "Epoch 9950/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5954 - val_loss: 5.2112\n",
      "Epoch 9951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7299 - val_loss: 4.8825\n",
      "Epoch 9952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6057 - val_loss: 4.8511\n",
      "Epoch 9953/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7270 - val_loss: 5.0951\n",
      "Epoch 9954/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6890 - val_loss: 4.9974\n",
      "Epoch 9955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7360 - val_loss: 4.9565\n",
      "Epoch 9956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6004 - val_loss: 4.9682\n",
      "Epoch 9957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5673 - val_loss: 4.8782\n",
      "Epoch 9958/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6614 - val_loss: 4.8701\n",
      "Epoch 9959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8501 - val_loss: 5.5775\n",
      "Epoch 9960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9412 - val_loss: 5.2092\n",
      "Epoch 9961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1217 - val_loss: 4.9543\n",
      "Epoch 9962/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8922 - val_loss: 4.9469\n",
      "Epoch 9963/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6867 - val_loss: 5.6759\n",
      "Epoch 9964/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7115 - val_loss: 4.9067\n",
      "Epoch 9965/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5908 - val_loss: 4.8733\n",
      "Epoch 9966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7301 - val_loss: 4.8727\n",
      "Epoch 9967/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6664 - val_loss: 4.8551\n",
      "Epoch 9968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7861 - val_loss: 4.8823\n",
      "Epoch 9969/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5721 - val_loss: 4.9025\n",
      "Epoch 9970/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6124 - val_loss: 4.8761\n",
      "Epoch 9971/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6379 - val_loss: 4.8984\n",
      "Epoch 9972/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7695 - val_loss: 4.9965\n",
      "Epoch 9973/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6435 - val_loss: 5.1259\n",
      "Epoch 9974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6452 - val_loss: 4.8744\n",
      "Epoch 9975/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6669 - val_loss: 5.0678\n",
      "Epoch 9976/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5730 - val_loss: 4.8614\n",
      "Epoch 9977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6717 - val_loss: 4.9465\n",
      "Epoch 9978/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8571 - val_loss: 5.0445\n",
      "Epoch 9979/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6618 - val_loss: 4.8661\n",
      "Epoch 9980/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6333 - val_loss: 4.8874\n",
      "Epoch 9981/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6050 - val_loss: 4.8776\n",
      "Epoch 9982/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6622 - val_loss: 4.8626\n",
      "Epoch 9983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6095 - val_loss: 4.8598\n",
      "Epoch 9984/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6498 - val_loss: 5.0346\n",
      "Epoch 9985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5740 - val_loss: 4.8896\n",
      "Epoch 9986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6676 - val_loss: 4.9234\n",
      "Epoch 9987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6278 - val_loss: 5.4166\n",
      "Epoch 9988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6348 - val_loss: 4.8835\n",
      "Epoch 9989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5723 - val_loss: 5.0187\n",
      "Epoch 9990/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7314 - val_loss: 5.0644\n",
      "Epoch 9991/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6435 - val_loss: 4.8932\n",
      "Epoch 9992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6264 - val_loss: 4.9000\n",
      "Epoch 9993/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7595 - val_loss: 4.9364\n",
      "Epoch 9994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6017 - val_loss: 4.8565\n",
      "Epoch 9995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5714 - val_loss: 4.8770\n",
      "Epoch 9996/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6914 - val_loss: 5.2173\n",
      "Epoch 9997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6077 - val_loss: 4.8613\n",
      "Epoch 9998/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6568 - val_loss: 5.0104\n",
      "Epoch 9999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7282 - val_loss: 4.9398\n",
      "Epoch 10000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6172 - val_loss: 4.8858\n",
      "Epoch 10001/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5216 - val_loss: 4.8543\n",
      "Epoch 10002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5939 - val_loss: 4.9834\n",
      "Epoch 10003/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6158 - val_loss: 4.9169\n",
      "Epoch 10004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5464 - val_loss: 4.9438\n",
      "Epoch 10005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5587 - val_loss: 4.8967\n",
      "Epoch 10006/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5766 - val_loss: 4.8650\n",
      "Epoch 10007/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5575 - val_loss: 4.9975\n",
      "Epoch 10008/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7475 - val_loss: 4.8785\n",
      "Epoch 10009/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5807 - val_loss: 4.9388\n",
      "Epoch 10010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6906 - val_loss: 4.8818\n",
      "Epoch 10011/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8527 - val_loss: 5.3468\n",
      "Epoch 10012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8983 - val_loss: 4.9030\n",
      "Epoch 10013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9034 - val_loss: 5.0681\n",
      "Epoch 10014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7104 - val_loss: 5.0844\n",
      "Epoch 10015/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7250 - val_loss: 5.4194\n",
      "Epoch 10016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0089 - val_loss: 4.8876\n",
      "Epoch 10017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6966 - val_loss: 4.9470\n",
      "Epoch 10018/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6310 - val_loss: 4.9132\n",
      "Epoch 10019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5480 - val_loss: 5.1541\n",
      "Epoch 10020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7170 - val_loss: 4.8577\n",
      "Epoch 10021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6566 - val_loss: 5.0341\n",
      "Epoch 10022/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6335 - val_loss: 4.8738\n",
      "Epoch 10023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6574 - val_loss: 4.9063\n",
      "Epoch 10024/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.7899 - val_loss: 4.8621\n",
      "Epoch 10025/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6801 - val_loss: 4.8854\n",
      "Epoch 10026/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7580 - val_loss: 4.8379\n",
      "Epoch 10027/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5629 - val_loss: 4.8823\n",
      "Epoch 10028/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5956 - val_loss: 5.0156\n",
      "Epoch 10029/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6595 - val_loss: 4.9107\n",
      "Epoch 10030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5518 - val_loss: 4.8733\n",
      "Epoch 10031/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6592 - val_loss: 4.9114\n",
      "Epoch 10032/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7424 - val_loss: 4.8517\n",
      "Epoch 10033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5973 - val_loss: 4.8875\n",
      "Epoch 10034/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6262 - val_loss: 4.9432\n",
      "Epoch 10035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5326 - val_loss: 4.8667\n",
      "Epoch 10036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6550 - val_loss: 4.8742\n",
      "Epoch 10037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5792 - val_loss: 4.9454\n",
      "Epoch 10038/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5624 - val_loss: 4.8768\n",
      "Epoch 10039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5599 - val_loss: 4.8637\n",
      "Epoch 10040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5933 - val_loss: 4.8559\n",
      "Epoch 10041/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7338 - val_loss: 4.9486\n",
      "Epoch 10042/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6135 - val_loss: 4.8898\n",
      "Epoch 10043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6124 - val_loss: 4.9254\n",
      "Epoch 10044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7268 - val_loss: 5.1197\n",
      "Epoch 10045/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6387 - val_loss: 4.8939\n",
      "Epoch 10046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5435 - val_loss: 4.9947\n",
      "Epoch 10047/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5913 - val_loss: 4.8423\n",
      "Epoch 10048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6401 - val_loss: 4.9500\n",
      "Epoch 10049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7551 - val_loss: 5.2502\n",
      "Epoch 10050/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6189 - val_loss: 4.8447\n",
      "Epoch 10051/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5409 - val_loss: 4.8686\n",
      "Epoch 10052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7340 - val_loss: 5.5963\n",
      "Epoch 10053/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.2929 - val_loss: 4.9233\n",
      "Epoch 10054/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6382 - val_loss: 4.9926\n",
      "Epoch 10055/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5615 - val_loss: 4.9782\n",
      "Epoch 10056/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5446 - val_loss: 4.8602\n",
      "Epoch 10057/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5731 - val_loss: 4.8675\n",
      "Epoch 10058/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6690 - val_loss: 4.8765\n",
      "Epoch 10059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6194 - val_loss: 4.8622\n",
      "Epoch 10060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5729 - val_loss: 4.8900\n",
      "Epoch 10061/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5740 - val_loss: 5.2848\n",
      "Epoch 10062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7645 - val_loss: 4.8766\n",
      "Epoch 10063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5691 - val_loss: 4.9518\n",
      "Epoch 10064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5849 - val_loss: 5.1040\n",
      "Epoch 10065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6110 - val_loss: 4.8608\n",
      "Epoch 10066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6072 - val_loss: 4.8406\n",
      "Epoch 10067/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5145 - val_loss: 5.0797\n",
      "Epoch 10068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7025 - val_loss: 4.8495\n",
      "Epoch 10069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6961 - val_loss: 6.0319\n",
      "Epoch 10070/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6509 - val_loss: 4.8260\n",
      "Epoch 10071/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7117 - val_loss: 5.1203\n",
      "Epoch 10072/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9611 - val_loss: 5.1486\n",
      "Epoch 10073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6159 - val_loss: 4.8604\n",
      "Epoch 10074/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6161 - val_loss: 4.9292\n",
      "Epoch 10075/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5740 - val_loss: 5.1424\n",
      "Epoch 10076/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0539 - val_loss: 4.9373\n",
      "Epoch 10077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5744 - val_loss: 4.8630\n",
      "Epoch 10078/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6570 - val_loss: 4.8504\n",
      "Epoch 10079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6155 - val_loss: 4.8582\n",
      "Epoch 10080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6160 - val_loss: 4.8565\n",
      "Epoch 10081/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5714 - val_loss: 4.8340\n",
      "Epoch 10082/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5702 - val_loss: 4.8752\n",
      "Epoch 10083/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5484 - val_loss: 5.2973\n",
      "Epoch 10084/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6822 - val_loss: 5.2168\n",
      "Epoch 10085/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5841 - val_loss: 5.0226\n",
      "Epoch 10086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7074 - val_loss: 5.1184\n",
      "Epoch 10087/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7563 - val_loss: 4.8783\n",
      "Epoch 10088/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5408 - val_loss: 5.0005\n",
      "Epoch 10089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6278 - val_loss: 4.8983\n",
      "Epoch 10090/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6219 - val_loss: 4.8508\n",
      "Epoch 10091/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5949 - val_loss: 4.8869\n",
      "Epoch 10092/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5379 - val_loss: 4.8809\n",
      "Epoch 10093/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5482 - val_loss: 4.8584\n",
      "Epoch 10094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5681 - val_loss: 4.8603\n",
      "Epoch 10095/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5711 - val_loss: 4.8664\n",
      "Epoch 10096/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6336 - val_loss: 5.0079\n",
      "Epoch 10097/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5825 - val_loss: 4.9315\n",
      "Epoch 10098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6160 - val_loss: 4.8707\n",
      "Epoch 10099/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6829 - val_loss: 4.8866\n",
      "Epoch 10100/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7349 - val_loss: 4.8804\n",
      "Epoch 10101/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5432 - val_loss: 4.8632\n",
      "Epoch 10102/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5406 - val_loss: 4.8892\n",
      "Epoch 10103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5613 - val_loss: 4.8888\n",
      "Epoch 10104/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6959 - val_loss: 5.1262\n",
      "Epoch 10105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6357 - val_loss: 4.8195\n",
      "Epoch 10106/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7051 - val_loss: 4.8943\n",
      "Epoch 10107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8948 - val_loss: 4.8988\n",
      "Epoch 10108/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9077 - val_loss: 4.9033\n",
      "Epoch 10109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6637 - val_loss: 4.8466\n",
      "Epoch 10110/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6318 - val_loss: 5.0340\n",
      "Epoch 10111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7310 - val_loss: 5.3092\n",
      "Epoch 10112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6858 - val_loss: 4.8780\n",
      "Epoch 10113/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5441 - val_loss: 4.9151\n",
      "Epoch 10114/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5942 - val_loss: 4.8500\n",
      "Epoch 10115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5866 - val_loss: 4.8708\n",
      "Epoch 10116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5858 - val_loss: 4.8718\n",
      "Epoch 10117/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6369 - val_loss: 4.9040\n",
      "Epoch 10118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5798 - val_loss: 5.1037\n",
      "Epoch 10119/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5419 - val_loss: 4.8653\n",
      "Epoch 10120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5924 - val_loss: 4.8795\n",
      "Epoch 10121/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6375 - val_loss: 4.8392\n",
      "Epoch 10122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5739 - val_loss: 5.0986\n",
      "Epoch 10123/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7387 - val_loss: 5.1067\n",
      "Epoch 10124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6210 - val_loss: 4.9282\n",
      "Epoch 10125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5992 - val_loss: 4.9639\n",
      "Epoch 10126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6075 - val_loss: 5.0333\n",
      "Epoch 10127/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6755 - val_loss: 4.9802\n",
      "Epoch 10128/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5626 - val_loss: 5.1796\n",
      "Epoch 10129/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6496 - val_loss: 5.0353\n",
      "Epoch 10130/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5895 - val_loss: 5.0169\n",
      "Epoch 10131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5634 - val_loss: 4.9066\n",
      "Epoch 10132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6403 - val_loss: 4.8595\n",
      "Epoch 10133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6598 - val_loss: 4.8564\n",
      "Epoch 10134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6743 - val_loss: 4.8911\n",
      "Epoch 10135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7012 - val_loss: 4.9379\n",
      "Epoch 10136/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6156 - val_loss: 4.8540\n",
      "Epoch 10137/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5878 - val_loss: 4.8722\n",
      "Epoch 10138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5914 - val_loss: 4.8777\n",
      "Epoch 10139/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5887 - val_loss: 5.0290\n",
      "Epoch 10140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7101 - val_loss: 5.0252\n",
      "Epoch 10141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6076 - val_loss: 4.9913\n",
      "Epoch 10142/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7142 - val_loss: 4.9020\n",
      "Epoch 10143/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6909 - val_loss: 4.9316\n",
      "Epoch 10144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7988 - val_loss: 4.9573\n",
      "Epoch 10145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1431 - val_loss: 5.0550\n",
      "Epoch 10146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7843 - val_loss: 4.8466\n",
      "Epoch 10147/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6737 - val_loss: 4.9672\n",
      "Epoch 10148/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9325 - val_loss: 5.0055\n",
      "Epoch 10149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6120 - val_loss: 4.8447\n",
      "Epoch 10150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5894 - val_loss: 4.9874\n",
      "Epoch 10151/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5904 - val_loss: 4.8496\n",
      "Epoch 10152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5520 - val_loss: 4.8694\n",
      "Epoch 10153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6684 - val_loss: 5.0537\n",
      "Epoch 10154/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6589 - val_loss: 5.0365\n",
      "Epoch 10155/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5695 - val_loss: 4.9813\n",
      "Epoch 10156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7683 - val_loss: 4.9384\n",
      "Epoch 10157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8196 - val_loss: 4.8637\n",
      "Epoch 10158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6228 - val_loss: 4.8770\n",
      "Epoch 10159/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5457 - val_loss: 4.8671\n",
      "Epoch 10160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5707 - val_loss: 4.9682\n",
      "Epoch 10161/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5958 - val_loss: 4.8415\n",
      "Epoch 10162/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6013 - val_loss: 4.8520\n",
      "Epoch 10163/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6079 - val_loss: 5.2818\n",
      "Epoch 10164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5997 - val_loss: 4.9544\n",
      "Epoch 10165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5965 - val_loss: 5.0049\n",
      "Epoch 10166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6452 - val_loss: 5.2462\n",
      "Epoch 10167/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7907 - val_loss: 5.0354\n",
      "Epoch 10168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5848 - val_loss: 4.8749\n",
      "Epoch 10169/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5883 - val_loss: 4.8510\n",
      "Epoch 10170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5863 - val_loss: 5.0317\n",
      "Epoch 10171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6317 - val_loss: 5.1835\n",
      "Epoch 10172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7274 - val_loss: 4.8655\n",
      "Epoch 10173/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5592 - val_loss: 4.8384\n",
      "Epoch 10174/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6591 - val_loss: 5.0185\n",
      "Epoch 10175/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6752 - val_loss: 4.8620\n",
      "Epoch 10176/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6005 - val_loss: 4.9215\n",
      "Epoch 10177/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6566 - val_loss: 4.8410\n",
      "Epoch 10178/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6730 - val_loss: 4.8784\n",
      "Epoch 10179/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9268 - val_loss: 5.5278\n",
      "Epoch 10180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7640 - val_loss: 5.1655\n",
      "Epoch 10181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5894 - val_loss: 4.8840\n",
      "Epoch 10182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5242 - val_loss: 5.0082\n",
      "Epoch 10183/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6348 - val_loss: 4.9849\n",
      "Epoch 10184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6245 - val_loss: 4.9145\n",
      "Epoch 10185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6716 - val_loss: 5.3278\n",
      "Epoch 10186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5764 - val_loss: 4.8780\n",
      "Epoch 10187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5365 - val_loss: 4.8464\n",
      "Epoch 10188/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5888 - val_loss: 5.3101\n",
      "Epoch 10189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6663 - val_loss: 5.3285\n",
      "Epoch 10190/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7140 - val_loss: 4.9975\n",
      "Epoch 10191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5666 - val_loss: 4.9207\n",
      "Epoch 10192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5732 - val_loss: 5.3199\n",
      "Epoch 10193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7825 - val_loss: 5.0354\n",
      "Epoch 10194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7653 - val_loss: 4.8732\n",
      "Epoch 10195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7518 - val_loss: 5.1713\n",
      "Epoch 10196/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7872 - val_loss: 4.9886\n",
      "Epoch 10197/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5585 - val_loss: 4.9224\n",
      "Epoch 10198/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5432 - val_loss: 4.8588\n",
      "Epoch 10199/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5364 - val_loss: 4.8452\n",
      "Epoch 10200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5334 - val_loss: 4.9724\n",
      "Epoch 10201/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5502 - val_loss: 5.1209\n",
      "Epoch 10202/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6203 - val_loss: 4.9181\n",
      "Epoch 10203/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5899 - val_loss: 4.8454\n",
      "Epoch 10204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5860 - val_loss: 4.8712\n",
      "Epoch 10205/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5710 - val_loss: 5.0605\n",
      "Epoch 10206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5911 - val_loss: 4.9840\n",
      "Epoch 10207/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6784 - val_loss: 4.9137\n",
      "Epoch 10208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7144 - val_loss: 4.8659\n",
      "Epoch 10209/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6819 - val_loss: 4.8906\n",
      "Epoch 10210/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6880 - val_loss: 4.8642\n",
      "Epoch 10211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5837 - val_loss: 4.8729\n",
      "Epoch 10212/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7250 - val_loss: 5.1448\n",
      "Epoch 10213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7069 - val_loss: 5.0663\n",
      "Epoch 10214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7109 - val_loss: 4.8430\n",
      "Epoch 10215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6300 - val_loss: 4.9858\n",
      "Epoch 10216/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6058 - val_loss: 4.9008\n",
      "Epoch 10217/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6960 - val_loss: 4.9032\n",
      "Epoch 10218/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6192 - val_loss: 4.9072\n",
      "Epoch 10219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5632 - val_loss: 4.8681\n",
      "Epoch 10220/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5630 - val_loss: 4.9128\n",
      "Epoch 10221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5349 - val_loss: 4.8802\n",
      "Epoch 10222/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5548 - val_loss: 4.8758\n",
      "Epoch 10223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6183 - val_loss: 4.8428\n",
      "Epoch 10224/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5832 - val_loss: 4.9992\n",
      "Epoch 10225/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5614 - val_loss: 4.8778\n",
      "Epoch 10226/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5629 - val_loss: 4.8457\n",
      "Epoch 10227/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7310 - val_loss: 4.8525\n",
      "Epoch 10228/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6117 - val_loss: 4.9736\n",
      "Epoch 10229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6044 - val_loss: 4.9676\n",
      "Epoch 10230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5340 - val_loss: 4.9211\n",
      "Epoch 10231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6061 - val_loss: 5.2924\n",
      "Epoch 10232/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6618 - val_loss: 4.8604\n",
      "Epoch 10233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6233 - val_loss: 4.8754\n",
      "Epoch 10234/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5782 - val_loss: 4.9718\n",
      "Epoch 10235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6022 - val_loss: 4.8722\n",
      "Epoch 10236/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5837 - val_loss: 4.8928\n",
      "Epoch 10237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5788 - val_loss: 4.8755\n",
      "Epoch 10238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5878 - val_loss: 4.9237\n",
      "Epoch 10239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7475 - val_loss: 4.8871\n",
      "Epoch 10240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6511 - val_loss: 4.9730\n",
      "Epoch 10241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8386 - val_loss: 5.0553\n",
      "Epoch 10242/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6080 - val_loss: 4.9096\n",
      "Epoch 10243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5722 - val_loss: 4.8778\n",
      "Epoch 10244/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6477 - val_loss: 4.8570\n",
      "Epoch 10245/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5650 - val_loss: 4.9855\n",
      "Epoch 10246/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7906 - val_loss: 5.1787\n",
      "Epoch 10247/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7548 - val_loss: 5.3824\n",
      "Epoch 10248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7944 - val_loss: 5.2201\n",
      "Epoch 10249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6409 - val_loss: 4.9511\n",
      "Epoch 10250/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6414 - val_loss: 5.0144\n",
      "Epoch 10251/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6585 - val_loss: 4.8476\n",
      "Epoch 10252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6048 - val_loss: 5.0991\n",
      "Epoch 10253/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6195 - val_loss: 4.9135\n",
      "Epoch 10254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5763 - val_loss: 4.8954\n",
      "Epoch 10255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6213 - val_loss: 4.9035\n",
      "Epoch 10256/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6773 - val_loss: 5.1368\n",
      "Epoch 10257/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7130 - val_loss: 5.1977\n",
      "Epoch 10258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6382 - val_loss: 4.8449\n",
      "Epoch 10259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6083 - val_loss: 5.1977\n",
      "Epoch 10260/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6516 - val_loss: 4.9810\n",
      "Epoch 10261/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5481 - val_loss: 5.0613\n",
      "Epoch 10262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6118 - val_loss: 5.4705\n",
      "Epoch 10263/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8680 - val_loss: 4.8848\n",
      "Epoch 10264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7149 - val_loss: 4.8842\n",
      "Epoch 10265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6016 - val_loss: 5.1354\n",
      "Epoch 10266/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7476 - val_loss: 4.9329\n",
      "Epoch 10267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5968 - val_loss: 5.1812\n",
      "Epoch 10268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7488 - val_loss: 4.8970\n",
      "Epoch 10269/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6992 - val_loss: 4.8892\n",
      "Epoch 10270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6589 - val_loss: 5.2784\n",
      "Epoch 10271/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5970 - val_loss: 4.9493\n",
      "Epoch 10272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5521 - val_loss: 4.8827\n",
      "Epoch 10273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5997 - val_loss: 4.8912\n",
      "Epoch 10274/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5208 - val_loss: 4.8629\n",
      "Epoch 10275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5703 - val_loss: 4.8906\n",
      "Epoch 10276/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7242 - val_loss: 5.0545\n",
      "Epoch 10277/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5953 - val_loss: 4.9109\n",
      "Epoch 10278/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6744 - val_loss: 4.9774\n",
      "Epoch 10279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8059 - val_loss: 5.3948\n",
      "Epoch 10280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9879 - val_loss: 5.3576\n",
      "Epoch 10281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7060 - val_loss: 5.1940\n",
      "Epoch 10282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7147 - val_loss: 4.8387\n",
      "Epoch 10283/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5765 - val_loss: 4.8846\n",
      "Epoch 10284/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8603 - val_loss: 4.9369\n",
      "Epoch 10285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6604 - val_loss: 4.8802\n",
      "Epoch 10286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5720 - val_loss: 4.8750\n",
      "Epoch 10287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5490 - val_loss: 4.8694\n",
      "Epoch 10288/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7392 - val_loss: 5.2915\n",
      "Epoch 10289/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7428 - val_loss: 4.8864\n",
      "Epoch 10290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5880 - val_loss: 4.8935\n",
      "Epoch 10291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5886 - val_loss: 4.9560\n",
      "Epoch 10292/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6718 - val_loss: 4.8740\n",
      "Epoch 10293/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6140 - val_loss: 4.9218\n",
      "Epoch 10294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7863 - val_loss: 5.1532\n",
      "Epoch 10295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9835 - val_loss: 4.9288\n",
      "Epoch 10296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6216 - val_loss: 4.9906\n",
      "Epoch 10297/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6220 - val_loss: 5.1059\n",
      "Epoch 10298/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6210 - val_loss: 4.9019\n",
      "Epoch 10299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6164 - val_loss: 5.2178\n",
      "Epoch 10300/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6849 - val_loss: 4.9278\n",
      "Epoch 10301/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6248 - val_loss: 5.0052\n",
      "Epoch 10302/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8128 - val_loss: 6.0450\n",
      "Epoch 10303/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9557 - val_loss: 5.1136\n",
      "Epoch 10304/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5740 - val_loss: 4.8196\n",
      "Epoch 10305/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6596 - val_loss: 4.9178\n",
      "Epoch 10306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6738 - val_loss: 4.8567\n",
      "Epoch 10307/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6223 - val_loss: 4.8818\n",
      "Epoch 10308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6684 - val_loss: 4.8899\n",
      "Epoch 10309/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5877 - val_loss: 4.8520\n",
      "Epoch 10310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5509 - val_loss: 4.8694\n",
      "Epoch 10311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6176 - val_loss: 4.8589\n",
      "Epoch 10312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5268 - val_loss: 4.9172\n",
      "Epoch 10313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5987 - val_loss: 4.8868\n",
      "Epoch 10314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6552 - val_loss: 4.8692\n",
      "Epoch 10315/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6712 - val_loss: 4.9852\n",
      "Epoch 10316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7103 - val_loss: 4.8650\n",
      "Epoch 10317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5929 - val_loss: 4.8769\n",
      "Epoch 10318/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5432 - val_loss: 4.9110\n",
      "Epoch 10319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6447 - val_loss: 5.0644\n",
      "Epoch 10320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6662 - val_loss: 5.0515\n",
      "Epoch 10321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6328 - val_loss: 4.9106\n",
      "Epoch 10322/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5873 - val_loss: 4.8348\n",
      "Epoch 10323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5869 - val_loss: 4.9360\n",
      "Epoch 10324/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6291 - val_loss: 4.9320\n",
      "Epoch 10325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5967 - val_loss: 4.9158\n",
      "Epoch 10326/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5544 - val_loss: 4.9745\n",
      "Epoch 10327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7292 - val_loss: 4.8400\n",
      "Epoch 10328/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8689 - val_loss: 5.4752\n",
      "Epoch 10329/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7909 - val_loss: 4.9059\n",
      "Epoch 10330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6523 - val_loss: 4.8681\n",
      "Epoch 10331/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6423 - val_loss: 5.1550\n",
      "Epoch 10332/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6129 - val_loss: 4.8600\n",
      "Epoch 10333/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6363 - val_loss: 4.9977\n",
      "Epoch 10334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6300 - val_loss: 4.8905\n",
      "Epoch 10335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6099 - val_loss: 5.0535\n",
      "Epoch 10336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5698 - val_loss: 4.8962\n",
      "Epoch 10337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7060 - val_loss: 4.9922\n",
      "Epoch 10338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7470 - val_loss: 4.8848\n",
      "Epoch 10339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5758 - val_loss: 4.8567\n",
      "Epoch 10340/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6891 - val_loss: 5.0533\n",
      "Epoch 10341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6845 - val_loss: 4.9486\n",
      "Epoch 10342/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6528 - val_loss: 4.8770\n",
      "Epoch 10343/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6222 - val_loss: 4.8747\n",
      "Epoch 10344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5795 - val_loss: 5.2367\n",
      "Epoch 10345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8294 - val_loss: 4.8794\n",
      "Epoch 10346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5257 - val_loss: 4.9114\n",
      "Epoch 10347/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5757 - val_loss: 4.9183\n",
      "Epoch 10348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6053 - val_loss: 5.1251\n",
      "Epoch 10349/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6428 - val_loss: 4.9938\n",
      "Epoch 10350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6793 - val_loss: 4.9416\n",
      "Epoch 10351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6606 - val_loss: 4.8565\n",
      "Epoch 10352/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6567 - val_loss: 4.9815\n",
      "Epoch 10353/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6891 - val_loss: 4.8324\n",
      "Epoch 10354/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5892 - val_loss: 4.9107\n",
      "Epoch 10355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6684 - val_loss: 4.9106\n",
      "Epoch 10356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5815 - val_loss: 5.0101\n",
      "Epoch 10357/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8458 - val_loss: 4.9024\n",
      "Epoch 10358/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7339 - val_loss: 4.9045\n",
      "Epoch 10359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6530 - val_loss: 4.9266\n",
      "Epoch 10360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8031 - val_loss: 5.3300\n",
      "Epoch 10361/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1085 - val_loss: 4.9418\n",
      "Epoch 10362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8164 - val_loss: 5.1004\n",
      "Epoch 10363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6323 - val_loss: 5.2425\n",
      "Epoch 10364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6261 - val_loss: 4.8807\n",
      "Epoch 10365/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6034 - val_loss: 4.8492\n",
      "Epoch 10366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5552 - val_loss: 4.9947\n",
      "Epoch 10367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6578 - val_loss: 4.9190\n",
      "Epoch 10368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9387 - val_loss: 4.8666\n",
      "Epoch 10369/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7125 - val_loss: 4.8786\n",
      "Epoch 10370/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5705 - val_loss: 4.8450\n",
      "Epoch 10371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5767 - val_loss: 5.0602\n",
      "Epoch 10372/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5200 - val_loss: 4.8497\n",
      "Epoch 10373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7400 - val_loss: 4.8528\n",
      "Epoch 10374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5569 - val_loss: 5.0311\n",
      "Epoch 10375/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6320 - val_loss: 4.8966\n",
      "Epoch 10376/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5313 - val_loss: 4.8424\n",
      "Epoch 10377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5915 - val_loss: 4.9009\n",
      "Epoch 10378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5316 - val_loss: 4.8579\n",
      "Epoch 10379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5705 - val_loss: 4.8491\n",
      "Epoch 10380/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7076 - val_loss: 4.8874\n",
      "Epoch 10381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5673 - val_loss: 4.9304\n",
      "Epoch 10382/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5909 - val_loss: 4.8418\n",
      "Epoch 10383/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6251 - val_loss: 4.8615\n",
      "Epoch 10384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6724 - val_loss: 4.8784\n",
      "Epoch 10385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6996 - val_loss: 4.8268\n",
      "Epoch 10386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5718 - val_loss: 4.8604\n",
      "Epoch 10387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6240 - val_loss: 4.9145\n",
      "Epoch 10388/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5485 - val_loss: 4.8562\n",
      "Epoch 10389/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5797 - val_loss: 4.8639\n",
      "Epoch 10390/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5537 - val_loss: 4.8652\n",
      "Epoch 10391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5421 - val_loss: 5.0052\n",
      "Epoch 10392/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6323 - val_loss: 4.8948\n",
      "Epoch 10393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7784 - val_loss: 4.8678\n",
      "Epoch 10394/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5537 - val_loss: 4.8852\n",
      "Epoch 10395/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5931 - val_loss: 4.8835\n",
      "Epoch 10396/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8086 - val_loss: 4.9310\n",
      "Epoch 10397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6034 - val_loss: 4.8489\n",
      "Epoch 10398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6468 - val_loss: 4.8257\n",
      "Epoch 10399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7292 - val_loss: 4.8432\n",
      "Epoch 10400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6009 - val_loss: 4.8978\n",
      "Epoch 10401/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6399 - val_loss: 5.0818\n",
      "Epoch 10402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6631 - val_loss: 5.0108\n",
      "Epoch 10403/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5698 - val_loss: 4.9409\n",
      "Epoch 10404/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5946 - val_loss: 5.0951\n",
      "Epoch 10405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8344 - val_loss: 4.8762\n",
      "Epoch 10406/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6792 - val_loss: 4.8420\n",
      "Epoch 10407/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5370 - val_loss: 4.8591\n",
      "Epoch 10408/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5693 - val_loss: 4.8357\n",
      "Epoch 10409/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6011 - val_loss: 5.1765\n",
      "Epoch 10410/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6645 - val_loss: 4.8916\n",
      "Epoch 10411/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5787 - val_loss: 4.9306\n",
      "Epoch 10412/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6877 - val_loss: 5.0833\n",
      "Epoch 10413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7611 - val_loss: 4.9019\n",
      "Epoch 10414/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6592 - val_loss: 4.8857\n",
      "Epoch 10415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5582 - val_loss: 4.8466\n",
      "Epoch 10416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8108 - val_loss: 4.8891\n",
      "Epoch 10417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6870 - val_loss: 4.9182\n",
      "Epoch 10418/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7037 - val_loss: 5.1568\n",
      "Epoch 10419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8865 - val_loss: 4.8868\n",
      "Epoch 10420/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5486 - val_loss: 4.9899\n",
      "Epoch 10421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6896 - val_loss: 4.8956\n",
      "Epoch 10422/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7402 - val_loss: 4.8539\n",
      "Epoch 10423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5277 - val_loss: 4.8642\n",
      "Epoch 10424/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5723 - val_loss: 4.8352\n",
      "Epoch 10425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5473 - val_loss: 5.2135\n",
      "Epoch 10426/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7650 - val_loss: 4.8689\n",
      "Epoch 10427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5523 - val_loss: 4.8683\n",
      "Epoch 10428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6949 - val_loss: 5.0675\n",
      "Epoch 10429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6698 - val_loss: 4.8887\n",
      "Epoch 10430/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7226 - val_loss: 4.8532\n",
      "Epoch 10431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6464 - val_loss: 4.9596\n",
      "Epoch 10432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5983 - val_loss: 5.0753\n",
      "Epoch 10433/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5878 - val_loss: 4.8763\n",
      "Epoch 10434/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6992 - val_loss: 4.9329\n",
      "Epoch 10435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7219 - val_loss: 5.1732\n",
      "Epoch 10436/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5816 - val_loss: 4.8644\n",
      "Epoch 10437/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6073 - val_loss: 4.8728\n",
      "Epoch 10438/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.7416 - val_loss: 4.8770\n",
      "Epoch 10439/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5694 - val_loss: 4.8636\n",
      "Epoch 10440/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6222 - val_loss: 4.8563\n",
      "Epoch 10441/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5872 - val_loss: 4.8814\n",
      "Epoch 10442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5617 - val_loss: 4.8873\n",
      "Epoch 10443/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5651 - val_loss: 4.8696\n",
      "Epoch 10444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6010 - val_loss: 4.8643\n",
      "Epoch 10445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5727 - val_loss: 4.8700\n",
      "Epoch 10446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5811 - val_loss: 4.9259\n",
      "Epoch 10447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5894 - val_loss: 5.1067\n",
      "Epoch 10448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6261 - val_loss: 4.8261\n",
      "Epoch 10449/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5803 - val_loss: 4.8656\n",
      "Epoch 10450/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6878 - val_loss: 4.9160\n",
      "Epoch 10451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5831 - val_loss: 4.9022\n",
      "Epoch 10452/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7706 - val_loss: 4.8663\n",
      "Epoch 10453/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6571 - val_loss: 4.9001\n",
      "Epoch 10454/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6321 - val_loss: 4.8997\n",
      "Epoch 10455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6333 - val_loss: 4.8362\n",
      "Epoch 10456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5471 - val_loss: 5.3024\n",
      "Epoch 10457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9745 - val_loss: 5.3324\n",
      "Epoch 10458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6472 - val_loss: 5.0085\n",
      "Epoch 10459/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6684 - val_loss: 4.8525\n",
      "Epoch 10460/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5593 - val_loss: 4.8792\n",
      "Epoch 10461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6608 - val_loss: 4.8622\n",
      "Epoch 10462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5575 - val_loss: 4.8549\n",
      "Epoch 10463/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5597 - val_loss: 4.8305\n",
      "Epoch 10464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5591 - val_loss: 4.8470\n",
      "Epoch 10465/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5415 - val_loss: 4.8581\n",
      "Epoch 10466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6171 - val_loss: 4.8499\n",
      "Epoch 10467/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5985 - val_loss: 4.8483\n",
      "Epoch 10468/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5393 - val_loss: 5.7020\n",
      "Epoch 10469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7813 - val_loss: 4.8321\n",
      "Epoch 10470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6157 - val_loss: 5.0603\n",
      "Epoch 10471/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5799 - val_loss: 4.9993\n",
      "Epoch 10472/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6312 - val_loss: 5.1117\n",
      "Epoch 10473/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6165 - val_loss: 4.9201\n",
      "Epoch 10474/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5278 - val_loss: 4.9952\n",
      "Epoch 10475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8251 - val_loss: 4.8799\n",
      "Epoch 10476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5728 - val_loss: 5.0459\n",
      "Epoch 10477/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8590 - val_loss: 5.6937\n",
      "Epoch 10478/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8879 - val_loss: 5.2583\n",
      "Epoch 10479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6733 - val_loss: 4.9156\n",
      "Epoch 10480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5319 - val_loss: 4.8656\n",
      "Epoch 10481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6063 - val_loss: 5.0335\n",
      "Epoch 10482/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5638 - val_loss: 4.8613\n",
      "Epoch 10483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7430 - val_loss: 4.8815\n",
      "Epoch 10484/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8363 - val_loss: 4.9943\n",
      "Epoch 10485/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7264 - val_loss: 4.8751\n",
      "Epoch 10486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5631 - val_loss: 4.8680\n",
      "Epoch 10487/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5647 - val_loss: 4.8901\n",
      "Epoch 10488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5732 - val_loss: 5.2168\n",
      "Epoch 10489/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7538 - val_loss: 5.9536\n",
      "Epoch 10490/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9687 - val_loss: 5.0890\n",
      "Epoch 10491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9651 - val_loss: 4.9463\n",
      "Epoch 10492/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6527 - val_loss: 5.3221\n",
      "Epoch 10493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9040 - val_loss: 5.0348\n",
      "Epoch 10494/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8127 - val_loss: 5.1830\n",
      "Epoch 10495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7643 - val_loss: 4.8816\n",
      "Epoch 10496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6478 - val_loss: 4.8978\n",
      "Epoch 10497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8613 - val_loss: 5.1350\n",
      "Epoch 10498/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6515 - val_loss: 5.0809\n",
      "Epoch 10499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6310 - val_loss: 5.2989\n",
      "Epoch 10500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6275 - val_loss: 4.8901\n",
      "Epoch 10501/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5532 - val_loss: 4.8460\n",
      "Epoch 10502/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8379 - val_loss: 4.9140\n",
      "Epoch 10503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6410 - val_loss: 5.0198\n",
      "Epoch 10504/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5852 - val_loss: 4.8464\n",
      "Epoch 10505/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6498 - val_loss: 4.9898\n",
      "Epoch 10506/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5176 - val_loss: 5.0789\n",
      "Epoch 10507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5861 - val_loss: 4.9799\n",
      "Epoch 10508/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6161 - val_loss: 4.8651\n",
      "Epoch 10509/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7218 - val_loss: 4.8280\n",
      "Epoch 10510/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7283 - val_loss: 4.9172\n",
      "Epoch 10511/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6079 - val_loss: 5.1034\n",
      "Epoch 10512/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6112 - val_loss: 5.0088\n",
      "Epoch 10513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6312 - val_loss: 5.0424\n",
      "Epoch 10514/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5331 - val_loss: 4.8396\n",
      "Epoch 10515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6589 - val_loss: 4.8734\n",
      "Epoch 10516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7137 - val_loss: 4.9889\n",
      "Epoch 10517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6892 - val_loss: 5.0831\n",
      "Epoch 10518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8951 - val_loss: 4.9086\n",
      "Epoch 10519/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7302 - val_loss: 4.9224\n",
      "Epoch 10520/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7007 - val_loss: 4.9191\n",
      "Epoch 10521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5800 - val_loss: 4.8496\n",
      "Epoch 10522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6070 - val_loss: 4.9143\n",
      "Epoch 10523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5632 - val_loss: 4.8567\n",
      "Epoch 10524/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5722 - val_loss: 5.0035\n",
      "Epoch 10525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5798 - val_loss: 4.9873\n",
      "Epoch 10526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6459 - val_loss: 4.9346\n",
      "Epoch 10527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8464 - val_loss: 4.8796\n",
      "Epoch 10528/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6197 - val_loss: 4.8713\n",
      "Epoch 10529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6084 - val_loss: 4.9186\n",
      "Epoch 10530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5852 - val_loss: 4.8537\n",
      "Epoch 10531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5589 - val_loss: 5.4574\n",
      "Epoch 10532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6375 - val_loss: 4.9151\n",
      "Epoch 10533/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5555 - val_loss: 4.9630\n",
      "Epoch 10534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6512 - val_loss: 4.8668\n",
      "Epoch 10535/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5840 - val_loss: 4.8387\n",
      "Epoch 10536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5511 - val_loss: 4.8976\n",
      "Epoch 10537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5886 - val_loss: 4.8976\n",
      "Epoch 10538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5999 - val_loss: 4.8457\n",
      "Epoch 10539/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7006 - val_loss: 4.9582\n",
      "Epoch 10540/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6183 - val_loss: 4.9837\n",
      "Epoch 10541/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9246 - val_loss: 4.9823\n",
      "Epoch 10542/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6583 - val_loss: 4.8287\n",
      "Epoch 10543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7205 - val_loss: 4.8622\n",
      "Epoch 10544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6329 - val_loss: 4.8261\n",
      "Epoch 10545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5551 - val_loss: 4.8636\n",
      "Epoch 10546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6083 - val_loss: 5.0748\n",
      "Epoch 10547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7310 - val_loss: 5.0170\n",
      "Epoch 10548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5780 - val_loss: 4.8765\n",
      "Epoch 10549/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6088 - val_loss: 5.0355\n",
      "Epoch 10550/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6438 - val_loss: 4.8544\n",
      "Epoch 10551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7289 - val_loss: 4.8531\n",
      "Epoch 10552/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5999 - val_loss: 4.8676\n",
      "Epoch 10553/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6098 - val_loss: 4.8340\n",
      "Epoch 10554/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6847 - val_loss: 4.9401\n",
      "Epoch 10555/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.8320 - val_loss: 4.8967\n",
      "Epoch 10556/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7013 - val_loss: 4.8494\n",
      "Epoch 10557/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5566 - val_loss: 5.0631\n",
      "Epoch 10558/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7558 - val_loss: 5.3599\n",
      "Epoch 10559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6722 - val_loss: 4.8374\n",
      "Epoch 10560/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6005 - val_loss: 4.8515\n",
      "Epoch 10561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6477 - val_loss: 4.8630\n",
      "Epoch 10562/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5650 - val_loss: 4.8890\n",
      "Epoch 10563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6469 - val_loss: 4.9262\n",
      "Epoch 10564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5911 - val_loss: 4.8615\n",
      "Epoch 10565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6043 - val_loss: 5.0258\n",
      "Epoch 10566/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5930 - val_loss: 4.9638\n",
      "Epoch 10567/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6096 - val_loss: 4.9432\n",
      "Epoch 10568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6226 - val_loss: 4.9595\n",
      "Epoch 10569/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8225 - val_loss: 5.1524\n",
      "Epoch 10570/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7460 - val_loss: 4.8331\n",
      "Epoch 10571/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5806 - val_loss: 4.8476\n",
      "Epoch 10572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6109 - val_loss: 4.8673\n",
      "Epoch 10573/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5532 - val_loss: 5.1101\n",
      "Epoch 10574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5557 - val_loss: 4.8591\n",
      "Epoch 10575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6025 - val_loss: 4.9021\n",
      "Epoch 10576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6231 - val_loss: 5.0274\n",
      "Epoch 10577/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7075 - val_loss: 4.8582\n",
      "Epoch 10578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6141 - val_loss: 4.9928\n",
      "Epoch 10579/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7756 - val_loss: 4.9106\n",
      "Epoch 10580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6980 - val_loss: 4.9393\n",
      "Epoch 10581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6157 - val_loss: 4.9144\n",
      "Epoch 10582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5846 - val_loss: 4.8522\n",
      "Epoch 10583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5999 - val_loss: 5.0449\n",
      "Epoch 10584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5593 - val_loss: 4.9242\n",
      "Epoch 10585/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6149 - val_loss: 4.8772\n",
      "Epoch 10586/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7293 - val_loss: 4.8527\n",
      "Epoch 10587/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5861 - val_loss: 4.8882\n",
      "Epoch 10588/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6042 - val_loss: 4.9068\n",
      "Epoch 10589/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7135 - val_loss: 4.8589\n",
      "Epoch 10590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5854 - val_loss: 4.8462\n",
      "Epoch 10591/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5821 - val_loss: 4.8394\n",
      "Epoch 10592/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.6559 - val_loss: 4.9826\n",
      "Epoch 10593/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6975 - val_loss: 4.9927\n",
      "Epoch 10594/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7288 - val_loss: 4.9740\n",
      "Epoch 10595/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8971 - val_loss: 5.1596\n",
      "Epoch 10596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6286 - val_loss: 4.9071\n",
      "Epoch 10597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7094 - val_loss: 4.8344\n",
      "Epoch 10598/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6818 - val_loss: 4.9087\n",
      "Epoch 10599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7023 - val_loss: 4.8532\n",
      "Epoch 10600/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6659 - val_loss: 4.8784\n",
      "Epoch 10601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8121 - val_loss: 4.8325\n",
      "Epoch 10602/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7231 - val_loss: 6.0040\n",
      "Epoch 10603/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7215 - val_loss: 4.8906\n",
      "Epoch 10604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6301 - val_loss: 5.2348\n",
      "Epoch 10605/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7328 - val_loss: 4.8718\n",
      "Epoch 10606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6012 - val_loss: 4.9575\n",
      "Epoch 10607/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7692 - val_loss: 4.8574\n",
      "Epoch 10608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5594 - val_loss: 4.8614\n",
      "Epoch 10609/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5351 - val_loss: 4.8896\n",
      "Epoch 10610/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5820 - val_loss: 4.8657\n",
      "Epoch 10611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6609 - val_loss: 4.9484\n",
      "Epoch 10612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6090 - val_loss: 4.8440\n",
      "Epoch 10613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7007 - val_loss: 4.8266\n",
      "Epoch 10614/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5736 - val_loss: 4.8723\n",
      "Epoch 10615/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6275 - val_loss: 4.8762\n",
      "Epoch 10616/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5409 - val_loss: 4.8742\n",
      "Epoch 10617/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5717 - val_loss: 4.8506\n",
      "Epoch 10618/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5469 - val_loss: 5.1080\n",
      "Epoch 10619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8212 - val_loss: 4.8513\n",
      "Epoch 10620/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5583 - val_loss: 4.8389\n",
      "Epoch 10621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5664 - val_loss: 4.8769\n",
      "Epoch 10622/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5774 - val_loss: 4.8351\n",
      "Epoch 10623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5891 - val_loss: 5.4013\n",
      "Epoch 10624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8716 - val_loss: 5.1586\n",
      "Epoch 10625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6122 - val_loss: 4.8705\n",
      "Epoch 10626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5817 - val_loss: 5.0304\n",
      "Epoch 10627/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8493 - val_loss: 5.1680\n",
      "Epoch 10628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6830 - val_loss: 4.8534\n",
      "Epoch 10629/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6080 - val_loss: 4.9590\n",
      "Epoch 10630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6197 - val_loss: 4.9836\n",
      "Epoch 10631/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7603 - val_loss: 4.9062\n",
      "Epoch 10632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6705 - val_loss: 4.9037\n",
      "Epoch 10633/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7597 - val_loss: 4.9214\n",
      "Epoch 10634/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6116 - val_loss: 4.9348\n",
      "Epoch 10635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6088 - val_loss: 4.8804\n",
      "Epoch 10636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6381 - val_loss: 4.8728\n",
      "Epoch 10637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6001 - val_loss: 5.0419\n",
      "Epoch 10638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6262 - val_loss: 4.8430\n",
      "Epoch 10639/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6444 - val_loss: 4.9958\n",
      "Epoch 10640/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5490 - val_loss: 4.8357\n",
      "Epoch 10641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6177 - val_loss: 4.8996\n",
      "Epoch 10642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6160 - val_loss: 4.8723\n",
      "Epoch 10643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6810 - val_loss: 4.8853\n",
      "Epoch 10644/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6753 - val_loss: 4.8885\n",
      "Epoch 10645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5798 - val_loss: 5.0664\n",
      "Epoch 10646/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5964 - val_loss: 4.8392\n",
      "Epoch 10647/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.6497 - val_loss: 4.9133\n",
      "Epoch 10648/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6056 - val_loss: 5.0512\n",
      "Epoch 10649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5452 - val_loss: 4.8932\n",
      "Epoch 10650/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6444 - val_loss: 4.8790\n",
      "Epoch 10651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7316 - val_loss: 4.8729\n",
      "Epoch 10652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7445 - val_loss: 4.9016\n",
      "Epoch 10653/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9080 - val_loss: 4.9619\n",
      "Epoch 10654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7867 - val_loss: 4.8870\n",
      "Epoch 10655/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5602 - val_loss: 4.8271\n",
      "Epoch 10656/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5569 - val_loss: 5.1391\n",
      "Epoch 10657/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5908 - val_loss: 5.0156\n",
      "Epoch 10658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7451 - val_loss: 4.8703\n",
      "Epoch 10659/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5858 - val_loss: 4.8959\n",
      "Epoch 10660/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7560 - val_loss: 4.8396\n",
      "Epoch 10661/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0981 - val_loss: 4.8798\n",
      "Epoch 10662/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6513 - val_loss: 4.8491\n",
      "Epoch 10663/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5512 - val_loss: 4.8589\n",
      "Epoch 10664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6454 - val_loss: 5.2157\n",
      "Epoch 10665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5762 - val_loss: 4.8918\n",
      "Epoch 10666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6590 - val_loss: 5.0361\n",
      "Epoch 10667/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6147 - val_loss: 4.8691\n",
      "Epoch 10668/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.553 - 0s 46us/step - loss: 4.5186 - val_loss: 5.5886\n",
      "Epoch 10669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6863 - val_loss: 4.8500\n",
      "Epoch 10670/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5756 - val_loss: 5.0955\n",
      "Epoch 10671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6361 - val_loss: 5.1852\n",
      "Epoch 10672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6969 - val_loss: 4.8400\n",
      "Epoch 10673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9662 - val_loss: 5.4305\n",
      "Epoch 10674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7543 - val_loss: 4.8646\n",
      "Epoch 10675/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6299 - val_loss: 4.9502\n",
      "Epoch 10676/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7425 - val_loss: 4.8927\n",
      "Epoch 10677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6307 - val_loss: 4.9469\n",
      "Epoch 10678/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5766 - val_loss: 4.8948\n",
      "Epoch 10679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8106 - val_loss: 4.9764\n",
      "Epoch 10680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7052 - val_loss: 4.8408\n",
      "Epoch 10681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5904 - val_loss: 4.8459\n",
      "Epoch 10682/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7940 - val_loss: 4.9591\n",
      "Epoch 10683/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6572 - val_loss: 4.9313\n",
      "Epoch 10684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7211 - val_loss: 5.0386\n",
      "Epoch 10685/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5722 - val_loss: 5.3345\n",
      "Epoch 10686/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6380 - val_loss: 4.9280\n",
      "Epoch 10687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6297 - val_loss: 4.9188\n",
      "Epoch 10688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5867 - val_loss: 5.0750\n",
      "Epoch 10689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7241 - val_loss: 4.8924\n",
      "Epoch 10690/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5419 - val_loss: 4.8639\n",
      "Epoch 10691/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5868 - val_loss: 4.9192\n",
      "Epoch 10692/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5793 - val_loss: 4.8489\n",
      "Epoch 10693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5767 - val_loss: 4.8698\n",
      "Epoch 10694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6265 - val_loss: 4.9155\n",
      "Epoch 10695/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6063 - val_loss: 4.8947\n",
      "Epoch 10696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5447 - val_loss: 5.0444\n",
      "Epoch 10697/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8083 - val_loss: 4.9489\n",
      "Epoch 10698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8195 - val_loss: 4.8970\n",
      "Epoch 10699/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6100 - val_loss: 4.9672\n",
      "Epoch 10700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5167 - val_loss: 4.9380\n",
      "Epoch 10701/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6868 - val_loss: 4.8631\n",
      "Epoch 10702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7283 - val_loss: 5.1253\n",
      "Epoch 10703/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5979 - val_loss: 4.9858\n",
      "Epoch 10704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6358 - val_loss: 4.8313\n",
      "Epoch 10705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6804 - val_loss: 5.2005\n",
      "Epoch 10706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6163 - val_loss: 4.8613\n",
      "Epoch 10707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5704 - val_loss: 4.8989\n",
      "Epoch 10708/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6563 - val_loss: 4.8375\n",
      "Epoch 10709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6270 - val_loss: 4.9557\n",
      "Epoch 10710/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.6762 - val_loss: 4.8549\n",
      "Epoch 10711/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5724 - val_loss: 4.8590\n",
      "Epoch 10712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6100 - val_loss: 4.8332\n",
      "Epoch 10713/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6539 - val_loss: 4.8639\n",
      "Epoch 10714/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6732 - val_loss: 4.9222\n",
      "Epoch 10715/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5513 - val_loss: 4.9930\n",
      "Epoch 10716/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5880 - val_loss: 5.0973\n",
      "Epoch 10717/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0203 - val_loss: 4.8524\n",
      "Epoch 10718/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5634 - val_loss: 4.9108\n",
      "Epoch 10719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5611 - val_loss: 4.9389\n",
      "Epoch 10720/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5714 - val_loss: 4.8226\n",
      "Epoch 10721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5547 - val_loss: 4.9716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5513 - val_loss: 4.8738\n",
      "Epoch 10723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5986 - val_loss: 4.9402\n",
      "Epoch 10724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5973 - val_loss: 4.8388\n",
      "Epoch 10725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6053 - val_loss: 4.8695\n",
      "Epoch 10726/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5589 - val_loss: 4.8875\n",
      "Epoch 10727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5997 - val_loss: 4.9810\n",
      "Epoch 10728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6321 - val_loss: 5.0061\n",
      "Epoch 10729/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6166 - val_loss: 5.1664\n",
      "Epoch 10730/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6165 - val_loss: 4.8373\n",
      "Epoch 10731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5941 - val_loss: 4.8823\n",
      "Epoch 10732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5799 - val_loss: 5.1896\n",
      "Epoch 10733/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5709 - val_loss: 4.9303\n",
      "Epoch 10734/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7647 - val_loss: 4.9488\n",
      "Epoch 10735/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5196 - val_loss: 4.9010\n",
      "Epoch 10736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6090 - val_loss: 4.8925\n",
      "Epoch 10737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5448 - val_loss: 4.9684\n",
      "Epoch 10738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5766 - val_loss: 4.8891\n",
      "Epoch 10739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5983 - val_loss: 4.9841\n",
      "Epoch 10740/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7357 - val_loss: 4.8682\n",
      "Epoch 10741/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5990 - val_loss: 4.8621\n",
      "Epoch 10742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7223 - val_loss: 5.3119\n",
      "Epoch 10743/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5606 - val_loss: 4.8226\n",
      "Epoch 10744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5604 - val_loss: 4.9156\n",
      "Epoch 10745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5815 - val_loss: 4.8682\n",
      "Epoch 10746/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5467 - val_loss: 5.0328\n",
      "Epoch 10747/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6285 - val_loss: 4.9795\n",
      "Epoch 10748/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5430 - val_loss: 5.1009\n",
      "Epoch 10749/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7140 - val_loss: 4.8587\n",
      "Epoch 10750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6680 - val_loss: 4.9258\n",
      "Epoch 10751/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5387 - val_loss: 4.8539\n",
      "Epoch 10752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5527 - val_loss: 4.8869\n",
      "Epoch 10753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6704 - val_loss: 4.8812\n",
      "Epoch 10754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5497 - val_loss: 4.8618\n",
      "Epoch 10755/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5785 - val_loss: 5.1277\n",
      "Epoch 10756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6791 - val_loss: 4.8930\n",
      "Epoch 10757/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5958 - val_loss: 4.8846\n",
      "Epoch 10758/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5834 - val_loss: 4.9335\n",
      "Epoch 10759/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5970 - val_loss: 4.8494\n",
      "Epoch 10760/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4926 - val_loss: 4.8596\n",
      "Epoch 10761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6308 - val_loss: 4.9361\n",
      "Epoch 10762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5633 - val_loss: 4.8688\n",
      "Epoch 10763/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5457 - val_loss: 4.8641\n",
      "Epoch 10764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6823 - val_loss: 5.4558\n",
      "Epoch 10765/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7277 - val_loss: 4.9276\n",
      "Epoch 10766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5147 - val_loss: 4.9235\n",
      "Epoch 10767/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8166 - val_loss: 5.1548\n",
      "Epoch 10768/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7109 - val_loss: 4.8496\n",
      "Epoch 10769/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5161 - val_loss: 4.8432\n",
      "Epoch 10770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5609 - val_loss: 4.8529\n",
      "Epoch 10771/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5342 - val_loss: 4.9904\n",
      "Epoch 10772/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7570 - val_loss: 4.9470\n",
      "Epoch 10773/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9439 - val_loss: 4.8743\n",
      "Epoch 10774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6284 - val_loss: 4.8633\n",
      "Epoch 10775/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5936 - val_loss: 4.8780\n",
      "Epoch 10776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6847 - val_loss: 5.9132\n",
      "Epoch 10777/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9694 - val_loss: 6.3170\n",
      "Epoch 10778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.3733 - val_loss: 5.0677\n",
      "Epoch 10779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7733 - val_loss: 4.9694\n",
      "Epoch 10780/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7112 - val_loss: 4.9092\n",
      "Epoch 10781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6238 - val_loss: 4.8372\n",
      "Epoch 10782/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6343 - val_loss: 5.1187\n",
      "Epoch 10783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7395 - val_loss: 4.8509\n",
      "Epoch 10784/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.4991 - val_loss: 4.8413\n",
      "Epoch 10785/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6194 - val_loss: 4.9107\n",
      "Epoch 10786/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5424 - val_loss: 4.8317\n",
      "Epoch 10787/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6601 - val_loss: 5.0037\n",
      "Epoch 10788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6313 - val_loss: 4.9009\n",
      "Epoch 10789/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6049 - val_loss: 4.9533\n",
      "Epoch 10790/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5403 - val_loss: 4.8974\n",
      "Epoch 10791/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6097 - val_loss: 4.9048\n",
      "Epoch 10792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5583 - val_loss: 4.9473\n",
      "Epoch 10793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5322 - val_loss: 5.1734\n",
      "Epoch 10794/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6480 - val_loss: 4.9800\n",
      "Epoch 10795/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6148 - val_loss: 4.8350\n",
      "Epoch 10796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5959 - val_loss: 4.9750\n",
      "Epoch 10797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7631 - val_loss: 4.8543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10798/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9553 - val_loss: 4.8896\n",
      "Epoch 10799/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6590 - val_loss: 5.2959\n",
      "Epoch 10800/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8137 - val_loss: 5.1063\n",
      "Epoch 10801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5747 - val_loss: 5.0247\n",
      "Epoch 10802/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0183 - val_loss: 6.2086\n",
      "Epoch 10803/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8381 - val_loss: 4.9667\n",
      "Epoch 10804/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6086 - val_loss: 4.8525\n",
      "Epoch 10805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5186 - val_loss: 4.8409\n",
      "Epoch 10806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6135 - val_loss: 4.8467\n",
      "Epoch 10807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6823 - val_loss: 4.8568\n",
      "Epoch 10808/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.6776 - val_loss: 4.8846\n",
      "Epoch 10809/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5469 - val_loss: 4.8649\n",
      "Epoch 10810/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5169 - val_loss: 4.8644\n",
      "Epoch 10811/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6254 - val_loss: 4.8563\n",
      "Epoch 10812/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7507 - val_loss: 5.1542\n",
      "Epoch 10813/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6335 - val_loss: 4.8705\n",
      "Epoch 10814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6314 - val_loss: 5.0590\n",
      "Epoch 10815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7452 - val_loss: 4.9106\n",
      "Epoch 10816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5917 - val_loss: 4.8323\n",
      "Epoch 10817/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6324 - val_loss: 4.8162\n",
      "Epoch 10818/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5563 - val_loss: 4.9071\n",
      "Epoch 10819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6077 - val_loss: 4.8250\n",
      "Epoch 10820/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5311 - val_loss: 4.8432\n",
      "Epoch 10821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6464 - val_loss: 4.8364\n",
      "Epoch 10822/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5608 - val_loss: 4.8709\n",
      "Epoch 10823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5525 - val_loss: 5.4337\n",
      "Epoch 10824/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6217 - val_loss: 4.8283\n",
      "Epoch 10825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7262 - val_loss: 4.8653\n",
      "Epoch 10826/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5783 - val_loss: 4.9181\n",
      "Epoch 10827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5731 - val_loss: 4.8333\n",
      "Epoch 10828/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5649 - val_loss: 4.8435\n",
      "Epoch 10829/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.123 - 0s 46us/step - loss: 4.5701 - val_loss: 4.8845\n",
      "Epoch 10830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5378 - val_loss: 5.0275\n",
      "Epoch 10831/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7411 - val_loss: 5.5343\n",
      "Epoch 10832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8441 - val_loss: 4.8719\n",
      "Epoch 10833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7237 - val_loss: 5.2160\n",
      "Epoch 10834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6507 - val_loss: 4.8308\n",
      "Epoch 10835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5352 - val_loss: 4.8489\n",
      "Epoch 10836/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5897 - val_loss: 4.8981\n",
      "Epoch 10837/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5400 - val_loss: 4.9976\n",
      "Epoch 10838/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6429 - val_loss: 4.8810\n",
      "Epoch 10839/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5087 - val_loss: 4.8460\n",
      "Epoch 10840/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7245 - val_loss: 4.8427\n",
      "Epoch 10841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5723 - val_loss: 4.8552\n",
      "Epoch 10842/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6022 - val_loss: 4.9941\n",
      "Epoch 10843/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5245 - val_loss: 5.1456\n",
      "Epoch 10844/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5637 - val_loss: 4.8808\n",
      "Epoch 10845/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5936 - val_loss: 5.3467\n",
      "Epoch 10846/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5808 - val_loss: 5.4924\n",
      "Epoch 10847/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8226 - val_loss: 5.0659\n",
      "Epoch 10848/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5960 - val_loss: 4.8773\n",
      "Epoch 10849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6544 - val_loss: 4.8446\n",
      "Epoch 10850/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6528 - val_loss: 4.9157\n",
      "Epoch 10851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7249 - val_loss: 4.9371\n",
      "Epoch 10852/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5525 - val_loss: 4.8739\n",
      "Epoch 10853/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5618 - val_loss: 4.8706\n",
      "Epoch 10854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5919 - val_loss: 4.9875\n",
      "Epoch 10855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6716 - val_loss: 5.0721\n",
      "Epoch 10856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6515 - val_loss: 4.8247\n",
      "Epoch 10857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5612 - val_loss: 4.8819\n",
      "Epoch 10858/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5388 - val_loss: 4.8674\n",
      "Epoch 10859/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5895 - val_loss: 4.9493\n",
      "Epoch 10860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5548 - val_loss: 4.8488\n",
      "Epoch 10861/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6321 - val_loss: 5.7902\n",
      "Epoch 10862/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6865 - val_loss: 4.8659\n",
      "Epoch 10863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5561 - val_loss: 5.1415\n",
      "Epoch 10864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7551 - val_loss: 5.1256\n",
      "Epoch 10865/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8500 - val_loss: 5.5184\n",
      "Epoch 10866/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6109 - val_loss: 4.8579\n",
      "Epoch 10867/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6002 - val_loss: 4.8649\n",
      "Epoch 10868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5260 - val_loss: 5.2064\n",
      "Epoch 10869/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8078 - val_loss: 4.8885\n",
      "Epoch 10870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0752 - val_loss: 5.2967\n",
      "Epoch 10871/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8512 - val_loss: 4.8885\n",
      "Epoch 10872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5406 - val_loss: 4.9440\n",
      "Epoch 10873/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7392 - val_loss: 5.1034\n",
      "Epoch 10874/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6070 - val_loss: 4.8425\n",
      "Epoch 10875/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5511 - val_loss: 4.8718\n",
      "Epoch 10876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5301 - val_loss: 4.8388\n",
      "Epoch 10877/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5726 - val_loss: 5.0481\n",
      "Epoch 10878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5446 - val_loss: 4.9360\n",
      "Epoch 10879/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5219 - val_loss: 4.8646\n",
      "Epoch 10880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7841 - val_loss: 5.2735\n",
      "Epoch 10881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7850 - val_loss: 5.2641\n",
      "Epoch 10882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5883 - val_loss: 5.0134\n",
      "Epoch 10883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5683 - val_loss: 4.9461\n",
      "Epoch 10884/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6757 - val_loss: 4.8824\n",
      "Epoch 10885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6895 - val_loss: 5.9061\n",
      "Epoch 10886/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7298 - val_loss: 4.8887\n",
      "Epoch 10887/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5934 - val_loss: 4.8588\n",
      "Epoch 10888/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6023 - val_loss: 4.8410\n",
      "Epoch 10889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5319 - val_loss: 4.9624\n",
      "Epoch 10890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5979 - val_loss: 4.9246\n",
      "Epoch 10891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6096 - val_loss: 5.1531\n",
      "Epoch 10892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6941 - val_loss: 5.4648\n",
      "Epoch 10893/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7375 - val_loss: 4.8559\n",
      "Epoch 10894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5620 - val_loss: 4.8296\n",
      "Epoch 10895/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5487 - val_loss: 4.8715\n",
      "Epoch 10896/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5470 - val_loss: 4.9136\n",
      "Epoch 10897/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5900 - val_loss: 4.8577\n",
      "Epoch 10898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5368 - val_loss: 4.8372\n",
      "Epoch 10899/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5842 - val_loss: 4.8322\n",
      "Epoch 10900/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6408 - val_loss: 4.8853\n",
      "Epoch 10901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5839 - val_loss: 4.9057\n",
      "Epoch 10902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6233 - val_loss: 4.8321\n",
      "Epoch 10903/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8487 - val_loss: 5.5369\n",
      "Epoch 10904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6194 - val_loss: 4.8977\n",
      "Epoch 10905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5904 - val_loss: 4.9229\n",
      "Epoch 10906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5278 - val_loss: 4.8646\n",
      "Epoch 10907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5863 - val_loss: 5.3049\n",
      "Epoch 10908/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7955 - val_loss: 4.8780\n",
      "Epoch 10909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6363 - val_loss: 4.8590\n",
      "Epoch 10910/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6324 - val_loss: 4.8706\n",
      "Epoch 10911/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5700 - val_loss: 4.8721\n",
      "Epoch 10912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5824 - val_loss: 4.8456\n",
      "Epoch 10913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7238 - val_loss: 4.8259\n",
      "Epoch 10914/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7753 - val_loss: 4.8335\n",
      "Epoch 10915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6304 - val_loss: 5.2874\n",
      "Epoch 10916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6109 - val_loss: 4.8401\n",
      "Epoch 10917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5897 - val_loss: 4.9618\n",
      "Epoch 10918/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6190 - val_loss: 4.8573\n",
      "Epoch 10919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5333 - val_loss: 5.1600\n",
      "Epoch 10920/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7179 - val_loss: 5.1396\n",
      "Epoch 10921/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6406 - val_loss: 4.8166\n",
      "Epoch 10922/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6499 - val_loss: 4.8467\n",
      "Epoch 10923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6989 - val_loss: 4.8420\n",
      "Epoch 10924/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5426 - val_loss: 4.9367\n",
      "Epoch 10925/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5874 - val_loss: 4.8898\n",
      "Epoch 10926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5645 - val_loss: 4.9230\n",
      "Epoch 10927/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6586 - val_loss: 4.9757\n",
      "Epoch 10928/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5818 - val_loss: 5.0882\n",
      "Epoch 10929/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5484 - val_loss: 4.8950\n",
      "Epoch 10930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5585 - val_loss: 4.8691\n",
      "Epoch 10931/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7748 - val_loss: 4.8997\n",
      "Epoch 10932/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7071 - val_loss: 4.9293\n",
      "Epoch 10933/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6555 - val_loss: 4.9792\n",
      "Epoch 10934/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6792 - val_loss: 4.8337\n",
      "Epoch 10935/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5170 - val_loss: 4.8749\n",
      "Epoch 10936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5542 - val_loss: 4.8991\n",
      "Epoch 10937/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5621 - val_loss: 4.8514\n",
      "Epoch 10938/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5875 - val_loss: 4.8721\n",
      "Epoch 10939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5578 - val_loss: 4.8351\n",
      "Epoch 10940/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6877 - val_loss: 4.8350\n",
      "Epoch 10941/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6026 - val_loss: 4.8673\n",
      "Epoch 10942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6048 - val_loss: 5.1382\n",
      "Epoch 10943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6283 - val_loss: 4.8596\n",
      "Epoch 10944/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6037 - val_loss: 4.8564\n",
      "Epoch 10945/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8012 - val_loss: 4.9971\n",
      "Epoch 10946/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6816 - val_loss: 4.8867\n",
      "Epoch 10947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5868 - val_loss: 5.0633\n",
      "Epoch 10948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6155 - val_loss: 5.0593\n",
      "Epoch 10949/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6404 - val_loss: 4.8453\n",
      "Epoch 10950/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5616 - val_loss: 4.8550\n",
      "Epoch 10951/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5626 - val_loss: 5.0184\n",
      "Epoch 10952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7933 - val_loss: 5.2432\n",
      "Epoch 10953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7207 - val_loss: 4.8661\n",
      "Epoch 10954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5485 - val_loss: 4.8544\n",
      "Epoch 10955/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5017 - val_loss: 4.8620\n",
      "Epoch 10956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6346 - val_loss: 4.8344\n",
      "Epoch 10957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5633 - val_loss: 4.8494\n",
      "Epoch 10958/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6886 - val_loss: 5.0722\n",
      "Epoch 10959/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9759 - val_loss: 4.9409\n",
      "Epoch 10960/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8109 - val_loss: 4.8548\n",
      "Epoch 10961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5881 - val_loss: 4.8474\n",
      "Epoch 10962/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6118 - val_loss: 5.1858\n",
      "Epoch 10963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6183 - val_loss: 4.8349\n",
      "Epoch 10964/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5842 - val_loss: 4.8788\n",
      "Epoch 10965/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6027 - val_loss: 4.8423\n",
      "Epoch 10966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6089 - val_loss: 4.9564\n",
      "Epoch 10967/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5511 - val_loss: 4.8851\n",
      "Epoch 10968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5038 - val_loss: 5.1892\n",
      "Epoch 10969/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7793 - val_loss: 4.9504\n",
      "Epoch 10970/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6543 - val_loss: 4.9194\n",
      "Epoch 10971/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6876 - val_loss: 4.8464\n",
      "Epoch 10972/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6679 - val_loss: 4.8497\n",
      "Epoch 10973/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7115 - val_loss: 4.9004\n",
      "Epoch 10974/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5819 - val_loss: 5.0120\n",
      "Epoch 10975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5648 - val_loss: 4.8852\n",
      "Epoch 10976/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5378 - val_loss: 4.8351\n",
      "Epoch 10977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5390 - val_loss: 4.8897\n",
      "Epoch 10978/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5916 - val_loss: 5.0458\n",
      "Epoch 10979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6680 - val_loss: 5.0913\n",
      "Epoch 10980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8776 - val_loss: 5.1331\n",
      "Epoch 10981/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8320 - val_loss: 5.0011\n",
      "Epoch 10982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6533 - val_loss: 4.8925\n",
      "Epoch 10983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7389 - val_loss: 4.8376\n",
      "Epoch 10984/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5619 - val_loss: 4.8518\n",
      "Epoch 10985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5753 - val_loss: 4.8576\n",
      "Epoch 10986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5984 - val_loss: 4.8316\n",
      "Epoch 10987/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5365 - val_loss: 4.9229\n",
      "Epoch 10988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6648 - val_loss: 4.9416\n",
      "Epoch 10989/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6068 - val_loss: 4.9715\n",
      "Epoch 10990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5642 - val_loss: 4.9474\n",
      "Epoch 10991/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5734 - val_loss: 4.9347\n",
      "Epoch 10992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6471 - val_loss: 4.8382\n",
      "Epoch 10993/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6415 - val_loss: 4.8306\n",
      "Epoch 10994/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7622 - val_loss: 4.8310\n",
      "Epoch 10995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7658 - val_loss: 5.0180\n",
      "Epoch 10996/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6058 - val_loss: 5.0153\n",
      "Epoch 10997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5851 - val_loss: 4.9694\n",
      "Epoch 10998/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8724 - val_loss: 4.9203\n",
      "Epoch 10999/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6219 - val_loss: 4.8488\n",
      "Epoch 11000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5200 - val_loss: 5.0069\n",
      "Epoch 11001/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6059 - val_loss: 5.1046\n",
      "Epoch 11002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6859 - val_loss: 5.1880\n",
      "Epoch 11003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6484 - val_loss: 4.8527\n",
      "Epoch 11004/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5121 - val_loss: 4.8266\n",
      "Epoch 11005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5181 - val_loss: 4.8065\n",
      "Epoch 11006/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5559 - val_loss: 4.8506\n",
      "Epoch 11007/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7829 - val_loss: 4.9142\n",
      "Epoch 11008/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5092 - val_loss: 4.8385\n",
      "Epoch 11009/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5245 - val_loss: 4.8346\n",
      "Epoch 11010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5193 - val_loss: 4.9129\n",
      "Epoch 11011/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6854 - val_loss: 5.1137\n",
      "Epoch 11012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6640 - val_loss: 5.0185\n",
      "Epoch 11013/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5955 - val_loss: 4.8171\n",
      "Epoch 11014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5400 - val_loss: 4.8444\n",
      "Epoch 11015/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5690 - val_loss: 4.9002\n",
      "Epoch 11016/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5001 - val_loss: 4.8435\n",
      "Epoch 11017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6172 - val_loss: 5.6438\n",
      "Epoch 11018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8650 - val_loss: 4.8948\n",
      "Epoch 11019/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6402 - val_loss: 5.0860\n",
      "Epoch 11020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6601 - val_loss: 4.8642\n",
      "Epoch 11021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5157 - val_loss: 4.9557\n",
      "Epoch 11022/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6926 - val_loss: 5.2381\n",
      "Epoch 11023/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5641 - val_loss: 4.9145\n",
      "Epoch 11024/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6330 - val_loss: 4.9018\n",
      "Epoch 11025/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6297 - val_loss: 5.0062\n",
      "Epoch 11026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5976 - val_loss: 4.8241\n",
      "Epoch 11027/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5523 - val_loss: 4.9284\n",
      "Epoch 11028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5995 - val_loss: 4.8841\n",
      "Epoch 11029/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8252 - val_loss: 4.8991\n",
      "Epoch 11030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7176 - val_loss: 4.8968\n",
      "Epoch 11031/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6417 - val_loss: 4.8427\n",
      "Epoch 11032/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5609 - val_loss: 4.8433\n",
      "Epoch 11033/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5741 - val_loss: 4.9031\n",
      "Epoch 11034/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6188 - val_loss: 4.8292\n",
      "Epoch 11035/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5355 - val_loss: 5.0229\n",
      "Epoch 11036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5619 - val_loss: 4.9940\n",
      "Epoch 11037/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7552 - val_loss: 5.1625\n",
      "Epoch 11038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8062 - val_loss: 4.8164\n",
      "Epoch 11039/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6610 - val_loss: 4.8613\n",
      "Epoch 11040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5735 - val_loss: 4.8657\n",
      "Epoch 11041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5331 - val_loss: 4.8574\n",
      "Epoch 11042/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7366 - val_loss: 4.8519\n",
      "Epoch 11043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5233 - val_loss: 4.9024\n",
      "Epoch 11044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6060 - val_loss: 4.8429\n",
      "Epoch 11045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5143 - val_loss: 5.0095\n",
      "Epoch 11046/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6041 - val_loss: 4.9369\n",
      "Epoch 11047/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6286 - val_loss: 4.9361\n",
      "Epoch 11048/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5831 - val_loss: 4.8543\n",
      "Epoch 11049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8072 - val_loss: 5.1468\n",
      "Epoch 11050/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6268 - val_loss: 5.0875\n",
      "Epoch 11051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5417 - val_loss: 4.8425\n",
      "Epoch 11052/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5629 - val_loss: 5.0767\n",
      "Epoch 11053/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5718 - val_loss: 4.8508\n",
      "Epoch 11054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7662 - val_loss: 4.8420\n",
      "Epoch 11055/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9471 - val_loss: 5.0527\n",
      "Epoch 11056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7033 - val_loss: 5.0704\n",
      "Epoch 11057/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5826 - val_loss: 4.8085\n",
      "Epoch 11058/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5639 - val_loss: 4.9665\n",
      "Epoch 11059/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5555 - val_loss: 4.8745\n",
      "Epoch 11060/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5492 - val_loss: 4.8574\n",
      "Epoch 11061/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5365 - val_loss: 4.8898\n",
      "Epoch 11062/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6593 - val_loss: 4.9088\n",
      "Epoch 11063/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6363 - val_loss: 4.8514\n",
      "Epoch 11064/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6383 - val_loss: 4.9762\n",
      "Epoch 11065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5782 - val_loss: 4.8690\n",
      "Epoch 11066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5167 - val_loss: 4.9150\n",
      "Epoch 11067/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5232 - val_loss: 4.8249\n",
      "Epoch 11068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5703 - val_loss: 4.8518\n",
      "Epoch 11069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6302 - val_loss: 5.0512\n",
      "Epoch 11070/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7551 - val_loss: 4.9432\n",
      "Epoch 11071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5270 - val_loss: 5.5358\n",
      "Epoch 11072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6367 - val_loss: 5.2233\n",
      "Epoch 11073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5937 - val_loss: 5.0206\n",
      "Epoch 11074/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8294 - val_loss: 5.5428\n",
      "Epoch 11075/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6390 - val_loss: 5.1593\n",
      "Epoch 11076/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7359 - val_loss: 4.8411\n",
      "Epoch 11077/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7847 - val_loss: 5.0585\n",
      "Epoch 11078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7787 - val_loss: 5.1716\n",
      "Epoch 11079/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5757 - val_loss: 4.8703\n",
      "Epoch 11080/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5264 - val_loss: 4.8504\n",
      "Epoch 11081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5396 - val_loss: 4.8794\n",
      "Epoch 11082/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6186 - val_loss: 4.8490\n",
      "Epoch 11083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6965 - val_loss: 4.8599\n",
      "Epoch 11084/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6176 - val_loss: 4.8752\n",
      "Epoch 11085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5507 - val_loss: 4.8864\n",
      "Epoch 11086/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5697 - val_loss: 5.0159\n",
      "Epoch 11087/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6005 - val_loss: 4.8638\n",
      "Epoch 11088/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6190 - val_loss: 4.8783\n",
      "Epoch 11089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6159 - val_loss: 5.0301\n",
      "Epoch 11090/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6599 - val_loss: 4.8136\n",
      "Epoch 11091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5856 - val_loss: 4.8573\n",
      "Epoch 11092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6398 - val_loss: 4.8351\n",
      "Epoch 11093/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5960 - val_loss: 4.8394\n",
      "Epoch 11094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7570 - val_loss: 4.9520\n",
      "Epoch 11095/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7366 - val_loss: 4.8919\n",
      "Epoch 11096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6261 - val_loss: 4.8282\n",
      "Epoch 11097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6180 - val_loss: 4.8333\n",
      "Epoch 11098/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5983 - val_loss: 4.8191\n",
      "Epoch 11099/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6685 - val_loss: 4.8962\n",
      "Epoch 11100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8116 - val_loss: 4.8354\n",
      "Epoch 11101/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5579 - val_loss: 5.0060\n",
      "Epoch 11102/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6971 - val_loss: 5.0126\n",
      "Epoch 11103/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7462 - val_loss: 5.1736\n",
      "Epoch 11104/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5872 - val_loss: 4.8680\n",
      "Epoch 11105/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5453 - val_loss: 4.8436\n",
      "Epoch 11106/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5699 - val_loss: 4.8389\n",
      "Epoch 11107/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6152 - val_loss: 5.1059\n",
      "Epoch 11108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7176 - val_loss: 4.9063\n",
      "Epoch 11109/20000\n",
      "685/685 [==============================] - 0s 137us/step - loss: 4.6322 - val_loss: 4.8464\n",
      "Epoch 11110/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5803 - val_loss: 4.9709\n",
      "Epoch 11111/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5728 - val_loss: 5.0422\n",
      "Epoch 11112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6161 - val_loss: 5.1484\n",
      "Epoch 11113/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7084 - val_loss: 4.8839\n",
      "Epoch 11114/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6360 - val_loss: 4.8790\n",
      "Epoch 11115/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7849 - val_loss: 4.8258\n",
      "Epoch 11116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5299 - val_loss: 4.8836\n",
      "Epoch 11117/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4860 - val_loss: 4.9413\n",
      "Epoch 11118/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5202 - val_loss: 4.8571\n",
      "Epoch 11119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6748 - val_loss: 4.9548\n",
      "Epoch 11120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5956 - val_loss: 4.8464\n",
      "Epoch 11121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5152 - val_loss: 4.8330\n",
      "Epoch 11122/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5607 - val_loss: 4.8601\n",
      "Epoch 11123/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6244 - val_loss: 5.1773\n",
      "Epoch 11124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6462 - val_loss: 4.8499\n",
      "Epoch 11125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5537 - val_loss: 5.2721\n",
      "Epoch 11126/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7108 - val_loss: 4.9762\n",
      "Epoch 11127/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6649 - val_loss: 4.8649\n",
      "Epoch 11128/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6228 - val_loss: 4.8603\n",
      "Epoch 11129/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5656 - val_loss: 5.0730\n",
      "Epoch 11130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6170 - val_loss: 4.8443\n",
      "Epoch 11131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8018 - val_loss: 4.8450\n",
      "Epoch 11132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5719 - val_loss: 4.8215\n",
      "Epoch 11133/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5631 - val_loss: 5.1598\n",
      "Epoch 11134/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 5.0135 - val_loss: 5.2831\n",
      "Epoch 11135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8718 - val_loss: 4.9418\n",
      "Epoch 11136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8570 - val_loss: 5.2336\n",
      "Epoch 11137/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.031 - 0s 46us/step - loss: 4.7852 - val_loss: 5.0305\n",
      "Epoch 11138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6232 - val_loss: 4.8151\n",
      "Epoch 11139/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7787 - val_loss: 4.8260\n",
      "Epoch 11140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6319 - val_loss: 4.9907\n",
      "Epoch 11141/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6412 - val_loss: 5.0378\n",
      "Epoch 11142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6416 - val_loss: 4.8655\n",
      "Epoch 11143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6318 - val_loss: 5.2383\n",
      "Epoch 11144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8568 - val_loss: 5.0799\n",
      "Epoch 11145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7544 - val_loss: 4.8158\n",
      "Epoch 11146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5578 - val_loss: 4.8889\n",
      "Epoch 11147/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5710 - val_loss: 5.1819\n",
      "Epoch 11148/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6950 - val_loss: 4.8166\n",
      "Epoch 11149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6558 - val_loss: 4.8188\n",
      "Epoch 11150/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5761 - val_loss: 5.0688\n",
      "Epoch 11151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5682 - val_loss: 4.9159\n",
      "Epoch 11152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5478 - val_loss: 4.9834\n",
      "Epoch 11153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6003 - val_loss: 4.9107\n",
      "Epoch 11154/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5459 - val_loss: 4.9421\n",
      "Epoch 11155/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6153 - val_loss: 4.8284\n",
      "Epoch 11156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5349 - val_loss: 4.8427\n",
      "Epoch 11157/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5804 - val_loss: 4.8328\n",
      "Epoch 11158/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6252 - val_loss: 4.8344\n",
      "Epoch 11159/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5757 - val_loss: 5.1822\n",
      "Epoch 11160/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5435 - val_loss: 4.8908\n",
      "Epoch 11161/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5267 - val_loss: 4.9483\n",
      "Epoch 11162/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5613 - val_loss: 4.8406\n",
      "Epoch 11163/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5642 - val_loss: 5.0786\n",
      "Epoch 11164/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5822 - val_loss: 4.8638\n",
      "Epoch 11165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6158 - val_loss: 4.8421\n",
      "Epoch 11166/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5639 - val_loss: 4.8530\n",
      "Epoch 11167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5440 - val_loss: 4.9112\n",
      "Epoch 11168/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5740 - val_loss: 5.0224\n",
      "Epoch 11169/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6051 - val_loss: 4.8867\n",
      "Epoch 11170/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7489 - val_loss: 5.4132\n",
      "Epoch 11171/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6772 - val_loss: 4.8961\n",
      "Epoch 11172/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5446 - val_loss: 4.9166\n",
      "Epoch 11173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5623 - val_loss: 4.8436\n",
      "Epoch 11174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5421 - val_loss: 4.8389\n",
      "Epoch 11175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5672 - val_loss: 5.0255\n",
      "Epoch 11176/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5580 - val_loss: 4.8304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7223 - val_loss: 5.0830\n",
      "Epoch 11178/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6075 - val_loss: 4.8767\n",
      "Epoch 11179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5749 - val_loss: 4.8745\n",
      "Epoch 11180/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5603 - val_loss: 5.1327\n",
      "Epoch 11181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7289 - val_loss: 5.2527\n",
      "Epoch 11182/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6363 - val_loss: 4.8238\n",
      "Epoch 11183/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6485 - val_loss: 4.8578\n",
      "Epoch 11184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6339 - val_loss: 4.8153\n",
      "Epoch 11185/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5309 - val_loss: 4.8166\n",
      "Epoch 11186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7744 - val_loss: 5.5205\n",
      "Epoch 11187/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8154 - val_loss: 5.3876\n",
      "Epoch 11188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6583 - val_loss: 4.8513\n",
      "Epoch 11189/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5207 - val_loss: 4.9283\n",
      "Epoch 11190/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5591 - val_loss: 4.9139\n",
      "Epoch 11191/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5039 - val_loss: 4.8909\n",
      "Epoch 11192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5880 - val_loss: 4.9264\n",
      "Epoch 11193/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7438 - val_loss: 4.8574\n",
      "Epoch 11194/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6079 - val_loss: 4.9273\n",
      "Epoch 11195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5524 - val_loss: 5.0590\n",
      "Epoch 11196/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5374 - val_loss: 5.0394\n",
      "Epoch 11197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7446 - val_loss: 4.8385\n",
      "Epoch 11198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5938 - val_loss: 4.8464\n",
      "Epoch 11199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8568 - val_loss: 4.8512\n",
      "Epoch 11200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5975 - val_loss: 4.8382\n",
      "Epoch 11201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5376 - val_loss: 4.8300\n",
      "Epoch 11202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6682 - val_loss: 5.0578\n",
      "Epoch 11203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8287 - val_loss: 5.0245\n",
      "Epoch 11204/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6369 - val_loss: 5.1282\n",
      "Epoch 11205/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9628 - val_loss: 4.8444\n",
      "Epoch 11206/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6219 - val_loss: 5.0355\n",
      "Epoch 11207/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6299 - val_loss: 4.9453\n",
      "Epoch 11208/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9341 - val_loss: 4.8268\n",
      "Epoch 11209/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5958 - val_loss: 4.8319\n",
      "Epoch 11210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6079 - val_loss: 5.0908\n",
      "Epoch 11211/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6325 - val_loss: 4.8668\n",
      "Epoch 11212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5335 - val_loss: 4.8665\n",
      "Epoch 11213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5609 - val_loss: 4.8665\n",
      "Epoch 11214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6266 - val_loss: 4.8389\n",
      "Epoch 11215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5824 - val_loss: 5.0341\n",
      "Epoch 11216/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5989 - val_loss: 4.8187\n",
      "Epoch 11217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5651 - val_loss: 4.8585\n",
      "Epoch 11218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5689 - val_loss: 5.1289\n",
      "Epoch 11219/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7405 - val_loss: 4.8468\n",
      "Epoch 11220/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6631 - val_loss: 4.8642\n",
      "Epoch 11221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6301 - val_loss: 4.9158\n",
      "Epoch 11222/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6643 - val_loss: 5.0232\n",
      "Epoch 11223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7829 - val_loss: 4.8910\n",
      "Epoch 11224/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7482 - val_loss: 4.8603\n",
      "Epoch 11225/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6644 - val_loss: 4.8869\n",
      "Epoch 11226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5526 - val_loss: 4.8588\n",
      "Epoch 11227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5559 - val_loss: 4.8520\n",
      "Epoch 11228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5668 - val_loss: 5.2384\n",
      "Epoch 11229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5822 - val_loss: 4.8162\n",
      "Epoch 11230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5502 - val_loss: 4.9019\n",
      "Epoch 11231/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5867 - val_loss: 4.8271\n",
      "Epoch 11232/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6007 - val_loss: 4.8975\n",
      "Epoch 11233/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7319 - val_loss: 5.0697\n",
      "Epoch 11234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6033 - val_loss: 4.9936\n",
      "Epoch 11235/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6951 - val_loss: 5.1657\n",
      "Epoch 11236/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7119 - val_loss: 5.2000\n",
      "Epoch 11237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6562 - val_loss: 4.8677\n",
      "Epoch 11238/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5565 - val_loss: 5.0145\n",
      "Epoch 11239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6607 - val_loss: 4.9835\n",
      "Epoch 11240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5499 - val_loss: 4.8208\n",
      "Epoch 11241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5415 - val_loss: 4.9100\n",
      "Epoch 11242/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5575 - val_loss: 4.8334\n",
      "Epoch 11243/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5554 - val_loss: 4.9456\n",
      "Epoch 11244/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6635 - val_loss: 4.9045\n",
      "Epoch 11245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7319 - val_loss: 5.2243\n",
      "Epoch 11246/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7627 - val_loss: 4.8539\n",
      "Epoch 11247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5608 - val_loss: 4.8716\n",
      "Epoch 11248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5781 - val_loss: 4.8543\n",
      "Epoch 11249/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5246 - val_loss: 4.8995\n",
      "Epoch 11250/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5695 - val_loss: 4.9420\n",
      "Epoch 11251/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6978 - val_loss: 4.8732\n",
      "Epoch 11252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6889 - val_loss: 4.8604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11253/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6595 - val_loss: 5.1511\n",
      "Epoch 11254/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6115 - val_loss: 5.1872\n",
      "Epoch 11255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7329 - val_loss: 5.3577\n",
      "Epoch 11256/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6576 - val_loss: 5.1627\n",
      "Epoch 11257/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7165 - val_loss: 5.1134\n",
      "Epoch 11258/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6151 - val_loss: 4.8492\n",
      "Epoch 11259/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4961 - val_loss: 4.8431\n",
      "Epoch 11260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6095 - val_loss: 4.9758\n",
      "Epoch 11261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6606 - val_loss: 4.8870\n",
      "Epoch 11262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5211 - val_loss: 4.8418\n",
      "Epoch 11263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6049 - val_loss: 4.8281\n",
      "Epoch 11264/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5269 - val_loss: 4.9065\n",
      "Epoch 11265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7210 - val_loss: 5.1978\n",
      "Epoch 11266/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7147 - val_loss: 4.8548\n",
      "Epoch 11267/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6799 - val_loss: 5.0040\n",
      "Epoch 11268/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6480 - val_loss: 4.8839\n",
      "Epoch 11269/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5465 - val_loss: 4.8244\n",
      "Epoch 11270/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5594 - val_loss: 5.0796\n",
      "Epoch 11271/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5771 - val_loss: 4.8799\n",
      "Epoch 11272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5194 - val_loss: 4.8929\n",
      "Epoch 11273/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5382 - val_loss: 4.8516\n",
      "Epoch 11274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5656 - val_loss: 4.8797\n",
      "Epoch 11275/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5086 - val_loss: 4.9964\n",
      "Epoch 11276/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7280 - val_loss: 4.9103\n",
      "Epoch 11277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6803 - val_loss: 4.9327\n",
      "Epoch 11278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7741 - val_loss: 4.9599\n",
      "Epoch 11279/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6743 - val_loss: 4.8383\n",
      "Epoch 11280/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6099 - val_loss: 5.0691\n",
      "Epoch 11281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9842 - val_loss: 5.4130\n",
      "Epoch 11282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7027 - val_loss: 4.8753\n",
      "Epoch 11283/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5282 - val_loss: 4.8405\n",
      "Epoch 11284/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5218 - val_loss: 4.9479\n",
      "Epoch 11285/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5761 - val_loss: 4.8461\n",
      "Epoch 11286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5629 - val_loss: 5.0102\n",
      "Epoch 11287/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5673 - val_loss: 4.9983\n",
      "Epoch 11288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6782 - val_loss: 5.0917\n",
      "Epoch 11289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6092 - val_loss: 5.0177\n",
      "Epoch 11290/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6448 - val_loss: 4.8528\n",
      "Epoch 11291/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5284 - val_loss: 5.1847\n",
      "Epoch 11292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8983 - val_loss: 5.1729\n",
      "Epoch 11293/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0256 - val_loss: 5.1441\n",
      "Epoch 11294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8326 - val_loss: 5.6940\n",
      "Epoch 11295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6762 - val_loss: 5.2059\n",
      "Epoch 11296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6677 - val_loss: 4.8394\n",
      "Epoch 11297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5152 - val_loss: 4.8714\n",
      "Epoch 11298/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5450 - val_loss: 4.8620\n",
      "Epoch 11299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5582 - val_loss: 4.8348\n",
      "Epoch 11300/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7705 - val_loss: 4.9288\n",
      "Epoch 11301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5945 - val_loss: 4.9814\n",
      "Epoch 11302/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5654 - val_loss: 4.9339\n",
      "Epoch 11303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6495 - val_loss: 4.9181\n",
      "Epoch 11304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8729 - val_loss: 5.0150\n",
      "Epoch 11305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5497 - val_loss: 4.8854\n",
      "Epoch 11306/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.5536 - val_loss: 4.8490\n",
      "Epoch 11307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5376 - val_loss: 5.0101\n",
      "Epoch 11308/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5936 - val_loss: 4.8766\n",
      "Epoch 11309/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5592 - val_loss: 4.9831\n",
      "Epoch 11310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5256 - val_loss: 5.0659\n",
      "Epoch 11311/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5875 - val_loss: 4.8135\n",
      "Epoch 11312/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6249 - val_loss: 4.9465\n",
      "Epoch 11313/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5797 - val_loss: 4.8450\n",
      "Epoch 11314/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6712 - val_loss: 4.8370\n",
      "Epoch 11315/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6099 - val_loss: 4.9467\n",
      "Epoch 11316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5467 - val_loss: 4.9605\n",
      "Epoch 11317/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6737 - val_loss: 5.1338\n",
      "Epoch 11318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5532 - val_loss: 4.8494\n",
      "Epoch 11319/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5224 - val_loss: 4.9135\n",
      "Epoch 11320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6048 - val_loss: 4.8141\n",
      "Epoch 11321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5602 - val_loss: 4.8427\n",
      "Epoch 11322/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5800 - val_loss: 4.9400\n",
      "Epoch 11323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8460 - val_loss: 5.0546\n",
      "Epoch 11324/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8394 - val_loss: 5.5350\n",
      "Epoch 11325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7305 - val_loss: 4.8476\n",
      "Epoch 11326/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5221 - val_loss: 5.0376\n",
      "Epoch 11327/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7597 - val_loss: 4.9354\n",
      "Epoch 11328/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6388 - val_loss: 4.9820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5912 - val_loss: 4.9290\n",
      "Epoch 11330/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6458 - val_loss: 4.9069\n",
      "Epoch 11331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5430 - val_loss: 4.8472\n",
      "Epoch 11332/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5101 - val_loss: 4.9254\n",
      "Epoch 11333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5824 - val_loss: 4.9676\n",
      "Epoch 11334/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7268 - val_loss: 4.9477\n",
      "Epoch 11335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5289 - val_loss: 4.9344\n",
      "Epoch 11336/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5231 - val_loss: 5.0091\n",
      "Epoch 11337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5947 - val_loss: 4.8234\n",
      "Epoch 11338/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5931 - val_loss: 4.9023\n",
      "Epoch 11339/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5480 - val_loss: 4.8195\n",
      "Epoch 11340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5142 - val_loss: 4.8741\n",
      "Epoch 11341/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7948 - val_loss: 5.2319\n",
      "Epoch 11342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6962 - val_loss: 4.8067\n",
      "Epoch 11343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6865 - val_loss: 4.8290\n",
      "Epoch 11344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6735 - val_loss: 4.8990\n",
      "Epoch 11345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5559 - val_loss: 4.8184\n",
      "Epoch 11346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5176 - val_loss: 4.8209\n",
      "Epoch 11347/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5385 - val_loss: 4.8693\n",
      "Epoch 11348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5744 - val_loss: 4.8507\n",
      "Epoch 11349/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5597 - val_loss: 4.8431\n",
      "Epoch 11350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6958 - val_loss: 4.8447\n",
      "Epoch 11351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7256 - val_loss: 4.8599\n",
      "Epoch 11352/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6788 - val_loss: 4.9559\n",
      "Epoch 11353/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6138 - val_loss: 4.9121\n",
      "Epoch 11354/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6958 - val_loss: 5.2845\n",
      "Epoch 11355/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8853 - val_loss: 4.8734\n",
      "Epoch 11356/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6764 - val_loss: 4.8945\n",
      "Epoch 11357/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5253 - val_loss: 5.2650\n",
      "Epoch 11358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8960 - val_loss: 4.8476\n",
      "Epoch 11359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6418 - val_loss: 4.8683\n",
      "Epoch 11360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5190 - val_loss: 5.0371\n",
      "Epoch 11361/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5705 - val_loss: 4.8667\n",
      "Epoch 11362/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6009 - val_loss: 5.4510\n",
      "Epoch 11363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8373 - val_loss: 5.5059\n",
      "Epoch 11364/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7989 - val_loss: 4.9233\n",
      "Epoch 11365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6644 - val_loss: 4.9015\n",
      "Epoch 11366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5837 - val_loss: 4.8656\n",
      "Epoch 11367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6526 - val_loss: 4.8373\n",
      "Epoch 11368/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6156 - val_loss: 4.9828\n",
      "Epoch 11369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6359 - val_loss: 4.8879\n",
      "Epoch 11370/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6445 - val_loss: 4.8402\n",
      "Epoch 11371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5459 - val_loss: 4.8333\n",
      "Epoch 11372/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5302 - val_loss: 4.8434\n",
      "Epoch 11373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6946 - val_loss: 5.1029\n",
      "Epoch 11374/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8042 - val_loss: 4.8726\n",
      "Epoch 11375/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5839 - val_loss: 5.0518\n",
      "Epoch 11376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7724 - val_loss: 5.0542\n",
      "Epoch 11377/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5628 - val_loss: 4.8071\n",
      "Epoch 11378/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5574 - val_loss: 4.8348\n",
      "Epoch 11379/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5546 - val_loss: 4.8461\n",
      "Epoch 11380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6583 - val_loss: 5.0035\n",
      "Epoch 11381/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9169 - val_loss: 4.8590\n",
      "Epoch 11382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6075 - val_loss: 5.2675\n",
      "Epoch 11383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6621 - val_loss: 4.8727\n",
      "Epoch 11384/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5366 - val_loss: 4.8819\n",
      "Epoch 11385/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5329 - val_loss: 4.9329\n",
      "Epoch 11386/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7022 - val_loss: 4.8159\n",
      "Epoch 11387/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6791 - val_loss: 5.0470\n",
      "Epoch 11388/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0143 - val_loss: 4.9387\n",
      "Epoch 11389/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5649 - val_loss: 4.8602\n",
      "Epoch 11390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5349 - val_loss: 4.8849\n",
      "Epoch 11391/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5205 - val_loss: 4.8410\n",
      "Epoch 11392/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6032 - val_loss: 4.9087\n",
      "Epoch 11393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6664 - val_loss: 4.8516\n",
      "Epoch 11394/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6476 - val_loss: 5.0428\n",
      "Epoch 11395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7327 - val_loss: 4.9851\n",
      "Epoch 11396/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5688 - val_loss: 5.1997\n",
      "Epoch 11397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7162 - val_loss: 5.1123\n",
      "Epoch 11398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6374 - val_loss: 4.9497\n",
      "Epoch 11399/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4954 - val_loss: 5.0909\n",
      "Epoch 11400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6828 - val_loss: 4.8371\n",
      "Epoch 11401/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5588 - val_loss: 4.8463\n",
      "Epoch 11402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6607 - val_loss: 4.9141\n",
      "Epoch 11403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5302 - val_loss: 4.8992\n",
      "Epoch 11404/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5833 - val_loss: 5.0771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6079 - val_loss: 4.9947\n",
      "Epoch 11406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6486 - val_loss: 5.1560\n",
      "Epoch 11407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6165 - val_loss: 4.8542\n",
      "Epoch 11408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6694 - val_loss: 5.1481\n",
      "Epoch 11409/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7454 - val_loss: 4.9542\n",
      "Epoch 11410/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5615 - val_loss: 4.8179\n",
      "Epoch 11411/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6955 - val_loss: 4.8542\n",
      "Epoch 11412/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5301 - val_loss: 4.8296\n",
      "Epoch 11413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5193 - val_loss: 4.9738\n",
      "Epoch 11414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5231 - val_loss: 4.8628\n",
      "Epoch 11415/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6361 - val_loss: 4.8329\n",
      "Epoch 11416/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5385 - val_loss: 4.9240\n",
      "Epoch 11417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6969 - val_loss: 4.8659\n",
      "Epoch 11418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5371 - val_loss: 4.9863\n",
      "Epoch 11419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6148 - val_loss: 4.8764\n",
      "Epoch 11420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5837 - val_loss: 4.8421\n",
      "Epoch 11421/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5506 - val_loss: 4.8235\n",
      "Epoch 11422/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5521 - val_loss: 5.0291\n",
      "Epoch 11423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5764 - val_loss: 4.8143\n",
      "Epoch 11424/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6581 - val_loss: 4.8404\n",
      "Epoch 11425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7121 - val_loss: 5.1770\n",
      "Epoch 11426/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8623 - val_loss: 5.1643\n",
      "Epoch 11427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6082 - val_loss: 4.8333\n",
      "Epoch 11428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5490 - val_loss: 4.8546\n",
      "Epoch 11429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5643 - val_loss: 5.0864\n",
      "Epoch 11430/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5270 - val_loss: 4.9063\n",
      "Epoch 11431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5284 - val_loss: 4.8413\n",
      "Epoch 11432/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5197 - val_loss: 4.8501\n",
      "Epoch 11433/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5277 - val_loss: 4.8421\n",
      "Epoch 11434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5581 - val_loss: 4.8566\n",
      "Epoch 11435/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6163 - val_loss: 4.8486\n",
      "Epoch 11436/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6350 - val_loss: 5.0062\n",
      "Epoch 11437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9017 - val_loss: 5.0563\n",
      "Epoch 11438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6159 - val_loss: 4.8484\n",
      "Epoch 11439/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5354 - val_loss: 4.8766\n",
      "Epoch 11440/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5713 - val_loss: 4.8527\n",
      "Epoch 11441/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5135 - val_loss: 4.8884\n",
      "Epoch 11442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5165 - val_loss: 4.8700\n",
      "Epoch 11443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6281 - val_loss: 4.9678\n",
      "Epoch 11444/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6847 - val_loss: 4.9690\n",
      "Epoch 11445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6084 - val_loss: 4.9480\n",
      "Epoch 11446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6047 - val_loss: 4.9333\n",
      "Epoch 11447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5198 - val_loss: 4.9047\n",
      "Epoch 11448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6742 - val_loss: 4.8803\n",
      "Epoch 11449/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5486 - val_loss: 4.9826\n",
      "Epoch 11450/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6459 - val_loss: 4.8731\n",
      "Epoch 11451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5434 - val_loss: 4.8692\n",
      "Epoch 11452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5379 - val_loss: 4.8730\n",
      "Epoch 11453/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6199 - val_loss: 4.8304\n",
      "Epoch 11454/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6163 - val_loss: 4.9081\n",
      "Epoch 11455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5201 - val_loss: 4.8477\n",
      "Epoch 11456/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5958 - val_loss: 4.8854\n",
      "Epoch 11457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6900 - val_loss: 5.0703\n",
      "Epoch 11458/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5928 - val_loss: 4.8495\n",
      "Epoch 11459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5722 - val_loss: 5.0194\n",
      "Epoch 11460/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6463 - val_loss: 5.6560\n",
      "Epoch 11461/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6708 - val_loss: 4.9012\n",
      "Epoch 11462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5610 - val_loss: 5.0137\n",
      "Epoch 11463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5378 - val_loss: 4.8831\n",
      "Epoch 11464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5949 - val_loss: 4.9583\n",
      "Epoch 11465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5185 - val_loss: 4.8537\n",
      "Epoch 11466/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5129 - val_loss: 4.8593\n",
      "Epoch 11467/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5150 - val_loss: 4.9773\n",
      "Epoch 11468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5733 - val_loss: 4.8897\n",
      "Epoch 11469/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5017 - val_loss: 4.8516\n",
      "Epoch 11470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6629 - val_loss: 4.8458\n",
      "Epoch 11471/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5388 - val_loss: 4.8370\n",
      "Epoch 11472/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6331 - val_loss: 4.9015\n",
      "Epoch 11473/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5393 - val_loss: 4.8320\n",
      "Epoch 11474/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5288 - val_loss: 4.8412\n",
      "Epoch 11475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5080 - val_loss: 4.9016\n",
      "Epoch 11476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5489 - val_loss: 4.8589\n",
      "Epoch 11477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6538 - val_loss: 4.8875\n",
      "Epoch 11478/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5586 - val_loss: 4.8455\n",
      "Epoch 11479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5981 - val_loss: 4.9370\n",
      "Epoch 11480/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5833 - val_loss: 4.8519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5731 - val_loss: 4.8735\n",
      "Epoch 11482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6759 - val_loss: 5.0775\n",
      "Epoch 11483/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0070 - val_loss: 4.8755\n",
      "Epoch 11484/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6045 - val_loss: 4.8482\n",
      "Epoch 11485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5974 - val_loss: 4.9621\n",
      "Epoch 11486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5686 - val_loss: 4.9164\n",
      "Epoch 11487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7849 - val_loss: 4.8722\n",
      "Epoch 11488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6164 - val_loss: 4.9464\n",
      "Epoch 11489/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6393 - val_loss: 4.9633\n",
      "Epoch 11490/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5824 - val_loss: 4.8658\n",
      "Epoch 11491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5503 - val_loss: 5.0522\n",
      "Epoch 11492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5207 - val_loss: 4.9737\n",
      "Epoch 11493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5268 - val_loss: 4.9296\n",
      "Epoch 11494/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6726 - val_loss: 4.8185\n",
      "Epoch 11495/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8191 - val_loss: 4.8640\n",
      "Epoch 11496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5894 - val_loss: 4.8980\n",
      "Epoch 11497/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5766 - val_loss: 5.0212\n",
      "Epoch 11498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5713 - val_loss: 4.8937\n",
      "Epoch 11499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6331 - val_loss: 4.9018\n",
      "Epoch 11500/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5862 - val_loss: 4.8780\n",
      "Epoch 11501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5506 - val_loss: 4.9624\n",
      "Epoch 11502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5022 - val_loss: 5.0212\n",
      "Epoch 11503/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6646 - val_loss: 4.8310\n",
      "Epoch 11504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5074 - val_loss: 4.9466\n",
      "Epoch 11505/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8255 - val_loss: 5.7680\n",
      "Epoch 11506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8699 - val_loss: 4.9184\n",
      "Epoch 11507/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6180 - val_loss: 5.0778\n",
      "Epoch 11508/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7247 - val_loss: 5.6693\n",
      "Epoch 11509/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6492 - val_loss: 4.9492\n",
      "Epoch 11510/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5853 - val_loss: 4.9152\n",
      "Epoch 11511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4969 - val_loss: 4.9205\n",
      "Epoch 11512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6455 - val_loss: 5.0797\n",
      "Epoch 11513/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7258 - val_loss: 4.9056\n",
      "Epoch 11514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6183 - val_loss: 5.0708\n",
      "Epoch 11515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5668 - val_loss: 4.9255\n",
      "Epoch 11516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5497 - val_loss: 4.8222\n",
      "Epoch 11517/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5136 - val_loss: 4.9359\n",
      "Epoch 11518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5803 - val_loss: 4.9371\n",
      "Epoch 11519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4920 - val_loss: 4.8916\n",
      "Epoch 11520/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6250 - val_loss: 4.8975\n",
      "Epoch 11521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7210 - val_loss: 5.0922\n",
      "Epoch 11522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5912 - val_loss: 4.9091\n",
      "Epoch 11523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5465 - val_loss: 4.8279\n",
      "Epoch 11524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6059 - val_loss: 4.8329\n",
      "Epoch 11525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6047 - val_loss: 4.9052\n",
      "Epoch 11526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5646 - val_loss: 4.8243\n",
      "Epoch 11527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6479 - val_loss: 4.8964\n",
      "Epoch 11528/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5482 - val_loss: 4.8422\n",
      "Epoch 11529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5584 - val_loss: 5.2693\n",
      "Epoch 11530/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6036 - val_loss: 4.8558\n",
      "Epoch 11531/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5495 - val_loss: 5.1426\n",
      "Epoch 11532/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6562 - val_loss: 5.0061\n",
      "Epoch 11533/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6308 - val_loss: 4.8704\n",
      "Epoch 11534/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.5636 - val_loss: 5.0039\n",
      "Epoch 11535/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6378 - val_loss: 5.3617\n",
      "Epoch 11536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8151 - val_loss: 5.2026\n",
      "Epoch 11537/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6749 - val_loss: 4.9185\n",
      "Epoch 11538/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5945 - val_loss: 5.0111\n",
      "Epoch 11539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5928 - val_loss: 4.8286\n",
      "Epoch 11540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6081 - val_loss: 4.9722\n",
      "Epoch 11541/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.7767 - val_loss: 4.8896\n",
      "Epoch 11542/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5749 - val_loss: 5.0607\n",
      "Epoch 11543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6260 - val_loss: 4.8610\n",
      "Epoch 11544/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8772 - val_loss: 5.0709\n",
      "Epoch 11545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6090 - val_loss: 4.8226\n",
      "Epoch 11546/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5882 - val_loss: 4.8247\n",
      "Epoch 11547/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5368 - val_loss: 4.8522\n",
      "Epoch 11548/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5189 - val_loss: 4.8436\n",
      "Epoch 11549/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5175 - val_loss: 4.9518\n",
      "Epoch 11550/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9732 - val_loss: 4.9567\n",
      "Epoch 11551/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5483 - val_loss: 5.0206\n",
      "Epoch 11552/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5690 - val_loss: 4.9047\n",
      "Epoch 11553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5860 - val_loss: 5.0152\n",
      "Epoch 11554/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6060 - val_loss: 4.8448\n",
      "Epoch 11555/20000\n",
      "685/685 [==============================] - 0s 137us/step - loss: 4.5472 - val_loss: 4.8514\n",
      "Epoch 11556/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5636 - val_loss: 4.8843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11557/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5533 - val_loss: 4.9381\n",
      "Epoch 11558/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6134 - val_loss: 4.9010\n",
      "Epoch 11559/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5067 - val_loss: 4.8216\n",
      "Epoch 11560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5175 - val_loss: 4.8245\n",
      "Epoch 11561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5072 - val_loss: 4.7967\n",
      "Epoch 11562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6744 - val_loss: 4.8723\n",
      "Epoch 11563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5719 - val_loss: 4.8690\n",
      "Epoch 11564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5880 - val_loss: 4.8364\n",
      "Epoch 11565/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5335 - val_loss: 5.0018\n",
      "Epoch 11566/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7433 - val_loss: 4.8267\n",
      "Epoch 11567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6149 - val_loss: 4.8270\n",
      "Epoch 11568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6174 - val_loss: 4.8551\n",
      "Epoch 11569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5374 - val_loss: 4.8287\n",
      "Epoch 11570/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6737 - val_loss: 4.8162\n",
      "Epoch 11571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6463 - val_loss: 4.8577\n",
      "Epoch 11572/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5453 - val_loss: 5.1810\n",
      "Epoch 11573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7642 - val_loss: 5.1063\n",
      "Epoch 11574/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5648 - val_loss: 4.8294\n",
      "Epoch 11575/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5264 - val_loss: 5.1017\n",
      "Epoch 11576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5467 - val_loss: 4.8260\n",
      "Epoch 11577/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5048 - val_loss: 4.8370\n",
      "Epoch 11578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5863 - val_loss: 4.9008\n",
      "Epoch 11579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5848 - val_loss: 4.9012\n",
      "Epoch 11580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6045 - val_loss: 4.9034\n",
      "Epoch 11581/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5326 - val_loss: 4.8354\n",
      "Epoch 11582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5581 - val_loss: 4.8880\n",
      "Epoch 11583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6243 - val_loss: 4.8724\n",
      "Epoch 11584/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5620 - val_loss: 5.0777\n",
      "Epoch 11585/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6098 - val_loss: 5.0439\n",
      "Epoch 11586/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6145 - val_loss: 4.8896\n",
      "Epoch 11587/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5404 - val_loss: 4.8671\n",
      "Epoch 11588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5960 - val_loss: 5.0474\n",
      "Epoch 11589/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5625 - val_loss: 4.8772\n",
      "Epoch 11590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5392 - val_loss: 4.8704\n",
      "Epoch 11591/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5939 - val_loss: 5.4048\n",
      "Epoch 11592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6157 - val_loss: 4.8653\n",
      "Epoch 11593/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5168 - val_loss: 5.0127\n",
      "Epoch 11594/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5497 - val_loss: 4.8991\n",
      "Epoch 11595/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.5463 - val_loss: 5.0234\n",
      "Epoch 11596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6542 - val_loss: 4.8562\n",
      "Epoch 11597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5569 - val_loss: 4.8349\n",
      "Epoch 11598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7126 - val_loss: 4.8341\n",
      "Epoch 11599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5301 - val_loss: 4.9220\n",
      "Epoch 11600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6183 - val_loss: 4.8745\n",
      "Epoch 11601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5694 - val_loss: 4.8438\n",
      "Epoch 11602/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5940 - val_loss: 4.9687\n",
      "Epoch 11603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5309 - val_loss: 5.1213\n",
      "Epoch 11604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5665 - val_loss: 4.9162\n",
      "Epoch 11605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5121 - val_loss: 4.9201\n",
      "Epoch 11606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5139 - val_loss: 4.8770\n",
      "Epoch 11607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7578 - val_loss: 5.0305\n",
      "Epoch 11608/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5410 - val_loss: 4.8960\n",
      "Epoch 11609/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6657 - val_loss: 5.0105\n",
      "Epoch 11610/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5878 - val_loss: 4.9752\n",
      "Epoch 11611/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5801 - val_loss: 4.8521\n",
      "Epoch 11612/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5238 - val_loss: 4.8801\n",
      "Epoch 11613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5794 - val_loss: 5.2193\n",
      "Epoch 11614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8521 - val_loss: 5.0243\n",
      "Epoch 11615/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5711 - val_loss: 5.1388\n",
      "Epoch 11616/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6097 - val_loss: 4.8974\n",
      "Epoch 11617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5671 - val_loss: 4.9213\n",
      "Epoch 11618/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5174 - val_loss: 4.8221\n",
      "Epoch 11619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4953 - val_loss: 4.8540\n",
      "Epoch 11620/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5098 - val_loss: 4.8508\n",
      "Epoch 11621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5336 - val_loss: 4.8786\n",
      "Epoch 11622/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5198 - val_loss: 4.8803\n",
      "Epoch 11623/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.6067 - val_loss: 4.8690\n",
      "Epoch 11624/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6476 - val_loss: 4.8808\n",
      "Epoch 11625/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5736 - val_loss: 4.8532\n",
      "Epoch 11626/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6111 - val_loss: 5.0080\n",
      "Epoch 11627/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6634 - val_loss: 5.2251\n",
      "Epoch 11628/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5618 - val_loss: 4.8272\n",
      "Epoch 11629/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5770 - val_loss: 5.1111\n",
      "Epoch 11630/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5549 - val_loss: 5.0741\n",
      "Epoch 11631/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5655 - val_loss: 4.8452\n",
      "Epoch 11632/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5480 - val_loss: 4.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11633/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5499 - val_loss: 5.0689\n",
      "Epoch 11634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6715 - val_loss: 5.0745\n",
      "Epoch 11635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6704 - val_loss: 4.9218\n",
      "Epoch 11636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6474 - val_loss: 4.9921\n",
      "Epoch 11637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5095 - val_loss: 4.8673\n",
      "Epoch 11638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6202 - val_loss: 4.9102\n",
      "Epoch 11639/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5207 - val_loss: 4.8695\n",
      "Epoch 11640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6091 - val_loss: 5.3303\n",
      "Epoch 11641/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5927 - val_loss: 4.8616\n",
      "Epoch 11642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6078 - val_loss: 4.9274\n",
      "Epoch 11643/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5656 - val_loss: 5.4415\n",
      "Epoch 11644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6382 - val_loss: 5.0322\n",
      "Epoch 11645/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6177 - val_loss: 4.8240\n",
      "Epoch 11646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5467 - val_loss: 4.8399\n",
      "Epoch 11647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5021 - val_loss: 4.9147\n",
      "Epoch 11648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5122 - val_loss: 4.9965\n",
      "Epoch 11649/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7387 - val_loss: 5.1395\n",
      "Epoch 11650/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9393 - val_loss: 4.8483\n",
      "Epoch 11651/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7571 - val_loss: 4.8638\n",
      "Epoch 11652/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6711 - val_loss: 4.9951\n",
      "Epoch 11653/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6147 - val_loss: 4.8462\n",
      "Epoch 11654/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6527 - val_loss: 4.8492\n",
      "Epoch 11655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6642 - val_loss: 5.0118\n",
      "Epoch 11656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7755 - val_loss: 4.9677\n",
      "Epoch 11657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6496 - val_loss: 4.8389\n",
      "Epoch 11658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7090 - val_loss: 4.8785\n",
      "Epoch 11659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5890 - val_loss: 4.8796\n",
      "Epoch 11660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5223 - val_loss: 4.8603\n",
      "Epoch 11661/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5417 - val_loss: 4.9139\n",
      "Epoch 11662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6321 - val_loss: 4.8787\n",
      "Epoch 11663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5365 - val_loss: 4.8592\n",
      "Epoch 11664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5506 - val_loss: 4.8656\n",
      "Epoch 11665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6098 - val_loss: 4.8391\n",
      "Epoch 11666/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7076 - val_loss: 4.8250\n",
      "Epoch 11667/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6147 - val_loss: 4.8049\n",
      "Epoch 11668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5474 - val_loss: 4.9838\n",
      "Epoch 11669/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9535 - val_loss: 5.2888\n",
      "Epoch 11670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7473 - val_loss: 4.9538\n",
      "Epoch 11671/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6726 - val_loss: 4.8194\n",
      "Epoch 11672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6538 - val_loss: 4.8685\n",
      "Epoch 11673/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6109 - val_loss: 4.9431\n",
      "Epoch 11674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6098 - val_loss: 5.3741\n",
      "Epoch 11675/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5716 - val_loss: 4.8571\n",
      "Epoch 11676/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4854 - val_loss: 5.4952\n",
      "Epoch 11677/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6432 - val_loss: 5.0509\n",
      "Epoch 11678/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7275 - val_loss: 5.9109\n",
      "Epoch 11679/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9238 - val_loss: 5.2232\n",
      "Epoch 11680/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7512 - val_loss: 5.4121\n",
      "Epoch 11681/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7199 - val_loss: 4.8362\n",
      "Epoch 11682/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5499 - val_loss: 4.8780\n",
      "Epoch 11683/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5532 - val_loss: 4.9616\n",
      "Epoch 11684/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6704 - val_loss: 4.8513\n",
      "Epoch 11685/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6540 - val_loss: 4.9141\n",
      "Epoch 11686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5513 - val_loss: 4.8466\n",
      "Epoch 11687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5035 - val_loss: 4.8329\n",
      "Epoch 11688/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6203 - val_loss: 4.9060\n",
      "Epoch 11689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6289 - val_loss: 5.0421\n",
      "Epoch 11690/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6850 - val_loss: 4.9212\n",
      "Epoch 11691/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6156 - val_loss: 4.8390\n",
      "Epoch 11692/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5137 - val_loss: 4.9412\n",
      "Epoch 11693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4966 - val_loss: 4.8805\n",
      "Epoch 11694/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6453 - val_loss: 4.9112\n",
      "Epoch 11695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5566 - val_loss: 5.3462\n",
      "Epoch 11696/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6697 - val_loss: 5.1963\n",
      "Epoch 11697/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5873 - val_loss: 4.9674\n",
      "Epoch 11698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5101 - val_loss: 4.8178\n",
      "Epoch 11699/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7251 - val_loss: 4.8681\n",
      "Epoch 11700/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5871 - val_loss: 4.8727\n",
      "Epoch 11701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5235 - val_loss: 5.0296\n",
      "Epoch 11702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7164 - val_loss: 4.8356\n",
      "Epoch 11703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5903 - val_loss: 4.8699\n",
      "Epoch 11704/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4903 - val_loss: 4.8889\n",
      "Epoch 11705/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5743 - val_loss: 4.8759\n",
      "Epoch 11706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7949 - val_loss: 5.0893\n",
      "Epoch 11707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6012 - val_loss: 4.8382\n",
      "Epoch 11708/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7050 - val_loss: 5.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11709/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7358 - val_loss: 4.9519\n",
      "Epoch 11710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5235 - val_loss: 4.7972\n",
      "Epoch 11711/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5146 - val_loss: 4.9164\n",
      "Epoch 11712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7144 - val_loss: 4.8933\n",
      "Epoch 11713/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6979 - val_loss: 4.8304\n",
      "Epoch 11714/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0803 - val_loss: 5.5715\n",
      "Epoch 11715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8251 - val_loss: 4.9991\n",
      "Epoch 11716/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6218 - val_loss: 5.1545\n",
      "Epoch 11717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7106 - val_loss: 5.2117\n",
      "Epoch 11718/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6570 - val_loss: 4.8346\n",
      "Epoch 11719/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5289 - val_loss: 4.8486\n",
      "Epoch 11720/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5150 - val_loss: 4.9669\n",
      "Epoch 11721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5361 - val_loss: 4.9534\n",
      "Epoch 11722/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5100 - val_loss: 4.9050\n",
      "Epoch 11723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6501 - val_loss: 4.8721\n",
      "Epoch 11724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6436 - val_loss: 5.5165\n",
      "Epoch 11725/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5556 - val_loss: 4.8231\n",
      "Epoch 11726/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5395 - val_loss: 5.0863\n",
      "Epoch 11727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9164 - val_loss: 4.8400\n",
      "Epoch 11728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6110 - val_loss: 4.8690\n",
      "Epoch 11729/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7306 - val_loss: 4.8789\n",
      "Epoch 11730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6581 - val_loss: 4.8449\n",
      "Epoch 11731/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5212 - val_loss: 4.8970\n",
      "Epoch 11732/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6095 - val_loss: 4.8628\n",
      "Epoch 11733/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7797 - val_loss: 4.9973\n",
      "Epoch 11734/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0259 - val_loss: 5.2278\n",
      "Epoch 11735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6656 - val_loss: 5.4947\n",
      "Epoch 11736/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6775 - val_loss: 4.9056\n",
      "Epoch 11737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4877 - val_loss: 4.8234\n",
      "Epoch 11738/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5327 - val_loss: 4.9283\n",
      "Epoch 11739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5638 - val_loss: 4.8366\n",
      "Epoch 11740/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5219 - val_loss: 4.8309\n",
      "Epoch 11741/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6734 - val_loss: 5.1482\n",
      "Epoch 11742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6424 - val_loss: 4.8426\n",
      "Epoch 11743/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6573 - val_loss: 4.9707\n",
      "Epoch 11744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7904 - val_loss: 4.9950\n",
      "Epoch 11745/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7933 - val_loss: 5.1481\n",
      "Epoch 11746/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6099 - val_loss: 4.9785\n",
      "Epoch 11747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5391 - val_loss: 5.0622\n",
      "Epoch 11748/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5236 - val_loss: 4.8784\n",
      "Epoch 11749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6238 - val_loss: 4.8849\n",
      "Epoch 11750/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9320 - val_loss: 4.8323\n",
      "Epoch 11751/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7710 - val_loss: 4.9090\n",
      "Epoch 11752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5929 - val_loss: 4.9143\n",
      "Epoch 11753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5253 - val_loss: 4.8260\n",
      "Epoch 11754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6395 - val_loss: 5.0217\n",
      "Epoch 11755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6753 - val_loss: 4.8242\n",
      "Epoch 11756/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5883 - val_loss: 4.8381\n",
      "Epoch 11757/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5959 - val_loss: 5.2281\n",
      "Epoch 11758/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5438 - val_loss: 4.8846\n",
      "Epoch 11759/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6783 - val_loss: 5.0223\n",
      "Epoch 11760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7027 - val_loss: 4.8776\n",
      "Epoch 11761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5652 - val_loss: 4.9689\n",
      "Epoch 11762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7562 - val_loss: 4.8184\n",
      "Epoch 11763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6149 - val_loss: 4.8385\n",
      "Epoch 11764/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5394 - val_loss: 4.8333\n",
      "Epoch 11765/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5919 - val_loss: 4.8870\n",
      "Epoch 11766/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5974 - val_loss: 4.8903\n",
      "Epoch 11767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6384 - val_loss: 4.9808\n",
      "Epoch 11768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6721 - val_loss: 4.8389\n",
      "Epoch 11769/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5694 - val_loss: 5.0054\n",
      "Epoch 11770/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5555 - val_loss: 4.9084\n",
      "Epoch 11771/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8222 - val_loss: 4.9962\n",
      "Epoch 11772/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6114 - val_loss: 4.9488\n",
      "Epoch 11773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7046 - val_loss: 5.2758\n",
      "Epoch 11774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6496 - val_loss: 4.8551\n",
      "Epoch 11775/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6586 - val_loss: 4.8273\n",
      "Epoch 11776/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6024 - val_loss: 5.0824\n",
      "Epoch 11777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6269 - val_loss: 5.1826\n",
      "Epoch 11778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5686 - val_loss: 4.8774\n",
      "Epoch 11779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6023 - val_loss: 5.0436\n",
      "Epoch 11780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6238 - val_loss: 5.2090\n",
      "Epoch 11781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6002 - val_loss: 4.8349\n",
      "Epoch 11782/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6019 - val_loss: 4.9610\n",
      "Epoch 11783/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.5749 - val_loss: 4.8628\n",
      "Epoch 11784/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5559 - val_loss: 4.8402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11785/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5624 - val_loss: 4.8536\n",
      "Epoch 11786/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8473 - val_loss: 5.1587\n",
      "Epoch 11787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5525 - val_loss: 5.0421\n",
      "Epoch 11788/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7275 - val_loss: 5.0350\n",
      "Epoch 11789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8936 - val_loss: 5.2332\n",
      "Epoch 11790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8777 - val_loss: 5.1287\n",
      "Epoch 11791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5954 - val_loss: 4.8253\n",
      "Epoch 11792/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5306 - val_loss: 4.8412\n",
      "Epoch 11793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5248 - val_loss: 5.0979\n",
      "Epoch 11794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6677 - val_loss: 4.8279\n",
      "Epoch 11795/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5866 - val_loss: 4.8754\n",
      "Epoch 11796/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5785 - val_loss: 4.9070\n",
      "Epoch 11797/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5222 - val_loss: 4.9020\n",
      "Epoch 11798/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5674 - val_loss: 4.9426\n",
      "Epoch 11799/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7324 - val_loss: 4.9452\n",
      "Epoch 11800/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.8037 - val_loss: 4.8109\n",
      "Epoch 11801/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5999 - val_loss: 4.9896\n",
      "Epoch 11802/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5358 - val_loss: 4.8581\n",
      "Epoch 11803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7740 - val_loss: 4.8947\n",
      "Epoch 11804/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5700 - val_loss: 4.8109\n",
      "Epoch 11805/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5434 - val_loss: 4.8350\n",
      "Epoch 11806/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8557 - val_loss: 4.8739\n",
      "Epoch 11807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7407 - val_loss: 4.8491\n",
      "Epoch 11808/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6988 - val_loss: 4.8264\n",
      "Epoch 11809/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.260 - 0s 46us/step - loss: 4.6749 - val_loss: 5.2397\n",
      "Epoch 11810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6075 - val_loss: 5.3084\n",
      "Epoch 11811/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6254 - val_loss: 5.0118\n",
      "Epoch 11812/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5169 - val_loss: 4.8268\n",
      "Epoch 11813/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5519 - val_loss: 5.2332\n",
      "Epoch 11814/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5958 - val_loss: 4.8638\n",
      "Epoch 11815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5392 - val_loss: 4.9047\n",
      "Epoch 11816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4946 - val_loss: 5.0425\n",
      "Epoch 11817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5807 - val_loss: 4.9826\n",
      "Epoch 11818/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8022 - val_loss: 4.8339\n",
      "Epoch 11819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7131 - val_loss: 4.9428\n",
      "Epoch 11820/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6783 - val_loss: 4.8312\n",
      "Epoch 11821/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5144 - val_loss: 5.1268\n",
      "Epoch 11822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7696 - val_loss: 4.9175\n",
      "Epoch 11823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9739 - val_loss: 5.0806\n",
      "Epoch 11824/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7381 - val_loss: 5.0106\n",
      "Epoch 11825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7495 - val_loss: 4.8534\n",
      "Epoch 11826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7729 - val_loss: 4.8211\n",
      "Epoch 11827/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6299 - val_loss: 4.9221\n",
      "Epoch 11828/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5463 - val_loss: 4.8709\n",
      "Epoch 11829/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5209 - val_loss: 4.8328\n",
      "Epoch 11830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6523 - val_loss: 5.5667\n",
      "Epoch 11831/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8010 - val_loss: 4.8379\n",
      "Epoch 11832/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6136 - val_loss: 4.9998\n",
      "Epoch 11833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6631 - val_loss: 4.9248\n",
      "Epoch 11834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5537 - val_loss: 4.8345\n",
      "Epoch 11835/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5678 - val_loss: 4.8558\n",
      "Epoch 11836/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6311 - val_loss: 4.8729\n",
      "Epoch 11837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5056 - val_loss: 4.8625\n",
      "Epoch 11838/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6123 - val_loss: 4.9765\n",
      "Epoch 11839/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6094 - val_loss: 4.8564\n",
      "Epoch 11840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5391 - val_loss: 5.0478\n",
      "Epoch 11841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5299 - val_loss: 4.8169\n",
      "Epoch 11842/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5141 - val_loss: 4.8663\n",
      "Epoch 11843/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5335 - val_loss: 4.8400\n",
      "Epoch 11844/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5117 - val_loss: 4.8460\n",
      "Epoch 11845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5260 - val_loss: 4.8886\n",
      "Epoch 11846/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5519 - val_loss: 4.9087\n",
      "Epoch 11847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5756 - val_loss: 4.8184\n",
      "Epoch 11848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5827 - val_loss: 4.9655\n",
      "Epoch 11849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6235 - val_loss: 4.8968\n",
      "Epoch 11850/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5610 - val_loss: 5.0312\n",
      "Epoch 11851/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5774 - val_loss: 4.8440\n",
      "Epoch 11852/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5759 - val_loss: 5.3077\n",
      "Epoch 11853/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7971 - val_loss: 4.9811\n",
      "Epoch 11854/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7907 - val_loss: 5.3097\n",
      "Epoch 11855/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5873 - val_loss: 4.8268\n",
      "Epoch 11856/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5159 - val_loss: 4.8648\n",
      "Epoch 11857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5946 - val_loss: 4.9398\n",
      "Epoch 11858/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7265 - val_loss: 5.0749\n",
      "Epoch 11859/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6668 - val_loss: 5.3389\n",
      "Epoch 11860/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6781 - val_loss: 5.3398\n",
      "Epoch 11861/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5382 - val_loss: 4.8774\n",
      "Epoch 11862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5351 - val_loss: 4.8270\n",
      "Epoch 11863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4956 - val_loss: 4.9190\n",
      "Epoch 11864/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4997 - val_loss: 4.8394\n",
      "Epoch 11865/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5117 - val_loss: 5.1131\n",
      "Epoch 11866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6739 - val_loss: 5.1674\n",
      "Epoch 11867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6493 - val_loss: 4.8114\n",
      "Epoch 11868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5476 - val_loss: 4.8161\n",
      "Epoch 11869/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5307 - val_loss: 4.9082\n",
      "Epoch 11870/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6849 - val_loss: 4.9646\n",
      "Epoch 11871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8720 - val_loss: 5.2805\n",
      "Epoch 11872/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5565 - val_loss: 4.9181\n",
      "Epoch 11873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5624 - val_loss: 4.8542\n",
      "Epoch 11874/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5750 - val_loss: 5.3030\n",
      "Epoch 11875/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7847 - val_loss: 4.8037\n",
      "Epoch 11876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5584 - val_loss: 4.8812\n",
      "Epoch 11877/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5700 - val_loss: 5.2362\n",
      "Epoch 11878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7751 - val_loss: 4.9757\n",
      "Epoch 11879/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6369 - val_loss: 5.1495\n",
      "Epoch 11880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8444 - val_loss: 4.9734\n",
      "Epoch 11881/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.082 - 0s 46us/step - loss: 4.7101 - val_loss: 4.7986\n",
      "Epoch 11882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5528 - val_loss: 4.8009\n",
      "Epoch 11883/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6028 - val_loss: 4.9203\n",
      "Epoch 11884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5948 - val_loss: 4.9351\n",
      "Epoch 11885/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7378 - val_loss: 4.8351\n",
      "Epoch 11886/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6694 - val_loss: 4.8151\n",
      "Epoch 11887/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5515 - val_loss: 5.1162\n",
      "Epoch 11888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7655 - val_loss: 4.8523\n",
      "Epoch 11889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8214 - val_loss: 5.0090\n",
      "Epoch 11890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6452 - val_loss: 4.8592\n",
      "Epoch 11891/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6101 - val_loss: 5.1376\n",
      "Epoch 11892/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5513 - val_loss: 4.9385\n",
      "Epoch 11893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5576 - val_loss: 4.9569\n",
      "Epoch 11894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5475 - val_loss: 4.9806\n",
      "Epoch 11895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5704 - val_loss: 4.8522\n",
      "Epoch 11896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5177 - val_loss: 4.9636\n",
      "Epoch 11897/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5482 - val_loss: 4.8704\n",
      "Epoch 11898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5353 - val_loss: 4.9714\n",
      "Epoch 11899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5549 - val_loss: 4.9174\n",
      "Epoch 11900/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6104 - val_loss: 4.9549\n",
      "Epoch 11901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7236 - val_loss: 4.9094\n",
      "Epoch 11902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5663 - val_loss: 4.8505\n",
      "Epoch 11903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5553 - val_loss: 4.8419\n",
      "Epoch 11904/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5246 - val_loss: 4.8794\n",
      "Epoch 11905/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7516 - val_loss: 5.3199\n",
      "Epoch 11906/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7714 - val_loss: 4.8697\n",
      "Epoch 11907/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5639 - val_loss: 4.8089\n",
      "Epoch 11908/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5729 - val_loss: 4.8421\n",
      "Epoch 11909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5303 - val_loss: 4.8305\n",
      "Epoch 11910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5767 - val_loss: 4.9000\n",
      "Epoch 11911/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5007 - val_loss: 4.8589\n",
      "Epoch 11912/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5834 - val_loss: 4.8781\n",
      "Epoch 11913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6349 - val_loss: 4.9117\n",
      "Epoch 11914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5123 - val_loss: 5.1591\n",
      "Epoch 11915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6135 - val_loss: 4.8264\n",
      "Epoch 11916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5200 - val_loss: 4.9880\n",
      "Epoch 11917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5173 - val_loss: 4.9215\n",
      "Epoch 11918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5377 - val_loss: 4.8365\n",
      "Epoch 11919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4982 - val_loss: 4.8269\n",
      "Epoch 11920/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6975 - val_loss: 5.1460\n",
      "Epoch 11921/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7201 - val_loss: 5.0524\n",
      "Epoch 11922/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6568 - val_loss: 4.8382\n",
      "Epoch 11923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7248 - val_loss: 4.8251\n",
      "Epoch 11924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5102 - val_loss: 4.9047\n",
      "Epoch 11925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6011 - val_loss: 4.8548\n",
      "Epoch 11926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6873 - val_loss: 5.0531\n",
      "Epoch 11927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6297 - val_loss: 4.9111\n",
      "Epoch 11928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6286 - val_loss: 5.1283\n",
      "Epoch 11929/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6232 - val_loss: 4.9006\n",
      "Epoch 11930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6472 - val_loss: 5.5078\n",
      "Epoch 11931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7867 - val_loss: 4.8380\n",
      "Epoch 11932/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7981 - val_loss: 5.1474\n",
      "Epoch 11933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6845 - val_loss: 4.8842\n",
      "Epoch 11934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5615 - val_loss: 4.8324\n",
      "Epoch 11935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5572 - val_loss: 4.9305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5416 - val_loss: 5.0772\n",
      "Epoch 11937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5371 - val_loss: 4.9458\n",
      "Epoch 11938/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5189 - val_loss: 5.0940\n",
      "Epoch 11939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6301 - val_loss: 4.8366\n",
      "Epoch 11940/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5468 - val_loss: 4.8306\n",
      "Epoch 11941/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5223 - val_loss: 4.8553\n",
      "Epoch 11942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5249 - val_loss: 4.8830\n",
      "Epoch 11943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5474 - val_loss: 4.8291\n",
      "Epoch 11944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4958 - val_loss: 4.8331\n",
      "Epoch 11945/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6292 - val_loss: 4.8570\n",
      "Epoch 11946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6896 - val_loss: 4.8974\n",
      "Epoch 11947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6127 - val_loss: 4.8229\n",
      "Epoch 11948/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6045 - val_loss: 4.8509\n",
      "Epoch 11949/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5868 - val_loss: 5.0929\n",
      "Epoch 11950/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5583 - val_loss: 4.9630\n",
      "Epoch 11951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6026 - val_loss: 4.8838\n",
      "Epoch 11952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5318 - val_loss: 5.1855\n",
      "Epoch 11953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5942 - val_loss: 4.9079\n",
      "Epoch 11954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6515 - val_loss: 5.1175\n",
      "Epoch 11955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7993 - val_loss: 5.3478\n",
      "Epoch 11956/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6587 - val_loss: 5.1910\n",
      "Epoch 11957/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7390 - val_loss: 4.9301\n",
      "Epoch 11958/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6277 - val_loss: 4.8274\n",
      "Epoch 11959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6205 - val_loss: 4.9559\n",
      "Epoch 11960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5302 - val_loss: 4.9285\n",
      "Epoch 11961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5775 - val_loss: 4.9329\n",
      "Epoch 11962/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5824 - val_loss: 4.8919\n",
      "Epoch 11963/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6574 - val_loss: 4.8920\n",
      "Epoch 11964/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7467 - val_loss: 4.9437\n",
      "Epoch 11965/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6028 - val_loss: 4.8502\n",
      "Epoch 11966/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5157 - val_loss: 4.8173\n",
      "Epoch 11967/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5373 - val_loss: 4.8901\n",
      "Epoch 11968/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5473 - val_loss: 5.0329\n",
      "Epoch 11969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7125 - val_loss: 5.0649\n",
      "Epoch 11970/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6251 - val_loss: 4.8252\n",
      "Epoch 11971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5006 - val_loss: 4.8695\n",
      "Epoch 11972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5094 - val_loss: 5.0032\n",
      "Epoch 11973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6913 - val_loss: 4.8238\n",
      "Epoch 11974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5970 - val_loss: 4.8634\n",
      "Epoch 11975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5823 - val_loss: 4.9152\n",
      "Epoch 11976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5404 - val_loss: 4.8113\n",
      "Epoch 11977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7232 - val_loss: 4.7887\n",
      "Epoch 11978/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6289 - val_loss: 4.9603\n",
      "Epoch 11979/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6928 - val_loss: 4.9788\n",
      "Epoch 11980/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5710 - val_loss: 4.9023\n",
      "Epoch 11981/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5671 - val_loss: 5.2164\n",
      "Epoch 11982/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5321 - val_loss: 4.9390\n",
      "Epoch 11983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5790 - val_loss: 4.8619\n",
      "Epoch 11984/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5542 - val_loss: 4.8179\n",
      "Epoch 11985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5212 - val_loss: 4.8300\n",
      "Epoch 11986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6841 - val_loss: 4.8621\n",
      "Epoch 11987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5137 - val_loss: 4.8579\n",
      "Epoch 11988/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7396 - val_loss: 4.8293\n",
      "Epoch 11989/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6716 - val_loss: 4.9344\n",
      "Epoch 11990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6536 - val_loss: 4.8383\n",
      "Epoch 11991/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4834 - val_loss: 4.8626\n",
      "Epoch 11992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6015 - val_loss: 5.0749\n",
      "Epoch 11993/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6083 - val_loss: 5.1162\n",
      "Epoch 11994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6551 - val_loss: 5.0483\n",
      "Epoch 11995/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6464 - val_loss: 5.1237\n",
      "Epoch 11996/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6433 - val_loss: 4.8237\n",
      "Epoch 11997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7213 - val_loss: 4.8201\n",
      "Epoch 11998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5991 - val_loss: 4.8377\n",
      "Epoch 11999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5138 - val_loss: 4.8676\n",
      "Epoch 12000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6612 - val_loss: 4.8872\n",
      "Epoch 12001/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5326 - val_loss: 4.8452\n",
      "Epoch 12002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5803 - val_loss: 4.8628\n",
      "Epoch 12003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5667 - val_loss: 4.8737\n",
      "Epoch 12004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5136 - val_loss: 4.8818\n",
      "Epoch 12005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5583 - val_loss: 4.9079\n",
      "Epoch 12006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5845 - val_loss: 4.8950\n",
      "Epoch 12007/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6538 - val_loss: 4.9262\n",
      "Epoch 12008/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5971 - val_loss: 4.8539\n",
      "Epoch 12009/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7209 - val_loss: 4.8204\n",
      "Epoch 12010/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5951 - val_loss: 4.8772\n",
      "Epoch 12011/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5366 - val_loss: 4.8186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5103 - val_loss: 4.8496\n",
      "Epoch 12013/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5516 - val_loss: 5.1975\n",
      "Epoch 12014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5715 - val_loss: 5.2549\n",
      "Epoch 12015/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6735 - val_loss: 5.2143\n",
      "Epoch 12016/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5744 - val_loss: 4.9118\n",
      "Epoch 12017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5149 - val_loss: 4.8574\n",
      "Epoch 12018/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6677 - val_loss: 4.8647\n",
      "Epoch 12019/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6652 - val_loss: 4.8041\n",
      "Epoch 12020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6620 - val_loss: 4.8582\n",
      "Epoch 12021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5584 - val_loss: 4.8480\n",
      "Epoch 12022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5785 - val_loss: 4.8412\n",
      "Epoch 12023/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7324 - val_loss: 4.8440\n",
      "Epoch 12024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5329 - val_loss: 4.8244\n",
      "Epoch 12025/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5399 - val_loss: 4.8184\n",
      "Epoch 12026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5752 - val_loss: 4.8129\n",
      "Epoch 12027/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6304 - val_loss: 4.8205\n",
      "Epoch 12028/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6074 - val_loss: 4.9514\n",
      "Epoch 12029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6727 - val_loss: 4.8101\n",
      "Epoch 12030/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5966 - val_loss: 4.8890\n",
      "Epoch 12031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5437 - val_loss: 4.8139\n",
      "Epoch 12032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4858 - val_loss: 4.8231\n",
      "Epoch 12033/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6467 - val_loss: 4.8191\n",
      "Epoch 12034/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4901 - val_loss: 4.8282\n",
      "Epoch 12035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5004 - val_loss: 4.8200\n",
      "Epoch 12036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5152 - val_loss: 4.8716\n",
      "Epoch 12037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5742 - val_loss: 4.9573\n",
      "Epoch 12038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5569 - val_loss: 4.8711\n",
      "Epoch 12039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6217 - val_loss: 4.9027\n",
      "Epoch 12040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7934 - val_loss: 4.8652\n",
      "Epoch 12041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5251 - val_loss: 5.0806\n",
      "Epoch 12042/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5869 - val_loss: 4.8424\n",
      "Epoch 12043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5253 - val_loss: 5.0641\n",
      "Epoch 12044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5342 - val_loss: 4.8417\n",
      "Epoch 12045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5048 - val_loss: 5.2829\n",
      "Epoch 12046/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8155 - val_loss: 4.9190\n",
      "Epoch 12047/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5899 - val_loss: 4.8179\n",
      "Epoch 12048/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5474 - val_loss: 4.8249\n",
      "Epoch 12049/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5658 - val_loss: 4.8386\n",
      "Epoch 12050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4952 - val_loss: 4.8085\n",
      "Epoch 12051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5710 - val_loss: 4.8561\n",
      "Epoch 12052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5204 - val_loss: 4.9335\n",
      "Epoch 12053/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7087 - val_loss: 4.9035\n",
      "Epoch 12054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6089 - val_loss: 4.9588\n",
      "Epoch 12055/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5748 - val_loss: 4.8999\n",
      "Epoch 12056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6640 - val_loss: 5.1300\n",
      "Epoch 12057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6612 - val_loss: 5.2688\n",
      "Epoch 12058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5877 - val_loss: 4.9819\n",
      "Epoch 12059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6208 - val_loss: 6.0426\n",
      "Epoch 12060/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6668 - val_loss: 4.8518\n",
      "Epoch 12061/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6128 - val_loss: 4.9337\n",
      "Epoch 12062/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6043 - val_loss: 4.8512\n",
      "Epoch 12063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6178 - val_loss: 4.8026\n",
      "Epoch 12064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5779 - val_loss: 4.8311\n",
      "Epoch 12065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5061 - val_loss: 4.8444\n",
      "Epoch 12066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4917 - val_loss: 4.8508\n",
      "Epoch 12067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5952 - val_loss: 4.8464\n",
      "Epoch 12068/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5768 - val_loss: 5.1051\n",
      "Epoch 12069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5835 - val_loss: 4.8084\n",
      "Epoch 12070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6371 - val_loss: 4.9426\n",
      "Epoch 12071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7124 - val_loss: 4.8353\n",
      "Epoch 12072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6506 - val_loss: 4.8351\n",
      "Epoch 12073/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7448 - val_loss: 4.8916\n",
      "Epoch 12074/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.502 - 0s 46us/step - loss: 4.6822 - val_loss: 5.5984\n",
      "Epoch 12075/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.9771 - val_loss: 4.9017\n",
      "Epoch 12076/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5732 - val_loss: 4.8983\n",
      "Epoch 12077/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5345 - val_loss: 4.8622\n",
      "Epoch 12078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7383 - val_loss: 4.8363\n",
      "Epoch 12079/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8894 - val_loss: 4.9799\n",
      "Epoch 12080/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5993 - val_loss: 4.8437\n",
      "Epoch 12081/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6688 - val_loss: 4.8874\n",
      "Epoch 12082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5867 - val_loss: 4.9216\n",
      "Epoch 12083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6112 - val_loss: 5.0987\n",
      "Epoch 12084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5854 - val_loss: 4.8486\n",
      "Epoch 12085/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6232 - val_loss: 4.8464\n",
      "Epoch 12086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6466 - val_loss: 4.8604\n",
      "Epoch 12087/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6213 - val_loss: 4.8949\n",
      "Epoch 12088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5876 - val_loss: 4.8095\n",
      "Epoch 12089/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5352 - val_loss: 4.8224\n",
      "Epoch 12090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5511 - val_loss: 4.8458\n",
      "Epoch 12091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5829 - val_loss: 5.2063\n",
      "Epoch 12092/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6166 - val_loss: 5.1721\n",
      "Epoch 12093/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6236 - val_loss: 5.0750\n",
      "Epoch 12094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5339 - val_loss: 4.8401\n",
      "Epoch 12095/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5830 - val_loss: 4.8607\n",
      "Epoch 12096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5890 - val_loss: 4.8711\n",
      "Epoch 12097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5454 - val_loss: 4.8116\n",
      "Epoch 12098/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5445 - val_loss: 5.3534\n",
      "Epoch 12099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5650 - val_loss: 4.8333\n",
      "Epoch 12100/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5043 - val_loss: 4.9613\n",
      "Epoch 12101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5651 - val_loss: 4.8568\n",
      "Epoch 12102/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7696 - val_loss: 4.8457\n",
      "Epoch 12103/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5654 - val_loss: 4.8425\n",
      "Epoch 12104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6748 - val_loss: 4.7968\n",
      "Epoch 12105/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5478 - val_loss: 5.2198\n",
      "Epoch 12106/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8117 - val_loss: 5.3518\n",
      "Epoch 12107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5372 - val_loss: 4.8137\n",
      "Epoch 12108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5907 - val_loss: 4.8270\n",
      "Epoch 12109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5970 - val_loss: 5.0071\n",
      "Epoch 12110/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5456 - val_loss: 4.8419\n",
      "Epoch 12111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6039 - val_loss: 4.9588\n",
      "Epoch 12112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5938 - val_loss: 4.9900\n",
      "Epoch 12113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4684 - val_loss: 5.4072\n",
      "Epoch 12114/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7276 - val_loss: 4.8192\n",
      "Epoch 12115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5567 - val_loss: 4.8359\n",
      "Epoch 12116/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5019 - val_loss: 5.0149\n",
      "Epoch 12117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5902 - val_loss: 4.8283\n",
      "Epoch 12118/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5831 - val_loss: 4.9940\n",
      "Epoch 12119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7440 - val_loss: 5.0994\n",
      "Epoch 12120/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5182 - val_loss: 5.0452\n",
      "Epoch 12121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5249 - val_loss: 4.9396\n",
      "Epoch 12122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5577 - val_loss: 4.9039\n",
      "Epoch 12123/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5495 - val_loss: 4.9335\n",
      "Epoch 12124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5222 - val_loss: 4.8247\n",
      "Epoch 12125/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6091 - val_loss: 4.8101\n",
      "Epoch 12126/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5656 - val_loss: 4.8992\n",
      "Epoch 12127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6945 - val_loss: 4.8062\n",
      "Epoch 12128/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6232 - val_loss: 4.7970\n",
      "Epoch 12129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5691 - val_loss: 4.8441\n",
      "Epoch 12130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6229 - val_loss: 4.8193\n",
      "Epoch 12131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5396 - val_loss: 5.0437\n",
      "Epoch 12132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5181 - val_loss: 4.8300\n",
      "Epoch 12133/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5785 - val_loss: 4.8698\n",
      "Epoch 12134/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5367 - val_loss: 4.8233\n",
      "Epoch 12135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5549 - val_loss: 4.8396\n",
      "Epoch 12136/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5679 - val_loss: 4.9516\n",
      "Epoch 12137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6317 - val_loss: 4.7985\n",
      "Epoch 12138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5444 - val_loss: 4.8350\n",
      "Epoch 12139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5641 - val_loss: 5.0914\n",
      "Epoch 12140/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7861 - val_loss: 5.8740\n",
      "Epoch 12141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7895 - val_loss: 4.8771\n",
      "Epoch 12142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6154 - val_loss: 4.8112\n",
      "Epoch 12143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5346 - val_loss: 4.9414\n",
      "Epoch 12144/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5663 - val_loss: 5.0029\n",
      "Epoch 12145/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5557 - val_loss: 4.9782\n",
      "Epoch 12146/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6761 - val_loss: 4.8418\n",
      "Epoch 12147/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5161 - val_loss: 5.2349\n",
      "Epoch 12148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6064 - val_loss: 4.8200\n",
      "Epoch 12149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6324 - val_loss: 4.9805\n",
      "Epoch 12150/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5829 - val_loss: 4.8177\n",
      "Epoch 12151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5314 - val_loss: 4.8239\n",
      "Epoch 12152/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5232 - val_loss: 4.8187\n",
      "Epoch 12153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5294 - val_loss: 5.0131\n",
      "Epoch 12154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5778 - val_loss: 4.8852\n",
      "Epoch 12155/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5724 - val_loss: 4.8812\n",
      "Epoch 12156/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5414 - val_loss: 4.8278\n",
      "Epoch 12157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5307 - val_loss: 4.8421\n",
      "Epoch 12158/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.573 - 0s 46us/step - loss: 4.7110 - val_loss: 5.1033\n",
      "Epoch 12159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6966 - val_loss: 4.8247\n",
      "Epoch 12160/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5423 - val_loss: 4.8286\n",
      "Epoch 12161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5719 - val_loss: 4.8150\n",
      "Epoch 12162/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5567 - val_loss: 5.0258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12163/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6241 - val_loss: 4.9123\n",
      "Epoch 12164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7999 - val_loss: 5.3565\n",
      "Epoch 12165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8537 - val_loss: 4.7957\n",
      "Epoch 12166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6188 - val_loss: 4.8431\n",
      "Epoch 12167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5407 - val_loss: 4.9147\n",
      "Epoch 12168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5132 - val_loss: 4.9265\n",
      "Epoch 12169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5594 - val_loss: 4.8249\n",
      "Epoch 12170/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7579 - val_loss: 4.8182\n",
      "Epoch 12171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4872 - val_loss: 4.8539\n",
      "Epoch 12172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5770 - val_loss: 4.8447\n",
      "Epoch 12173/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8426 - val_loss: 4.9950\n",
      "Epoch 12174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6141 - val_loss: 5.1920\n",
      "Epoch 12175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6124 - val_loss: 4.8226\n",
      "Epoch 12176/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6571 - val_loss: 4.7930\n",
      "Epoch 12177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5381 - val_loss: 4.8499\n",
      "Epoch 12178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5981 - val_loss: 4.8846\n",
      "Epoch 12179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5520 - val_loss: 4.8216\n",
      "Epoch 12180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5105 - val_loss: 4.9025\n",
      "Epoch 12181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7454 - val_loss: 4.8427\n",
      "Epoch 12182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5203 - val_loss: 4.8982\n",
      "Epoch 12183/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6198 - val_loss: 4.8270\n",
      "Epoch 12184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5134 - val_loss: 4.7919\n",
      "Epoch 12185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5663 - val_loss: 4.8829\n",
      "Epoch 12186/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5290 - val_loss: 5.0149\n",
      "Epoch 12187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5573 - val_loss: 5.1791\n",
      "Epoch 12188/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5680 - val_loss: 4.8534\n",
      "Epoch 12189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6898 - val_loss: 4.9855\n",
      "Epoch 12190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6759 - val_loss: 4.7988\n",
      "Epoch 12191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5009 - val_loss: 4.8795\n",
      "Epoch 12192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5275 - val_loss: 4.8540\n",
      "Epoch 12193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7459 - val_loss: 4.8484\n",
      "Epoch 12194/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5071 - val_loss: 4.8347\n",
      "Epoch 12195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5408 - val_loss: 4.8915\n",
      "Epoch 12196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7385 - val_loss: 4.8161\n",
      "Epoch 12197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5070 - val_loss: 5.1236\n",
      "Epoch 12198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7877 - val_loss: 4.9492\n",
      "Epoch 12199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5274 - val_loss: 4.8508\n",
      "Epoch 12200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5175 - val_loss: 4.8912\n",
      "Epoch 12201/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5444 - val_loss: 4.8179\n",
      "Epoch 12202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5195 - val_loss: 4.8204\n",
      "Epoch 12203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5348 - val_loss: 4.8693\n",
      "Epoch 12204/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5553 - val_loss: 4.8959\n",
      "Epoch 12205/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5317 - val_loss: 5.1618\n",
      "Epoch 12206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6439 - val_loss: 5.1864\n",
      "Epoch 12207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6186 - val_loss: 4.8674\n",
      "Epoch 12208/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6131 - val_loss: 5.0283\n",
      "Epoch 12209/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6690 - val_loss: 4.8132\n",
      "Epoch 12210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6267 - val_loss: 4.9469\n",
      "Epoch 12211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7416 - val_loss: 4.9870\n",
      "Epoch 12212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7663 - val_loss: 5.0958\n",
      "Epoch 12213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8477 - val_loss: 4.8589\n",
      "Epoch 12214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8850 - val_loss: 4.8655\n",
      "Epoch 12215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6179 - val_loss: 4.8831\n",
      "Epoch 12216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5287 - val_loss: 5.3120\n",
      "Epoch 12217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6939 - val_loss: 5.0453\n",
      "Epoch 12218/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7222 - val_loss: 4.8248\n",
      "Epoch 12219/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6134 - val_loss: 4.8423\n",
      "Epoch 12220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5946 - val_loss: 4.8322\n",
      "Epoch 12221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5124 - val_loss: 4.8135\n",
      "Epoch 12222/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6939 - val_loss: 4.9571\n",
      "Epoch 12223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7330 - val_loss: 4.8330\n",
      "Epoch 12224/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5963 - val_loss: 4.8304\n",
      "Epoch 12225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5527 - val_loss: 4.8664\n",
      "Epoch 12226/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5511 - val_loss: 4.8889\n",
      "Epoch 12227/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6221 - val_loss: 5.1329\n",
      "Epoch 12228/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6128 - val_loss: 4.8728\n",
      "Epoch 12229/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.6960 - val_loss: 4.8585\n",
      "Epoch 12230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5314 - val_loss: 4.8716\n",
      "Epoch 12231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6400 - val_loss: 5.0511\n",
      "Epoch 12232/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5473 - val_loss: 4.9502\n",
      "Epoch 12233/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5482 - val_loss: 5.0107\n",
      "Epoch 12234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6028 - val_loss: 5.2003\n",
      "Epoch 12235/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7285 - val_loss: 5.1490\n",
      "Epoch 12236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6398 - val_loss: 4.9627\n",
      "Epoch 12237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6709 - val_loss: 5.1954\n",
      "Epoch 12238/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5850 - val_loss: 5.4277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12239/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7424 - val_loss: 5.1507\n",
      "Epoch 12240/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5678 - val_loss: 4.8790\n",
      "Epoch 12241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6709 - val_loss: 4.9432\n",
      "Epoch 12242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6801 - val_loss: 4.8073\n",
      "Epoch 12243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5789 - val_loss: 4.9066\n",
      "Epoch 12244/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5586 - val_loss: 4.8764\n",
      "Epoch 12245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6953 - val_loss: 4.8584\n",
      "Epoch 12246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5629 - val_loss: 4.8993\n",
      "Epoch 12247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5457 - val_loss: 4.8733\n",
      "Epoch 12248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5569 - val_loss: 4.8514\n",
      "Epoch 12249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6200 - val_loss: 4.9579\n",
      "Epoch 12250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6260 - val_loss: 4.8154\n",
      "Epoch 12251/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5117 - val_loss: 5.2440\n",
      "Epoch 12252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5689 - val_loss: 4.8765\n",
      "Epoch 12253/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6116 - val_loss: 5.1114\n",
      "Epoch 12254/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6641 - val_loss: 5.5455\n",
      "Epoch 12255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6667 - val_loss: 4.9145\n",
      "Epoch 12256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7288 - val_loss: 5.0191\n",
      "Epoch 12257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6664 - val_loss: 5.0588\n",
      "Epoch 12258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5305 - val_loss: 4.8095\n",
      "Epoch 12259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5103 - val_loss: 4.8139\n",
      "Epoch 12260/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5539 - val_loss: 4.8437\n",
      "Epoch 12261/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6165 - val_loss: 4.8278\n",
      "Epoch 12262/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5886 - val_loss: 4.8417\n",
      "Epoch 12263/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8244 - val_loss: 4.8103\n",
      "Epoch 12264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6032 - val_loss: 4.9870\n",
      "Epoch 12265/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6809 - val_loss: 4.8132\n",
      "Epoch 12266/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5213 - val_loss: 4.9287\n",
      "Epoch 12267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6361 - val_loss: 4.8184\n",
      "Epoch 12268/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5321 - val_loss: 4.8201\n",
      "Epoch 12269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5206 - val_loss: 4.8465\n",
      "Epoch 12270/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5188 - val_loss: 4.9113\n",
      "Epoch 12271/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5550 - val_loss: 5.8044\n",
      "Epoch 12272/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7131 - val_loss: 4.9211\n",
      "Epoch 12273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5092 - val_loss: 4.8023\n",
      "Epoch 12274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5885 - val_loss: 4.9319\n",
      "Epoch 12275/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7475 - val_loss: 4.8979\n",
      "Epoch 12276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6029 - val_loss: 5.0896\n",
      "Epoch 12277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5697 - val_loss: 4.8223\n",
      "Epoch 12278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5675 - val_loss: 4.8241\n",
      "Epoch 12279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5829 - val_loss: 4.9888\n",
      "Epoch 12280/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7557 - val_loss: 5.4417\n",
      "Epoch 12281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6870 - val_loss: 4.9413\n",
      "Epoch 12282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6091 - val_loss: 5.0539\n",
      "Epoch 12283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5625 - val_loss: 4.8215\n",
      "Epoch 12284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6698 - val_loss: 4.8256\n",
      "Epoch 12285/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7865 - val_loss: 4.8373\n",
      "Epoch 12286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5873 - val_loss: 4.9832\n",
      "Epoch 12287/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6746 - val_loss: 4.8414\n",
      "Epoch 12288/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5276 - val_loss: 5.0492\n",
      "Epoch 12289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5516 - val_loss: 5.0101\n",
      "Epoch 12290/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5176 - val_loss: 4.9942\n",
      "Epoch 12291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5349 - val_loss: 4.8854\n",
      "Epoch 12292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4909 - val_loss: 4.8327\n",
      "Epoch 12293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6822 - val_loss: 4.7910\n",
      "Epoch 12294/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5501 - val_loss: 5.0601\n",
      "Epoch 12295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5816 - val_loss: 4.7934\n",
      "Epoch 12296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5424 - val_loss: 4.8205\n",
      "Epoch 12297/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5411 - val_loss: 4.8385\n",
      "Epoch 12298/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5411 - val_loss: 4.8203\n",
      "Epoch 12299/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5882 - val_loss: 4.8637\n",
      "Epoch 12300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4774 - val_loss: 4.9637\n",
      "Epoch 12301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4737 - val_loss: 4.8576\n",
      "Epoch 12302/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5296 - val_loss: 4.8173\n",
      "Epoch 12303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5336 - val_loss: 4.9651\n",
      "Epoch 12304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8448 - val_loss: 5.0011\n",
      "Epoch 12305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6578 - val_loss: 5.1398\n",
      "Epoch 12306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8451 - val_loss: 5.0511\n",
      "Epoch 12307/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7803 - val_loss: 4.8421\n",
      "Epoch 12308/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6449 - val_loss: 4.7967\n",
      "Epoch 12309/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5887 - val_loss: 4.8002\n",
      "Epoch 12310/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5465 - val_loss: 4.8103\n",
      "Epoch 12311/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5279 - val_loss: 4.8057\n",
      "Epoch 12312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6599 - val_loss: 4.7984\n",
      "Epoch 12313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5855 - val_loss: 4.8362\n",
      "Epoch 12314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6145 - val_loss: 4.8135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12315/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6649 - val_loss: 4.8991\n",
      "Epoch 12316/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6662 - val_loss: 4.9676\n",
      "Epoch 12317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6694 - val_loss: 5.3214\n",
      "Epoch 12318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7291 - val_loss: 4.9325\n",
      "Epoch 12319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5347 - val_loss: 4.8962\n",
      "Epoch 12320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6450 - val_loss: 4.8089\n",
      "Epoch 12321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5303 - val_loss: 4.8508\n",
      "Epoch 12322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5010 - val_loss: 4.8173\n",
      "Epoch 12323/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6606 - val_loss: 4.8374\n",
      "Epoch 12324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5174 - val_loss: 4.8383\n",
      "Epoch 12325/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.002 - 0s 68us/step - loss: 4.5077 - val_loss: 4.8417\n",
      "Epoch 12326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5724 - val_loss: 4.7984\n",
      "Epoch 12327/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6292 - val_loss: 4.8776\n",
      "Epoch 12328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5204 - val_loss: 4.8201\n",
      "Epoch 12329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5842 - val_loss: 4.8945\n",
      "Epoch 12330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5306 - val_loss: 4.8336\n",
      "Epoch 12331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5119 - val_loss: 4.9588\n",
      "Epoch 12332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6580 - val_loss: 4.8173\n",
      "Epoch 12333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8423 - val_loss: 5.0617\n",
      "Epoch 12334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6495 - val_loss: 4.8088\n",
      "Epoch 12335/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5507 - val_loss: 4.8574\n",
      "Epoch 12336/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6967 - val_loss: 5.1833\n",
      "Epoch 12337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7803 - val_loss: 5.1583\n",
      "Epoch 12338/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6802 - val_loss: 4.9092\n",
      "Epoch 12339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5615 - val_loss: 4.8164\n",
      "Epoch 12340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6302 - val_loss: 4.8878\n",
      "Epoch 12341/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7513 - val_loss: 4.8846\n",
      "Epoch 12342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6448 - val_loss: 4.8897\n",
      "Epoch 12343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5421 - val_loss: 5.2144\n",
      "Epoch 12344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5915 - val_loss: 4.9662\n",
      "Epoch 12345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5109 - val_loss: 5.0648\n",
      "Epoch 12346/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5537 - val_loss: 4.8454\n",
      "Epoch 12347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5811 - val_loss: 5.0132\n",
      "Epoch 12348/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4886 - val_loss: 4.8512\n",
      "Epoch 12349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5226 - val_loss: 4.8196\n",
      "Epoch 12350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5578 - val_loss: 4.8880\n",
      "Epoch 12351/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5418 - val_loss: 4.9694\n",
      "Epoch 12352/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6337 - val_loss: 4.8718\n",
      "Epoch 12353/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6050 - val_loss: 4.8367\n",
      "Epoch 12354/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5787 - val_loss: 5.0809\n",
      "Epoch 12355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6801 - val_loss: 5.0813\n",
      "Epoch 12356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7961 - val_loss: 5.3482\n",
      "Epoch 12357/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8591 - val_loss: 4.8303\n",
      "Epoch 12358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5092 - val_loss: 4.9671\n",
      "Epoch 12359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6883 - val_loss: 4.9471\n",
      "Epoch 12360/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4901 - val_loss: 4.8106\n",
      "Epoch 12361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5603 - val_loss: 4.9001\n",
      "Epoch 12362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5193 - val_loss: 4.8744\n",
      "Epoch 12363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5344 - val_loss: 5.0180\n",
      "Epoch 12364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6496 - val_loss: 5.2603\n",
      "Epoch 12365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6809 - val_loss: 5.1204\n",
      "Epoch 12366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6048 - val_loss: 4.8289\n",
      "Epoch 12367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6471 - val_loss: 4.8968\n",
      "Epoch 12368/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6054 - val_loss: 4.9232\n",
      "Epoch 12369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6677 - val_loss: 4.8352\n",
      "Epoch 12370/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5045 - val_loss: 4.8627\n",
      "Epoch 12371/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7390 - val_loss: 5.4920\n",
      "Epoch 12372/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9121 - val_loss: 5.7138\n",
      "Epoch 12373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8556 - val_loss: 4.8126\n",
      "Epoch 12374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5213 - val_loss: 4.8722\n",
      "Epoch 12375/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6844 - val_loss: 5.1614\n",
      "Epoch 12376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6202 - val_loss: 4.8188\n",
      "Epoch 12377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5419 - val_loss: 4.8505\n",
      "Epoch 12378/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.6890 - val_loss: 4.8548\n",
      "Epoch 12379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6983 - val_loss: 4.8107\n",
      "Epoch 12380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6594 - val_loss: 5.2178\n",
      "Epoch 12381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7244 - val_loss: 4.8975\n",
      "Epoch 12382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6415 - val_loss: 4.9648\n",
      "Epoch 12383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4846 - val_loss: 4.8412\n",
      "Epoch 12384/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5819 - val_loss: 5.1741\n",
      "Epoch 12385/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5365 - val_loss: 4.8993\n",
      "Epoch 12386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5725 - val_loss: 4.9111\n",
      "Epoch 12387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6499 - val_loss: 5.2599\n",
      "Epoch 12388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6661 - val_loss: 4.8166\n",
      "Epoch 12389/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5239 - val_loss: 4.8502\n",
      "Epoch 12390/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5278 - val_loss: 4.9758\n",
      "Epoch 12391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5634 - val_loss: 4.8363\n",
      "Epoch 12392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5149 - val_loss: 4.8924\n",
      "Epoch 12393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6479 - val_loss: 4.9914\n",
      "Epoch 12394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6029 - val_loss: 4.8557\n",
      "Epoch 12395/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4978 - val_loss: 4.9858\n",
      "Epoch 12396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6235 - val_loss: 4.8373\n",
      "Epoch 12397/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6977 - val_loss: 4.8104\n",
      "Epoch 12398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6746 - val_loss: 4.8962\n",
      "Epoch 12399/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6713 - val_loss: 4.9239\n",
      "Epoch 12400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5369 - val_loss: 5.0261\n",
      "Epoch 12401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5911 - val_loss: 5.1254\n",
      "Epoch 12402/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6844 - val_loss: 5.0162\n",
      "Epoch 12403/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5606 - val_loss: 4.8889\n",
      "Epoch 12404/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5037 - val_loss: 4.8353\n",
      "Epoch 12405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4801 - val_loss: 4.8084\n",
      "Epoch 12406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6062 - val_loss: 5.4138\n",
      "Epoch 12407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7013 - val_loss: 4.8553\n",
      "Epoch 12408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4982 - val_loss: 4.9744\n",
      "Epoch 12409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5006 - val_loss: 4.8999\n",
      "Epoch 12410/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7727 - val_loss: 5.0524\n",
      "Epoch 12411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5350 - val_loss: 4.8520\n",
      "Epoch 12412/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6524 - val_loss: 4.8092\n",
      "Epoch 12413/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5458 - val_loss: 4.9054\n",
      "Epoch 12414/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4985 - val_loss: 5.5638\n",
      "Epoch 12415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8223 - val_loss: 4.8354\n",
      "Epoch 12416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6871 - val_loss: 4.8810\n",
      "Epoch 12417/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5568 - val_loss: 5.0012\n",
      "Epoch 12418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5842 - val_loss: 4.8504\n",
      "Epoch 12419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7390 - val_loss: 5.4265\n",
      "Epoch 12420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5645 - val_loss: 4.8017\n",
      "Epoch 12421/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5903 - val_loss: 4.8189\n",
      "Epoch 12422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6379 - val_loss: 4.8459\n",
      "Epoch 12423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4939 - val_loss: 4.8791\n",
      "Epoch 12424/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5036 - val_loss: 4.9072\n",
      "Epoch 12425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5683 - val_loss: 4.8682\n",
      "Epoch 12426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6249 - val_loss: 4.9445\n",
      "Epoch 12427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5580 - val_loss: 4.9350\n",
      "Epoch 12428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5933 - val_loss: 4.8651\n",
      "Epoch 12429/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6558 - val_loss: 4.8676\n",
      "Epoch 12430/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6986 - val_loss: 4.9057\n",
      "Epoch 12431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7418 - val_loss: 5.0604\n",
      "Epoch 12432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6475 - val_loss: 4.8152\n",
      "Epoch 12433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7020 - val_loss: 4.8167\n",
      "Epoch 12434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4785 - val_loss: 4.9559\n",
      "Epoch 12435/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5542 - val_loss: 5.1263\n",
      "Epoch 12436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5694 - val_loss: 4.9967\n",
      "Epoch 12437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6577 - val_loss: 5.2954\n",
      "Epoch 12438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5658 - val_loss: 4.8762\n",
      "Epoch 12439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5218 - val_loss: 4.9792\n",
      "Epoch 12440/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6090 - val_loss: 4.8531\n",
      "Epoch 12441/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4816 - val_loss: 4.8623\n",
      "Epoch 12442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5146 - val_loss: 4.8215\n",
      "Epoch 12443/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5994 - val_loss: 4.8241\n",
      "Epoch 12444/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5342 - val_loss: 5.0170\n",
      "Epoch 12445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5634 - val_loss: 4.8175\n",
      "Epoch 12446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5592 - val_loss: 4.8749\n",
      "Epoch 12447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5380 - val_loss: 4.9254\n",
      "Epoch 12448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5452 - val_loss: 4.8436\n",
      "Epoch 12449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5984 - val_loss: 4.8500\n",
      "Epoch 12450/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5309 - val_loss: 5.1399\n",
      "Epoch 12451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5147 - val_loss: 4.8535\n",
      "Epoch 12452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5948 - val_loss: 4.8747\n",
      "Epoch 12453/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5549 - val_loss: 4.8353\n",
      "Epoch 12454/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5805 - val_loss: 5.2511\n",
      "Epoch 12455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7129 - val_loss: 4.9565\n",
      "Epoch 12456/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5169 - val_loss: 4.8803\n",
      "Epoch 12457/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4934 - val_loss: 4.9505\n",
      "Epoch 12458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7035 - val_loss: 4.9887\n",
      "Epoch 12459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5073 - val_loss: 4.7973\n",
      "Epoch 12460/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5456 - val_loss: 4.8145\n",
      "Epoch 12461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7608 - val_loss: 5.0047\n",
      "Epoch 12462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8378 - val_loss: 5.1703\n",
      "Epoch 12463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8685 - val_loss: 4.8649\n",
      "Epoch 12464/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5579 - val_loss: 4.9720\n",
      "Epoch 12465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5647 - val_loss: 4.8586\n",
      "Epoch 12466/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5688 - val_loss: 5.4139\n",
      "Epoch 12467/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8457 - val_loss: 5.2541\n",
      "Epoch 12468/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7489 - val_loss: 5.4039\n",
      "Epoch 12469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7204 - val_loss: 5.0144\n",
      "Epoch 12470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6027 - val_loss: 5.0891\n",
      "Epoch 12471/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5709 - val_loss: 4.9381\n",
      "Epoch 12472/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5743 - val_loss: 4.9770\n",
      "Epoch 12473/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9333 - val_loss: 4.9296\n",
      "Epoch 12474/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6305 - val_loss: 4.9111\n",
      "Epoch 12475/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6281 - val_loss: 5.6296\n",
      "Epoch 12476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8707 - val_loss: 4.8424\n",
      "Epoch 12477/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8267 - val_loss: 5.0243\n",
      "Epoch 12478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6458 - val_loss: 4.8730\n",
      "Epoch 12479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5306 - val_loss: 4.8341\n",
      "Epoch 12480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5220 - val_loss: 4.8235\n",
      "Epoch 12481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6045 - val_loss: 4.8063\n",
      "Epoch 12482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7896 - val_loss: 4.8107\n",
      "Epoch 12483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6584 - val_loss: 4.8400\n",
      "Epoch 12484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5510 - val_loss: 4.8336\n",
      "Epoch 12485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6062 - val_loss: 4.8277\n",
      "Epoch 12486/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6259 - val_loss: 4.8263\n",
      "Epoch 12487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6107 - val_loss: 4.9724\n",
      "Epoch 12488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5419 - val_loss: 4.8186\n",
      "Epoch 12489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6221 - val_loss: 4.8378\n",
      "Epoch 12490/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5737 - val_loss: 4.9262\n",
      "Epoch 12491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5785 - val_loss: 5.1064\n",
      "Epoch 12492/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5368 - val_loss: 4.8539\n",
      "Epoch 12493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5356 - val_loss: 4.8430\n",
      "Epoch 12494/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6833 - val_loss: 4.8508\n",
      "Epoch 12495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7005 - val_loss: 4.8837\n",
      "Epoch 12496/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5224 - val_loss: 4.8267\n",
      "Epoch 12497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5610 - val_loss: 4.8541\n",
      "Epoch 12498/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4915 - val_loss: 4.8128\n",
      "Epoch 12499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6123 - val_loss: 4.8211\n",
      "Epoch 12500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6678 - val_loss: 4.8349\n",
      "Epoch 12501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6392 - val_loss: 4.8170\n",
      "Epoch 12502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6095 - val_loss: 4.8076\n",
      "Epoch 12503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5589 - val_loss: 4.8357\n",
      "Epoch 12504/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6703 - val_loss: 4.9011\n",
      "Epoch 12505/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6699 - val_loss: 4.7937\n",
      "Epoch 12506/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5192 - val_loss: 4.8200\n",
      "Epoch 12507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5393 - val_loss: 4.8345\n",
      "Epoch 12508/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5327 - val_loss: 5.2691\n",
      "Epoch 12509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5913 - val_loss: 5.2125\n",
      "Epoch 12510/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5577 - val_loss: 5.5570\n",
      "Epoch 12511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5849 - val_loss: 4.8132\n",
      "Epoch 12512/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6318 - val_loss: 5.2082\n",
      "Epoch 12513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6797 - val_loss: 4.9900\n",
      "Epoch 12514/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5295 - val_loss: 4.8287\n",
      "Epoch 12515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5887 - val_loss: 4.8495\n",
      "Epoch 12516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5544 - val_loss: 4.8879\n",
      "Epoch 12517/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5198 - val_loss: 4.8365\n",
      "Epoch 12518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5081 - val_loss: 4.8269\n",
      "Epoch 12519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5505 - val_loss: 4.8906\n",
      "Epoch 12520/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5649 - val_loss: 5.3049\n",
      "Epoch 12521/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6436 - val_loss: 4.9054\n",
      "Epoch 12522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9362 - val_loss: 5.1714\n",
      "Epoch 12523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8143 - val_loss: 5.1863\n",
      "Epoch 12524/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9248 - val_loss: 5.2916\n",
      "Epoch 12525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6790 - val_loss: 4.8371\n",
      "Epoch 12526/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5897 - val_loss: 4.8654\n",
      "Epoch 12527/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5074 - val_loss: 4.8296\n",
      "Epoch 12528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5450 - val_loss: 4.8232\n",
      "Epoch 12529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5269 - val_loss: 5.0476\n",
      "Epoch 12530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5303 - val_loss: 4.8653\n",
      "Epoch 12531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5505 - val_loss: 4.8175\n",
      "Epoch 12532/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4880 - val_loss: 4.8944\n",
      "Epoch 12533/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6124 - val_loss: 5.1288\n",
      "Epoch 12534/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6330 - val_loss: 4.8407\n",
      "Epoch 12535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6417 - val_loss: 5.3307\n",
      "Epoch 12536/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6068 - val_loss: 4.9210\n",
      "Epoch 12537/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5392 - val_loss: 4.8021\n",
      "Epoch 12538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4720 - val_loss: 4.8369\n",
      "Epoch 12539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5866 - val_loss: 4.8548\n",
      "Epoch 12540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5949 - val_loss: 4.8665\n",
      "Epoch 12541/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5305 - val_loss: 4.8616\n",
      "Epoch 12542/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6321 - val_loss: 4.7973\n",
      "Epoch 12543/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6823 - val_loss: 4.8056\n",
      "Epoch 12544/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6277 - val_loss: 4.7867\n",
      "Epoch 12545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5225 - val_loss: 4.8718\n",
      "Epoch 12546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5388 - val_loss: 4.9007\n",
      "Epoch 12547/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5335 - val_loss: 4.8207\n",
      "Epoch 12548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5435 - val_loss: 4.8344\n",
      "Epoch 12549/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5471 - val_loss: 4.8620\n",
      "Epoch 12550/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4782 - val_loss: 5.0105\n",
      "Epoch 12551/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5629 - val_loss: 4.8054\n",
      "Epoch 12552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5161 - val_loss: 4.8163\n",
      "Epoch 12553/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8051 - val_loss: 4.9763\n",
      "Epoch 12554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6406 - val_loss: 4.8318\n",
      "Epoch 12555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5554 - val_loss: 4.8775\n",
      "Epoch 12556/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5671 - val_loss: 4.8253\n",
      "Epoch 12557/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5392 - val_loss: 4.8750\n",
      "Epoch 12558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5151 - val_loss: 4.8052\n",
      "Epoch 12559/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5413 - val_loss: 4.8066\n",
      "Epoch 12560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6066 - val_loss: 4.9190\n",
      "Epoch 12561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6114 - val_loss: 5.4605\n",
      "Epoch 12562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6592 - val_loss: 5.0898\n",
      "Epoch 12563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5628 - val_loss: 4.8720\n",
      "Epoch 12564/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5093 - val_loss: 4.8213\n",
      "Epoch 12565/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5692 - val_loss: 5.0675\n",
      "Epoch 12566/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7187 - val_loss: 4.8111\n",
      "Epoch 12567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5164 - val_loss: 4.8570\n",
      "Epoch 12568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5369 - val_loss: 5.1849\n",
      "Epoch 12569/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8075 - val_loss: 5.1351\n",
      "Epoch 12570/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6598 - val_loss: 4.9736\n",
      "Epoch 12571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5177 - val_loss: 4.8354\n",
      "Epoch 12572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5451 - val_loss: 4.8357\n",
      "Epoch 12573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5248 - val_loss: 4.7898\n",
      "Epoch 12574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4987 - val_loss: 4.8030\n",
      "Epoch 12575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5066 - val_loss: 4.8653\n",
      "Epoch 12576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5007 - val_loss: 4.8651\n",
      "Epoch 12577/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5159 - val_loss: 4.8518\n",
      "Epoch 12578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5855 - val_loss: 5.0014\n",
      "Epoch 12579/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6158 - val_loss: 4.9493\n",
      "Epoch 12580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4826 - val_loss: 5.0397\n",
      "Epoch 12581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8415 - val_loss: 4.9011\n",
      "Epoch 12582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5548 - val_loss: 4.8263\n",
      "Epoch 12583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4772 - val_loss: 4.8089\n",
      "Epoch 12584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5099 - val_loss: 4.8830\n",
      "Epoch 12585/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5168 - val_loss: 4.8147\n",
      "Epoch 12586/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5801 - val_loss: 4.7887\n",
      "Epoch 12587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5129 - val_loss: 5.0871\n",
      "Epoch 12588/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5559 - val_loss: 4.9546\n",
      "Epoch 12589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6219 - val_loss: 4.8768\n",
      "Epoch 12590/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8671 - val_loss: 5.0356\n",
      "Epoch 12591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8586 - val_loss: 4.9608\n",
      "Epoch 12592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6245 - val_loss: 4.8145\n",
      "Epoch 12593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6310 - val_loss: 4.8963\n",
      "Epoch 12594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9745 - val_loss: 5.4022\n",
      "Epoch 12595/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8873 - val_loss: 5.3717\n",
      "Epoch 12596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5570 - val_loss: 4.8380\n",
      "Epoch 12597/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6286 - val_loss: 4.9538\n",
      "Epoch 12598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5560 - val_loss: 4.8086\n",
      "Epoch 12599/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5333 - val_loss: 4.8260\n",
      "Epoch 12600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5235 - val_loss: 4.8199\n",
      "Epoch 12601/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5302 - val_loss: 5.5587\n",
      "Epoch 12602/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5290 - val_loss: 4.8116\n",
      "Epoch 12603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6432 - val_loss: 4.8475\n",
      "Epoch 12604/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5067 - val_loss: 4.9246\n",
      "Epoch 12605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5061 - val_loss: 4.8036\n",
      "Epoch 12606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5184 - val_loss: 4.8162\n",
      "Epoch 12607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4938 - val_loss: 4.9329\n",
      "Epoch 12608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5603 - val_loss: 4.8949\n",
      "Epoch 12609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6411 - val_loss: 4.8710\n",
      "Epoch 12610/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5602 - val_loss: 4.8227\n",
      "Epoch 12611/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5375 - val_loss: 4.8189\n",
      "Epoch 12612/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4989 - val_loss: 4.9107\n",
      "Epoch 12613/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6001 - val_loss: 4.8529\n",
      "Epoch 12614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6028 - val_loss: 4.8610\n",
      "Epoch 12615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5234 - val_loss: 4.8898\n",
      "Epoch 12616/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5534 - val_loss: 4.9433\n",
      "Epoch 12617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6648 - val_loss: 4.8313\n",
      "Epoch 12618/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5006 - val_loss: 4.7961\n",
      "Epoch 12619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5296 - val_loss: 4.8246\n",
      "Epoch 12620/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6076 - val_loss: 4.8760\n",
      "Epoch 12621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9208 - val_loss: 4.9087\n",
      "Epoch 12622/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6437 - val_loss: 4.8656\n",
      "Epoch 12623/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5123 - val_loss: 4.8295\n",
      "Epoch 12624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5401 - val_loss: 4.8399\n",
      "Epoch 12625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6036 - val_loss: 4.9531\n",
      "Epoch 12626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4753 - val_loss: 4.8109\n",
      "Epoch 12627/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6260 - val_loss: 5.1144\n",
      "Epoch 12628/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5145 - val_loss: 4.8381\n",
      "Epoch 12629/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5427 - val_loss: 4.8636\n",
      "Epoch 12630/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6158 - val_loss: 4.8412\n",
      "Epoch 12631/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.5683 - val_loss: 5.0015\n",
      "Epoch 12632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5363 - val_loss: 4.8296\n",
      "Epoch 12633/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5982 - val_loss: 4.8505\n",
      "Epoch 12634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4879 - val_loss: 4.7880\n",
      "Epoch 12635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5025 - val_loss: 5.0279\n",
      "Epoch 12636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5046 - val_loss: 4.8497\n",
      "Epoch 12637/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5089 - val_loss: 4.8742\n",
      "Epoch 12638/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6775 - val_loss: 4.8141\n",
      "Epoch 12639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6018 - val_loss: 4.8258\n",
      "Epoch 12640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5867 - val_loss: 4.8235\n",
      "Epoch 12641/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5285 - val_loss: 4.8247\n",
      "Epoch 12642/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5526 - val_loss: 4.8261\n",
      "Epoch 12643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5319 - val_loss: 4.8600\n",
      "Epoch 12644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5658 - val_loss: 4.8546\n",
      "Epoch 12645/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7093 - val_loss: 4.8981\n",
      "Epoch 12646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7668 - val_loss: 4.8056\n",
      "Epoch 12647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5652 - val_loss: 4.9765\n",
      "Epoch 12648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5993 - val_loss: 4.8252\n",
      "Epoch 12649/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5412 - val_loss: 4.8329\n",
      "Epoch 12650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5450 - val_loss: 4.9255\n",
      "Epoch 12651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5532 - val_loss: 4.8808\n",
      "Epoch 12652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4777 - val_loss: 5.1866\n",
      "Epoch 12653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5920 - val_loss: 4.8533\n",
      "Epoch 12654/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5245 - val_loss: 4.8352\n",
      "Epoch 12655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6317 - val_loss: 5.2920\n",
      "Epoch 12656/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9619 - val_loss: 5.1176\n",
      "Epoch 12657/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5593 - val_loss: 4.7925\n",
      "Epoch 12658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5280 - val_loss: 4.9422\n",
      "Epoch 12659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5374 - val_loss: 4.8455\n",
      "Epoch 12660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6672 - val_loss: 4.8388\n",
      "Epoch 12661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7183 - val_loss: 4.9128\n",
      "Epoch 12662/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5623 - val_loss: 4.8034\n",
      "Epoch 12663/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6043 - val_loss: 4.9964\n",
      "Epoch 12664/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.617 - 0s 46us/step - loss: 4.6437 - val_loss: 4.9599\n",
      "Epoch 12665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7154 - val_loss: 5.0022\n",
      "Epoch 12666/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6078 - val_loss: 4.8232\n",
      "Epoch 12667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6805 - val_loss: 4.8045\n",
      "Epoch 12668/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6311 - val_loss: 4.9289\n",
      "Epoch 12669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7272 - val_loss: 5.2172\n",
      "Epoch 12670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6887 - val_loss: 4.8981\n",
      "Epoch 12671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5605 - val_loss: 4.8159\n",
      "Epoch 12672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5423 - val_loss: 4.8154\n",
      "Epoch 12673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6292 - val_loss: 4.8976\n",
      "Epoch 12674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8301 - val_loss: 5.0111\n",
      "Epoch 12675/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7521 - val_loss: 4.8239\n",
      "Epoch 12676/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6109 - val_loss: 5.5586\n",
      "Epoch 12677/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8153 - val_loss: 4.9062\n",
      "Epoch 12678/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5548 - val_loss: 5.1511\n",
      "Epoch 12679/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8788 - val_loss: 5.1035\n",
      "Epoch 12680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5335 - val_loss: 4.7945\n",
      "Epoch 12681/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5237 - val_loss: 4.9190\n",
      "Epoch 12682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9059 - val_loss: 5.4524\n",
      "Epoch 12683/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6857 - val_loss: 4.8131\n",
      "Epoch 12684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5415 - val_loss: 4.8407\n",
      "Epoch 12685/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7636 - val_loss: 4.8491\n",
      "Epoch 12686/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5323 - val_loss: 4.9525\n",
      "Epoch 12687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9474 - val_loss: 5.1137\n",
      "Epoch 12688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6894 - val_loss: 5.1926\n",
      "Epoch 12689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6664 - val_loss: 5.3839\n",
      "Epoch 12690/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7200 - val_loss: 4.8221\n",
      "Epoch 12691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5742 - val_loss: 4.8345\n",
      "Epoch 12692/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6710 - val_loss: 4.9301\n",
      "Epoch 12693/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4823 - val_loss: 4.8334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12694/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5608 - val_loss: 4.8163\n",
      "Epoch 12695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6236 - val_loss: 4.8962\n",
      "Epoch 12696/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7090 - val_loss: 4.9226\n",
      "Epoch 12697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5570 - val_loss: 4.8113\n",
      "Epoch 12698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5314 - val_loss: 5.0623\n",
      "Epoch 12699/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5341 - val_loss: 4.7988\n",
      "Epoch 12700/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4767 - val_loss: 4.8031\n",
      "Epoch 12701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6302 - val_loss: 4.8233\n",
      "Epoch 12702/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9343 - val_loss: 4.9642\n",
      "Epoch 12703/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5998 - val_loss: 4.8333\n",
      "Epoch 12704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5558 - val_loss: 4.8118\n",
      "Epoch 12705/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 6.383 - 0s 46us/step - loss: 4.6370 - val_loss: 5.0829\n",
      "Epoch 12706/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6826 - val_loss: 4.8644\n",
      "Epoch 12707/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6479 - val_loss: 4.8194\n",
      "Epoch 12708/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6643 - val_loss: 5.3997\n",
      "Epoch 12709/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7962 - val_loss: 4.8698\n",
      "Epoch 12710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5826 - val_loss: 4.9620\n",
      "Epoch 12711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5916 - val_loss: 4.8705\n",
      "Epoch 12712/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5210 - val_loss: 4.7903\n",
      "Epoch 12713/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6183 - val_loss: 4.9357\n",
      "Epoch 12714/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5544 - val_loss: 4.8184\n",
      "Epoch 12715/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5348 - val_loss: 4.8293\n",
      "Epoch 12716/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6168 - val_loss: 4.7965\n",
      "Epoch 12717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6552 - val_loss: 5.5398\n",
      "Epoch 12718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7270 - val_loss: 5.1441\n",
      "Epoch 12719/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6024 - val_loss: 4.8913\n",
      "Epoch 12720/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5518 - val_loss: 5.0976\n",
      "Epoch 12721/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8450 - val_loss: 4.9117\n",
      "Epoch 12722/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6450 - val_loss: 4.7973\n",
      "Epoch 12723/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5734 - val_loss: 5.1827\n",
      "Epoch 12724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5246 - val_loss: 4.8288\n",
      "Epoch 12725/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5044 - val_loss: 4.8908\n",
      "Epoch 12726/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5839 - val_loss: 5.0225\n",
      "Epoch 12727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6528 - val_loss: 5.0014\n",
      "Epoch 12728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5011 - val_loss: 4.7995\n",
      "Epoch 12729/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.5259 - val_loss: 4.8236\n",
      "Epoch 12730/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5316 - val_loss: 4.8852\n",
      "Epoch 12731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5638 - val_loss: 4.9242\n",
      "Epoch 12732/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6201 - val_loss: 4.8130\n",
      "Epoch 12733/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5728 - val_loss: 4.8073\n",
      "Epoch 12734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4792 - val_loss: 4.9674\n",
      "Epoch 12735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6315 - val_loss: 5.0444\n",
      "Epoch 12736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4955 - val_loss: 4.8990\n",
      "Epoch 12737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5486 - val_loss: 4.9528\n",
      "Epoch 12738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5231 - val_loss: 4.9187\n",
      "Epoch 12739/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5627 - val_loss: 4.8801\n",
      "Epoch 12740/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5828 - val_loss: 4.8734\n",
      "Epoch 12741/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5161 - val_loss: 4.8194\n",
      "Epoch 12742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4967 - val_loss: 4.8988\n",
      "Epoch 12743/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6345 - val_loss: 4.7934\n",
      "Epoch 12744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5587 - val_loss: 5.5231\n",
      "Epoch 12745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7269 - val_loss: 4.8061\n",
      "Epoch 12746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5443 - val_loss: 4.8423\n",
      "Epoch 12747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5711 - val_loss: 4.9852\n",
      "Epoch 12748/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7015 - val_loss: 5.1283\n",
      "Epoch 12749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6572 - val_loss: 4.8623\n",
      "Epoch 12750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5210 - val_loss: 4.9025\n",
      "Epoch 12751/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5123 - val_loss: 5.0156\n",
      "Epoch 12752/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7742 - val_loss: 4.8273\n",
      "Epoch 12753/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7936 - val_loss: 4.7814\n",
      "Epoch 12754/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5830 - val_loss: 4.8389\n",
      "Epoch 12755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7214 - val_loss: 5.1017\n",
      "Epoch 12756/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6889 - val_loss: 4.9006\n",
      "Epoch 12757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5409 - val_loss: 4.7963\n",
      "Epoch 12758/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5058 - val_loss: 4.9544\n",
      "Epoch 12759/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6177 - val_loss: 5.0141\n",
      "Epoch 12760/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5356 - val_loss: 4.8457\n",
      "Epoch 12761/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5203 - val_loss: 5.0049\n",
      "Epoch 12762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6395 - val_loss: 5.1140\n",
      "Epoch 12763/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5899 - val_loss: 5.0522\n",
      "Epoch 12764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5719 - val_loss: 4.8087\n",
      "Epoch 12765/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5790 - val_loss: 4.8041\n",
      "Epoch 12766/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4617 - val_loss: 5.0679\n",
      "Epoch 12767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5540 - val_loss: 4.8577\n",
      "Epoch 12768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5229 - val_loss: 4.8932\n",
      "Epoch 12769/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.7586 - val_loss: 4.8144\n",
      "Epoch 12770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6185 - val_loss: 4.9117\n",
      "Epoch 12771/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5018 - val_loss: 4.8457\n",
      "Epoch 12772/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6229 - val_loss: 4.8495\n",
      "Epoch 12773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5613 - val_loss: 4.8113\n",
      "Epoch 12774/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5824 - val_loss: 4.9188\n",
      "Epoch 12775/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6284 - val_loss: 4.8682\n",
      "Epoch 12776/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6451 - val_loss: 4.8545\n",
      "Epoch 12777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6301 - val_loss: 4.8251\n",
      "Epoch 12778/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4899 - val_loss: 4.8123\n",
      "Epoch 12779/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5459 - val_loss: 4.8937\n",
      "Epoch 12780/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4903 - val_loss: 4.8158\n",
      "Epoch 12781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5729 - val_loss: 4.8209\n",
      "Epoch 12782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5996 - val_loss: 4.9864\n",
      "Epoch 12783/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6567 - val_loss: 4.9426\n",
      "Epoch 12784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5449 - val_loss: 4.8099\n",
      "Epoch 12785/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6664 - val_loss: 4.8126\n",
      "Epoch 12786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5556 - val_loss: 4.8134\n",
      "Epoch 12787/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.543 - 0s 46us/step - loss: 4.4971 - val_loss: 5.0233\n",
      "Epoch 12788/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5734 - val_loss: 4.8214\n",
      "Epoch 12789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5211 - val_loss: 4.8590\n",
      "Epoch 12790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6333 - val_loss: 4.8225\n",
      "Epoch 12791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5383 - val_loss: 5.0715\n",
      "Epoch 12792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7192 - val_loss: 4.8992\n",
      "Epoch 12793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5212 - val_loss: 4.8628\n",
      "Epoch 12794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6022 - val_loss: 5.3457\n",
      "Epoch 12795/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7489 - val_loss: 4.9671\n",
      "Epoch 12796/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6576 - val_loss: 4.8225\n",
      "Epoch 12797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5555 - val_loss: 5.0297\n",
      "Epoch 12798/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.6667 - val_loss: 4.9164\n",
      "Epoch 12799/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5158 - val_loss: 4.8404\n",
      "Epoch 12800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5870 - val_loss: 5.0312\n",
      "Epoch 12801/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5271 - val_loss: 4.8143\n",
      "Epoch 12802/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6395 - val_loss: 5.0440\n",
      "Epoch 12803/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6606 - val_loss: 4.8317\n",
      "Epoch 12804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5718 - val_loss: 4.8697\n",
      "Epoch 12805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6145 - val_loss: 4.9138\n",
      "Epoch 12806/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5273 - val_loss: 4.8007\n",
      "Epoch 12807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9909 - val_loss: 4.7875\n",
      "Epoch 12808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8580 - val_loss: 4.8175\n",
      "Epoch 12809/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5259 - val_loss: 5.3783\n",
      "Epoch 12810/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9337 - val_loss: 4.8803\n",
      "Epoch 12811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6570 - val_loss: 4.8881\n",
      "Epoch 12812/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5539 - val_loss: 5.0665\n",
      "Epoch 12813/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6704 - val_loss: 5.1197\n",
      "Epoch 12814/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5204 - val_loss: 5.0077\n",
      "Epoch 12815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7186 - val_loss: 4.8193\n",
      "Epoch 12816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6332 - val_loss: 4.8008\n",
      "Epoch 12817/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5786 - val_loss: 4.8219\n",
      "Epoch 12818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5030 - val_loss: 4.8733\n",
      "Epoch 12819/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5028 - val_loss: 4.8314\n",
      "Epoch 12820/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5403 - val_loss: 4.8370\n",
      "Epoch 12821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5509 - val_loss: 4.8779\n",
      "Epoch 12822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5310 - val_loss: 4.8188\n",
      "Epoch 12823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6260 - val_loss: 4.8698\n",
      "Epoch 12824/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.2014 - val_loss: 5.0576\n",
      "Epoch 12825/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5870 - val_loss: 4.8099\n",
      "Epoch 12826/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5400 - val_loss: 4.9525\n",
      "Epoch 12827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6010 - val_loss: 4.8614\n",
      "Epoch 12828/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5529 - val_loss: 4.9657\n",
      "Epoch 12829/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6626 - val_loss: 5.1603\n",
      "Epoch 12830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6235 - val_loss: 5.2854\n",
      "Epoch 12831/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6904 - val_loss: 5.2032\n",
      "Epoch 12832/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4946 - val_loss: 4.8405\n",
      "Epoch 12833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5899 - val_loss: 4.8317\n",
      "Epoch 12834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4802 - val_loss: 4.8643\n",
      "Epoch 12835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5424 - val_loss: 4.8018\n",
      "Epoch 12836/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5685 - val_loss: 4.8888\n",
      "Epoch 12837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6291 - val_loss: 4.8838\n",
      "Epoch 12838/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8246 - val_loss: 4.8805\n",
      "Epoch 12839/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7929 - val_loss: 5.0414\n",
      "Epoch 12840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6145 - val_loss: 5.2578\n",
      "Epoch 12841/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5088 - val_loss: 4.9325\n",
      "Epoch 12842/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7194 - val_loss: 4.9244\n",
      "Epoch 12843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8488 - val_loss: 5.0991\n",
      "Epoch 12844/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6626 - val_loss: 4.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7192 - val_loss: 5.4157\n",
      "Epoch 12846/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.5753 - val_loss: 4.7953\n",
      "Epoch 12847/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.4866 - val_loss: 4.8105\n",
      "Epoch 12848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5637 - val_loss: 4.8373\n",
      "Epoch 12849/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5196 - val_loss: 4.8426\n",
      "Epoch 12850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5821 - val_loss: 4.8204\n",
      "Epoch 12851/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5286 - val_loss: 4.8347\n",
      "Epoch 12852/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5444 - val_loss: 5.1901\n",
      "Epoch 12853/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5578 - val_loss: 5.1257\n",
      "Epoch 12854/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6257 - val_loss: 5.4663\n",
      "Epoch 12855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6956 - val_loss: 4.7977\n",
      "Epoch 12856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5206 - val_loss: 4.9880\n",
      "Epoch 12857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6606 - val_loss: 4.8051\n",
      "Epoch 12858/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5657 - val_loss: 4.8128\n",
      "Epoch 12859/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6206 - val_loss: 4.9861\n",
      "Epoch 12860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7460 - val_loss: 4.7838\n",
      "Epoch 12861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5898 - val_loss: 4.8140\n",
      "Epoch 12862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5395 - val_loss: 4.8152\n",
      "Epoch 12863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5390 - val_loss: 4.8462\n",
      "Epoch 12864/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7159 - val_loss: 4.8331\n",
      "Epoch 12865/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.698 - 0s 46us/step - loss: 4.5327 - val_loss: 4.9496\n",
      "Epoch 12866/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6352 - val_loss: 4.8685\n",
      "Epoch 12867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6576 - val_loss: 4.8090\n",
      "Epoch 12868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6423 - val_loss: 4.8288\n",
      "Epoch 12869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5361 - val_loss: 5.1473\n",
      "Epoch 12870/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9535 - val_loss: 5.1651\n",
      "Epoch 12871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5311 - val_loss: 4.9277\n",
      "Epoch 12872/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5671 - val_loss: 4.9468\n",
      "Epoch 12873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5507 - val_loss: 4.8130\n",
      "Epoch 12874/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5609 - val_loss: 5.2012\n",
      "Epoch 12875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6862 - val_loss: 5.0103\n",
      "Epoch 12876/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6630 - val_loss: 5.2677\n",
      "Epoch 12877/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7190 - val_loss: 4.9378\n",
      "Epoch 12878/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5369 - val_loss: 4.8162\n",
      "Epoch 12879/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6303 - val_loss: 4.8076\n",
      "Epoch 12880/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6011 - val_loss: 4.8398\n",
      "Epoch 12881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5262 - val_loss: 4.8113\n",
      "Epoch 12882/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4974 - val_loss: 4.8255\n",
      "Epoch 12883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5181 - val_loss: 4.8313\n",
      "Epoch 12884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5139 - val_loss: 4.8240\n",
      "Epoch 12885/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5003 - val_loss: 4.9072\n",
      "Epoch 12886/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4963 - val_loss: 4.8112\n",
      "Epoch 12887/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5064 - val_loss: 4.8617\n",
      "Epoch 12888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5560 - val_loss: 4.8253\n",
      "Epoch 12889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5039 - val_loss: 4.8267\n",
      "Epoch 12890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5902 - val_loss: 4.8351\n",
      "Epoch 12891/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6025 - val_loss: 4.8174\n",
      "Epoch 12892/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5742 - val_loss: 4.9687\n",
      "Epoch 12893/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5474 - val_loss: 4.8524\n",
      "Epoch 12894/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6641 - val_loss: 4.7880\n",
      "Epoch 12895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5650 - val_loss: 4.7843\n",
      "Epoch 12896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4828 - val_loss: 4.8298\n",
      "Epoch 12897/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5253 - val_loss: 4.8334\n",
      "Epoch 12898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5381 - val_loss: 4.9030\n",
      "Epoch 12899/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5741 - val_loss: 4.8252\n",
      "Epoch 12900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5439 - val_loss: 4.8639\n",
      "Epoch 12901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7082 - val_loss: 5.0984\n",
      "Epoch 12902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5191 - val_loss: 5.0554\n",
      "Epoch 12903/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5590 - val_loss: 4.9946\n",
      "Epoch 12904/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5004 - val_loss: 4.8068\n",
      "Epoch 12905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5511 - val_loss: 4.8230\n",
      "Epoch 12906/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5867 - val_loss: 4.8252\n",
      "Epoch 12907/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6060 - val_loss: 4.7894\n",
      "Epoch 12908/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6611 - val_loss: 5.0126\n",
      "Epoch 12909/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6097 - val_loss: 4.7900\n",
      "Epoch 12910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5574 - val_loss: 4.7966\n",
      "Epoch 12911/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5789 - val_loss: 5.3999\n",
      "Epoch 12912/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6254 - val_loss: 4.8380\n",
      "Epoch 12913/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7264 - val_loss: 4.8719\n",
      "Epoch 12914/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5618 - val_loss: 4.9429\n",
      "Epoch 12915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6012 - val_loss: 4.8227\n",
      "Epoch 12916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4893 - val_loss: 5.2117\n",
      "Epoch 12917/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7017 - val_loss: 5.2978\n",
      "Epoch 12918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7805 - val_loss: 5.0484\n",
      "Epoch 12919/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6012 - val_loss: 5.2784\n",
      "Epoch 12920/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.8572 - val_loss: 5.1432\n",
      "Epoch 12921/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8530 - val_loss: 5.9352\n",
      "Epoch 12922/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8983 - val_loss: 4.7893\n",
      "Epoch 12923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5720 - val_loss: 4.8080\n",
      "Epoch 12924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4970 - val_loss: 4.8377\n",
      "Epoch 12925/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5448 - val_loss: 4.8200\n",
      "Epoch 12926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5464 - val_loss: 4.7989\n",
      "Epoch 12927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4851 - val_loss: 4.8224\n",
      "Epoch 12928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6223 - val_loss: 4.7869\n",
      "Epoch 12929/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5500 - val_loss: 4.8252\n",
      "Epoch 12930/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5412 - val_loss: 5.0300\n",
      "Epoch 12931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4831 - val_loss: 4.9158\n",
      "Epoch 12932/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6132 - val_loss: 5.0981\n",
      "Epoch 12933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6657 - val_loss: 5.0556\n",
      "Epoch 12934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6039 - val_loss: 4.9467\n",
      "Epoch 12935/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4949 - val_loss: 4.8032\n",
      "Epoch 12936/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5883 - val_loss: 4.9219\n",
      "Epoch 12937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6572 - val_loss: 4.8241\n",
      "Epoch 12938/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4934 - val_loss: 4.8332\n",
      "Epoch 12939/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5450 - val_loss: 4.8924\n",
      "Epoch 12940/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5105 - val_loss: 4.8109\n",
      "Epoch 12941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4989 - val_loss: 4.8186\n",
      "Epoch 12942/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5501 - val_loss: 4.8071\n",
      "Epoch 12943/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5034 - val_loss: 4.8542\n",
      "Epoch 12944/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5691 - val_loss: 4.8267\n",
      "Epoch 12945/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6423 - val_loss: 4.8687\n",
      "Epoch 12946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5425 - val_loss: 4.7977\n",
      "Epoch 12947/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5912 - val_loss: 4.8567\n",
      "Epoch 12948/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5529 - val_loss: 4.8166\n",
      "Epoch 12949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4784 - val_loss: 4.9883\n",
      "Epoch 12950/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6546 - val_loss: 4.8274\n",
      "Epoch 12951/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5504 - val_loss: 4.8002\n",
      "Epoch 12952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4996 - val_loss: 4.8061\n",
      "Epoch 12953/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5007 - val_loss: 4.8664\n",
      "Epoch 12954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5754 - val_loss: 4.8029\n",
      "Epoch 12955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7907 - val_loss: 4.8829\n",
      "Epoch 12956/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5540 - val_loss: 5.0747\n",
      "Epoch 12957/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5501 - val_loss: 4.8981\n",
      "Epoch 12958/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4846 - val_loss: 4.9237\n",
      "Epoch 12959/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6577 - val_loss: 4.8091\n",
      "Epoch 12960/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6545 - val_loss: 5.0272\n",
      "Epoch 12961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6169 - val_loss: 4.9575\n",
      "Epoch 12962/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6477 - val_loss: 4.9227\n",
      "Epoch 12963/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7200 - val_loss: 4.8601\n",
      "Epoch 12964/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5385 - val_loss: 4.8919\n",
      "Epoch 12965/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5410 - val_loss: 5.0543\n",
      "Epoch 12966/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6344 - val_loss: 4.8876\n",
      "Epoch 12967/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6759 - val_loss: 4.7737\n",
      "Epoch 12968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5147 - val_loss: 4.9045\n",
      "Epoch 12969/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5362 - val_loss: 4.8092\n",
      "Epoch 12970/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5359 - val_loss: 4.8696\n",
      "Epoch 12971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6962 - val_loss: 4.8513\n",
      "Epoch 12972/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6536 - val_loss: 5.4450\n",
      "Epoch 12973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6683 - val_loss: 4.9484\n",
      "Epoch 12974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5609 - val_loss: 4.8863\n",
      "Epoch 12975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4855 - val_loss: 4.8169\n",
      "Epoch 12976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5542 - val_loss: 4.9149\n",
      "Epoch 12977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5150 - val_loss: 4.7877\n",
      "Epoch 12978/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4891 - val_loss: 4.8229\n",
      "Epoch 12979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5640 - val_loss: 5.1874\n",
      "Epoch 12980/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7742 - val_loss: 5.1272\n",
      "Epoch 12981/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5070 - val_loss: 4.9069\n",
      "Epoch 12982/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6978 - val_loss: 4.8658\n",
      "Epoch 12983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5082 - val_loss: 4.7976\n",
      "Epoch 12984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4857 - val_loss: 4.8057\n",
      "Epoch 12985/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5296 - val_loss: 4.8042\n",
      "Epoch 12986/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5000 - val_loss: 4.7901\n",
      "Epoch 12987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6940 - val_loss: 4.9049\n",
      "Epoch 12988/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6021 - val_loss: 4.8463\n",
      "Epoch 12989/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6283 - val_loss: 5.2103\n",
      "Epoch 12990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5452 - val_loss: 4.9177\n",
      "Epoch 12991/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7391 - val_loss: 4.8280\n",
      "Epoch 12992/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5878 - val_loss: 5.1608\n",
      "Epoch 12993/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7048 - val_loss: 4.9333\n",
      "Epoch 12994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5429 - val_loss: 5.6892\n",
      "Epoch 12995/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8034 - val_loss: 5.7215\n",
      "Epoch 12996/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6753 - val_loss: 4.8939\n",
      "Epoch 12997/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6250 - val_loss: 4.8265\n",
      "Epoch 12998/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6778 - val_loss: 4.8925\n",
      "Epoch 12999/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5438 - val_loss: 5.0875\n",
      "Epoch 13000/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6939 - val_loss: 4.8597\n",
      "Epoch 13001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5611 - val_loss: 4.9712\n",
      "Epoch 13002/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.293 - 0s 46us/step - loss: 4.5141 - val_loss: 4.8181\n",
      "Epoch 13003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5882 - val_loss: 4.8972\n",
      "Epoch 13004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5514 - val_loss: 4.8429\n",
      "Epoch 13005/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5473 - val_loss: 4.8241\n",
      "Epoch 13006/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5371 - val_loss: 4.8267\n",
      "Epoch 13007/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8282 - val_loss: 4.9231\n",
      "Epoch 13008/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6498 - val_loss: 4.8146\n",
      "Epoch 13009/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5305 - val_loss: 4.8399\n",
      "Epoch 13010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5816 - val_loss: 5.0077\n",
      "Epoch 13011/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6294 - val_loss: 4.8702\n",
      "Epoch 13012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6516 - val_loss: 5.1076\n",
      "Epoch 13013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5608 - val_loss: 4.7890\n",
      "Epoch 13014/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6160 - val_loss: 4.8729\n",
      "Epoch 13015/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5138 - val_loss: 4.8124\n",
      "Epoch 13016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5751 - val_loss: 4.9381\n",
      "Epoch 13017/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5889 - val_loss: 4.8155\n",
      "Epoch 13018/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5705 - val_loss: 4.8733\n",
      "Epoch 13019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5768 - val_loss: 4.8192\n",
      "Epoch 13020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6115 - val_loss: 4.7822\n",
      "Epoch 13021/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5838 - val_loss: 4.7964\n",
      "Epoch 13022/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6046 - val_loss: 4.8651\n",
      "Epoch 13023/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5340 - val_loss: 4.8217\n",
      "Epoch 13024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6484 - val_loss: 4.7906\n",
      "Epoch 13025/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5115 - val_loss: 4.9570\n",
      "Epoch 13026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5482 - val_loss: 4.8189\n",
      "Epoch 13027/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6478 - val_loss: 4.8956\n",
      "Epoch 13028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5240 - val_loss: 4.8176\n",
      "Epoch 13029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5639 - val_loss: 4.8615\n",
      "Epoch 13030/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5147 - val_loss: 4.8119\n",
      "Epoch 13031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5494 - val_loss: 4.8368\n",
      "Epoch 13032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5372 - val_loss: 4.7931\n",
      "Epoch 13033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4761 - val_loss: 4.9187\n",
      "Epoch 13034/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5025 - val_loss: 4.8226\n",
      "Epoch 13035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5101 - val_loss: 4.8874\n",
      "Epoch 13036/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5385 - val_loss: 4.8916\n",
      "Epoch 13037/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6351 - val_loss: 4.8535\n",
      "Epoch 13038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5158 - val_loss: 4.8142\n",
      "Epoch 13039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6032 - val_loss: 4.8238\n",
      "Epoch 13040/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5349 - val_loss: 4.8243\n",
      "Epoch 13041/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5784 - val_loss: 4.8455\n",
      "Epoch 13042/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7063 - val_loss: 4.8727\n",
      "Epoch 13043/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5628 - val_loss: 4.9186\n",
      "Epoch 13044/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5359 - val_loss: 5.1014\n",
      "Epoch 13045/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4721 - val_loss: 4.8733\n",
      "Epoch 13046/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5602 - val_loss: 4.8205\n",
      "Epoch 13047/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5105 - val_loss: 4.8219\n",
      "Epoch 13048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5014 - val_loss: 4.8903\n",
      "Epoch 13049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5158 - val_loss: 5.2970\n",
      "Epoch 13050/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5321 - val_loss: 4.8213\n",
      "Epoch 13051/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5990 - val_loss: 5.1041\n",
      "Epoch 13052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5726 - val_loss: 4.8076\n",
      "Epoch 13053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5990 - val_loss: 4.9891\n",
      "Epoch 13054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8238 - val_loss: 4.8697\n",
      "Epoch 13055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6701 - val_loss: 4.8458\n",
      "Epoch 13056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6524 - val_loss: 4.8232\n",
      "Epoch 13057/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4749 - val_loss: 4.7916\n",
      "Epoch 13058/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4676 - val_loss: 4.8555\n",
      "Epoch 13059/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5067 - val_loss: 4.8322\n",
      "Epoch 13060/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6083 - val_loss: 4.8298\n",
      "Epoch 13061/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5169 - val_loss: 4.8141\n",
      "Epoch 13062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5208 - val_loss: 5.0245\n",
      "Epoch 13063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6357 - val_loss: 4.7934\n",
      "Epoch 13064/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6020 - val_loss: 4.8098\n",
      "Epoch 13065/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6548 - val_loss: 4.8687\n",
      "Epoch 13066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7190 - val_loss: 4.8132\n",
      "Epoch 13067/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5246 - val_loss: 4.8817\n",
      "Epoch 13068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5268 - val_loss: 4.8159\n",
      "Epoch 13069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4770 - val_loss: 4.8398\n",
      "Epoch 13070/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6461 - val_loss: 5.2323\n",
      "Epoch 13071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6312 - val_loss: 4.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13072/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6487 - val_loss: 4.9016\n",
      "Epoch 13073/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7085 - val_loss: 4.8068\n",
      "Epoch 13074/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6240 - val_loss: 4.8255\n",
      "Epoch 13075/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5258 - val_loss: 4.8330\n",
      "Epoch 13076/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6344 - val_loss: 4.7968\n",
      "Epoch 13077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6359 - val_loss: 4.8446\n",
      "Epoch 13078/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5797 - val_loss: 4.8335\n",
      "Epoch 13079/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5000 - val_loss: 4.8498\n",
      "Epoch 13080/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5120 - val_loss: 5.5055\n",
      "Epoch 13081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6656 - val_loss: 4.9137\n",
      "Epoch 13082/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4902 - val_loss: 4.7980\n",
      "Epoch 13083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6674 - val_loss: 4.8006\n",
      "Epoch 13084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9773 - val_loss: 4.8372\n",
      "Epoch 13085/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5950 - val_loss: 4.8890\n",
      "Epoch 13086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5102 - val_loss: 4.9161\n",
      "Epoch 13087/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5494 - val_loss: 4.8213\n",
      "Epoch 13088/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5444 - val_loss: 4.8705\n",
      "Epoch 13089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5390 - val_loss: 4.9644\n",
      "Epoch 13090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5162 - val_loss: 4.8450\n",
      "Epoch 13091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6062 - val_loss: 4.8227\n",
      "Epoch 13092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5041 - val_loss: 4.8216\n",
      "Epoch 13093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5927 - val_loss: 4.8288\n",
      "Epoch 13094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5449 - val_loss: 4.8825\n",
      "Epoch 13095/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5899 - val_loss: 4.9988\n",
      "Epoch 13096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5186 - val_loss: 4.8345\n",
      "Epoch 13097/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5303 - val_loss: 5.0842\n",
      "Epoch 13098/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6396 - val_loss: 5.2401\n",
      "Epoch 13099/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6215 - val_loss: 4.8977\n",
      "Epoch 13100/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7086 - val_loss: 4.8095\n",
      "Epoch 13101/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5919 - val_loss: 4.8920\n",
      "Epoch 13102/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0147 - val_loss: 4.9735\n",
      "Epoch 13103/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8400 - val_loss: 4.9028\n",
      "Epoch 13104/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5570 - val_loss: 5.0565\n",
      "Epoch 13105/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9036 - val_loss: 5.3574\n",
      "Epoch 13106/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.1173 - val_loss: 4.8031\n",
      "Epoch 13107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6323 - val_loss: 4.8562\n",
      "Epoch 13108/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5723 - val_loss: 4.8142\n",
      "Epoch 13109/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6214 - val_loss: 4.8060\n",
      "Epoch 13110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4816 - val_loss: 5.0076\n",
      "Epoch 13111/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7432 - val_loss: 5.1032\n",
      "Epoch 13112/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9283 - val_loss: 5.1636\n",
      "Epoch 13113/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7943 - val_loss: 4.8280\n",
      "Epoch 13114/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5884 - val_loss: 4.9255\n",
      "Epoch 13115/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6712 - val_loss: 4.8425\n",
      "Epoch 13116/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7979 - val_loss: 5.0898\n",
      "Epoch 13117/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5487 - val_loss: 5.0029\n",
      "Epoch 13118/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6838 - val_loss: 5.3927\n",
      "Epoch 13119/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5136 - val_loss: 4.8114\n",
      "Epoch 13120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4986 - val_loss: 4.8061\n",
      "Epoch 13121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5327 - val_loss: 4.8268\n",
      "Epoch 13122/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5101 - val_loss: 4.8483\n",
      "Epoch 13123/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5104 - val_loss: 4.8269\n",
      "Epoch 13124/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4986 - val_loss: 4.8037\n",
      "Epoch 13125/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4969 - val_loss: 4.9221\n",
      "Epoch 13126/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4931 - val_loss: 4.9696\n",
      "Epoch 13127/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4723 - val_loss: 4.8019\n",
      "Epoch 13128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7138 - val_loss: 4.8205\n",
      "Epoch 13129/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7101 - val_loss: 4.8814\n",
      "Epoch 13130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5986 - val_loss: 5.0322\n",
      "Epoch 13131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6846 - val_loss: 4.8415\n",
      "Epoch 13132/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4804 - val_loss: 4.8067\n",
      "Epoch 13133/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5841 - val_loss: 4.7994\n",
      "Epoch 13134/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5583 - val_loss: 5.0012\n",
      "Epoch 13135/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8435 - val_loss: 4.8659\n",
      "Epoch 13136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5669 - val_loss: 4.8142\n",
      "Epoch 13137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4878 - val_loss: 4.9140\n",
      "Epoch 13138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5021 - val_loss: 4.8415\n",
      "Epoch 13139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4974 - val_loss: 5.0590\n",
      "Epoch 13140/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4712 - val_loss: 4.9032\n",
      "Epoch 13141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5544 - val_loss: 4.8217\n",
      "Epoch 13142/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7033 - val_loss: 5.0101\n",
      "Epoch 13143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0077 - val_loss: 5.0264\n",
      "Epoch 13144/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6501 - val_loss: 5.1031\n",
      "Epoch 13145/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5140 - val_loss: 4.8359\n",
      "Epoch 13146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6174 - val_loss: 4.8960\n",
      "Epoch 13147/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7108 - val_loss: 4.8425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13148/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5554 - val_loss: 4.8691\n",
      "Epoch 13149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6198 - val_loss: 4.8433\n",
      "Epoch 13150/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6807 - val_loss: 4.8466\n",
      "Epoch 13151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5455 - val_loss: 4.8652\n",
      "Epoch 13152/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5569 - val_loss: 5.1578\n",
      "Epoch 13153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5981 - val_loss: 4.8402\n",
      "Epoch 13154/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5093 - val_loss: 4.8301\n",
      "Epoch 13155/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6344 - val_loss: 4.8704\n",
      "Epoch 13156/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5075 - val_loss: 4.8682\n",
      "Epoch 13157/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5553 - val_loss: 5.0044\n",
      "Epoch 13158/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9953 - val_loss: 5.1886\n",
      "Epoch 13159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9040 - val_loss: 4.9587\n",
      "Epoch 13160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6367 - val_loss: 4.9733\n",
      "Epoch 13161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8543 - val_loss: 4.8604\n",
      "Epoch 13162/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6268 - val_loss: 4.9610\n",
      "Epoch 13163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7062 - val_loss: 4.8319\n",
      "Epoch 13164/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5090 - val_loss: 5.0339\n",
      "Epoch 13165/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5558 - val_loss: 4.8113\n",
      "Epoch 13166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4999 - val_loss: 4.8921\n",
      "Epoch 13167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5318 - val_loss: 4.8777\n",
      "Epoch 13168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4922 - val_loss: 4.9440\n",
      "Epoch 13169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5247 - val_loss: 5.0006\n",
      "Epoch 13170/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4818 - val_loss: 4.8539\n",
      "Epoch 13171/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4682 - val_loss: 4.8401\n",
      "Epoch 13172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5197 - val_loss: 5.2298\n",
      "Epoch 13173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6721 - val_loss: 4.8551\n",
      "Epoch 13174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5045 - val_loss: 5.1298\n",
      "Epoch 13175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8174 - val_loss: 5.4401\n",
      "Epoch 13176/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8676 - val_loss: 4.8262\n",
      "Epoch 13177/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5462 - val_loss: 4.8235\n",
      "Epoch 13178/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6403 - val_loss: 5.2316\n",
      "Epoch 13179/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9525 - val_loss: 4.8630\n",
      "Epoch 13180/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6605 - val_loss: 4.9287\n",
      "Epoch 13181/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4999 - val_loss: 5.4081\n",
      "Epoch 13182/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5809 - val_loss: 4.8576\n",
      "Epoch 13183/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6959 - val_loss: 4.8654\n",
      "Epoch 13184/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5987 - val_loss: 4.8199\n",
      "Epoch 13185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5735 - val_loss: 4.8869\n",
      "Epoch 13186/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5400 - val_loss: 4.7913\n",
      "Epoch 13187/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5087 - val_loss: 4.7930\n",
      "Epoch 13188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6243 - val_loss: 4.7923\n",
      "Epoch 13189/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5659 - val_loss: 4.8401\n",
      "Epoch 13190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6279 - val_loss: 4.8032\n",
      "Epoch 13191/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5201 - val_loss: 4.9785\n",
      "Epoch 13192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1056 - val_loss: 6.8346\n",
      "Epoch 13193/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0573 - val_loss: 4.9511\n",
      "Epoch 13194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5286 - val_loss: 5.0288\n",
      "Epoch 13195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5544 - val_loss: 5.3737\n",
      "Epoch 13196/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7606 - val_loss: 4.7843\n",
      "Epoch 13197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6382 - val_loss: 4.8246\n",
      "Epoch 13198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5639 - val_loss: 4.8509\n",
      "Epoch 13199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6025 - val_loss: 5.0653\n",
      "Epoch 13200/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6606 - val_loss: 4.8029\n",
      "Epoch 13201/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6491 - val_loss: 4.8108\n",
      "Epoch 13202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4705 - val_loss: 4.8417\n",
      "Epoch 13203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5194 - val_loss: 4.8083\n",
      "Epoch 13204/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4920 - val_loss: 4.8144\n",
      "Epoch 13205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5015 - val_loss: 4.8815\n",
      "Epoch 13206/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4758 - val_loss: 4.9788\n",
      "Epoch 13207/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5274 - val_loss: 4.8121\n",
      "Epoch 13208/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5403 - val_loss: 4.7925\n",
      "Epoch 13209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5349 - val_loss: 4.9088\n",
      "Epoch 13210/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5860 - val_loss: 4.7975\n",
      "Epoch 13211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6786 - val_loss: 4.8263\n",
      "Epoch 13212/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4786 - val_loss: 4.9097\n",
      "Epoch 13213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4643 - val_loss: 4.7979\n",
      "Epoch 13214/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5318 - val_loss: 5.0280\n",
      "Epoch 13215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5905 - val_loss: 5.0948\n",
      "Epoch 13216/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6254 - val_loss: 5.0775\n",
      "Epoch 13217/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6568 - val_loss: 4.8639\n",
      "Epoch 13218/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8705 - val_loss: 5.2156\n",
      "Epoch 13219/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6854 - val_loss: 4.8280\n",
      "Epoch 13220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4808 - val_loss: 4.8256\n",
      "Epoch 13221/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4969 - val_loss: 5.1713\n",
      "Epoch 13222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7345 - val_loss: 5.3108\n",
      "Epoch 13223/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7145 - val_loss: 4.8113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13224/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7988 - val_loss: 4.8107\n",
      "Epoch 13225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4930 - val_loss: 4.8763\n",
      "Epoch 13226/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6346 - val_loss: 4.8019\n",
      "Epoch 13227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5782 - val_loss: 4.9543\n",
      "Epoch 13228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4826 - val_loss: 5.0375\n",
      "Epoch 13229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5638 - val_loss: 4.9714\n",
      "Epoch 13230/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5281 - val_loss: 4.9531\n",
      "Epoch 13231/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5551 - val_loss: 4.8051\n",
      "Epoch 13232/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5466 - val_loss: 4.7995\n",
      "Epoch 13233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5081 - val_loss: 4.8303\n",
      "Epoch 13234/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5314 - val_loss: 4.8107\n",
      "Epoch 13235/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5734 - val_loss: 4.8775\n",
      "Epoch 13236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5510 - val_loss: 4.8295\n",
      "Epoch 13237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6192 - val_loss: 4.9311\n",
      "Epoch 13238/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6225 - val_loss: 4.9182\n",
      "Epoch 13239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5104 - val_loss: 4.7882\n",
      "Epoch 13240/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4766 - val_loss: 4.8966\n",
      "Epoch 13241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4821 - val_loss: 5.0630\n",
      "Epoch 13242/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7902 - val_loss: 5.2063\n",
      "Epoch 13243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7287 - val_loss: 4.9067\n",
      "Epoch 13244/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5041 - val_loss: 4.8547\n",
      "Epoch 13245/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5262 - val_loss: 4.8241\n",
      "Epoch 13246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5212 - val_loss: 4.8046\n",
      "Epoch 13247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4779 - val_loss: 4.8097\n",
      "Epoch 13248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5034 - val_loss: 4.7965\n",
      "Epoch 13249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5436 - val_loss: 5.1171\n",
      "Epoch 13250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7025 - val_loss: 4.8377\n",
      "Epoch 13251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6437 - val_loss: 4.8279\n",
      "Epoch 13252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5167 - val_loss: 4.8135\n",
      "Epoch 13253/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5652 - val_loss: 4.8299\n",
      "Epoch 13254/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6813 - val_loss: 4.8114\n",
      "Epoch 13255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5131 - val_loss: 4.8779\n",
      "Epoch 13256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5331 - val_loss: 4.7940\n",
      "Epoch 13257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5194 - val_loss: 4.8129\n",
      "Epoch 13258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4884 - val_loss: 4.8350\n",
      "Epoch 13259/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5408 - val_loss: 4.7990\n",
      "Epoch 13260/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5223 - val_loss: 4.8119\n",
      "Epoch 13261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6621 - val_loss: 4.8765\n",
      "Epoch 13262/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5800 - val_loss: 4.9113\n",
      "Epoch 13263/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5966 - val_loss: 4.9360\n",
      "Epoch 13264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4924 - val_loss: 4.8308\n",
      "Epoch 13265/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5589 - val_loss: 5.2110\n",
      "Epoch 13266/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5052 - val_loss: 4.9705\n",
      "Epoch 13267/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5063 - val_loss: 5.2587\n",
      "Epoch 13268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5950 - val_loss: 5.0103\n",
      "Epoch 13269/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5498 - val_loss: 5.2861\n",
      "Epoch 13270/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7591 - val_loss: 5.1495\n",
      "Epoch 13271/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7622 - val_loss: 5.0156\n",
      "Epoch 13272/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5902 - val_loss: 4.8268\n",
      "Epoch 13273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7000 - val_loss: 5.1966\n",
      "Epoch 13274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7826 - val_loss: 4.9641\n",
      "Epoch 13275/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8641 - val_loss: 4.8022\n",
      "Epoch 13276/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5601 - val_loss: 5.0589\n",
      "Epoch 13277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7868 - val_loss: 5.0339\n",
      "Epoch 13278/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.996 - 0s 46us/step - loss: 4.6376 - val_loss: 4.9837\n",
      "Epoch 13279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5785 - val_loss: 5.2618\n",
      "Epoch 13280/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6931 - val_loss: 5.0124\n",
      "Epoch 13281/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5489 - val_loss: 4.7955\n",
      "Epoch 13282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5442 - val_loss: 4.8632\n",
      "Epoch 13283/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5019 - val_loss: 4.9002\n",
      "Epoch 13284/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6937 - val_loss: 4.7781\n",
      "Epoch 13285/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7198 - val_loss: 5.3301\n",
      "Epoch 13286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5830 - val_loss: 4.8034\n",
      "Epoch 13287/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5752 - val_loss: 4.7973\n",
      "Epoch 13288/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5478 - val_loss: 4.8366\n",
      "Epoch 13289/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9294 - val_loss: 4.8039\n",
      "Epoch 13290/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5531 - val_loss: 4.9757\n",
      "Epoch 13291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6753 - val_loss: 4.8095\n",
      "Epoch 13292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6231 - val_loss: 4.8853\n",
      "Epoch 13293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6354 - val_loss: 5.0753\n",
      "Epoch 13294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5813 - val_loss: 4.8820\n",
      "Epoch 13295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6887 - val_loss: 5.1870\n",
      "Epoch 13296/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6566 - val_loss: 5.0740\n",
      "Epoch 13297/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5577 - val_loss: 5.0819\n",
      "Epoch 13298/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5747 - val_loss: 4.9868\n",
      "Epoch 13299/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5323 - val_loss: 4.8100\n",
      "Epoch 13300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5384 - val_loss: 4.8303\n",
      "Epoch 13301/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6159 - val_loss: 4.8130\n",
      "Epoch 13302/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5439 - val_loss: 4.8130\n",
      "Epoch 13303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6119 - val_loss: 4.7768\n",
      "Epoch 13304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5828 - val_loss: 4.8291\n",
      "Epoch 13305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4828 - val_loss: 4.8704\n",
      "Epoch 13306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5340 - val_loss: 4.8432\n",
      "Epoch 13307/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5580 - val_loss: 4.8521\n",
      "Epoch 13308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5456 - val_loss: 5.0069\n",
      "Epoch 13309/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5169 - val_loss: 5.0899\n",
      "Epoch 13310/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4780 - val_loss: 4.8210\n",
      "Epoch 13311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4770 - val_loss: 4.8429\n",
      "Epoch 13312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4924 - val_loss: 4.8147\n",
      "Epoch 13313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5092 - val_loss: 4.8183\n",
      "Epoch 13314/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5183 - val_loss: 4.9233\n",
      "Epoch 13315/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5737 - val_loss: 4.8507\n",
      "Epoch 13316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5376 - val_loss: 4.8360\n",
      "Epoch 13317/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5352 - val_loss: 4.8341\n",
      "Epoch 13318/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6457 - val_loss: 4.8535\n",
      "Epoch 13319/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5191 - val_loss: 4.8738\n",
      "Epoch 13320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6164 - val_loss: 4.8249\n",
      "Epoch 13321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5278 - val_loss: 4.8413\n",
      "Epoch 13322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5394 - val_loss: 4.8534\n",
      "Epoch 13323/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4811 - val_loss: 4.8212\n",
      "Epoch 13324/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5353 - val_loss: 4.8838\n",
      "Epoch 13325/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5704 - val_loss: 4.9722\n",
      "Epoch 13326/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5385 - val_loss: 4.9138\n",
      "Epoch 13327/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4962 - val_loss: 4.8494\n",
      "Epoch 13328/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5543 - val_loss: 4.8006\n",
      "Epoch 13329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5961 - val_loss: 4.8500\n",
      "Epoch 13330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5451 - val_loss: 4.8609\n",
      "Epoch 13331/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5454 - val_loss: 4.8195\n",
      "Epoch 13332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4614 - val_loss: 4.8629\n",
      "Epoch 13333/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4954 - val_loss: 4.8336\n",
      "Epoch 13334/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7935 - val_loss: 5.1008\n",
      "Epoch 13335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6085 - val_loss: 4.8683\n",
      "Epoch 13336/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5257 - val_loss: 4.8517\n",
      "Epoch 13337/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4912 - val_loss: 4.8347\n",
      "Epoch 13338/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5449 - val_loss: 5.2663\n",
      "Epoch 13339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6050 - val_loss: 5.1872\n",
      "Epoch 13340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6036 - val_loss: 4.8757\n",
      "Epoch 13341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5896 - val_loss: 4.8435\n",
      "Epoch 13342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6391 - val_loss: 5.1628\n",
      "Epoch 13343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6662 - val_loss: 4.9958\n",
      "Epoch 13344/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6530 - val_loss: 4.9940\n",
      "Epoch 13345/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5293 - val_loss: 4.9174\n",
      "Epoch 13346/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5231 - val_loss: 4.8377\n",
      "Epoch 13347/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4948 - val_loss: 4.8303\n",
      "Epoch 13348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5102 - val_loss: 4.8244\n",
      "Epoch 13349/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6850 - val_loss: 4.8733\n",
      "Epoch 13350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5213 - val_loss: 4.8341\n",
      "Epoch 13351/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6300 - val_loss: 4.7868\n",
      "Epoch 13352/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5636 - val_loss: 4.8342\n",
      "Epoch 13353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8109 - val_loss: 4.8948\n",
      "Epoch 13354/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5591 - val_loss: 5.0460\n",
      "Epoch 13355/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5836 - val_loss: 4.9402\n",
      "Epoch 13356/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6651 - val_loss: 4.8365\n",
      "Epoch 13357/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5349 - val_loss: 4.8654\n",
      "Epoch 13358/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5304 - val_loss: 4.8406\n",
      "Epoch 13359/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5184 - val_loss: 4.9978\n",
      "Epoch 13360/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5708 - val_loss: 5.0148\n",
      "Epoch 13361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5659 - val_loss: 5.0449\n",
      "Epoch 13362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9851 - val_loss: 5.1937\n",
      "Epoch 13363/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6951 - val_loss: 4.9608\n",
      "Epoch 13364/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7544 - val_loss: 4.7889\n",
      "Epoch 13365/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6487 - val_loss: 4.8147\n",
      "Epoch 13366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5794 - val_loss: 5.2587\n",
      "Epoch 13367/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7770 - val_loss: 5.2714\n",
      "Epoch 13368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6104 - val_loss: 4.9604\n",
      "Epoch 13369/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4826 - val_loss: 4.8302\n",
      "Epoch 13370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5293 - val_loss: 4.8189\n",
      "Epoch 13371/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5760 - val_loss: 4.8793\n",
      "Epoch 13372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5489 - val_loss: 4.8463\n",
      "Epoch 13373/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5638 - val_loss: 5.1268\n",
      "Epoch 13374/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5642 - val_loss: 5.2197\n",
      "Epoch 13375/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5645 - val_loss: 5.0017\n",
      "Epoch 13376/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5528 - val_loss: 4.8345\n",
      "Epoch 13377/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5534 - val_loss: 4.8346\n",
      "Epoch 13378/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5004 - val_loss: 4.8325\n",
      "Epoch 13379/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5178 - val_loss: 4.8353\n",
      "Epoch 13380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6795 - val_loss: 4.8309\n",
      "Epoch 13381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6901 - val_loss: 4.8217\n",
      "Epoch 13382/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6065 - val_loss: 5.0053\n",
      "Epoch 13383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6973 - val_loss: 5.0507\n",
      "Epoch 13384/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5046 - val_loss: 5.1976\n",
      "Epoch 13385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5496 - val_loss: 4.8299\n",
      "Epoch 13386/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5209 - val_loss: 4.8129\n",
      "Epoch 13387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4673 - val_loss: 5.3150\n",
      "Epoch 13388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6644 - val_loss: 5.0292\n",
      "Epoch 13389/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5768 - val_loss: 4.9077\n",
      "Epoch 13390/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5080 - val_loss: 5.0383\n",
      "Epoch 13391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5308 - val_loss: 4.8209\n",
      "Epoch 13392/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6959 - val_loss: 5.1692\n",
      "Epoch 13393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6738 - val_loss: 4.8903\n",
      "Epoch 13394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5330 - val_loss: 5.0059\n",
      "Epoch 13395/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5227 - val_loss: 4.8148\n",
      "Epoch 13396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5882 - val_loss: 4.7964\n",
      "Epoch 13397/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6062 - val_loss: 5.0715\n",
      "Epoch 13398/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0191 - val_loss: 5.5201\n",
      "Epoch 13399/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9784 - val_loss: 4.7972\n",
      "Epoch 13400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6381 - val_loss: 4.9425\n",
      "Epoch 13401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6879 - val_loss: 4.9280\n",
      "Epoch 13402/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5280 - val_loss: 4.7995\n",
      "Epoch 13403/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5433 - val_loss: 4.9445\n",
      "Epoch 13404/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5920 - val_loss: 5.0996\n",
      "Epoch 13405/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7474 - val_loss: 4.8000\n",
      "Epoch 13406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5339 - val_loss: 4.9378\n",
      "Epoch 13407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5541 - val_loss: 4.7798\n",
      "Epoch 13408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6219 - val_loss: 4.8898\n",
      "Epoch 13409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5610 - val_loss: 5.0828\n",
      "Epoch 13410/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5175 - val_loss: 5.0179\n",
      "Epoch 13411/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5929 - val_loss: 4.9722\n",
      "Epoch 13412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6162 - val_loss: 5.0271\n",
      "Epoch 13413/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5590 - val_loss: 4.8004\n",
      "Epoch 13414/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6114 - val_loss: 4.8521\n",
      "Epoch 13415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5276 - val_loss: 4.7961\n",
      "Epoch 13416/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5245 - val_loss: 4.7988\n",
      "Epoch 13417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5517 - val_loss: 5.0779\n",
      "Epoch 13418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7645 - val_loss: 4.8720\n",
      "Epoch 13419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5674 - val_loss: 4.8264\n",
      "Epoch 13420/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6218 - val_loss: 5.0245\n",
      "Epoch 13421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5908 - val_loss: 4.8367\n",
      "Epoch 13422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5154 - val_loss: 5.0008\n",
      "Epoch 13423/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5307 - val_loss: 4.8231\n",
      "Epoch 13424/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6130 - val_loss: 4.8807\n",
      "Epoch 13425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7046 - val_loss: 5.0181\n",
      "Epoch 13426/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6129 - val_loss: 5.3046\n",
      "Epoch 13427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5061 - val_loss: 4.8098\n",
      "Epoch 13428/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7058 - val_loss: 4.9543\n",
      "Epoch 13429/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6466 - val_loss: 4.8535\n",
      "Epoch 13430/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5856 - val_loss: 4.8950\n",
      "Epoch 13431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7745 - val_loss: 4.8201\n",
      "Epoch 13432/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5139 - val_loss: 4.9198\n",
      "Epoch 13433/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4924 - val_loss: 4.8042\n",
      "Epoch 13434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4904 - val_loss: 4.8529\n",
      "Epoch 13435/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5372 - val_loss: 4.9933\n",
      "Epoch 13436/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6217 - val_loss: 4.8240\n",
      "Epoch 13437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4938 - val_loss: 4.8841\n",
      "Epoch 13438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4874 - val_loss: 4.8034\n",
      "Epoch 13439/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5046 - val_loss: 4.8069\n",
      "Epoch 13440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5067 - val_loss: 4.9645\n",
      "Epoch 13441/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5664 - val_loss: 5.0780\n",
      "Epoch 13442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5942 - val_loss: 5.2847\n",
      "Epoch 13443/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5236 - val_loss: 4.8284\n",
      "Epoch 13444/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5744 - val_loss: 5.3012\n",
      "Epoch 13445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6865 - val_loss: 4.9224\n",
      "Epoch 13446/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5483 - val_loss: 4.8723\n",
      "Epoch 13447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5390 - val_loss: 4.9902\n",
      "Epoch 13448/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.716 - 0s 46us/step - loss: 4.5066 - val_loss: 4.8140\n",
      "Epoch 13449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5445 - val_loss: 4.7927\n",
      "Epoch 13450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4957 - val_loss: 5.0346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13451/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7140 - val_loss: 4.9511\n",
      "Epoch 13452/20000\n",
      "685/685 [==============================] - 0s 114us/step - loss: 4.7673 - val_loss: 4.8752\n",
      "Epoch 13453/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8225 - val_loss: 4.8113\n",
      "Epoch 13454/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5409 - val_loss: 5.1351\n",
      "Epoch 13455/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4903 - val_loss: 4.8230\n",
      "Epoch 13456/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5613 - val_loss: 4.8340\n",
      "Epoch 13457/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5594 - val_loss: 4.9143\n",
      "Epoch 13458/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6723 - val_loss: 4.8648\n",
      "Epoch 13459/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6302 - val_loss: 4.9529\n",
      "Epoch 13460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6843 - val_loss: 4.9489\n",
      "Epoch 13461/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5997 - val_loss: 4.7911\n",
      "Epoch 13462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5161 - val_loss: 4.9033\n",
      "Epoch 13463/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4730 - val_loss: 4.8938\n",
      "Epoch 13464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8290 - val_loss: 5.1206\n",
      "Epoch 13465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6062 - val_loss: 4.8147\n",
      "Epoch 13466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5822 - val_loss: 4.8614\n",
      "Epoch 13467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4939 - val_loss: 4.8909\n",
      "Epoch 13468/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4937 - val_loss: 4.7872\n",
      "Epoch 13469/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6507 - val_loss: 5.0101\n",
      "Epoch 13470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5584 - val_loss: 4.9415\n",
      "Epoch 13471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5535 - val_loss: 5.4810\n",
      "Epoch 13472/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8954 - val_loss: 5.2334\n",
      "Epoch 13473/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5166 - val_loss: 4.9992\n",
      "Epoch 13474/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5464 - val_loss: 4.8076\n",
      "Epoch 13475/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6315 - val_loss: 4.8079\n",
      "Epoch 13476/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6293 - val_loss: 4.7843\n",
      "Epoch 13477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5288 - val_loss: 5.3526\n",
      "Epoch 13478/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8315 - val_loss: 5.3412\n",
      "Epoch 13479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8109 - val_loss: 4.8150\n",
      "Epoch 13480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4887 - val_loss: 4.7943\n",
      "Epoch 13481/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5177 - val_loss: 4.7925\n",
      "Epoch 13482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8106 - val_loss: 5.0737\n",
      "Epoch 13483/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6339 - val_loss: 4.9117\n",
      "Epoch 13484/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5559 - val_loss: 4.7758\n",
      "Epoch 13485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5007 - val_loss: 4.9343\n",
      "Epoch 13486/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5632 - val_loss: 4.8098\n",
      "Epoch 13487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5384 - val_loss: 4.8855\n",
      "Epoch 13488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5946 - val_loss: 4.8719\n",
      "Epoch 13489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6911 - val_loss: 4.9338\n",
      "Epoch 13490/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5096 - val_loss: 4.7678\n",
      "Epoch 13491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6519 - val_loss: 4.7622\n",
      "Epoch 13492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5895 - val_loss: 5.1036\n",
      "Epoch 13493/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6798 - val_loss: 4.8417\n",
      "Epoch 13494/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4715 - val_loss: 4.8295\n",
      "Epoch 13495/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5096 - val_loss: 4.7925\n",
      "Epoch 13496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5787 - val_loss: 4.9381\n",
      "Epoch 13497/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5091 - val_loss: 4.8558\n",
      "Epoch 13498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6115 - val_loss: 4.8030\n",
      "Epoch 13499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7069 - val_loss: 4.9480\n",
      "Epoch 13500/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5664 - val_loss: 4.9983\n",
      "Epoch 13501/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5027 - val_loss: 4.8692\n",
      "Epoch 13502/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5086 - val_loss: 4.8215\n",
      "Epoch 13503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7900 - val_loss: 4.8003\n",
      "Epoch 13504/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5578 - val_loss: 4.9267\n",
      "Epoch 13505/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6086 - val_loss: 4.9734\n",
      "Epoch 13506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7084 - val_loss: 4.7919\n",
      "Epoch 13507/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8088 - val_loss: 4.9083\n",
      "Epoch 13508/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5276 - val_loss: 4.8277\n",
      "Epoch 13509/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5142 - val_loss: 4.9017\n",
      "Epoch 13510/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8321 - val_loss: 4.9741\n",
      "Epoch 13511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7250 - val_loss: 5.0889\n",
      "Epoch 13512/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9294 - val_loss: 5.1056\n",
      "Epoch 13513/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5465 - val_loss: 4.7999\n",
      "Epoch 13514/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6113 - val_loss: 4.7838\n",
      "Epoch 13515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5052 - val_loss: 5.0652\n",
      "Epoch 13516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4913 - val_loss: 4.9403\n",
      "Epoch 13517/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5803 - val_loss: 5.4679\n",
      "Epoch 13518/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6630 - val_loss: 4.9869\n",
      "Epoch 13519/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6221 - val_loss: 4.9429\n",
      "Epoch 13520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7947 - val_loss: 4.8117\n",
      "Epoch 13521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4916 - val_loss: 4.8408\n",
      "Epoch 13522/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6438 - val_loss: 4.8486\n",
      "Epoch 13523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5681 - val_loss: 4.9263\n",
      "Epoch 13524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4923 - val_loss: 4.8824\n",
      "Epoch 13525/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4849 - val_loss: 4.7881\n",
      "Epoch 13526/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5342 - val_loss: 4.8059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6762 - val_loss: 4.8422\n",
      "Epoch 13528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5047 - val_loss: 4.8110\n",
      "Epoch 13529/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5452 - val_loss: 4.8477\n",
      "Epoch 13530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4750 - val_loss: 4.8623\n",
      "Epoch 13531/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5044 - val_loss: 4.8154\n",
      "Epoch 13532/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5340 - val_loss: 4.9588\n",
      "Epoch 13533/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5418 - val_loss: 4.8327\n",
      "Epoch 13534/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5632 - val_loss: 4.8776\n",
      "Epoch 13535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5121 - val_loss: 4.7836\n",
      "Epoch 13536/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5243 - val_loss: 5.0691\n",
      "Epoch 13537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7132 - val_loss: 5.5443\n",
      "Epoch 13538/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8130 - val_loss: 4.9550\n",
      "Epoch 13539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5933 - val_loss: 4.8016\n",
      "Epoch 13540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5335 - val_loss: 4.8612\n",
      "Epoch 13541/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.4953 - val_loss: 4.8457\n",
      "Epoch 13542/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5914 - val_loss: 4.7982\n",
      "Epoch 13543/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5268 - val_loss: 4.9823\n",
      "Epoch 13544/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6058 - val_loss: 5.0042\n",
      "Epoch 13545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5359 - val_loss: 4.8574\n",
      "Epoch 13546/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6476 - val_loss: 4.7821\n",
      "Epoch 13547/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5654 - val_loss: 4.7876\n",
      "Epoch 13548/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6935 - val_loss: 4.8865\n",
      "Epoch 13549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7320 - val_loss: 4.7962\n",
      "Epoch 13550/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5670 - val_loss: 4.8816\n",
      "Epoch 13551/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7585 - val_loss: 4.8157\n",
      "Epoch 13552/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0543 - val_loss: 4.8467\n",
      "Epoch 13553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7948 - val_loss: 4.7922\n",
      "Epoch 13554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5587 - val_loss: 4.8130\n",
      "Epoch 13555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4714 - val_loss: 4.9100\n",
      "Epoch 13556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4890 - val_loss: 5.0614\n",
      "Epoch 13557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4981 - val_loss: 4.8276\n",
      "Epoch 13558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5202 - val_loss: 4.8275\n",
      "Epoch 13559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5408 - val_loss: 4.8190\n",
      "Epoch 13560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6187 - val_loss: 4.8005\n",
      "Epoch 13561/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.4596 - val_loss: 4.8119\n",
      "Epoch 13562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5494 - val_loss: 4.9713\n",
      "Epoch 13563/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5595 - val_loss: 5.1328\n",
      "Epoch 13564/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6129 - val_loss: 4.8829\n",
      "Epoch 13565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5246 - val_loss: 4.8184\n",
      "Epoch 13566/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4987 - val_loss: 4.8316\n",
      "Epoch 13567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5414 - val_loss: 4.8023\n",
      "Epoch 13568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5640 - val_loss: 4.7871\n",
      "Epoch 13569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5449 - val_loss: 4.8843\n",
      "Epoch 13570/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5952 - val_loss: 4.7987\n",
      "Epoch 13571/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5869 - val_loss: 4.8039\n",
      "Epoch 13572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6134 - val_loss: 5.2313\n",
      "Epoch 13573/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9619 - val_loss: 5.0512\n",
      "Epoch 13574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5422 - val_loss: 4.8174\n",
      "Epoch 13575/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5015 - val_loss: 4.8111\n",
      "Epoch 13576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6434 - val_loss: 4.8379\n",
      "Epoch 13577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5525 - val_loss: 4.7663\n",
      "Epoch 13578/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5103 - val_loss: 5.1211\n",
      "Epoch 13579/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5558 - val_loss: 4.8503\n",
      "Epoch 13580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4993 - val_loss: 4.8274\n",
      "Epoch 13581/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6275 - val_loss: 4.8107\n",
      "Epoch 13582/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6332 - val_loss: 4.7935\n",
      "Epoch 13583/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5054 - val_loss: 4.7929\n",
      "Epoch 13584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7066 - val_loss: 4.8091\n",
      "Epoch 13585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6949 - val_loss: 5.1757\n",
      "Epoch 13586/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5881 - val_loss: 4.8243\n",
      "Epoch 13587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4908 - val_loss: 4.7779\n",
      "Epoch 13588/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5273 - val_loss: 4.8120\n",
      "Epoch 13589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4890 - val_loss: 4.9947\n",
      "Epoch 13590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5928 - val_loss: 4.8832\n",
      "Epoch 13591/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9256 - val_loss: 4.8134\n",
      "Epoch 13592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5700 - val_loss: 4.8484\n",
      "Epoch 13593/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5268 - val_loss: 4.8207\n",
      "Epoch 13594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4890 - val_loss: 4.8166\n",
      "Epoch 13595/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5892 - val_loss: 4.8289\n",
      "Epoch 13596/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5315 - val_loss: 5.0416\n",
      "Epoch 13597/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6340 - val_loss: 4.7946\n",
      "Epoch 13598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5849 - val_loss: 4.8066\n",
      "Epoch 13599/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5275 - val_loss: 4.7895\n",
      "Epoch 13600/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4992 - val_loss: 4.8156\n",
      "Epoch 13601/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5579 - val_loss: 4.8214\n",
      "Epoch 13602/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5120 - val_loss: 4.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4584 - val_loss: 4.8365\n",
      "Epoch 13604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5184 - val_loss: 4.8375\n",
      "Epoch 13605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6035 - val_loss: 4.8144\n",
      "Epoch 13606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7724 - val_loss: 4.8101\n",
      "Epoch 13607/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4672 - val_loss: 4.8614\n",
      "Epoch 13608/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4849 - val_loss: 4.8365\n",
      "Epoch 13609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4686 - val_loss: 5.0326\n",
      "Epoch 13610/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5993 - val_loss: 4.8274\n",
      "Epoch 13611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5427 - val_loss: 4.9996\n",
      "Epoch 13612/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5807 - val_loss: 4.8928\n",
      "Epoch 13613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5495 - val_loss: 4.9428\n",
      "Epoch 13614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5226 - val_loss: 4.8128\n",
      "Epoch 13615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5114 - val_loss: 4.8308\n",
      "Epoch 13616/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5206 - val_loss: 4.8333\n",
      "Epoch 13617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4652 - val_loss: 4.8539\n",
      "Epoch 13618/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5816 - val_loss: 4.9634\n",
      "Epoch 13619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5981 - val_loss: 4.8165\n",
      "Epoch 13620/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5770 - val_loss: 4.8434\n",
      "Epoch 13621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5876 - val_loss: 5.3059\n",
      "Epoch 13622/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6927 - val_loss: 4.7955\n",
      "Epoch 13623/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6054 - val_loss: 5.0852\n",
      "Epoch 13624/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7275 - val_loss: 4.8582\n",
      "Epoch 13625/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9414 - val_loss: 4.9450\n",
      "Epoch 13626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5783 - val_loss: 4.8232\n",
      "Epoch 13627/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5794 - val_loss: 5.2835\n",
      "Epoch 13628/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7827 - val_loss: 4.7981\n",
      "Epoch 13629/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6710 - val_loss: 4.8101\n",
      "Epoch 13630/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4891 - val_loss: 4.8955\n",
      "Epoch 13631/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7948 - val_loss: 4.8260\n",
      "Epoch 13632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5261 - val_loss: 4.9350\n",
      "Epoch 13633/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5498 - val_loss: 4.9000\n",
      "Epoch 13634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4994 - val_loss: 4.8469\n",
      "Epoch 13635/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5591 - val_loss: 4.8178\n",
      "Epoch 13636/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 5.0238 - val_loss: 4.9763\n",
      "Epoch 13637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5225 - val_loss: 4.7849\n",
      "Epoch 13638/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5222 - val_loss: 4.7942\n",
      "Epoch 13639/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5731 - val_loss: 4.8067\n",
      "Epoch 13640/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5423 - val_loss: 4.8509\n",
      "Epoch 13641/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5804 - val_loss: 5.0777\n",
      "Epoch 13642/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6494 - val_loss: 5.2592\n",
      "Epoch 13643/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6381 - val_loss: 5.0548\n",
      "Epoch 13644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4953 - val_loss: 4.9114\n",
      "Epoch 13645/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5200 - val_loss: 4.9794\n",
      "Epoch 13646/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5220 - val_loss: 4.8308\n",
      "Epoch 13647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6040 - val_loss: 4.8151\n",
      "Epoch 13648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6552 - val_loss: 4.9227\n",
      "Epoch 13649/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5188 - val_loss: 5.0392\n",
      "Epoch 13650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6416 - val_loss: 4.8747\n",
      "Epoch 13651/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5458 - val_loss: 4.8131\n",
      "Epoch 13652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5329 - val_loss: 5.0183\n",
      "Epoch 13653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5383 - val_loss: 4.8908\n",
      "Epoch 13654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5747 - val_loss: 5.0843\n",
      "Epoch 13655/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5247 - val_loss: 4.8554\n",
      "Epoch 13656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8417 - val_loss: 4.8348\n",
      "Epoch 13657/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5999 - val_loss: 4.8082\n",
      "Epoch 13658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6990 - val_loss: 4.8573\n",
      "Epoch 13659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5217 - val_loss: 4.8072\n",
      "Epoch 13660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5925 - val_loss: 5.1355\n",
      "Epoch 13661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5751 - val_loss: 5.1840\n",
      "Epoch 13662/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7640 - val_loss: 4.8786\n",
      "Epoch 13663/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5038 - val_loss: 4.7996\n",
      "Epoch 13664/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6005 - val_loss: 4.9147\n",
      "Epoch 13665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8013 - val_loss: 4.8458\n",
      "Epoch 13666/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5758 - val_loss: 5.0194\n",
      "Epoch 13667/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5106 - val_loss: 4.7978\n",
      "Epoch 13668/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5439 - val_loss: 4.9091\n",
      "Epoch 13669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5282 - val_loss: 4.8143\n",
      "Epoch 13670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5158 - val_loss: 4.8062\n",
      "Epoch 13671/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8249 - val_loss: 5.3301\n",
      "Epoch 13672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7752 - val_loss: 4.8109\n",
      "Epoch 13673/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6188 - val_loss: 4.8582\n",
      "Epoch 13674/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6656 - val_loss: 4.8893\n",
      "Epoch 13675/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5338 - val_loss: 4.9980\n",
      "Epoch 13676/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5435 - val_loss: 4.8585\n",
      "Epoch 13677/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7619 - val_loss: 5.0014\n",
      "Epoch 13678/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5562 - val_loss: 4.8203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5573 - val_loss: 4.8074\n",
      "Epoch 13680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5527 - val_loss: 4.8990\n",
      "Epoch 13681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5785 - val_loss: 4.8795\n",
      "Epoch 13682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5280 - val_loss: 4.8266\n",
      "Epoch 13683/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5620 - val_loss: 4.8794\n",
      "Epoch 13684/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5434 - val_loss: 4.9377\n",
      "Epoch 13685/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5334 - val_loss: 4.8227\n",
      "Epoch 13686/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6056 - val_loss: 5.0735\n",
      "Epoch 13687/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5851 - val_loss: 4.8106\n",
      "Epoch 13688/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6419 - val_loss: 4.8993\n",
      "Epoch 13689/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4632 - val_loss: 4.8694\n",
      "Epoch 13690/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.311 - 0s 46us/step - loss: 4.6314 - val_loss: 5.0890\n",
      "Epoch 13691/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5812 - val_loss: 4.8138\n",
      "Epoch 13692/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4956 - val_loss: 4.8906\n",
      "Epoch 13693/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6114 - val_loss: 5.1326\n",
      "Epoch 13694/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8842 - val_loss: 4.8414\n",
      "Epoch 13695/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5951 - val_loss: 5.0623\n",
      "Epoch 13696/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5413 - val_loss: 4.8184\n",
      "Epoch 13697/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5072 - val_loss: 4.8836\n",
      "Epoch 13698/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5522 - val_loss: 4.8375\n",
      "Epoch 13699/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6113 - val_loss: 4.7986\n",
      "Epoch 13700/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4762 - val_loss: 4.8313\n",
      "Epoch 13701/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5321 - val_loss: 5.0995\n",
      "Epoch 13702/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5349 - val_loss: 4.7952\n",
      "Epoch 13703/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5049 - val_loss: 4.7904\n",
      "Epoch 13704/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6517 - val_loss: 4.8645\n",
      "Epoch 13705/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4947 - val_loss: 4.9105\n",
      "Epoch 13706/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4717 - val_loss: 5.0087\n",
      "Epoch 13707/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7181 - val_loss: 4.7947\n",
      "Epoch 13708/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6233 - val_loss: 5.1745\n",
      "Epoch 13709/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5474 - val_loss: 4.8253\n",
      "Epoch 13710/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5135 - val_loss: 4.9331\n",
      "Epoch 13711/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5117 - val_loss: 4.8173\n",
      "Epoch 13712/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5718 - val_loss: 4.8289\n",
      "Epoch 13713/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4688 - val_loss: 5.0819\n",
      "Epoch 13714/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6098 - val_loss: 4.9945\n",
      "Epoch 13715/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5461 - val_loss: 4.8669\n",
      "Epoch 13716/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8132 - val_loss: 4.7901\n",
      "Epoch 13717/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5994 - val_loss: 4.9070\n",
      "Epoch 13718/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5148 - val_loss: 4.8878\n",
      "Epoch 13719/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5831 - val_loss: 4.8071\n",
      "Epoch 13720/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.4738 - val_loss: 5.0169\n",
      "Epoch 13721/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5039 - val_loss: 4.8164\n",
      "Epoch 13722/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6544 - val_loss: 5.1039\n",
      "Epoch 13723/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5365 - val_loss: 4.8082\n",
      "Epoch 13724/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5664 - val_loss: 4.8844\n",
      "Epoch 13725/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5579 - val_loss: 4.7789\n",
      "Epoch 13726/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4894 - val_loss: 4.8111\n",
      "Epoch 13727/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5396 - val_loss: 4.8001\n",
      "Epoch 13728/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5529 - val_loss: 4.7911\n",
      "Epoch 13729/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6210 - val_loss: 4.9290\n",
      "Epoch 13730/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6724 - val_loss: 4.8497\n",
      "Epoch 13731/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5238 - val_loss: 4.8238\n",
      "Epoch 13732/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4905 - val_loss: 4.9889\n",
      "Epoch 13733/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8867 - val_loss: 5.2718\n",
      "Epoch 13734/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5087 - val_loss: 4.8888\n",
      "Epoch 13735/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4996 - val_loss: 4.8738\n",
      "Epoch 13736/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5527 - val_loss: 5.2603\n",
      "Epoch 13737/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5465 - val_loss: 4.8262\n",
      "Epoch 13738/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4876 - val_loss: 4.8663\n",
      "Epoch 13739/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4520 - val_loss: 4.8593\n",
      "Epoch 13740/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5767 - val_loss: 5.0628\n",
      "Epoch 13741/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5287 - val_loss: 4.8137\n",
      "Epoch 13742/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5752 - val_loss: 4.9432\n",
      "Epoch 13743/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5585 - val_loss: 4.7808\n",
      "Epoch 13744/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5808 - val_loss: 4.7871\n",
      "Epoch 13745/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5656 - val_loss: 4.7905\n",
      "Epoch 13746/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5647 - val_loss: 4.8928\n",
      "Epoch 13747/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4980 - val_loss: 4.8651\n",
      "Epoch 13748/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4751 - val_loss: 4.8038\n",
      "Epoch 13749/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6517 - val_loss: 4.8484\n",
      "Epoch 13750/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8414 - val_loss: 4.8772\n",
      "Epoch 13751/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4526 - val_loss: 5.1114\n",
      "Epoch 13752/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8614 - val_loss: 4.9090\n",
      "Epoch 13753/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7300 - val_loss: 4.7929\n",
      "Epoch 13754/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5559 - val_loss: 4.8015\n",
      "Epoch 13755/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5509 - val_loss: 4.8056\n",
      "Epoch 13756/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.9812 - val_loss: 5.2799\n",
      "Epoch 13757/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7772 - val_loss: 4.9887\n",
      "Epoch 13758/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6406 - val_loss: 4.8733\n",
      "Epoch 13759/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5238 - val_loss: 4.9034\n",
      "Epoch 13760/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5367 - val_loss: 4.8050\n",
      "Epoch 13761/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4869 - val_loss: 4.9536\n",
      "Epoch 13762/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6084 - val_loss: 4.8068\n",
      "Epoch 13763/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5156 - val_loss: 4.9229\n",
      "Epoch 13764/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5473 - val_loss: 4.8534\n",
      "Epoch 13765/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7165 - val_loss: 4.8052\n",
      "Epoch 13766/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 4.438 - 0s 46us/step - loss: 4.5256 - val_loss: 4.8292\n",
      "Epoch 13767/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5336 - val_loss: 4.8605\n",
      "Epoch 13768/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5156 - val_loss: 4.8064\n",
      "Epoch 13769/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7179 - val_loss: 5.1870\n",
      "Epoch 13770/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8111 - val_loss: 6.1164\n",
      "Epoch 13771/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9303 - val_loss: 4.8607\n",
      "Epoch 13772/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6359 - val_loss: 4.7980\n",
      "Epoch 13773/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5564 - val_loss: 4.8172\n",
      "Epoch 13774/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5498 - val_loss: 4.8257\n",
      "Epoch 13775/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5116 - val_loss: 4.8510\n",
      "Epoch 13776/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5537 - val_loss: 5.5011\n",
      "Epoch 13777/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6012 - val_loss: 4.8491\n",
      "Epoch 13778/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6112 - val_loss: 5.0173\n",
      "Epoch 13779/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5984 - val_loss: 4.8024\n",
      "Epoch 13780/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6122 - val_loss: 5.0377\n",
      "Epoch 13781/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4750 - val_loss: 5.2879\n",
      "Epoch 13782/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5471 - val_loss: 4.8783\n",
      "Epoch 13783/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4959 - val_loss: 4.8293\n",
      "Epoch 13784/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7448 - val_loss: 5.0715\n",
      "Epoch 13785/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5598 - val_loss: 4.8750\n",
      "Epoch 13786/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6032 - val_loss: 4.7811\n",
      "Epoch 13787/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5009 - val_loss: 4.9731\n",
      "Epoch 13788/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5618 - val_loss: 4.8140\n",
      "Epoch 13789/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5993 - val_loss: 4.8875\n",
      "Epoch 13790/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5261 - val_loss: 5.1797\n",
      "Epoch 13791/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5989 - val_loss: 4.9157\n",
      "Epoch 13792/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6495 - val_loss: 4.9372\n",
      "Epoch 13793/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9053 - val_loss: 6.1781\n",
      "Epoch 13794/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8714 - val_loss: 4.7804\n",
      "Epoch 13795/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5563 - val_loss: 4.7895\n",
      "Epoch 13796/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5371 - val_loss: 4.8057\n",
      "Epoch 13797/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5506 - val_loss: 4.8142\n",
      "Epoch 13798/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6410 - val_loss: 4.8169\n",
      "Epoch 13799/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5084 - val_loss: 4.8153\n",
      "Epoch 13800/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6479 - val_loss: 4.9767\n",
      "Epoch 13801/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6027 - val_loss: 4.8473\n",
      "Epoch 13802/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7155 - val_loss: 4.8003\n",
      "Epoch 13803/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6179 - val_loss: 4.9680\n",
      "Epoch 13804/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6137 - val_loss: 5.9821\n",
      "Epoch 13805/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9301 - val_loss: 5.0028\n",
      "Epoch 13806/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5978 - val_loss: 4.8440\n",
      "Epoch 13807/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5675 - val_loss: 4.7865\n",
      "Epoch 13808/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4913 - val_loss: 4.8859\n",
      "Epoch 13809/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4644 - val_loss: 5.1262\n",
      "Epoch 13810/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6258 - val_loss: 4.8483\n",
      "Epoch 13811/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5359 - val_loss: 4.8664\n",
      "Epoch 13812/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5194 - val_loss: 4.8811\n",
      "Epoch 13813/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5188 - val_loss: 4.8513\n",
      "Epoch 13814/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5678 - val_loss: 4.8397\n",
      "Epoch 13815/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6161 - val_loss: 4.8260\n",
      "Epoch 13816/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8779 - val_loss: 5.1316\n",
      "Epoch 13817/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6278 - val_loss: 4.8648\n",
      "Epoch 13818/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6133 - val_loss: 4.8015\n",
      "Epoch 13819/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5237 - val_loss: 4.8173\n",
      "Epoch 13820/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5602 - val_loss: 5.3336\n",
      "Epoch 13821/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5230 - val_loss: 5.0828\n",
      "Epoch 13822/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5468 - val_loss: 5.0509\n",
      "Epoch 13823/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5898 - val_loss: 4.9858\n",
      "Epoch 13824/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5373 - val_loss: 4.9424\n",
      "Epoch 13825/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6829 - val_loss: 4.7909\n",
      "Epoch 13826/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7183 - val_loss: 5.1095\n",
      "Epoch 13827/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4710 - val_loss: 4.7829\n",
      "Epoch 13828/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.4960 - val_loss: 4.9001\n",
      "Epoch 13829/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.4663 - val_loss: 4.9458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13830/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5599 - val_loss: 4.9595\n",
      "Epoch 13831/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5142 - val_loss: 4.8437\n",
      "Epoch 13832/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6732 - val_loss: 4.7990\n",
      "Epoch 13833/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5193 - val_loss: 4.7989\n",
      "Epoch 13834/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6471 - val_loss: 5.7456\n",
      "Epoch 13835/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9624 - val_loss: 4.9022\n",
      "Epoch 13836/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8779 - val_loss: 4.8573\n",
      "Epoch 13837/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6144 - val_loss: 4.9472\n",
      "Epoch 13838/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5858 - val_loss: 4.8086\n",
      "Epoch 13839/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5379 - val_loss: 4.8683\n",
      "Epoch 13840/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4617 - val_loss: 4.7939\n",
      "Epoch 13841/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.5692 - val_loss: 4.7762\n",
      "Epoch 13842/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.4758 - val_loss: 4.8090\n",
      "Epoch 13843/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4785 - val_loss: 4.8671\n",
      "Epoch 13844/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5273 - val_loss: 4.8041\n",
      "Epoch 13845/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5252 - val_loss: 4.8268\n",
      "Epoch 13846/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7531 - val_loss: 4.8168\n",
      "Epoch 13847/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6025 - val_loss: 4.9080\n",
      "Epoch 13848/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4959 - val_loss: 4.8942\n",
      "Epoch 13849/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5563 - val_loss: 4.7714\n",
      "Epoch 13850/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5432 - val_loss: 4.8497\n",
      "Epoch 13851/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5644 - val_loss: 4.8031\n",
      "Epoch 13852/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5624 - val_loss: 4.7923\n",
      "Epoch 13853/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4967 - val_loss: 4.8453\n",
      "Epoch 13854/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4815 - val_loss: 4.7970\n",
      "Epoch 13855/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7120 - val_loss: 5.4424\n",
      "Epoch 13856/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8909 - val_loss: 6.0150\n",
      "Epoch 13857/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7570 - val_loss: 5.1314\n",
      "Epoch 13858/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6322 - val_loss: 4.8592\n",
      "Epoch 13859/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5022 - val_loss: 4.7854\n",
      "Epoch 13860/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5199 - val_loss: 4.9201\n",
      "Epoch 13861/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7333 - val_loss: 4.8672\n",
      "Epoch 13862/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5859 - val_loss: 4.7884\n",
      "Epoch 13863/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4730 - val_loss: 4.8174\n",
      "Epoch 13864/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5131 - val_loss: 4.8241\n",
      "Epoch 13865/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5058 - val_loss: 4.8135\n",
      "Epoch 13866/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5004 - val_loss: 4.8509\n",
      "Epoch 13867/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5954 - val_loss: 4.8340\n",
      "Epoch 13868/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8147 - val_loss: 5.1982\n",
      "Epoch 13869/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5773 - val_loss: 4.7780\n",
      "Epoch 13870/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5059 - val_loss: 4.8141\n",
      "Epoch 13871/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5601 - val_loss: 4.8096\n",
      "Epoch 13872/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6555 - val_loss: 4.8340\n",
      "Epoch 13873/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5211 - val_loss: 4.7999\n",
      "Epoch 13874/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4934 - val_loss: 4.8026\n",
      "Epoch 13875/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6397 - val_loss: 5.1732\n",
      "Epoch 13876/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7799 - val_loss: 5.1564\n",
      "Epoch 13877/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5137 - val_loss: 4.8179\n",
      "Epoch 13878/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5954 - val_loss: 5.8153\n",
      "Epoch 13879/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8921 - val_loss: 4.8874\n",
      "Epoch 13880/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4839 - val_loss: 4.7981\n",
      "Epoch 13881/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4934 - val_loss: 4.8148\n",
      "Epoch 13882/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5105 - val_loss: 4.8101\n",
      "Epoch 13883/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5860 - val_loss: 4.8717\n",
      "Epoch 13884/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5365 - val_loss: 4.7828\n",
      "Epoch 13885/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5187 - val_loss: 4.8454\n",
      "Epoch 13886/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5386 - val_loss: 4.8802\n",
      "Epoch 13887/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4723 - val_loss: 5.0113\n",
      "Epoch 13888/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6242 - val_loss: 4.7962\n",
      "Epoch 13889/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6921 - val_loss: 4.7921\n",
      "Epoch 13890/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5920 - val_loss: 4.7804\n",
      "Epoch 13891/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5705 - val_loss: 4.7792\n",
      "Epoch 13892/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5346 - val_loss: 5.1053\n",
      "Epoch 13893/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7892 - val_loss: 4.8769\n",
      "Epoch 13894/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5745 - val_loss: 4.9290\n",
      "Epoch 13895/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7801 - val_loss: 4.8750\n",
      "Epoch 13896/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6494 - val_loss: 4.9674\n",
      "Epoch 13897/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5396 - val_loss: 5.1803\n",
      "Epoch 13898/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5143 - val_loss: 4.8140\n",
      "Epoch 13899/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6701 - val_loss: 4.9555\n",
      "Epoch 13900/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5595 - val_loss: 4.8115\n",
      "Epoch 13901/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5625 - val_loss: 4.9121\n",
      "Epoch 13902/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6593 - val_loss: 5.0611\n",
      "Epoch 13903/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6167 - val_loss: 5.0507\n",
      "Epoch 13904/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6828 - val_loss: 5.2463\n",
      "Epoch 13905/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5458 - val_loss: 4.8179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13906/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5942 - val_loss: 4.8152\n",
      "Epoch 13907/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5901 - val_loss: 4.8386\n",
      "Epoch 13908/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5644 - val_loss: 4.9320\n",
      "Epoch 13909/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5850 - val_loss: 4.8123\n",
      "Epoch 13910/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5073 - val_loss: 4.8138\n",
      "Epoch 13911/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.604 - 0s 46us/step - loss: 4.7406 - val_loss: 4.8208\n",
      "Epoch 13912/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5915 - val_loss: 4.8197\n",
      "Epoch 13913/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6918 - val_loss: 4.7874\n",
      "Epoch 13914/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6138 - val_loss: 4.7860\n",
      "Epoch 13915/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7493 - val_loss: 5.0696\n",
      "Epoch 13916/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0027 - val_loss: 5.0162\n",
      "Epoch 13917/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7125 - val_loss: 4.9733\n",
      "Epoch 13918/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5619 - val_loss: 4.8334\n",
      "Epoch 13919/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4709 - val_loss: 4.7973\n",
      "Epoch 13920/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4892 - val_loss: 4.9862\n",
      "Epoch 13921/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 5.997 - 0s 46us/step - loss: 4.7200 - val_loss: 4.9406\n",
      "Epoch 13922/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7397 - val_loss: 4.7812\n",
      "Epoch 13923/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6462 - val_loss: 4.9047\n",
      "Epoch 13924/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4605 - val_loss: 4.8149\n",
      "Epoch 13925/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5227 - val_loss: 4.7810\n",
      "Epoch 13926/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5327 - val_loss: 4.8043\n",
      "Epoch 13927/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5488 - val_loss: 4.9238\n",
      "Epoch 13928/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5328 - val_loss: 4.8130\n",
      "Epoch 13929/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5180 - val_loss: 4.9353\n",
      "Epoch 13930/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5563 - val_loss: 4.9997\n",
      "Epoch 13931/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5829 - val_loss: 4.8496\n",
      "Epoch 13932/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5308 - val_loss: 4.8192\n",
      "Epoch 13933/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4993 - val_loss: 4.7930\n",
      "Epoch 13934/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4933 - val_loss: 4.9257\n",
      "Epoch 13935/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5076 - val_loss: 5.0820\n",
      "Epoch 13936/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5938 - val_loss: 4.7996\n",
      "Epoch 13937/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4629 - val_loss: 4.9754\n",
      "Epoch 13938/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7018 - val_loss: 5.8601\n",
      "Epoch 13939/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7233 - val_loss: 4.8610\n",
      "Epoch 13940/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4690 - val_loss: 4.8055\n",
      "Epoch 13941/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4602 - val_loss: 4.7948\n",
      "Epoch 13942/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5028 - val_loss: 5.3360\n",
      "Epoch 13943/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8619 - val_loss: 4.8356\n",
      "Epoch 13944/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6807 - val_loss: 4.8684\n",
      "Epoch 13945/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4877 - val_loss: 4.8217\n",
      "Epoch 13946/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4721 - val_loss: 4.8307\n",
      "Epoch 13947/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5385 - val_loss: 4.8171\n",
      "Epoch 13948/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5407 - val_loss: 4.9872\n",
      "Epoch 13949/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5718 - val_loss: 4.8202\n",
      "Epoch 13950/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5649 - val_loss: 4.7974\n",
      "Epoch 13951/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5145 - val_loss: 4.8415\n",
      "Epoch 13952/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5155 - val_loss: 4.9618\n",
      "Epoch 13953/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5499 - val_loss: 4.8626\n",
      "Epoch 13954/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6504 - val_loss: 4.8394\n",
      "Epoch 13955/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7374 - val_loss: 4.9641\n",
      "Epoch 13956/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5306 - val_loss: 4.8085\n",
      "Epoch 13957/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5292 - val_loss: 5.0261\n",
      "Epoch 13958/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5535 - val_loss: 5.2094\n",
      "Epoch 13959/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5381 - val_loss: 4.8400\n",
      "Epoch 13960/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4709 - val_loss: 4.7870\n",
      "Epoch 13961/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5852 - val_loss: 4.8081\n",
      "Epoch 13962/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5478 - val_loss: 4.8093\n",
      "Epoch 13963/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5559 - val_loss: 4.9321\n",
      "Epoch 13964/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5592 - val_loss: 5.0771\n",
      "Epoch 13965/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5630 - val_loss: 4.9086\n",
      "Epoch 13966/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4907 - val_loss: 4.8391\n",
      "Epoch 13967/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5452 - val_loss: 4.8069\n",
      "Epoch 13968/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5917 - val_loss: 4.7891\n",
      "Epoch 13969/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4994 - val_loss: 4.9076\n",
      "Epoch 13970/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7339 - val_loss: 5.1272\n",
      "Epoch 13971/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7332 - val_loss: 5.0422\n",
      "Epoch 13972/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.5702 - val_loss: 4.8850\n",
      "Epoch 13973/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6257 - val_loss: 4.7739\n",
      "Epoch 13974/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4657 - val_loss: 4.9912\n",
      "Epoch 13975/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6725 - val_loss: 5.0657\n",
      "Epoch 13976/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6788 - val_loss: 4.7745\n",
      "Epoch 13977/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5750 - val_loss: 4.8722\n",
      "Epoch 13978/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4843 - val_loss: 4.8223\n",
      "Epoch 13979/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5080 - val_loss: 4.8692\n",
      "Epoch 13980/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7908 - val_loss: 6.3745\n",
      "Epoch 13981/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.7945 - val_loss: 5.8447\n",
      "Epoch 13982/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6873 - val_loss: 4.8912\n",
      "Epoch 13983/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4979 - val_loss: 4.7870\n",
      "Epoch 13984/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5477 - val_loss: 4.9316\n",
      "Epoch 13985/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6771 - val_loss: 4.8385\n",
      "Epoch 13986/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6472 - val_loss: 4.8305\n",
      "Epoch 13987/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5217 - val_loss: 4.9054\n",
      "Epoch 13988/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7580 - val_loss: 4.9500\n",
      "Epoch 13989/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5230 - val_loss: 4.7916\n",
      "Epoch 13990/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4673 - val_loss: 4.7992\n",
      "Epoch 13991/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6475 - val_loss: 4.9004\n",
      "Epoch 13992/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6544 - val_loss: 4.8058\n",
      "Epoch 13993/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5358 - val_loss: 4.8016\n",
      "Epoch 13994/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5343 - val_loss: 5.0688\n",
      "Epoch 13995/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5249 - val_loss: 5.3637\n",
      "Epoch 13996/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5885 - val_loss: 4.7556\n",
      "Epoch 13997/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4714 - val_loss: 4.7774\n",
      "Epoch 13998/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6534 - val_loss: 4.8194\n",
      "Epoch 13999/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5667 - val_loss: 4.8488\n",
      "Epoch 14000/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5216 - val_loss: 4.8242\n",
      "Epoch 14001/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4846 - val_loss: 5.1568\n",
      "Epoch 14002/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5807 - val_loss: 4.8971\n",
      "Epoch 14003/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5622 - val_loss: 4.9973\n",
      "Epoch 14004/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5000 - val_loss: 4.7958\n",
      "Epoch 14005/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4581 - val_loss: 4.8344\n",
      "Epoch 14006/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5863 - val_loss: 4.8067\n",
      "Epoch 14007/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4735 - val_loss: 4.9976\n",
      "Epoch 14008/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5944 - val_loss: 4.8300\n",
      "Epoch 14009/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4587 - val_loss: 4.8366\n",
      "Epoch 14010/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5175 - val_loss: 4.8109\n",
      "Epoch 14011/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5768 - val_loss: 4.8523\n",
      "Epoch 14012/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6130 - val_loss: 4.8717\n",
      "Epoch 14013/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5471 - val_loss: 5.3166\n",
      "Epoch 14014/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7529 - val_loss: 4.8021\n",
      "Epoch 14015/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4726 - val_loss: 4.8125\n",
      "Epoch 14016/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4805 - val_loss: 4.7930\n",
      "Epoch 14017/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5056 - val_loss: 4.8459\n",
      "Epoch 14018/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7358 - val_loss: 4.9467\n",
      "Epoch 14019/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5399 - val_loss: 4.9322\n",
      "Epoch 14020/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5071 - val_loss: 4.8848\n",
      "Epoch 14021/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6356 - val_loss: 4.8195\n",
      "Epoch 14022/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7496 - val_loss: 4.9560\n",
      "Epoch 14023/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6565 - val_loss: 4.9642\n",
      "Epoch 14024/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4870 - val_loss: 4.9518\n",
      "Epoch 14025/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5296 - val_loss: 5.1027\n",
      "Epoch 14026/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8515 - val_loss: 5.3064\n",
      "Epoch 14027/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8569 - val_loss: 4.7917\n",
      "Epoch 14028/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6327 - val_loss: 4.7808\n",
      "Epoch 14029/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5347 - val_loss: 5.1120\n",
      "Epoch 14030/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7023 - val_loss: 4.8829\n",
      "Epoch 14031/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4968 - val_loss: 5.4201\n",
      "Epoch 14032/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8246 - val_loss: 5.0812\n",
      "Epoch 14033/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4940 - val_loss: 5.0340\n",
      "Epoch 14034/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6291 - val_loss: 4.8248\n",
      "Epoch 14035/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5612 - val_loss: 4.8664\n",
      "Epoch 14036/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4970 - val_loss: 4.9877\n",
      "Epoch 14037/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5077 - val_loss: 5.0647\n",
      "Epoch 14038/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5297 - val_loss: 4.7776\n",
      "Epoch 14039/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5110 - val_loss: 5.0964\n",
      "Epoch 14040/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5052 - val_loss: 4.7845\n",
      "Epoch 14041/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7155 - val_loss: 4.8587\n",
      "Epoch 14042/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6060 - val_loss: 4.7807\n",
      "Epoch 14043/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5162 - val_loss: 4.7830\n",
      "Epoch 14044/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8097 - val_loss: 4.7930\n",
      "Epoch 14045/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4821 - val_loss: 4.7924\n",
      "Epoch 14046/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5041 - val_loss: 5.1579\n",
      "Epoch 14047/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5201 - val_loss: 4.7851\n",
      "Epoch 14048/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4850 - val_loss: 4.8462\n",
      "Epoch 14049/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5203 - val_loss: 4.8087\n",
      "Epoch 14050/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6360 - val_loss: 4.7846\n",
      "Epoch 14051/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4514 - val_loss: 4.8239\n",
      "Epoch 14052/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5667 - val_loss: 5.4060\n",
      "Epoch 14053/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6766 - val_loss: 5.3094\n",
      "Epoch 14054/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6590 - val_loss: 5.1693\n",
      "Epoch 14055/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5530 - val_loss: 5.2574\n",
      "Epoch 14056/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.0952 - val_loss: 5.2712\n",
      "Epoch 14057/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.6380 - val_loss: 4.8247\n",
      "Epoch 14058/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6267 - val_loss: 4.7960\n",
      "Epoch 14059/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5813 - val_loss: 4.7918\n",
      "Epoch 14060/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7506 - val_loss: 4.7754\n",
      "Epoch 14061/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5996 - val_loss: 4.8102\n",
      "Epoch 14062/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5492 - val_loss: 4.8882\n",
      "Epoch 14063/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5780 - val_loss: 4.8215\n",
      "Epoch 14064/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4841 - val_loss: 4.8592\n",
      "Epoch 14065/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5173 - val_loss: 4.8849\n",
      "Epoch 14066/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4874 - val_loss: 4.9333\n",
      "Epoch 14067/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4974 - val_loss: 5.0200\n",
      "Epoch 14068/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5885 - val_loss: 4.8129\n",
      "Epoch 14069/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4986 - val_loss: 4.9418\n",
      "Epoch 14070/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5256 - val_loss: 4.7935\n",
      "Epoch 14071/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5637 - val_loss: 5.0440\n",
      "Epoch 14072/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6118 - val_loss: 4.8133\n",
      "Epoch 14073/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4820 - val_loss: 4.8774\n",
      "Epoch 14074/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5781 - val_loss: 4.9267\n",
      "Epoch 14075/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.154 - 0s 46us/step - loss: 4.5484 - val_loss: 5.0779\n",
      "Epoch 14076/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5050 - val_loss: 4.8583\n",
      "Epoch 14077/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6012 - val_loss: 4.9835\n",
      "Epoch 14078/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5326 - val_loss: 4.8123\n",
      "Epoch 14079/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6629 - val_loss: 4.7899\n",
      "Epoch 14080/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8028 - val_loss: 4.8392\n",
      "Epoch 14081/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7227 - val_loss: 5.2580\n",
      "Epoch 14082/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7341 - val_loss: 4.7832\n",
      "Epoch 14083/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5073 - val_loss: 4.7861\n",
      "Epoch 14084/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4981 - val_loss: 4.8271\n",
      "Epoch 14085/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5093 - val_loss: 4.7938\n",
      "Epoch 14086/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5782 - val_loss: 5.0622\n",
      "Epoch 14087/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6863 - val_loss: 4.9666\n",
      "Epoch 14088/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8672 - val_loss: 4.8031\n",
      "Epoch 14089/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5993 - val_loss: 4.8522\n",
      "Epoch 14090/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4804 - val_loss: 4.8625\n",
      "Epoch 14091/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4823 - val_loss: 4.7756\n",
      "Epoch 14092/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4958 - val_loss: 4.7904\n",
      "Epoch 14093/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4702 - val_loss: 5.0160\n",
      "Epoch 14094/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6128 - val_loss: 5.0142\n",
      "Epoch 14095/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6195 - val_loss: 4.8597\n",
      "Epoch 14096/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5175 - val_loss: 4.8834\n",
      "Epoch 14097/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5946 - val_loss: 4.8670\n",
      "Epoch 14098/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5124 - val_loss: 4.8676\n",
      "Epoch 14099/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5687 - val_loss: 4.8965\n",
      "Epoch 14100/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5613 - val_loss: 4.8614\n",
      "Epoch 14101/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4838 - val_loss: 4.8763\n",
      "Epoch 14102/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5139 - val_loss: 4.8290\n",
      "Epoch 14103/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6080 - val_loss: 4.8557\n",
      "Epoch 14104/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6251 - val_loss: 4.8813\n",
      "Epoch 14105/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7170 - val_loss: 4.8586\n",
      "Epoch 14106/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5747 - val_loss: 4.9404\n",
      "Epoch 14107/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5669 - val_loss: 4.9658\n",
      "Epoch 14108/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4954 - val_loss: 4.8831\n",
      "Epoch 14109/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5628 - val_loss: 4.8080\n",
      "Epoch 14110/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5173 - val_loss: 4.7925\n",
      "Epoch 14111/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6176 - val_loss: 4.8055\n",
      "Epoch 14112/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5939 - val_loss: 4.7814\n",
      "Epoch 14113/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.5591 - val_loss: 4.9828\n",
      "Epoch 14114/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7528 - val_loss: 4.8064\n",
      "Epoch 14115/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7147 - val_loss: 4.7792\n",
      "Epoch 14116/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4688 - val_loss: 4.8711\n",
      "Epoch 14117/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5154 - val_loss: 5.1820\n",
      "Epoch 14118/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6723 - val_loss: 4.8408\n",
      "Epoch 14119/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7676 - val_loss: 4.7938\n",
      "Epoch 14120/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4844 - val_loss: 4.8028\n",
      "Epoch 14121/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4616 - val_loss: 4.8080\n",
      "Epoch 14122/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6609 - val_loss: 4.7873\n",
      "Epoch 14123/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4754 - val_loss: 5.3077\n",
      "Epoch 14124/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5687 - val_loss: 5.3728\n",
      "Epoch 14125/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5643 - val_loss: 4.9045\n",
      "Epoch 14126/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7161 - val_loss: 4.7849\n",
      "Epoch 14127/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5446 - val_loss: 4.8073\n",
      "Epoch 14128/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5599 - val_loss: 4.9049\n",
      "Epoch 14129/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6932 - val_loss: 4.8926\n",
      "Epoch 14130/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5319 - val_loss: 4.8731\n",
      "Epoch 14131/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4835 - val_loss: 4.8576\n",
      "Epoch 14132/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5488 - val_loss: 5.0633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14133/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7273 - val_loss: 5.1323\n",
      "Epoch 14134/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5598 - val_loss: 4.9938\n",
      "Epoch 14135/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8699 - val_loss: 5.0436\n",
      "Epoch 14136/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5378 - val_loss: 4.8323\n",
      "Epoch 14137/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6702 - val_loss: 5.5457\n",
      "Epoch 14138/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6353 - val_loss: 4.8198\n",
      "Epoch 14139/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7606 - val_loss: 4.8236\n",
      "Epoch 14140/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6068 - val_loss: 4.8040\n",
      "Epoch 14141/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6266 - val_loss: 4.8606\n",
      "Epoch 14142/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5474 - val_loss: 4.8649\n",
      "Epoch 14143/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7043 - val_loss: 4.8461\n",
      "Epoch 14144/20000\n",
      "685/685 [==============================] - ETA: 0s - loss: 3.880 - 0s 46us/step - loss: 4.5142 - val_loss: 4.9149\n",
      "Epoch 14145/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4949 - val_loss: 4.9335\n",
      "Epoch 14146/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4810 - val_loss: 4.8057\n",
      "Epoch 14147/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6076 - val_loss: 4.8031\n",
      "Epoch 14148/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7095 - val_loss: 4.8239\n",
      "Epoch 14149/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5222 - val_loss: 4.7762\n",
      "Epoch 14150/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6085 - val_loss: 4.7952\n",
      "Epoch 14151/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5621 - val_loss: 4.7801\n",
      "Epoch 14152/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6744 - val_loss: 4.9792\n",
      "Epoch 14153/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5258 - val_loss: 4.9335\n",
      "Epoch 14154/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5548 - val_loss: 4.8018\n",
      "Epoch 14155/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6574 - val_loss: 4.8615\n",
      "Epoch 14156/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4732 - val_loss: 4.8027\n",
      "Epoch 14157/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4957 - val_loss: 4.8346\n",
      "Epoch 14158/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4894 - val_loss: 4.8295\n",
      "Epoch 14159/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7041 - val_loss: 4.8667\n",
      "Epoch 14160/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5516 - val_loss: 4.8502\n",
      "Epoch 14161/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4958 - val_loss: 4.8227\n",
      "Epoch 14162/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4973 - val_loss: 4.7830\n",
      "Epoch 14163/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5725 - val_loss: 4.8323\n",
      "Epoch 14164/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5040 - val_loss: 5.0918\n",
      "Epoch 14165/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6319 - val_loss: 4.7894\n",
      "Epoch 14166/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7851 - val_loss: 4.7746\n",
      "Epoch 14167/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7329 - val_loss: 5.0354\n",
      "Epoch 14168/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8841 - val_loss: 5.1456\n",
      "Epoch 14169/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7717 - val_loss: 5.2934\n",
      "Epoch 14170/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6595 - val_loss: 4.7844\n",
      "Epoch 14171/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5059 - val_loss: 4.8348\n",
      "Epoch 14172/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5371 - val_loss: 5.1345\n",
      "Epoch 14173/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5506 - val_loss: 5.0607\n",
      "Epoch 14174/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7245 - val_loss: 4.9717\n",
      "Epoch 14175/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5440 - val_loss: 4.8067\n",
      "Epoch 14176/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4629 - val_loss: 4.8918\n",
      "Epoch 14177/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4858 - val_loss: 4.8120\n",
      "Epoch 14178/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4817 - val_loss: 4.8343\n",
      "Epoch 14179/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4988 - val_loss: 4.8209\n",
      "Epoch 14180/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4887 - val_loss: 4.7858\n",
      "Epoch 14181/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5224 - val_loss: 4.7981\n",
      "Epoch 14182/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5278 - val_loss: 4.8388\n",
      "Epoch 14183/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5550 - val_loss: 4.8090\n",
      "Epoch 14184/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5296 - val_loss: 5.2057\n",
      "Epoch 14185/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5856 - val_loss: 4.7958\n",
      "Epoch 14186/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4757 - val_loss: 4.7903\n",
      "Epoch 14187/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6684 - val_loss: 4.8596\n",
      "Epoch 14188/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6269 - val_loss: 4.7941\n",
      "Epoch 14189/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5453 - val_loss: 4.8018\n",
      "Epoch 14190/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7472 - val_loss: 4.8936\n",
      "Epoch 14191/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4915 - val_loss: 4.7919\n",
      "Epoch 14192/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4957 - val_loss: 4.8669\n",
      "Epoch 14193/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5697 - val_loss: 4.9889\n",
      "Epoch 14194/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4947 - val_loss: 4.8220\n",
      "Epoch 14195/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5380 - val_loss: 5.0778\n",
      "Epoch 14196/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4650 - val_loss: 4.8484\n",
      "Epoch 14197/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7617 - val_loss: 4.9355\n",
      "Epoch 14198/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5450 - val_loss: 4.8209\n",
      "Epoch 14199/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4837 - val_loss: 4.8109\n",
      "Epoch 14200/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4836 - val_loss: 4.8079\n",
      "Epoch 14201/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5225 - val_loss: 4.9031\n",
      "Epoch 14202/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5048 - val_loss: 5.2106\n",
      "Epoch 14203/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6853 - val_loss: 4.8716\n",
      "Epoch 14204/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6102 - val_loss: 4.7879\n",
      "Epoch 14205/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6761 - val_loss: 4.8618\n",
      "Epoch 14206/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5426 - val_loss: 4.8187\n",
      "Epoch 14207/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5263 - val_loss: 4.8239\n",
      "Epoch 14208/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5499 - val_loss: 4.8679\n",
      "Epoch 14209/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5355 - val_loss: 4.8345\n",
      "Epoch 14210/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5691 - val_loss: 4.8447\n",
      "Epoch 14211/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5570 - val_loss: 5.0109\n",
      "Epoch 14212/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5675 - val_loss: 4.9526\n",
      "Epoch 14213/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5830 - val_loss: 4.8247\n",
      "Epoch 14214/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5082 - val_loss: 4.8337\n",
      "Epoch 14215/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5093 - val_loss: 4.7952\n",
      "Epoch 14216/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.4565 - val_loss: 4.9955\n",
      "Epoch 14217/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5313 - val_loss: 4.9583\n",
      "Epoch 14218/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5678 - val_loss: 4.8453\n",
      "Epoch 14219/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5076 - val_loss: 4.8304\n",
      "Epoch 14220/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6392 - val_loss: 4.7961\n",
      "Epoch 14221/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6347 - val_loss: 4.8419\n",
      "Epoch 14222/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4691 - val_loss: 4.7865\n",
      "Epoch 14223/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5083 - val_loss: 4.8501\n",
      "Epoch 14224/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5337 - val_loss: 5.1203\n",
      "Epoch 14225/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6356 - val_loss: 4.8086\n",
      "Epoch 14226/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5396 - val_loss: 4.9255\n",
      "Epoch 14227/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6052 - val_loss: 4.8258\n",
      "Epoch 14228/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6363 - val_loss: 4.8404\n",
      "Epoch 14229/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6453 - val_loss: 4.9044\n",
      "Epoch 14230/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5605 - val_loss: 5.3972\n",
      "Epoch 14231/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6209 - val_loss: 4.8567\n",
      "Epoch 14232/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5070 - val_loss: 4.8084\n",
      "Epoch 14233/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5600 - val_loss: 4.8080\n",
      "Epoch 14234/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5566 - val_loss: 4.8217\n",
      "Epoch 14235/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6656 - val_loss: 5.2125\n",
      "Epoch 14236/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7257 - val_loss: 4.9507\n",
      "Epoch 14237/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5000 - val_loss: 4.8888\n",
      "Epoch 14238/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4866 - val_loss: 4.8024\n",
      "Epoch 14239/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6816 - val_loss: 4.7945\n",
      "Epoch 14240/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5806 - val_loss: 4.8188\n",
      "Epoch 14241/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5566 - val_loss: 4.8062\n",
      "Epoch 14242/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5811 - val_loss: 4.9669\n",
      "Epoch 14243/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6891 - val_loss: 5.2128\n",
      "Epoch 14244/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6161 - val_loss: 4.8706\n",
      "Epoch 14245/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6096 - val_loss: 4.8420\n",
      "Epoch 14246/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6021 - val_loss: 4.7993\n",
      "Epoch 14247/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4890 - val_loss: 4.8729\n",
      "Epoch 14248/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6134 - val_loss: 4.8200\n",
      "Epoch 14249/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5415 - val_loss: 4.9230\n",
      "Epoch 14250/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7145 - val_loss: 4.8011\n",
      "Epoch 14251/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4764 - val_loss: 4.7713\n",
      "Epoch 14252/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5691 - val_loss: 4.8007\n",
      "Epoch 14253/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5418 - val_loss: 4.8226\n",
      "Epoch 14254/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5469 - val_loss: 5.0329\n",
      "Epoch 14255/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8696 - val_loss: 5.5131\n",
      "Epoch 14256/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5745 - val_loss: 4.8145\n",
      "Epoch 14257/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4744 - val_loss: 4.8668\n",
      "Epoch 14258/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5925 - val_loss: 4.8003\n",
      "Epoch 14259/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8720 - val_loss: 4.9714\n",
      "Epoch 14260/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7107 - val_loss: 4.7785\n",
      "Epoch 14261/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6009 - val_loss: 4.8825\n",
      "Epoch 14262/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6127 - val_loss: 4.8398\n",
      "Epoch 14263/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5981 - val_loss: 4.8306\n",
      "Epoch 14264/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5455 - val_loss: 4.8191\n",
      "Epoch 14265/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.4806 - val_loss: 4.9371\n",
      "Epoch 14266/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7555 - val_loss: 5.1058\n",
      "Epoch 14267/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.6367 - val_loss: 4.8452\n",
      "Epoch 14268/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4767 - val_loss: 4.8060\n",
      "Epoch 14269/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5281 - val_loss: 4.8614\n",
      "Epoch 14270/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4814 - val_loss: 4.8904\n",
      "Epoch 14271/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4946 - val_loss: 4.8567\n",
      "Epoch 14272/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6761 - val_loss: 4.8451\n",
      "Epoch 14273/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5831 - val_loss: 4.9063\n",
      "Epoch 14274/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5270 - val_loss: 4.8276\n",
      "Epoch 14275/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5469 - val_loss: 5.2305\n",
      "Epoch 14276/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6387 - val_loss: 5.0280\n",
      "Epoch 14277/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6513 - val_loss: 4.8680\n",
      "Epoch 14278/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6283 - val_loss: 5.3057\n",
      "Epoch 14279/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9842 - val_loss: 5.1660\n",
      "Epoch 14280/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5860 - val_loss: 5.0334\n",
      "Epoch 14281/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6265 - val_loss: 4.8089\n",
      "Epoch 14282/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5386 - val_loss: 4.8122\n",
      "Epoch 14283/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5333 - val_loss: 4.8479\n",
      "Epoch 14284/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.4895 - val_loss: 4.8593\n",
      "Epoch 14285/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4825 - val_loss: 4.8535\n",
      "Epoch 14286/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4586 - val_loss: 4.8070\n",
      "Epoch 14287/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4864 - val_loss: 4.9259\n",
      "Epoch 14288/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5018 - val_loss: 4.8222\n",
      "Epoch 14289/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4818 - val_loss: 4.8980\n",
      "Epoch 14290/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5565 - val_loss: 4.8460\n",
      "Epoch 14291/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7912 - val_loss: 4.8128\n",
      "Epoch 14292/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5654 - val_loss: 4.9590\n",
      "Epoch 14293/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5912 - val_loss: 4.8009\n",
      "Epoch 14294/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5118 - val_loss: 4.8440\n",
      "Epoch 14295/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6154 - val_loss: 4.7862\n",
      "Epoch 14296/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5333 - val_loss: 4.7849\n",
      "Epoch 14297/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5311 - val_loss: 4.7852\n",
      "Epoch 14298/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5836 - val_loss: 4.9301\n",
      "Epoch 14299/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4750 - val_loss: 4.8814\n",
      "Epoch 14300/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6008 - val_loss: 5.0494\n",
      "Epoch 14301/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6160 - val_loss: 4.8202\n",
      "Epoch 14302/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5723 - val_loss: 4.8751\n",
      "Epoch 14303/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6301 - val_loss: 4.7917\n",
      "Epoch 14304/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5023 - val_loss: 4.8117\n",
      "Epoch 14305/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5411 - val_loss: 4.7867\n",
      "Epoch 14306/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4773 - val_loss: 4.7852\n",
      "Epoch 14307/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6198 - val_loss: 4.9479\n",
      "Epoch 14308/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6151 - val_loss: 4.9237\n",
      "Epoch 14309/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5477 - val_loss: 4.8419\n",
      "Epoch 14310/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5417 - val_loss: 4.7712\n",
      "Epoch 14311/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5466 - val_loss: 4.8054\n",
      "Epoch 14312/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4616 - val_loss: 4.8782\n",
      "Epoch 14313/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5239 - val_loss: 4.8131\n",
      "Epoch 14314/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6129 - val_loss: 4.7802\n",
      "Epoch 14315/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5308 - val_loss: 4.8539\n",
      "Epoch 14316/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5634 - val_loss: 4.8766\n",
      "Epoch 14317/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5181 - val_loss: 4.8117\n",
      "Epoch 14318/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6167 - val_loss: 4.8987\n",
      "Epoch 14319/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6031 - val_loss: 4.8693\n",
      "Epoch 14320/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5949 - val_loss: 4.8145\n",
      "Epoch 14321/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7786 - val_loss: 4.8819\n",
      "Epoch 14322/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5899 - val_loss: 4.8240\n",
      "Epoch 14323/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5668 - val_loss: 4.9796\n",
      "Epoch 14324/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6932 - val_loss: 5.2227\n",
      "Epoch 14325/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5323 - val_loss: 4.8137\n",
      "Epoch 14326/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4954 - val_loss: 4.9034\n",
      "Epoch 14327/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5183 - val_loss: 4.9528\n",
      "Epoch 14328/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7535 - val_loss: 4.8328\n",
      "Epoch 14329/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5578 - val_loss: 4.9619\n",
      "Epoch 14330/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4988 - val_loss: 4.7949\n",
      "Epoch 14331/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4700 - val_loss: 4.9644\n",
      "Epoch 14332/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5705 - val_loss: 4.8051\n",
      "Epoch 14333/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5622 - val_loss: 4.8216\n",
      "Epoch 14334/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7911 - val_loss: 4.8216\n",
      "Epoch 14335/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5796 - val_loss: 4.8181\n",
      "Epoch 14336/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5420 - val_loss: 4.7783\n",
      "Epoch 14337/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5403 - val_loss: 4.9330\n",
      "Epoch 14338/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5696 - val_loss: 4.9788\n",
      "Epoch 14339/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5091 - val_loss: 4.8719\n",
      "Epoch 14340/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5680 - val_loss: 4.8241\n",
      "Epoch 14341/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5703 - val_loss: 4.8873\n",
      "Epoch 14342/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5532 - val_loss: 4.8098\n",
      "Epoch 14343/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6588 - val_loss: 4.9084\n",
      "Epoch 14344/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6210 - val_loss: 5.1794\n",
      "Epoch 14345/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7026 - val_loss: 4.8883\n",
      "Epoch 14346/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5779 - val_loss: 4.8114\n",
      "Epoch 14347/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4886 - val_loss: 4.8244\n",
      "Epoch 14348/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5921 - val_loss: 4.9861\n",
      "Epoch 14349/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5886 - val_loss: 4.8975\n",
      "Epoch 14350/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6220 - val_loss: 5.2711\n",
      "Epoch 14351/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6729 - val_loss: 5.0479\n",
      "Epoch 14352/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6334 - val_loss: 4.8210\n",
      "Epoch 14353/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6111 - val_loss: 5.1051\n",
      "Epoch 14354/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6067 - val_loss: 4.8358\n",
      "Epoch 14355/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6577 - val_loss: 5.1113\n",
      "Epoch 14356/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6995 - val_loss: 4.7923\n",
      "Epoch 14357/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9158 - val_loss: 4.8196\n",
      "Epoch 14358/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6457 - val_loss: 4.8174\n",
      "Epoch 14359/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5072 - val_loss: 4.8374\n",
      "Epoch 14360/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5196 - val_loss: 4.8116\n",
      "Epoch 14361/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8820 - val_loss: 4.9732\n",
      "Epoch 14362/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7365 - val_loss: 5.3787\n",
      "Epoch 14363/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5716 - val_loss: 4.9536\n",
      "Epoch 14364/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5481 - val_loss: 4.8755\n",
      "Epoch 14365/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4717 - val_loss: 4.9254\n",
      "Epoch 14366/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4989 - val_loss: 5.0114\n",
      "Epoch 14367/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5385 - val_loss: 4.7995\n",
      "Epoch 14368/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5067 - val_loss: 5.4971\n",
      "Epoch 14369/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7005 - val_loss: 4.9117\n",
      "Epoch 14370/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5131 - val_loss: 4.8011\n",
      "Epoch 14371/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4564 - val_loss: 5.2795\n",
      "Epoch 14372/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5837 - val_loss: 4.8039\n",
      "Epoch 14373/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5817 - val_loss: 4.8125\n",
      "Epoch 14374/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5320 - val_loss: 4.8161\n",
      "Epoch 14375/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4878 - val_loss: 4.8106\n",
      "Epoch 14376/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5414 - val_loss: 4.8437\n",
      "Epoch 14377/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5803 - val_loss: 4.9510\n",
      "Epoch 14378/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7142 - val_loss: 4.8091\n",
      "Epoch 14379/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5143 - val_loss: 4.7987\n",
      "Epoch 14380/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4888 - val_loss: 4.8145\n",
      "Epoch 14381/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6128 - val_loss: 4.9061\n",
      "Epoch 14382/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5606 - val_loss: 4.8734\n",
      "Epoch 14383/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5143 - val_loss: 4.8017\n",
      "Epoch 14384/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5755 - val_loss: 4.8038\n",
      "Epoch 14385/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7004 - val_loss: 4.8084\n",
      "Epoch 14386/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4977 - val_loss: 5.2069\n",
      "Epoch 14387/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5505 - val_loss: 4.8166\n",
      "Epoch 14388/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5231 - val_loss: 4.8304\n",
      "Epoch 14389/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5125 - val_loss: 4.8701\n",
      "Epoch 14390/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7043 - val_loss: 5.0838\n",
      "Epoch 14391/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5155 - val_loss: 4.8078\n",
      "Epoch 14392/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4974 - val_loss: 4.8866\n",
      "Epoch 14393/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5426 - val_loss: 4.8587\n",
      "Epoch 14394/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5477 - val_loss: 5.0123\n",
      "Epoch 14395/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6821 - val_loss: 5.1834\n",
      "Epoch 14396/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6263 - val_loss: 4.9191\n",
      "Epoch 14397/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5005 - val_loss: 4.8651\n",
      "Epoch 14398/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5013 - val_loss: 4.8476\n",
      "Epoch 14399/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6248 - val_loss: 4.8178\n",
      "Epoch 14400/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5460 - val_loss: 4.9789\n",
      "Epoch 14401/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4800 - val_loss: 4.9149\n",
      "Epoch 14402/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5067 - val_loss: 4.8709\n",
      "Epoch 14403/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4741 - val_loss: 4.8634\n",
      "Epoch 14404/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6370 - val_loss: 5.2668\n",
      "Epoch 14405/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6959 - val_loss: 4.9090\n",
      "Epoch 14406/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9384 - val_loss: 5.4109\n",
      "Epoch 14407/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6197 - val_loss: 4.8751\n",
      "Epoch 14408/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5398 - val_loss: 4.8103\n",
      "Epoch 14409/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4873 - val_loss: 4.9214\n",
      "Epoch 14410/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5140 - val_loss: 4.9781\n",
      "Epoch 14411/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8482 - val_loss: 4.8002\n",
      "Epoch 14412/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5687 - val_loss: 4.7838\n",
      "Epoch 14413/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4781 - val_loss: 4.8213\n",
      "Epoch 14414/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4692 - val_loss: 4.7875\n",
      "Epoch 14415/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4997 - val_loss: 5.1105\n",
      "Epoch 14416/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5505 - val_loss: 4.8925\n",
      "Epoch 14417/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6210 - val_loss: 4.7985\n",
      "Epoch 14418/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5873 - val_loss: 4.7757\n",
      "Epoch 14419/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4859 - val_loss: 4.7787\n",
      "Epoch 14420/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5388 - val_loss: 4.8668\n",
      "Epoch 14421/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5027 - val_loss: 5.0796\n",
      "Epoch 14422/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6615 - val_loss: 4.9491\n",
      "Epoch 14423/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5295 - val_loss: 4.8085\n",
      "Epoch 14424/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4723 - val_loss: 4.9505\n",
      "Epoch 14425/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5154 - val_loss: 5.1234\n",
      "Epoch 14426/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6062 - val_loss: 4.7731\n",
      "Epoch 14427/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5124 - val_loss: 4.7923\n",
      "Epoch 14428/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4866 - val_loss: 4.8670\n",
      "Epoch 14429/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5015 - val_loss: 4.8128\n",
      "Epoch 14430/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4342 - val_loss: 4.8714\n",
      "Epoch 14431/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7554 - val_loss: 4.8072\n",
      "Epoch 14432/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4807 - val_loss: 4.8198\n",
      "Epoch 14433/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5780 - val_loss: 5.1755\n",
      "Epoch 14434/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6904 - val_loss: 4.9310\n",
      "Epoch 14435/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5227 - val_loss: 4.8420\n",
      "Epoch 14436/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5319 - val_loss: 5.1982\n",
      "Epoch 14437/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6219 - val_loss: 5.0382\n",
      "Epoch 14438/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5197 - val_loss: 4.7976\n",
      "Epoch 14439/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5380 - val_loss: 4.8952\n",
      "Epoch 14440/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6886 - val_loss: 4.7868\n",
      "Epoch 14441/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5074 - val_loss: 4.8411\n",
      "Epoch 14442/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5125 - val_loss: 4.8317\n",
      "Epoch 14443/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6240 - val_loss: 4.8109\n",
      "Epoch 14444/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4728 - val_loss: 5.1586\n",
      "Epoch 14445/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6484 - val_loss: 4.7899\n",
      "Epoch 14446/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4793 - val_loss: 4.7849\n",
      "Epoch 14447/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5215 - val_loss: 4.8786\n",
      "Epoch 14448/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5269 - val_loss: 4.8325\n",
      "Epoch 14449/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5811 - val_loss: 4.8116\n",
      "Epoch 14450/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4954 - val_loss: 4.9486\n",
      "Epoch 14451/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4942 - val_loss: 4.9320\n",
      "Epoch 14452/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6016 - val_loss: 4.8235\n",
      "Epoch 14453/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5093 - val_loss: 4.7717\n",
      "Epoch 14454/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6088 - val_loss: 6.0020\n",
      "Epoch 14455/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9290 - val_loss: 4.8104\n",
      "Epoch 14456/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4731 - val_loss: 4.7906\n",
      "Epoch 14457/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5313 - val_loss: 4.8561\n",
      "Epoch 14458/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5495 - val_loss: 4.7963\n",
      "Epoch 14459/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4861 - val_loss: 4.9529\n",
      "Epoch 14460/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4671 - val_loss: 4.8241\n",
      "Epoch 14461/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4861 - val_loss: 5.3093\n",
      "Epoch 14462/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6744 - val_loss: 4.7926\n",
      "Epoch 14463/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5574 - val_loss: 5.0049\n",
      "Epoch 14464/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5918 - val_loss: 4.8343\n",
      "Epoch 14465/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6924 - val_loss: 4.8221\n",
      "Epoch 14466/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5820 - val_loss: 4.8780\n",
      "Epoch 14467/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7455 - val_loss: 5.2740\n",
      "Epoch 14468/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5840 - val_loss: 5.3043\n",
      "Epoch 14469/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6770 - val_loss: 5.0365\n",
      "Epoch 14470/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5364 - val_loss: 4.8005\n",
      "Epoch 14471/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5390 - val_loss: 4.8871\n",
      "Epoch 14472/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4605 - val_loss: 4.8219\n",
      "Epoch 14473/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5563 - val_loss: 4.7875\n",
      "Epoch 14474/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5206 - val_loss: 4.9018\n",
      "Epoch 14475/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7343 - val_loss: 4.8025\n",
      "Epoch 14476/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5032 - val_loss: 5.5539\n",
      "Epoch 14477/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.9635 - val_loss: 4.8258\n",
      "Epoch 14478/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5616 - val_loss: 4.9846\n",
      "Epoch 14479/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5621 - val_loss: 4.7843\n",
      "Epoch 14480/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6925 - val_loss: 4.9770\n",
      "Epoch 14481/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5591 - val_loss: 4.9421\n",
      "Epoch 14482/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4729 - val_loss: 4.8286\n",
      "Epoch 14483/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5337 - val_loss: 4.8622\n",
      "Epoch 14484/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4835 - val_loss: 4.8053\n",
      "Epoch 14485/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5180 - val_loss: 4.8417\n",
      "Epoch 14486/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5113 - val_loss: 5.1894\n",
      "Epoch 14487/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5640 - val_loss: 4.8586\n",
      "Epoch 14488/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7664 - val_loss: 5.8608\n",
      "Epoch 14489/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6013 - val_loss: 5.0527\n",
      "Epoch 14490/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5844 - val_loss: 4.8568\n",
      "Epoch 14491/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8289 - val_loss: 5.5934\n",
      "Epoch 14492/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6019 - val_loss: 5.1685\n",
      "Epoch 14493/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5618 - val_loss: 4.9643\n",
      "Epoch 14494/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4987 - val_loss: 4.7838\n",
      "Epoch 14495/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5623 - val_loss: 4.8343\n",
      "Epoch 14496/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5865 - val_loss: 4.7916\n",
      "Epoch 14497/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5305 - val_loss: 4.7817\n",
      "Epoch 14498/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5419 - val_loss: 5.3313\n",
      "Epoch 14499/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6187 - val_loss: 5.1309\n",
      "Epoch 14500/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6415 - val_loss: 4.7999\n",
      "Epoch 14501/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7489 - val_loss: 4.7947\n",
      "Epoch 14502/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5319 - val_loss: 4.8068\n",
      "Epoch 14503/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7856 - val_loss: 5.2487\n",
      "Epoch 14504/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5757 - val_loss: 4.8571\n",
      "Epoch 14505/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6511 - val_loss: 4.8825\n",
      "Epoch 14506/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5377 - val_loss: 4.8705\n",
      "Epoch 14507/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4739 - val_loss: 4.7785\n",
      "Epoch 14508/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5677 - val_loss: 5.0911\n",
      "Epoch 14509/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5778 - val_loss: 4.9887\n",
      "Epoch 14510/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5548 - val_loss: 5.1156\n",
      "Epoch 14511/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4982 - val_loss: 4.7997\n",
      "Epoch 14512/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.5197 - val_loss: 4.8299\n",
      "Epoch 14513/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4915 - val_loss: 4.8008\n",
      "Epoch 14514/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6454 - val_loss: 4.8414\n",
      "Epoch 14515/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4847 - val_loss: 4.8902\n",
      "Epoch 14516/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4906 - val_loss: 4.9268\n",
      "Epoch 14517/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.6843 - val_loss: 5.6482\n",
      "Epoch 14518/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5933 - val_loss: 5.0325\n",
      "Epoch 14519/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5824 - val_loss: 4.7824\n",
      "Epoch 14520/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5309 - val_loss: 4.9461\n",
      "Epoch 14521/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4882 - val_loss: 5.0651\n",
      "Epoch 14522/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6795 - val_loss: 4.8017\n",
      "Epoch 14523/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6494 - val_loss: 4.7917\n",
      "Epoch 14524/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8585 - val_loss: 4.8858\n",
      "Epoch 14525/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4789 - val_loss: 4.7709\n",
      "Epoch 14526/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4930 - val_loss: 4.7925\n",
      "Epoch 14527/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6166 - val_loss: 5.1497\n",
      "Epoch 14528/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7956 - val_loss: 4.9238\n",
      "Epoch 14529/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5904 - val_loss: 4.8385\n",
      "Epoch 14530/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6018 - val_loss: 4.8335\n",
      "Epoch 14531/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6122 - val_loss: 4.9349\n",
      "Epoch 14532/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5417 - val_loss: 4.7909\n",
      "Epoch 14533/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4942 - val_loss: 4.8352\n",
      "Epoch 14534/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5665 - val_loss: 4.8350\n",
      "Epoch 14535/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6162 - val_loss: 4.8260\n",
      "Epoch 14536/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5009 - val_loss: 4.7819\n",
      "Epoch 14537/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4670 - val_loss: 5.0421\n",
      "Epoch 14538/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6006 - val_loss: 5.2661\n",
      "Epoch 14539/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5491 - val_loss: 5.1277\n",
      "Epoch 14540/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8609 - val_loss: 5.3411\n",
      "Epoch 14541/20000\n",
      "685/685 [==============================] - 0s 91us/step - loss: 4.8015 - val_loss: 5.0732\n",
      "Epoch 14542/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6268 - val_loss: 4.8574\n",
      "Epoch 14543/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6194 - val_loss: 4.9870\n",
      "Epoch 14544/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5327 - val_loss: 4.7794\n",
      "Epoch 14545/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6078 - val_loss: 4.8522\n",
      "Epoch 14546/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4998 - val_loss: 4.8762\n",
      "Epoch 14547/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5043 - val_loss: 4.8723\n",
      "Epoch 14548/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5258 - val_loss: 4.8123\n",
      "Epoch 14549/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4778 - val_loss: 5.0405\n",
      "Epoch 14550/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.4969 - val_loss: 5.4920\n",
      "Epoch 14551/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6193 - val_loss: 4.8768\n",
      "Epoch 14552/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4954 - val_loss: 4.8160\n",
      "Epoch 14553/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7467 - val_loss: 4.9726\n",
      "Epoch 14554/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6841 - val_loss: 5.0305\n",
      "Epoch 14555/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7109 - val_loss: 4.9044\n",
      "Epoch 14556/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4773 - val_loss: 4.8410\n",
      "Epoch 14557/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5058 - val_loss: 4.9536\n",
      "Epoch 14558/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5059 - val_loss: 4.9267\n",
      "Epoch 14559/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5316 - val_loss: 4.8391\n",
      "Epoch 14560/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5414 - val_loss: 5.1596\n",
      "Epoch 14561/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7184 - val_loss: 4.9009\n",
      "Epoch 14562/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5915 - val_loss: 4.8918\n",
      "Epoch 14563/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5549 - val_loss: 4.8822\n",
      "Epoch 14564/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5443 - val_loss: 4.8144\n",
      "Epoch 14565/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4958 - val_loss: 4.9033\n",
      "Epoch 14566/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.9106 - val_loss: 4.8148\n",
      "Epoch 14567/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7011 - val_loss: 5.9306\n",
      "Epoch 14568/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8564 - val_loss: 4.8401\n",
      "Epoch 14569/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6175 - val_loss: 5.0133\n",
      "Epoch 14570/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6136 - val_loss: 5.0763\n",
      "Epoch 14571/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6295 - val_loss: 4.8951\n",
      "Epoch 14572/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5461 - val_loss: 5.0026\n",
      "Epoch 14573/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5153 - val_loss: 4.8851\n",
      "Epoch 14574/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5045 - val_loss: 4.8682\n",
      "Epoch 14575/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5201 - val_loss: 4.8151\n",
      "Epoch 14576/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.8563 - val_loss: 4.9890\n",
      "Epoch 14577/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6173 - val_loss: 4.9592\n",
      "Epoch 14578/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5533 - val_loss: 4.9038\n",
      "Epoch 14579/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5299 - val_loss: 4.8134\n",
      "Epoch 14580/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5324 - val_loss: 4.8253\n",
      "Epoch 14581/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5920 - val_loss: 5.1174\n",
      "Epoch 14582/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5755 - val_loss: 5.1913\n",
      "Epoch 14583/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6380 - val_loss: 5.4213\n",
      "Epoch 14584/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7687 - val_loss: 5.3209\n",
      "Epoch 14585/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 5.1778 - val_loss: 4.9874\n",
      "Epoch 14586/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7676 - val_loss: 4.9840\n",
      "Epoch 14587/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5683 - val_loss: 5.1283\n",
      "Epoch 14588/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 68us/step - loss: 4.5613 - val_loss: 4.8958\n",
      "Epoch 14589/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5164 - val_loss: 4.8888\n",
      "Epoch 14590/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6995 - val_loss: 4.8489\n",
      "Epoch 14591/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6298 - val_loss: 4.8344\n",
      "Epoch 14592/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6052 - val_loss: 4.9136\n",
      "Epoch 14593/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5278 - val_loss: 4.8545\n",
      "Epoch 14594/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4808 - val_loss: 4.8106\n",
      "Epoch 14595/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4828 - val_loss: 4.9470\n",
      "Epoch 14596/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6555 - val_loss: 5.1001\n",
      "Epoch 14597/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5083 - val_loss: 4.8288\n",
      "Epoch 14598/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4883 - val_loss: 4.8724\n",
      "Epoch 14599/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5279 - val_loss: 4.8052\n",
      "Epoch 14600/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5564 - val_loss: 4.8107\n",
      "Epoch 14601/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5111 - val_loss: 4.8914\n",
      "Epoch 14602/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4583 - val_loss: 4.7998\n",
      "Epoch 14603/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4914 - val_loss: 4.8163\n",
      "Epoch 14604/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5461 - val_loss: 5.1012\n",
      "Epoch 14605/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4659 - val_loss: 4.8541\n",
      "Epoch 14606/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4780 - val_loss: 4.8087\n",
      "Epoch 14607/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5067 - val_loss: 4.8555\n",
      "Epoch 14608/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4705 - val_loss: 5.5706\n",
      "Epoch 14609/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7574 - val_loss: 5.0180\n",
      "Epoch 14610/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5132 - val_loss: 5.1095\n",
      "Epoch 14611/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5014 - val_loss: 4.8230\n",
      "Epoch 14612/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7928 - val_loss: 4.8223\n",
      "Epoch 14613/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7125 - val_loss: 5.0294\n",
      "Epoch 14614/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7598 - val_loss: 4.7968\n",
      "Epoch 14615/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4855 - val_loss: 4.9661\n",
      "Epoch 14616/20000\n",
      "685/685 [==============================] - 0s 23us/step - loss: 4.4817 - val_loss: 4.7792\n",
      "Epoch 14617/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4484 - val_loss: 4.8645\n",
      "Epoch 14618/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5179 - val_loss: 4.8532\n",
      "Epoch 14619/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6097 - val_loss: 5.7536\n",
      "Epoch 14620/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.8274 - val_loss: 4.8620\n",
      "Epoch 14621/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5599 - val_loss: 5.1840\n",
      "Epoch 14622/20000\n",
      "685/685 [==============================] - 0s 69us/step - loss: 4.5500 - val_loss: 4.8040\n",
      "Epoch 14623/20000\n",
      "685/685 [==============================] - 0s 45us/step - loss: 4.5259 - val_loss: 5.0091\n",
      "Epoch 14624/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5697 - val_loss: 4.8179\n",
      "Epoch 14625/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5286 - val_loss: 4.9533\n",
      "Epoch 14626/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6032 - val_loss: 4.8795\n",
      "Epoch 14627/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6645 - val_loss: 4.9187\n",
      "Epoch 14628/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5311 - val_loss: 4.8921\n",
      "Epoch 14629/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5175 - val_loss: 4.8210\n",
      "Epoch 14630/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5037 - val_loss: 4.7860\n",
      "Epoch 14631/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6539 - val_loss: 4.7787\n",
      "Epoch 14632/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4999 - val_loss: 4.8429\n",
      "Epoch 14633/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5965 - val_loss: 4.9298\n",
      "Epoch 14634/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5805 - val_loss: 5.3048\n",
      "Epoch 14635/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5076 - val_loss: 4.7772\n",
      "Epoch 14636/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6019 - val_loss: 5.1244\n",
      "Epoch 14637/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5319 - val_loss: 4.8201\n",
      "Epoch 14638/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5256 - val_loss: 4.9141\n",
      "Epoch 14639/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6570 - val_loss: 4.8130\n",
      "Epoch 14640/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4663 - val_loss: 5.1243\n",
      "Epoch 14641/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5764 - val_loss: 4.8059\n",
      "Epoch 14642/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5010 - val_loss: 4.8142\n",
      "Epoch 14643/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5490 - val_loss: 4.8826\n",
      "Epoch 14644/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6004 - val_loss: 4.9180\n",
      "Epoch 14645/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7564 - val_loss: 4.8220\n",
      "Epoch 14646/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7010 - val_loss: 4.8356\n",
      "Epoch 14647/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4831 - val_loss: 4.9125\n",
      "Epoch 14648/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7075 - val_loss: 5.2856\n",
      "Epoch 14649/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5528 - val_loss: 4.8426\n",
      "Epoch 14650/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7208 - val_loss: 4.8243\n",
      "Epoch 14651/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5283 - val_loss: 4.8950\n",
      "Epoch 14652/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6482 - val_loss: 4.9783\n",
      "Epoch 14653/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6763 - val_loss: 4.8236\n",
      "Epoch 14654/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6096 - val_loss: 4.7736\n",
      "Epoch 14655/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5563 - val_loss: 4.8137\n",
      "Epoch 14656/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4868 - val_loss: 4.8196\n",
      "Epoch 14657/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7841 - val_loss: 5.1503\n",
      "Epoch 14658/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5772 - val_loss: 4.8253\n",
      "Epoch 14659/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4861 - val_loss: 4.8173\n",
      "Epoch 14660/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4681 - val_loss: 4.8233\n",
      "Epoch 14661/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5437 - val_loss: 5.0659\n",
      "Epoch 14662/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.7789 - val_loss: 4.9075\n",
      "Epoch 14663/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6680 - val_loss: 5.4771\n",
      "Epoch 14664/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 0s 46us/step - loss: 4.6491 - val_loss: 4.8017\n",
      "Epoch 14665/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.7120 - val_loss: 4.8130\n",
      "Epoch 14666/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6482 - val_loss: 4.8011\n",
      "Epoch 14667/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5580 - val_loss: 4.8238\n",
      "Epoch 14668/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4832 - val_loss: 4.8111\n",
      "Epoch 14669/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4887 - val_loss: 4.8013\n",
      "Epoch 14670/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.4861 - val_loss: 4.7916\n",
      "Epoch 14671/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5219 - val_loss: 4.8066\n",
      "Epoch 14672/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5211 - val_loss: 4.8435\n",
      "Epoch 14673/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5010 - val_loss: 4.9031\n",
      "Epoch 14674/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.4865 - val_loss: 4.8053\n",
      "Epoch 14675/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5594 - val_loss: 4.9663\n",
      "Epoch 14676/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5189 - val_loss: 4.7926\n",
      "Epoch 14677/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.5630 - val_loss: 5.2282\n",
      "Epoch 14678/20000\n",
      "685/685 [==============================] - 0s 68us/step - loss: 4.6103 - val_loss: 4.8650\n",
      "Epoch 14679/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5329 - val_loss: 4.8131\n",
      "Epoch 14680/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5141 - val_loss: 4.8576\n",
      "Epoch 14681/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.6633 - val_loss: 4.9733\n",
      "Epoch 14682/20000\n",
      "685/685 [==============================] - 0s 46us/step - loss: 4.5028 - val_loss: 4.8003\n"
     ]
    }
   ],
   "source": [
    "model = index_model()\n",
    "# training the neural network, validation set is included to check if overfitting occurs\n",
    "history = model.fit(x_train, y_train, epochs=20000, validation_data=(x_valid, y_valid)) # Storing all information during fitting in history variable\n",
    "# geting the performance by using validaiton set \n",
    "Error = model.evaluate(x_train, y_train)\n",
    "print(\"Error for the test set: \", Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Prediction vs Reality\n",
    "predicted_level = model.predict(x_test)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.plot(predicted_level, color = 'orangered', label = 'Prediction')\n",
    "plt.plot(y_test[0:,], color = 'royalblue', label = 'TecDAX')\n",
    "plt.title('TecDAX Index Prediction', fontweight='bold',fontsize=22)\n",
    "plt.xlabel('Predicted days',fontsize=18,color='black')\n",
    "plt.ylabel('Price',fontsize=18,color='black')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising Overfitting\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss_values = np.log(history_dict['val_loss'])\n",
    "acc = history_dict['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.plot(epochs, loss_values, color = 'orangered', label = 'Logarithmic Loss')\n",
    "plt.title('Adam - log losses', fontweight='bold',fontsize=22)\n",
    "plt.xlabel('Epochs',fontsize=18,color='black')\n",
    "plt.ylabel('Loss',fontsize=18,color='black')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting the TecDAX price development using the Facebook Prophet model.\n",
    "\n",
    "Not all forecasting problems can be solved by the same procedure. Prophet is optimized for business forecasting tasks, which have the following characteristics: hourly, daily, or weekly observations with at least a few months (preferably multiple years) of history. At its core, the Prophet procedure is an additive regression model with three main components:\n",
    "\n",
    " - A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data.\n",
    " - A yearly seasonal component modeled using Fourier series.\n",
    " - A weekly seasonal component using dummy variables.\n",
    " \n",
    "We use the TecDAX Index dataset imported in the exploratory analysis section to train the prophet model. However we will not include the year 2018 and the first half of 2019 in the training set. After training the prophet model we predict the latest 1,5 year we didnt include in the training set. The prediction results will be shown as a solid blue line with an 80% confidence intervall(light blue area). In orange color we represent the true Index price realised during the year 2018 and the first half of 2019.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cycle_analysis(data,split_date,cycle,mode='additive',forecast_plot = False,print_ind=False):\n",
    "    training = data[:split_date].iloc[:-1,]\n",
    "    testing = data[split_date:]\n",
    "    predict_period = len(pd.date_range(split_date,max(data.index)))\n",
    "    df = training.reset_index()\n",
    "    df.columns = ['ds','y']\n",
    "    model = Prophet(weekly_seasonality=False,yearly_seasonality=False,daily_seasonality=False)\n",
    "    model.add_seasonality('self_define_cycle',period=cycle,fourier_order=8,mode=mode)\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=predict_period)\n",
    "    forecast = model.predict(future)\n",
    "    if forecast_plot:\n",
    "        model.plot(forecast)\n",
    "        plt.plot(testing.index,testing.values,color='orangered',alpha=0.8)\n",
    "        plt.title(\"TecDAX Index\",fontweight='bold',fontsize=22)\n",
    "        plt.xlabel('Date',fontsize=18,color='black')\n",
    "        plt.ylabel('Price',fontsize=18,color='black')\n",
    "        plt.show()\n",
    "    ret = max(forecast.self_define_cycle)-min(forecast.self_define_cycle)\n",
    "    model_tb = forecast['yhat']\n",
    "    model_tb.index = forecast['ds'].map(lambda x:x.strftime(\"%Y-%m-%d\"))\n",
    "    return forecast\n",
    "\n",
    "       \n",
    "cycle_analysis(TecDAX,'2018-01-02',30,forecast_plot=True,print_ind=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting the TecDAX price development using a LSTM model approach.\n",
    "\n",
    "Again we use the TecDAX Index dataset from previous sections to train the prophet model. However this time we will not include the May 2019 in the training set. After training the prophet model we predict the month May that we didnt include in the training set. The prediction results will be shown as a solid orange line. In blue color we represent the true TecDAX Index price realised during the same month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping\n",
    "TecDAX = TecDAX.values.reshape(len(TecDAX),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data into train and test data\n",
    "train_size = int(len(TecDAX) * 0.993)\n",
    "test_size  = len(TecDAX) - train_size\n",
    "\n",
    "TecDAX_train, TecDAX_test = TecDAX[0:train_size,:], TecDAX[train_size:len(TecDAX),:]\n",
    "\n",
    "print(\"Training observations: \", len(TecDAX_train))\n",
    "print(\"Testing observations:  \", len(TecDAX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "TecDAX_train.shape\n",
    "training_set_scaled = sc.fit_transform(TecDAX_train)\n",
    "test_set_scaled     = sc.fit_transform(TecDAX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !! fitting the model can take significant amount of time and computation power !!\n",
    "\n",
    "# initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "# adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "# adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "# compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "# fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predicted stock price\n",
    "dataset_total = pd.concat((pd.Series(TecDAX_train[:,0]), pd.Series(TecDAX_test[:,0])), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(TecDAX_test) - 60:]\n",
    "inputs = inputs.values.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.plot(TecDAX_test[0:20,:], color = 'royalblue', label = 'TecDAX')\n",
    "plt.plot(predicted_stock_price, color = 'orangered', label = 'Prediction')\n",
    "plt.title('TecDAX Index Prediction', fontweight='bold',fontsize=22)\n",
    "plt.xlabel('Predicted days',fontsize=18,color='black')\n",
    "plt.ylabel('Price',fontsize=18,color='black')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
